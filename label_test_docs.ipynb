{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\sanskruti jajoo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\sanskruti jajoo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sanskruti jajoo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sanskruti jajoo\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sanskruti\n",
      "[nltk_data]     Jajoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sanskruti\n",
      "[nltk_data]     Jajoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sanskruti\n",
      "[nltk_data]     Jajoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled data saved to label_studio_input.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def load_text_files(folder_path):\n",
    "    texts = []\n",
    "    filenames = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                texts.append(content)\n",
    "                filenames.append(file)  # Only store the filename\n",
    "    return filenames, texts\n",
    "\n",
    "def preprocess_text(texts):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "        processed_texts.append(\" \".join(tokens))\n",
    "    \n",
    "    return processed_texts\n",
    "\n",
    "def cluster_texts(texts, num_clusters=2):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # Remap the labels: 0 -> 1, 1 -> 2 (adjust based on actual cluster interpretation)\n",
    "    remapped_labels = [label + 1 for label in labels]  # Change 0 -> 1, 1 -> 2\n",
    "\n",
    "    return remapped_labels\n",
    "\n",
    "def save_label_studio_format(filenames, texts, labels, output_file):\n",
    "    label_map = {1: \"finance\", 2: \"film\"}  # Adjust based on actual cluster interpretation\n",
    "    labeled_data = []\n",
    "    \n",
    "    for filename, content, label in zip(filenames, texts, labels):\n",
    "        content = content.replace(\"\\n\", \" \")  # Replace newlines with space or another separator\n",
    "        labeled_data.append({\n",
    "            \"data\": {\n",
    "                \"text\": content  # Make sure the 'text' field is present here\n",
    "            },\n",
    "            \"annotations\": [{\n",
    "                \"result\": [{\n",
    "                    \"value\": {\n",
    "                        \"choices\": [label_map[label]]  # Make sure the 'choices' is a list\n",
    "                    },\n",
    "                    \"from_name\": \"sentiment\",  # Specify the label name field\n",
    "                    \"to_name\": \"text\",  # Specify the to_name field that corresponds to 'data'\n",
    "                    \"type\": \"choices\",  # Label Studio expects this type for simple classification\n",
    "                }]\n",
    "            }]\n",
    "        })\n",
    "    \n",
    "    # Save the data to a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(labeled_data, f, indent=4)\n",
    "\n",
    "def main():\n",
    "    folder_path = \"./merged_folder\"\n",
    "    output_file = \"label_studio_input.json\"\n",
    "    \n",
    "    filenames, texts = load_text_files(folder_path)\n",
    "    preprocessed_texts = preprocess_text(texts)\n",
    "    labels = cluster_texts(preprocessed_texts)\n",
    "    save_label_studio_format(filenames, texts, labels, output_file)\n",
    "    \n",
    "    print(f\"Labeled data saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
