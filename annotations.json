[{"id":9,"annotations":[{"id":9,"completed_by":1,"result":[{"value":{"choices":["Other"]},"id":"wZmvw2Lqiq","from_name":"sentiment","to_name":"text","type":"choices","origin":"manual"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.309403Z","updated_at":"2025-03-23T13:58:48.528592Z","draft_created_at":null,"lead_time":53.76,"prediction":{},"result_count":1,"unique_id":"10bc8353-a249-44d1-8d1c-4ee565acbd9c","import_id":null,"last_action":null,"bulk_created":false,"task":9,"project":5,"updated_by":1,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],
"data":{"text":"    A film, also known as a movie or motion picture,[a] is a work of visual art that simulates experiences and otherwise communicates ideas, stories, perceptions, emotions, or atmosphere through the use of moving images that are generally, since the 1930s, synchronized with sound and (less commonly) other sensory stimulations.[1]  The name \"film\" originally referred to the thin layer of photochemical emulsion[2] on the celluloid strip that used to be the actual medium for recording and displaying motion pictures.  Many other terms exist for an individual motion-picture, including \"picture\", \"picture show\", \"moving picture\", \"photoplay\", and \"flick\". The most common term in the United States is \"movie\", while in Europe, \"film\" is preferred. Archaic terms include \"animated pictures\" and \"animated photography\".[citation needed]  \"Flick\" is, in general a slang term, first recorded in 1926. It originates in the verb flicker, owing to the flickering appearance of early films.[3]  Common terms for the field, in general, include \"the big screen\", \"the movies\", \"the silver screen\", and \"cinema\"; the last of these is commonly used, as an overarching term, in scholarly texts and critical essays. In the early years, the word \"sheet\" was sometimes used instead of \"screen\".[citation needed]  The word \"cinema\" is borrowed from the French cinéma, an abbreviation of cinématographe (term coined by the Lumière brothers in the 1890s), from Ancient Greek meaning \"recording movement\". The word is today usually used to refer to either a purpose-built venue for screening films, known as a movie theater in the US; the film industry; the overall art form of specifically just filmmaking.  The moving images of a film are created by photographing actual scenes with a motion-picture camera, by photographing drawings or miniature models using traditional animation techniques, by means of CGI and computer animation, or by a combination of some or all of these techniques, and other visual effects.  Before the introduction of digital production, a series of still images were recorded on a strip of chemically sensitized celluloid (photographic film stock), usually at a rate of 24 frames per second. The images are transmitted through a movie projector at the same rate as they were recorded, with a Geneva drive ensuring that each frame remains still during its short projection time. A rotating shutter causes stroboscopic intervals of darkness, but the viewer does not notice the interruptions due to flicker fusion. The apparent motion on the screen is the result of the fact that the visual sense cannot discern the individual images at high speeds, so the impressions of the images blend with the dark intervals and are thus linked together to produce the illusion of one moving image. An analogous optical soundtrack (a graphic recording of the spoken words, music, and other sounds) runs along a portion of the film exclusively reserved for it, and was not projected.  Contemporary films are usually fully digital through the entire process of production, distribution, and exhibition.  The art of film has drawn on several earlier traditions in fields such as oral storytelling, literature, theatre and visual arts. Forms of art and entertainment that had already featured moving or projected images include:  The stroboscopic animation principle was introduced in 1833 with the stroboscopic disc (better known as the phénakisticope) and later applied in the zoetrope (since 1866), the flip book (since 1868), and the praxinoscope (since 1877), before it became the basic principle for cinematography.  Experiments with early phénakisticope-based animation projectors were made at least as early as 1843 and publicly screened in 1847. Jules Duboscq marketed phénakisticope projection systems in France from c. 1853 until the 1890s.  Photography was introduced in 1839, but initially photographic emulsions needed such long exposures that the recording of moving subjects seemed impossible. At least as early as 1844, photographic series of subjects posed in different positions were created to either suggest a motion sequence or document a range of different viewing angles. The advent of stereoscopic photography, with early experiments in the 1840s and commercial success since the early 1850s, raised interest in completing the photographic medium with the addition of means to capture colour and motion. In 1849, Joseph Plateau published about the idea to combine his invention of the phénakisticope with the stereoscope, as suggested to him by stereoscope inventor Charles Wheatstone, and to use photographs of plaster sculptures in different positions to be animated in the combined device. In 1852, Jules Duboscq patented such an instrument as the \"Stéréoscope-fantascope, ou Bïoscope\", but he only marketed it very briefly, without success. One Bïoscope disc with stereoscopic photographs of a machine is in the Plateau collection of Ghent University, but no instruments or other discs have yet been found.  By the late 1850s the first examples of instantaneous photography came about and provided hope that motion photography would soon be possible, but it took a few decades before it was successfully combined with a method to record series of sequential images in real-time. In 1878, Eadweard Muybridge eventually managed to take a series of photographs of a running horse with a battery of cameras in a line along the track and published the results as The Horse in Motion on cabinet cards. Muybridge, as well as Étienne-Jules Marey, Ottomar Anschütz and many others, would create many more chronophotography studies. Muybridge had the contours of dozens of his chronophotographic series traced onto glass discs and projected them with his zoopraxiscope in his lectures from 1880 to 1895.  Anschütz made his first instantaneous photographs in 1881. He developed a portable camera that allowed shutter speeds as short as 1\/1000 of a second in 1882. The quality of his pictures was generally regarded as much higher than that of the chronophotography works of Muybridge and Étienne-Jules Marey.[4] In 1886, Anschütz developed the Electrotachyscope, an early device that displayed short motion picture loops with 24 glass plate photographs on a 1.5 meter wide rotating wheel that was hand-cranked to a speed of circa 30 frames per second. Different versions were shown at many international exhibitions, fairs, conventions, and arcades from 1887 until at least 1894. Starting in 1891, some 152 examples of a coin-operated peep-box Electrotachyscope model were manufactured by Siemens & Halske in Berlin and sold internationally.[5][4] Nearly 34,000 people paid to see it at the Berlin Exhibition Park in the summer of 1892. Others saw it in London or at the 1893 Chicago World's Fair. On 25 November 1894, Anschütz introduced a Electrotachyscope projector with a 6x8 meter screening in Berlin. Between 22 February and 30 March 1895, a total of circa 7,000 paying customers came to view a 1.5-hour show of some 40 scenes at a 300-seat hall in the old Reichstag building in Berlin.[6]  Émile Reynaud already mentioned the possibility of projecting the images of the Praxinoscope in his 1877 patent application. He presented a praxinoscope projection device at the Société française de photographie on 4 June 1880, but did not market his praxinoscope a projection before 1882. He then further developed the device into the Théâtre Optique which could project longer sequences with separate backgrounds, patented in 1888. He created several movies for the machine by painting images on hundreds of gelatin plates that were mounted into cardboard frames and attached to a cloth band. From 28 October 1892 to March 1900 Reynaud gave over 12,800 shows to a total of over 500,000 visitors at the Musée Grévin in Paris.  By the end of the 1880s, the introduction of lengths of celluloid photographic film and the invention of motion picture cameras, which could photograph a rapid sequence of images using only one lens, allowed action to be captured and stored on a single compact reel of film.  Movies were initially shown publicly to one person at a time through \"peep show\" devices such as the Electrotachyscope, Kinetoscope and the Mutoscope. Not much later, exhibitors managed to project films on large screens for theatre audiences.  The first public screenings of films at which admission was charged were made in 1895 by the American Woodville Latham and his sons, using films produced by their Eidoloscope company,[7] by the Skladanowsky brothers and by the – arguably better known – French brothers Auguste and Louis Lumière with ten of their own productions.[citation needed] Private screenings had preceded these by several months, with Latham's slightly predating the others'.[citation needed]  The earliest films were simply one static shot that showed an event or action with no editing or other cinematic techniques. Typical films showed employees leaving a factory gate, people walking in the street, and the view from the front of a trolley as it traveled a city's Main Street. According to legend, when a film showed a locomotive at high speed approaching the audience, the audience panicked and ran from the theater. Around the turn of the 20th century, films started stringing several scenes together to tell a story. (The filmmakers who first put several shots or scenes discovered that, when one shot follows another, that act establishes a relationship between the content in the separate shots in the minds of the viewer. It is this relationship that makes all film storytelling possible. In a simple example, if a person is shown looking out a window, whatever the next shot shows, it will be regarded as the view the person was seeing.) Each scene was a single stationary shot with the action occurring before it. The scenes were later broken up into multiple shots photographed from different distances and angles. Other techniques such as camera movement were developed as effective ways to tell a story with film. Until sound film became commercially practical in the late 1920s, motion pictures were a purely visual art, but these innovative silent films had gained a hold on the public imagination. Rather than leave audiences with only the noise of the projector as an accompaniment, theater owners hired a pianist or organist or, in large urban theaters, a full orchestra to play music that fit the mood of the film at any given moment. By the early 1920s, most films came with a prepared list of sheet music to be used for this purpose, and complete film scores were composed for major productions.  The rise of European cinema was interrupted by the outbreak of World War I, while the film industry in the United States flourished with the rise of Hollywood, typified most prominently by the innovative work of D. W. Griffith in The Birth of a Nation (1915) and Intolerance (1916). However, in the 1920s, European filmmakers such as Eisenstein, F. W. Murnau and Fritz Lang, in many ways inspired by the meteoric wartime progress of film through Griffith, along with the contributions of Charles Chaplin, Buster Keaton and others, quickly caught up with American film-making and continued to further advance the medium.  In the 1920s, the development of electronic sound recording technologies made it practical to incorporate a soundtrack of speech, music and sound effects synchronized with the action on the screen.[citation needed] The resulting sound films were initially distinguished from the usual silent \"moving pictures\" or \"movies\" by calling them \"talking pictures\" or \"talkies.\"[citation needed] The revolution they wrought was swift. By 1930, silent film was practically extinct in the US and already being referred to as \"the old medium.\"[citation needed]  The evolution of sound in cinema began with the idea of combining moving images with existing phonograph sound technology. Early sound-film systems, such as Thomas Edison's Kinetoscope and the Vitaphone used by Warner Bros., laid the groundwork for synchronized sound in film. The Vitaphone system, produced alongside Bell Telephone Company and Western Electric, faced initial resistance due to expensive equipping costs, but sound in cinema gained acceptance with movies like Don Juan (1926) and The Jazz Singer (1927).[8]  American film studios, while Europe standardized on Tobis-Klangfilm and Tri-Ergon systems. This new technology allowed for greater fluidity in film, giving rise to more complex and epic movies like King Kong (1933).[9]  As the television threat emerged in the 1940s and 1950s, the film industry needed to innovate to attract audiences. In terms of sound technology, this meant the development of surround sound and more sophisticated audio systems, such as Cinerama's seven-channel system. However, these advances required a large number of personnel to operate the equipment and maintain the sound experience in theaters.[9]  In 1966, Dolby Laboratories introduced the Dolby A noise reduction system, which became a standard in the recording industry and eliminated the hissing sound associated with earlier standardization efforts. Dolby Stereo, a revolutionary surround sound system, followed and allowed cinema designers to take acoustics into consideration when designing theaters. This innovation enabled audiences in smaller venues to enjoy comparable audio experiences to those in larger city theaters.[10]  Today, the future of sound in film remains uncertain, with potential influences from artificial intelligence, remastered audio, and personal viewing experiences shaping its development.[11][12] However, it is clear that the evolution of sound in cinema has been marked by continuous innovation and a desire to create more immersive and engaging experiences for audiences.  A significant technological advancement in film was the introduction of \"natural color,\" where color was captured directly from nature through photography, as opposed to being manually added to black-and-white prints using techniques like hand-coloring or stencil-coloring.[13][14] Early color processes often produced colors that appeared far from \"natural\".[15] Unlike the rapid transition from silent films to sound films, color's replacement of black-and-white happened more gradually.[16]  The crucial innovation was the three-strip version of the Technicolor process, first used in animated cartoons in 1932.[17][18] The process was later applied to live-action short films, specific sequences in feature films, and finally, for an entire feature film, Becky Sharp, in 1935.[19] Although the process was expensive, the positive public response, as evidenced by increased box office revenue, generally justified the additional cost.[13] Consequently, the number of films made in color gradually increased year after year.[20][21]  In the early 1950s, the proliferation of black-and-white television started seriously depressing North American theater attendance.[citation needed] In an attempt to lure audiences back into theaters, bigger screens were installed, widescreen processes, polarized 3D projection, and stereophonic sound were introduced, and more films were made in color, which soon became the rule rather than the exception. Some important mainstream Hollywood films were still being made in black-and-white as late as the mid-1960s, but they marked the end of an era. Color television receivers had been available in the US since the mid-1950s, but at first, they were very expensive and few broadcasts were in color. During the 1960s, prices gradually came down, color broadcasts became common, and sales boomed. The overwhelming public verdict in favor of color was clear. After the final flurry of black-and-white films had been released in mid-decade, all Hollywood studio productions were filmed in color, with the usual exceptions made only at the insistence of \"star\" filmmakers such as Peter Bogdanovich and Martin Scorsese.[citation needed]  The decades following the decline of the studio system in the 1960s saw changes in the production and style of film. Various New Wave movements (including the French New Wave, New German Cinema wave, Indian New Wave, Japanese New Wave, New Hollywood, and Egyptian New Wave) and the rise of film-school-educated independent filmmakers contributed to the changes the medium experienced in the latter half of the 20th century. Digital technology has been the driving force for change throughout the 1990s and into the 2000s. Digital 3D projection largely replaced earlier problem-prone 3D film systems and has become popular in the early 2010s.[citation needed]  \"Film theory\" seeks to develop concise and systematic concepts that apply to the study of film as art. The concept of film as an art-form began in 1911 with Ricciotto Canudo's manifest The Birth of the Sixth Art. The Moscow Film School, the oldest film school in the world, was founded in 1919, in order to teach about and research film theory. Formalist film theory, led by Rudolf Arnheim, Béla Balázs, and Siegfried Kracauer, emphasized how film differed from reality and thus could be considered a valid fine art. André Bazin reacted against this theory by arguing that film's artistic essence lay in its ability to mechanically reproduce reality, not in its differences from reality, and this gave rise to realist theory. More recent analysis spurred by Jacques Lacan's psychoanalysis and Ferdinand de Saussure's semiotics among other things has given rise to psychoanalytic film theory, structuralist film theory, feminist film theory, and others. On the other hand, critics from the analytical philosophy tradition, influenced by Wittgenstein, try to clarify misconceptions used in theoretical studies and produce analysis of a film's vocabulary and its link to a form of life.  Film is considered to have its own language. James Monaco wrote a classic text on film theory, titled \"How to Read a Film,\" that addresses this. Director Ingmar Bergman famously said, \"Andrei Tarkovsky for me is the greatest director, the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream.\" An example of the language is a sequence of back and forth images of one speaking actor's left profile, followed by another speaking actor's right profile, then a repetition of this, which is a language understood by the audience to indicate a conversation. This describes another theory of film, the 180-degree rule, as a visual story-telling device with an ability to place a viewer in a context of being psychologically present through the use of visual composition and editing. The \"Hollywood style\" includes this narrative theory, due to the overwhelming practice of the rule by movie studios based in Hollywood, California, during film's classical era. Another example of cinematic language is having a shot that zooms in on the forehead of an actor with an expression of silent reflection that cuts to a shot of a younger actor who vaguely resembles the first actor, indicating that the first person is remembering a past self, an edit of compositions that causes a time transition.  Montage is a film editing technique in which separate pieces of film are selected, edited, and assembled to create a new section or sequence within a film. This technique can be used to convey a narrative or to create an emotional or intellectual effect by juxtaposing different shots, often for the purpose of condensing time, space, or information. Montage can involve flashbacks, parallel action, or the interplay of various visual elements to enhance the storytelling or create symbolic meaning.[22]  The concept of montage emerged in the 1920s, with pioneering Soviet filmmakers such as Sergei Eisenstein and Lev Kuleshov developing the theory of montage. Eisenstein's film Battleship Potemkin (1925) is a prime example of the innovative use of montage, where he employed complex juxtapositions of images to create a visceral impact on the audience.[23]  As the art of montage evolved, filmmakers began incorporating musical and visual counterpoint to create a more dynamic and engaging experience for the viewer. The development of scene construction through mise-en-scène, editing, and special effects led to more sophisticated techniques that can be compared to those utilized in opera and ballet.[24]  The French New Wave movement of the late 1950s and 1960s also embraced the montage technique, with filmmakers such as Jean-Luc Godard and François Truffaut using montage to create distinctive and innovative films. This approach continues to be influential in contemporary cinema, with directors employing montage to create memorable sequences in their films.[25]  In contemporary cinema, montage continues to play an essential role in shaping narratives and creating emotional resonance. Filmmakers have adapted the traditional montage technique to suit the evolving aesthetics and storytelling styles of modern cinema.  As the medium of film continues to evolve, montage remains an integral aspect of visual storytelling, with filmmakers finding new and innovative ways to employ this powerful technique.  If a movie can illuminate the lives of other people who share this planet with us and show us not only how different they are but, how even so, they share the same dreams and hurts, then it deserves to be called great.  Film criticism is the analysis and evaluation of films. In general, these works can be divided into two categories: academic criticism by film scholars and journalistic film criticism that appears regularly in newspapers and other media. Film critics working for newspapers, magazines, and broadcast media mainly review new releases. Normally they only see any given film once and have only a day or two to formulate their opinions. Despite this, critics have an important impact on the audience response and attendance at films, especially those of certain genres. Mass marketed action, horror, and comedy films tend not to be greatly affected by a critic's overall judgment of a film. The plot summary and description of a film and the assessment of the director's and screenwriters' work that makes up the majority of most film reviews can still have an important impact on whether people decide to see a film. For prestige films such as most dramas and art films, the influence of reviews is important. Poor reviews from leading critics at major papers and magazines will often reduce audience interest and attendance.  The impact of a reviewer on a given film's box office performance is a matter of debate. Some observers claim that movie marketing in the 2000s is so intense, well-coordinated and well financed that reviewers cannot prevent a poorly written or filmed blockbuster from attaining market success. However, the cataclysmic failure of some heavily promoted films which were harshly reviewed, as well as the unexpected success of critically praised independent films indicates that extreme critical reactions can have considerable influence. Other observers note that positive film reviews have been shown to spark interest in little-known films. Conversely, there have been several films in which film companies have so little confidence that they refuse to give reviewers an advanced viewing to avoid widespread panning of the film. However, this usually backfires, as reviewers are wise to the tactic and warn the public that the film may not be worth seeing and the films often do poorly as a result. Journalist film critics are sometimes called film reviewers. Critics who take a more academic approach to films, through publishing in film journals and writing books about films using film theory or film studies approaches, study how film and filming techniques work, and what effect they have on people. Rather than having their reviews published in newspapers or appearing on television, their articles are published in scholarly journals or up-market magazines. They also tend to be affiliated with colleges or universities as professors or instructors.  The making and showing of motion pictures became a source of profit almost as soon as the process was invented. Upon seeing how successful their new invention, and its product, was in their native France, the Lumières quickly set about touring the Continent to exhibit the first films privately to royalty and publicly to the masses. In each country, they would normally add new, local scenes to their catalogue and, quickly enough, found local entrepreneurs in the various countries of Europe to buy their equipment and photograph, export, import, and screen additional product commercially. The Oberammergau Passion Play of 1898[31] was the first commercial motion picture ever produced. Other pictures soon followed, and motion pictures became a separate industry that overshadowed the vaudeville world. Dedicated theaters and companies formed specifically to produce and distribute films, while motion picture actors became major celebrities and commanded huge fees for their performances. By 1917 Charlie Chaplin had a contract that called for an annual salary of one million dollars. From 1931 to 1956, film was also the only image storage and playback system for television programming until the introduction of videotape recorders.  In the United States, much of the film industry is centered around Hollywood, California. Other regional centers exist in many parts of the world, such as Mumbai-centered Bollywood, the Indian film industry's Hindi cinema which produces the largest number of films in the world.[32] Though the expense involved in making films has led cinema production to concentrate under the auspices of movie studios, recent advances in affordable film making equipment have allowed independent film productions to flourish.  Profit is a key force in the industry, due to the costly and risky nature of filmmaking; many films have large cost overruns, an example being Kevin Costner's Waterworld. Yet many filmmakers strive to create works of lasting social significance. The Academy Awards (also known as \"the Oscars\") are the most prominent film awards in the United States, providing recognition each year to films, based on their artistic merits. There is also a large industry for educational and instructional films made in lieu of or in addition to lectures and texts. Revenue in the industry is sometimes volatile due to the reliance on blockbuster films released in movie theaters. The rise of alternative home entertainment has raised questions about the future of the cinema industry, and Hollywood employment has become less reliable, particularly for medium and low-budget films.[33]  Derivative academic fields of study may both interact with and develop independently of filmmaking, as in film theory and analysis. Fields of academic study have been created that are derivative or dependent on the existence of film, such as film criticism, film history, divisions of film propaganda in authoritarian governments, or psychological on subliminal effects (e.g., of a flashing soda can during a screening). These fields may further create derivative fields, such as a movie review section in a newspaper or a television guide. Sub-industries can spin off from film, such as popcorn makers, and film-related toys (e.g., Star Wars figures). Sub-industries of pre-existing industries may deal specifically with film, such as product placement and other advertising within films.  The terminology used for describing motion pictures varies considerably between British and American English. In British usage, the name of the medium is film. The word movie is understood but seldom used.[34][35] Additionally, the pictures (plural) is used somewhat frequently to refer to the place where movies are exhibited; in American English this may be called the movies, but that term is becoming outdated. In other countries, the place where movies are exhibited may be called a cinema or movie theatre.  By contrast, in the United States, movie is the predominant term for the medium. Although the words film and movie are sometimes used interchangeably, film is more often used when considering a work's artistic, theoretical, or technical aspects. The term movie more often refers to a work's entertainment or commercial aspects.  Further terminology is used to distinguish various forms and media used in the film industry. Motion pictures and moving pictures are frequently used terms for film and movie productions specifically intended for theatrical exhibition, such as Star Wars. DVD, Blu-ray Disc, and videotape are video formats that can reproduce a photochemical film. A reproduction based on such is called a transfer. After the advent of theatrical film as an industry, the television industry began using videotape as a recording medium. For many decades, tape was solely an analog medium onto which moving images could be either recorded or transferred. Film and filming refer to the photochemical medium that chemically records a visual image and the act of recording respectively. However, the act of shooting images with other visual media, such as with a digital camera, is still called filming, and the resulting works often called films as interchangeable to movies, despite not being shot on film. Silent films need not be utterly silent, but are films and movies without an audible dialogue, including those that have a musical accompaniment. The word talkies refers to the earliest sound films created to have audible dialogue recorded for playback along with the film, regardless of a musical accompaniment. Cinema either broadly encompasses both films and movies, or it is roughly synonymous with film and theatrical exhibition, and both are capitalized when referring to a category of art. The silver screen refers to the projection screen used to exhibit films and, by extension, is also used as a metonym for the entire film industry.  Widescreen refers to a larger width to height in the frame, compared to earlier historic aspect ratios.[36] A feature-length film, or feature film, is of a conventional full length, usually 60 minutes or more, and can commercially stand by itself without other films in a ticketed screening.[37] A short is a film that is not as long as a feature-length film, often screened with other shorts, or preceding a feature-length film. An independent is a film made outside the conventional film industry.  In US usage, one talks of a screening or projection of a movie or video on a screen at a public or private theater. In British English, a film showing happens at a cinema (never a theatre, which is a different medium and place altogether).[35] Cinema usually refers to an arena designed specifically to exhibit films, where the screen is affixed to a wall, while theatre usually refers to a place where live, non-recorded action or combination thereof occurs from a podium or other type of stage, including the amphitheatre. Theatres can still screen movies in them, though the theatre would be retrofitted to do so. One might propose going to the cinema when referring to the activity, or sometimes to the pictures in British English, whereas the US expression is usually going to the movies. A cinema usually shows a mass-marketed movie using a front-projection screen process with either a film projector or, more recently, with a digital projector. But, cinemas may also show theatrical movies from their home video transfers that include Blu-ray Disc, DVD, and videocassette when they possess sufficient projection quality or based upon need, such as movies that exist only in their transferred state, which may be due to the loss or deterioration of the film master and prints from which the movie originally existed. Due to the advent of digital film production and distribution, physical film might be absent entirely.  A double feature is a screening of two independently marketed, stand-alone feature films. A viewing is a watching of a film. Sales and at the box office refer to tickets sold at a theater, or more currently, rights sold for individual showings. A release is the distribution and often simultaneous screening of a film. A preview is a screening in advance of the main release.  Any film may also have a sequel, which portrays events following those in the film. Bride of Frankenstein is an early example. When there are more films than one with the same characters, story arcs, or subject themes, these movies become a series, such as the James Bond series. Existing outside a specific story timeline usually does not exclude a film from being part of a series. A film that portrays events occurring earlier in a timeline with those in another film, but is released after that film, is sometimes called a prequel, an example being Butch and Sundance: The Early Days.  The credits, or end credits, are a list that gives credit to the people involved in the production of a film. Films from before the 1970s usually start a film with credits, often ending with only a title card, saying \"The End\" or some equivalent, often an equivalent that depends on the language of the production.[citation needed] From then onward, a film's credits usually appear at the end of most films. However, films with credits that end a film often repeat some credits at or near the start of a film and therefore appear twice, such as that film's acting leads, while less frequently some appearing near or at the beginning only appear there, not at the end, which often happens to the director's credit. The credits appearing at or near the beginning of a film are usually called titles or beginning titles. A post-credits scene is a scene shown after the end of the credits. Ferris Bueller's Day Off has a post-credits scene in which Ferris tells the audience that the film is over and they should go home.  A film's cast refers to a collection of the actors and actresses who appear, or star, in a film. A star is an actor or actress, often a popular one, and in many cases, a celebrity who plays a central character in a film. Occasionally the word can also be used to refer to the fame of other members of the crew, such as a director or other personality, such as Martin Scorsese. A crew is usually interpreted as the people involved in a film's physical construction outside cast participation, and it could include directors, film editors, photographers, grips, gaffers, set decorators, prop masters, and costume designers. A person can both be part of a film's cast and crew, such as Woody Allen, who directed and starred in Take the Money and Run.  A film goer, movie goer, or film buff is a person who likes or often attends films and movies, and any of these, though more often the latter, could also see oneself as a student to films and movies or the filmic process. Intense interest in films, film theory, and film criticism, is known as cinephilia. A film enthusiast is known as a cinephile or cineaste.  Preview performance refers to a showing of a film to a select audience, usually for the purposes of corporate promotions, before the public film premiere itself. Previews are sometimes used to judge audience reaction, which if unexpectedly negative, may result in recutting or even refilming certain sections based on the audience response. One example of a film that was changed after a negative response from the test screening is 1982's First Blood. After the test audience responded very negatively to the death of protagonist John Rambo, a Vietnam veteran, at the end of the film, the company wrote and re-shot a new ending in which the character survives.[38]  Trailers or previews are advertisements for films that will be shown in 1 to 3 months at a cinema. Back in the early days of cinema, with theaters that had only one or two screens, only certain trailers were shown for the films that were going to be shown there. Later, when theaters added more screens or new theaters were built with a lot of screens, all different trailers were shown even if they were not going to play that film in that theater. Film studios realized that the more trailers that were shown (even if it was not going to be shown in that particular theater) the more patrons would go to a different theater to see the film when it came out. The term trailer comes from their having originally been shown at the end of a film program. That practice did not last long because patrons tended to leave the theater after the films ended, but the name has stuck. Trailers are now shown before the film (or the \"A film\" in a double feature program) begins. Film trailers are also common on DVDs and Blu-ray Discs, as well as on the Internet and mobile devices. Trailers are created to be engaging and interesting for viewers. As a result, in the Internet era, viewers often seek out trailers to watch them. Of the ten billion videos watched online annually in 2008, film trailers ranked third, after news and user-created videos.[39] A teaser is a much shorter preview or advertisement that lasts only 10 to 30 seconds. Teasers are used to get patrons excited about a film coming out in the next six to twelve months. Teasers may be produced even before the film production is completed.  Films are cultural artifacts created by specific cultures, facilitating intercultural dialogue. It is considered to be an important art form that provides entertainment and historical value, often visually documenting a period of time. The visual basis of the medium gives it a universal power of communication, often stretched further through the use of dubbing or subtitles to translate the dialog into other languages.[42] Just seeing a location in a film is linked to higher tourism to that location, demonstrating how powerful the suggestive nature of the medium can be.[43]  Film is used for a range of goals, including education and propaganda due its ability to effectively intercultural dialogue. When the purpose is primarily educational, a film is called an \"educational film\". Examples are recordings of academic lectures and experiments, or a film based on a classic novel. Film may be propaganda, in whole or in part, such as the films made by Leni Riefenstahl in Nazi Germany, US war film trailers during World War II, or artistic films made under Stalin by Sergei Eisenstein. They may also be works of political protest, as in the films of Andrzej Wajda, or more subtly, the films of Andrei Tarkovsky. The same film may be considered educational by some, and propaganda by others as the categorization of a film can be subjective.  At its core, the means to produce a film depend on the content the filmmaker wishes to show, and the apparatus for displaying it: the zoetrope merely requires a series of images on a strip of paper. Film production can, therefore, take as little as one person with a camera (or even without a camera, as in Stan Brakhage's 1963 film Mothlight), or thousands of actors, extras, and crew members for a live-action, feature-length epic. The necessary steps for almost any film can be boiled down to conception, planning, execution, revision, and distribution. The more involved the production, the more significant each of the steps becomes. In a typical production cycle of a Hollywood-style film, these main stages are defined as development, pre-production, production, post-production and distribution.  This production cycle usually takes three years. The first year is taken up with development. The second year comprises preproduction and production. The third year, post-production and distribution. The bigger the production, the more resources it takes, and the more important financing becomes; most feature films are artistic works from the creators' perspective (e.g., film director, cinematographer, screenwriter) and for-profit business entities for the production companies.  A film crew is a group of people hired by a film company, employed during the \"production\" or \"photography\" phase, for the purpose of producing a film or motion picture. Crew is distinguished from cast, who are the actors who appear in front of the camera or provide voices for characters in the film. The crew interacts with but is also distinct from the production staff, consisting of producers, managers, company representatives, their assistants, and those whose primary responsibility falls in pre-production or post-production phases, such as screenwriters and film editors. Communication between production and crew generally passes through the director and his\/her staff of assistants. Medium-to-large crews are generally divided into departments with well-defined hierarchies and standards for interaction and cooperation between the departments. Other than acting, the crew handles everything in the photography phase: props and costumes, shooting, sound, electrics (i.e., lights), sets, and production special effects. Caterers (known in the film industry as \"craft services\") are usually not considered part of the crew.  Film stock consists of transparent celluloid, acetate, or polyester base coated with an emulsion containing light-sensitive chemicals. Cellulose nitrate was the first type of film base used to record motion pictures, but due to its flammability was eventually replaced by safer materials. Stock widths and the film format for images on the reel have had a rich history, though most large commercial films are still shot on (and distributed to theaters) as 35 mm prints. Originally moving picture film was shot and projected at various speeds using hand-cranked cameras and projectors; though 1000 frames per minute (16⁠2\/3⁠ frame\/s) is generally cited as a standard silent speed, research indicates most films were shot between 16 frame\/s and 23 frame\/s and projected from 18 frame\/s on up (often reels included instructions on how fast each scene should be shown).[44] When synchronized sound film was introduced in the late 1920s, a constant speed was required for the sound head. 24 frames per second were chosen because it was the slowest (and thus cheapest) speed which allowed for sufficient sound quality.[45] The standard was set with Warner Bros.'s The Jazz Singer and their Vitaphone system in 1927.[46][47] Improvements since the late 19th century include the mechanization of cameras – allowing them to record at a consistent speed, quiet camera design – allowing sound recorded on-set to be usable without requiring large \"blimps\" to encase the camera, the invention of more sophisticated filmstocks and lenses, allowing directors to film in increasingly dim conditions, and the development of synchronized sound, allowing sound to be recorded at exactly the same speed as its corresponding action. The soundtrack can be recorded separately from shooting the film, but for live-action pictures, many parts of the soundtrack are usually recorded simultaneously.  As a medium, film is not limited to motion pictures, since the technology developed as the basis for photography. It can be used to present a progressive sequence of still images in the form of a slideshow. Film has also been incorporated into multimedia presentations and often has importance as primary historical documentation. However, historic films have problems in terms of preservation and storage, and the motion picture industry is exploring many alternatives. Most films on cellulose nitrate base have been copied onto modern safety films. Some studios save color films through the use of separation masters: three B&W negatives each exposed through red, green, or blue filters (essentially a reverse of the Technicolor process). Digital methods have also been used to restore films, although their continued obsolescence cycle makes them (as of 2006) a poor choice for long-term preservation. Film preservation of decaying film stock is a matter of concern to both film historians and archivists and to companies interested in preserving their existing products in order to make them available to future generations (and thereby increase revenue). Preservation is generally a higher concern for nitrate and single-strip color films, due to their high decay rates; black-and-white films on safety bases and color films preserved on Technicolor imbibition prints tend to keep up much better, assuming proper handling and storage.  Some films in recent decades have been recorded using analog video technology similar to that used in television production. Modern digital video cameras and digital projectors are gaining ground as well. These approaches are preferred by some film-makers, especially because footage shot with digital cinema can be evaluated and edited with non-linear editing systems (NLE) without waiting for the film stock to be processed. The migration was gradual, and as of 2005, most major motion pictures were still shot on film.[needs update]  Independent filmmaking often takes place outside Hollywood, or other major studio systems. An independent film (or indie film) is a film initially produced without financing or distribution from a major film studio. Creative, business and technological reasons have all contributed to the growth of the indie film scene in the late 20th and early 21st century. On the business side, the costs of big-budget studio films also lead to conservative choices in cast and crew. There is a trend in Hollywood towards co-financing (over two-thirds of the films put out by Warner Bros. in 2000 were joint ventures, up from 10% in 1987).[48] A hopeful director is almost never given the opportunity to get a job on a big-budget studio film unless he or she has significant industry experience in film or television. Also, the studios rarely produce films with unknown actors, particularly in lead roles.  Before the advent of digital alternatives, the cost of professional film equipment and stock was also a hurdle to being able to produce, direct, or star in a traditional studio film. But the advent of consumer camcorders in 1985, and more importantly, the arrival of high-resolution digital video in the early 1990s, have lowered the technology barrier to film production significantly. Both production and post-production costs have been significantly lowered; in the 2000s, the hardware and software for post-production can be installed in a commodity-based personal computer. Technologies such as DVDs, FireWire connections and a wide variety of professional and consumer-grade video editing software make film-making relatively affordable.  Since the introduction of digital video DV technology, the means of production have become more democratized. Filmmakers can conceivably shoot a film with a digital video camera and edit the film, create and edit the sound and music, and mix the final cut on a high-end home computer. However, while the means of production may be democratized, financing, distribution, and marketing remain difficult to accomplish outside the traditional system. Most independent filmmakers rely on film festivals to get their films noticed and sold for distribution. The arrival of internet-based video websites such as YouTube and Veoh has further changed the filmmaking landscape, enabling indie filmmakers to make their films available to the public.  An open content film is much like an independent film, but it is produced through open collaborations; its source material is available under a license which is permissive enough to allow other parties to create fan fiction or derivative works rather than a traditional copyright. Like independent filmmaking, open source filmmaking takes place outside Hollywood and other major studio systems. For example, the film Balloon was based on the real event during the Cold War.[49]  A fan film is a film or video inspired by a film, television program, comic book or a similar source, created by fans rather than by the source's copyright holders or creators. Fan filmmakers have traditionally been amateurs, but some of the most notable films have actually been produced by professional filmmakers as film school class projects or as demonstration reels. Fan films vary tremendously in length, from short faux-teaser trailers for non-existent motion pictures to rarer full-length motion pictures.  Film distribution is the process through which a film is made available for viewing by an audience. This is normally the task of a professional film distributor, who would determine the marketing strategy of the film, the media by which a film is to be exhibited or made available for viewing, and may set the release date and other matters. The film may be exhibited directly to the public either through a movie theater (historically the main way films were distributed) or television for personal home viewing (including on DVD-Video or Blu-ray Disc, video-on-demand, online downloading, television programs through broadcast syndication etc.). Other ways of distributing a film include rental or personal purchase of the film in a variety of media and formats, such as VHS tape or DVD, or Internet downloading or streaming using a computer.  Animation is a technique in which each frame of a film is produced individually, whether generated as a computer graphic, or by photographing a drawn image, or by repeatedly making small changes to a model unit (see claymation and stop motion), and then photographing the result with a special animation camera. When the frames are strung together and the resulting film is viewed at a speed of 16 or more frames per second, there is an illusion of continuous movement (due to the phi phenomenon). Generating such a film is very labor-intensive and tedious, though the development of computer animation has greatly sped up the process. Because animation is very time-consuming and often very expensive to produce, the majority of animation for TV and films comes from professional animation studios. However, the field of independent animation has existed at least since the 1950s, with animation being produced by independent studios (and sometimes by a single person). Several independent animation producers have gone on to enter the professional animation industry.  Limited animation is a way of increasing production and decreasing costs of animation by using \"short cuts\" in the animation process. This method was pioneered by UPA and popularized by Hanna-Barbera in the United States, and by Osamu Tezuka in Japan, and adapted by other studios as cartoons moved from movie theaters to television.[50] Although most animation studios are now using digital technologies in their productions, there is a specific style of animation that depends on film. Camera-less animation, made famous by film-makers like Norman McLaren, Len Lye, and Stan Brakhage, is painted and drawn directly onto pieces of film, and then run through a projector. "},
"meta":{},"created_at":"2025-03-22T14:25:42.273677Z","updated_at":"2025-03-23T13:58:48.657413Z","inner_id":1,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":1,"comment_authors":[]},
{"id":10,"annotations":[{"id":10,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.309403Z","updated_at":"2025-03-22T14:25:42.309403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"6fd320b0-057e-4834-a3cd-6a3babced6aa","import_id":null,"last_action":null,"bulk_created":false,"task":10,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A financial market is a market in which people trade financial securities and derivatives at low transaction costs. Some of the securities include stocks and bonds, raw materials and precious metals, which are known in the financial markets as commodities.  The term \"market\" is sometimes used for what are more strictly exchanges, that is, organizations that facilitate the trade in financial securities, e.g., a stock exchange or commodity exchange. This may be a physical location (such as the New York Stock Exchange (NYSE), London Stock Exchange (LSE), Johannesburg Stock Exchange JSE Limited (JSE), Bombay Stock Exchange (BSE), National Stock Exchange of India (NSE) or an electronic system such as NASDAQ. Much trading of stocks takes place on an exchange; still, corporate actions (merger, spinoff) are outside an exchange, while any two companies or people, for whatever reason, may agree to sell the stock from the one to the other without using an exchange.  Trading of currencies and bonds is largely on a bilateral basis, although some bonds trade on a stock exchange, and people are building electronic systems for these as well. There are also global initiatives such as the United Nations Sustainable Development Goal 10 which has a target to improve regulation and monitoring of global financial markets.[1]  Within the financial sector, the term \"financial markets\" is often used to refer just to the markets that are used to raise finances. For long term finance, they are usually called the capital markets; for short term finance, they are usually called money markets. The money market deals in short-term loans, generally for a period of a year or less. Another common use of the term is as a catchall for all the markets in the financial sector, as per examples in the breakdown below.  The capital markets may also be divided into primary markets and secondary markets. Newly formed (issued) securities are bought or sold in primary markets, such as during initial public offerings. Secondary markets allow investors to buy and sell existing securities. The transactions in primary markets exist between issuers and investors, while secondary market transactions exist among investors.  Liquidity is a crucial aspect of securities that are traded in secondary markets. Liquidity refers to the ease with which a security can be sold without a loss of value. Securities with an active secondary market mean that there are many buyers and sellers at a given point in time. Investors benefit from liquid securities because they can sell their assets whenever they want; an illiquid security may force the seller to get rid of their asset at a large discount.  Financial markets attract funds from investors and channels them to corporations—they thus allow corporations to finance their operations and achieve growth. Money markets allow firms to borrow funds on a short-term basis, while capital markets allow corporations to gain long-term funding to support expansion (known as maturity transformation).  Without financial markets, borrowers would have difficulty finding lenders themselves. Intermediaries such as banks, Investment Banks, and Boutique Investment Banks can help in this process. Banks take deposits from those who have money to save on the form of savings a\/c. They can then lend money from this pool of deposited money to those who seek to borrow. Banks popularly lend money in the form of loans and mortgages.  More complex transactions than a simple bank deposit require markets where lenders and their agents can meet borrowers and their agents, and where existing borrowing or lending commitments can be sold on to other parties. A good example of a financial market is a stock exchange. A company can raise money by selling shares to investors and its existing shares can be bought or sold.  The following table illustrates where financial markets fit in the relationship between lenders and borrowers:  The lender temporarily gives money to somebody else, on the condition of getting back the principal amount together with some interest or profit or charge.  Many individuals are not aware that they are lenders, but almost everybody does lend money in many ways. A person lends money when he or she:  Companies tend to be lenders of capital. When companies have surplus cash that is not needed for a short period of time, they may seek to make money from their cash surplus by lending it via short term markets called money markets. Alternatively, such companies may decide to return the cash surplus to their shareholders (e.g. via a share repurchase or dividend payment).  Banks can be lenders themselves as they are able to create new debt money in the form of deposits.  Governments borrow by issuing bonds. In the UK, the government also borrows from individuals by offering bank accounts and Premium Bonds. Government debt seems to be permanent. Indeed, the debt seemingly expands rather than being paid off. One strategy used by governments to reduce the value of the debt is to influence inflation.  Municipalities and local authorities may borrow in their own name as well as receiving funding from national governments. In the UK, this would cover an authority like Hampshire County Council.  Public Corporations typically include nationalized industries. These may include the postal services, railway companies and utility companies.  Many borrowers have difficulty raising money locally. They need to borrow internationally with the aid of Foreign exchange markets.  Borrowers having similar needs can form into a group of borrowers. They can also take an organizational form like Mutual Funds. They can provide mortgage on weight basis. The main advantage is that this lowers the cost of their borrowings.  During the 1980s and 1990s, a major growth sector in financial markets was the trade in so called derivatives.  In the financial markets, stock prices, share prices, bond prices, currency rates, interest rates and dividends go up and down, creating risk. Derivative products are financial products that are used to control risk or paradoxically exploit risk.[4] It is also called financial economics.  Derivative products or instruments help the issuers to gain an unusual profit from issuing the instruments. For using the help of these products a contract has to be made. Derivative contracts are mainly four types:[5]  Over the past few decades, the derivatives market has increased and become essential to the financial industry. As the market expands, establishing and improving the regulatory framework becomes particularly critical. In response to the systemic risks exposed by the global economic crisis in 2008, essential regulations such as the Dodd-Frank Act (US)[6] and the EU Market Fundamentals Regulation (MiFID II)[7] were enacted.  These regulations have significantly changed the market structure and strengthened supervision and risk management of the derivatives market. Although regulatory measures have enhanced market stability, they have also had a broad impact on market participants' operating models and strategies.  Seemingly, the most obvious buyers and sellers of currency are importers and exporters of goods. While this may have been true in the distant past,[when?] when international trade created the demand for currency markets, importers and exporters now represent only 1\/32 of foreign exchange dealing, according to the Bank for International Settlements.[8]  The picture of foreign currency transactions today shows:  Much effort has gone into the study of financial markets and how prices vary with time. Charles Dow, one of the founders of Dow Jones & Company and The Wall Street Journal, enunciated a set of ideas on the subject which are now called Dow theory. This is the basis of the so-called technical analysis method of attempting to predict future changes. One of the tenets of \"technical analysis\" is that market trends give an indication of the future, at least in the short term. The claims of the technical analysts are disputed by many academics, who claim that the evidence points rather to the random walk hypothesis, which states that the next change is not correlated to the last change. The role of human psychology in price variations also plays a significant factor. Large amounts of volatility often indicate the presence of strong emotional factors playing into the price. Fear can cause excessive drops in price and greed can create bubbles. In recent years the rise of algorithmic and high-frequency program trading has seen the adoption of momentum, ultra-short term moving average and other similar strategies which are based on technical as opposed to fundamental or theoretical concepts of market behaviour. For instance, according to a study published by the European Central Bank,[9] high frequency trading has a substantial correlation with news announcements and other relevant public information that are able to create wide price movements (e.g., interest rates decisions, trade of balances etc.)  The scale of changes in price over some unit of time is called the volatility. It was discovered by Benoit Mandelbrot that changes in prices do not follow a normal distribution, but are rather modeled better by Lévy stable distributions. The scale of change, or volatility, depends on the length of the time unit to a power a bit more than 1\/2. Large changes up or down are more likely than what one would calculate using a normal distribution with an estimated standard deviation.  Simply put, primary market is the market where the newly started company issued shares to the public for the first time through IPO (initial public offering). Secondary market is the market where the second hand securities are sold (security Commodity Markets). "},"meta":{},"created_at":"2025-03-22T14:25:42.273677Z","updated_at":"2025-03-22T14:25:42.273677Z","inner_id":2,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":11,"annotations":[{"id":11,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.309403Z","updated_at":"2025-03-22T14:25:42.309403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"d02f98fe-8b9c-4223-aa63-1350b1924e09","import_id":null,"last_action":null,"bulk_created":false,"task":11,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Venture capital (VC) is a form of private equity financing provided by firms or funds to startup, early-stage, and emerging companies, that have been deemed to have high growth potential or that have demonstrated high growth in terms of number of employees, annual revenue, scale of operations, etc. Venture capital firms or funds invest in these early-stage companies in exchange for equity, or an ownership stake. Venture capitalists take on the risk of financing start-ups in the hopes that some of the companies they support will become successful. Because startups face high uncertainty,[1] VC investments have high rates of failure. Start-ups are usually based on an innovative technology or business model and often come from high technology industries such as information technology (IT) or biotechnology.  Pre-seed and seed rounds are the initial stages of funding for a startup company,[2] typically occurring early in its development. During a seed round, entrepreneurs seek investment from angel investors, venture capital firms, or other sources to finance the initial operations and development of their business idea. Seed funding is often used to validate the concept, build a prototype, or conduct market research. This initial capital injection is crucial for startups to kickstart their journey and attract further investment in subsequent funding rounds.  Typical venture capital investments occur after an initial \"seed funding\" round. The first round of institutional venture capital to fund growth is called the Series A round. Venture capitalists provide this financing in the interest of generating a return through an eventual \"exit\" event, such as the company selling shares to the public for the first time in an initial public offering (IPO), or disposal of shares happening via a merger, via a sale to another entity such as a financial buyer in the private equity secondary market or via a sale to a trading company such as a competitor.  In addition to angel investing, equity crowdfunding and other seed funding options, venture capital is attractive for new companies with limited operating history that are too small to raise capital in the public markets and have not reached the point where they are able to secure a bank loan or complete a debt offering. In exchange for the high risk that venture capitalists assume by investing in smaller and early-stage companies, venture capitalists usually get significant control over company decisions, in addition to a significant portion of the companies' ownership (and consequently value). Companies who have reached a market valuation of over $1 billion are referred to as Unicorns. As of May 2024 there were a reported total of 1248 Unicorn companies.[3] Venture capitalists also often provide strategic advice to the company's executives on its business model and marketing strategies.  Venture capital is also a way in which the private and public sectors can construct an institution that systematically creates business networks for the new firms and industries so that they can progress and develop. This institution helps identify promising new firms and provide them with finance, technical expertise, mentoring, talent acquisition, strategic partnership, marketing \"know-how\", and business models. Once integrated into the business network, these firms are more likely to succeed, as they become \"nodes\" in the search networks for designing and building products in their domain.[4] However, venture capitalists' decisions are often biased, exhibiting for instance overconfidence and illusion of control, much like entrepreneurial decisions in general.[5]  Before World War II (1939–1945) venture capital was primarily the domain of wealthy individuals and families. J.P. Morgan, the Wallenbergs, the Vanderbilts, the Whitneys, the Rockefellers, and the Warburgs were notable investors in private companies. In 1938, Laurance S. Rockefeller helped finance the creation of both Eastern Air Lines and Douglas Aircraft, and the Rockefeller family had vast holdings in a variety of companies. Eric M. Warburg founded E.M. Warburg & Co. in 1938, which would ultimately become Warburg Pincus, with investments in both leveraged buyouts and venture capital. The Wallenberg family started Investor AB in 1916 in Sweden and were early investors in several Swedish companies such as ABB, Atlas Copco, and Ericsson in the first half of the 20th century.  Only after 1945 did \"true\" venture capital investment firms begin to emerge, notably with the founding of American Research and Development Corporation (ARDC) and J.H. Whitney & Company in 1946.[6][7]  Georges Doriot, the \"father of venture capitalism\",[8] along with Ralph Flanders and Karl Compton (former president of MIT) founded ARDC in 1946 to encourage private-sector investment in businesses run by soldiers returning from World War II. ARDC became the first institutional private-equity investment firm to raise capital from sources other than wealthy families. Unlike most present-day venture capital firms, ARDC was a publicly traded company. ARDC's most successful investment was its 1957 funding of Digital Equipment Corporation (DEC), which would later be valued at more than $355 million after its initial public offering in 1968. This represented a return of over 1200 times its investment and an annualized rate of return of 101% to ARDC.[9]  Former employees of ARDC went on to establish several prominent venture capital firms including Greylock Partners, founded in 1965 by Charlie Waite and Bill Elfers; Morgan, Holland Ventures, the predecessor of Flagship Ventures, founded in 1982 by James Morgan; Fidelity Ventures, now Volition Capital, founded in 1969 by Henry Hoagland; and Charles River Ventures, founded in 1970 by Richard Burnes.[10] ARDC continued investing until 1971, when Doriot retired. In 1972 Doriot merged ARDC with Textron after having invested in over 150 companies.[11]  John Hay Whitney (1904–1982) and his partner Benno Schmidt (1913–1999) founded J.H. Whitney & Company in 1946. Whitney had been investing since the 1930s, founding Pioneer Pictures in 1933 and acquiring a 15% interest in Technicolor Corporation with his cousin Cornelius Vanderbilt Whitney. Florida Foods Corporation proved Whitney's most famous investment. The company developed an innovative method for delivering nutrition to American soldiers, later known as Minute Maid orange juice and was sold to The Coca-Cola Company in 1960. J.H. Whitney & Company continued to make investments in leveraged buyout transactions and raised $750 million for its sixth institutional private-equity fund in 2005.[citation needed]  One of the first steps toward a professionally managed venture capital industry was the passage of the Small Business Investment Act of 1958. The 1958 Act officially allowed the U.S. Small Business Administration (SBA) to license private \"Small Business Investment Companies\" (SBICs) to help the financing and management of the small entrepreneurial businesses in the United States.[12] The Small Business Investment Act of 1958 provided tax breaks that helped contribute to the rise of private-equity firms.[13]  During the 1950s, putting a venture capital deal together may have required the help of two or three other organizations to complete the transaction. It was a business that was growing very rapidly, and as the business grew, the transactions grew exponentially.[14] Arthur Rock, one of the pioneers of Silicon Valley during his venturing the Fairchild Semiconductor is often credited with the introduction of the term \"venture capitalist\" that has since become widely accepted.[15]  During the 1960s and 1970s, venture capital firms focused their investment activity primarily on starting and expanding companies. More often than not, these companies were exploiting breakthroughs in electronic, medical, or data-processing technology. As a result, venture capital came to be almost synonymous with financing of technology ventures. An early West Coast venture capital company was Draper and Johnson Investment Company, formed in 1962[16] by William Henry Draper III and Franklin P. Johnson, Jr. In 1965, Sutter Hill Ventures acquired the portfolio of Draper and Johnson as a founding action.[17] Bill Draper and Paul Wythes were the founders, and Pitch Johnson formed Asset Management Company at that time.  It was also in the 1960s that the common form of private-equity fund, still in use today, emerged. Private-equity firms organized limited partnerships to hold investments in which the investment professionals served as general partner and the investors, who were passive limited partners, put up the capital. The compensation structure, still in use today, also emerged with limited partners paying an annual management fee of 1.0–2.5% and a carried interest typically representing up to 20% of the profits of the partnership.  The growth of the venture capital industry was fueled by the emergence of the independent investment firms on Sand Hill Road, beginning with Kleiner Perkins and Sequoia Capital in 1972. Located in Menlo Park, California, Kleiner Perkins, Sequoia and later venture capital firms would have access to the many semiconductor companies based in the Santa Clara Valley as well as early computer firms using their devices and programming and service companies.[note 1] Kleiner Perkins was the first venture capital firm to open an office on Sand Hill Road in 1972.[18]  Throughout the 1970s, a group of private-equity firms, focused primarily on venture capital investments, would be founded that would become the model for later leveraged buyout and venture capital investment firms. In 1973, with the number of new venture capital firms increasing, leading venture capitalists formed the National Venture Capital Association (NVCA). The NVCA was to serve as the industry trade group for the venture capital industry.[19] Venture capital firms suffered a temporary downturn in 1974, when the stock market crashed and investors were naturally wary of this new kind of investment fund.  It was not until 1978 that venture capital experienced its first major fundraising year, as the industry raised approximately $750 million. With the passage of the Employee Retirement Income Security Act (ERISA) in 1974, corporate pension funds were prohibited from holding certain risky investments including many investments in privately held companies. In 1978, the US Labor Department relaxed certain restrictions of the ERISA, under the \"prudent man rule\"[note 2], thus allowing corporate pension funds to invest in the asset class and providing a major source of capital available to venture capitalists.  The public successes of the venture capital industry in the 1970s and early 1980s (e.g., Digital Equipment Corporation, Apple Inc., Genentech) gave rise to a major proliferation of venture capital investment firms. From just a few dozen firms at the start of the decade, there were over 650 firms by the end of the 1980s, each searching for the next major \"home run\". The number of firms multiplied, and the capital managed by these firms increased from $3 billion to $31 billion over the course of the decade.[20]  The growth of the industry was hampered by sharply declining returns, and certain venture firms began posting losses for the first time. In addition to the increased competition among firms, several other factors affected returns. The market for initial public offerings cooled in the mid-1980s before collapsing after the stock market crash in 1987, and foreign corporations, particularly from Japan and Korea, flooded early-stage companies with capital.[20]  In response to the changing conditions, corporations that had sponsored in-house venture investment arms, including General Electric and Paine Webber either sold off or closed these venture capital units. Additionally, venture capital units within Chemical Bank and Continental Illinois National Bank, among others, began shifting their focus from funding early stage companies toward investments in more mature companies. Even industry founders J.H. Whitney & Company and Warburg Pincus began to transition toward leveraged buyouts and growth capital investments.[20][21][22]  By the end of the 1980s, venture capital returns were relatively low, particularly in comparison with their emerging leveraged buyout cousins, due in part to the competition for hot startups, excess supply of IPOs and the inexperience of many venture capital fund managers. Growth in the venture capital industry remained limited throughout the 1980s and the first half of the 1990s, increasing from $3 billion in 1983 to just over $4 billion more than a decade later in 1994.[23]  The advent of the World Wide Web in the early 1990s reinvigorated venture capital as investors saw companies with huge potential being formed. Netscape and Amazon (company) were founded in 1994, and Yahoo! in 1995. All were funded by venture capital. Internet IPOs—AOL in 1992; Netcom in 1994; UUNet, Spyglass and Netscape in 1995; Lycos, Excite, Yahoo!, CompuServe, Infoseek, C\/NET, and E*Trade in 1996; and Amazon, ONSALE, Go2Net, N2K, NextLink, and SportsLine in 1997—generated enormous returns for their venture capital investors. These returns, and the performance of the companies post-IPO, caused a rush of money into venture capital, increasing the number of venture capital funds raised from about 40 in 1991 to more than 400 in 2000, and the amount of money committed to the sector from $1.5 billion in 1991 to more than $90 billion in 2000.[24]  The bursting of the dot-com bubble in 2000 caused many venture capital firms to fail and financial results in the sector to decline.[citation needed]  The Nasdaq crash and technology slump that started in March 2000 shook virtually the entire venture capital industry as valuations for startup technology companies collapsed. Over the next two years, many venture firms had been forced to write-off large proportions of their investments, and many funds were significantly \"under water\" (the values of the fund's investments were below the amount of capital invested). Venture capital investors sought to reduce the size of commitments they had made to venture capital funds, and, in numerous instances, investors sought to unload existing commitments for cents on the dollar in the secondary market. By mid-2003, the venture capital industry had shriveled to about half its 2001 capacity. Nevertheless, PricewaterhouseCoopers' MoneyTree Survey[25] shows that total venture capital investments held steady at 2003 levels through the second quarter of 2005.[citation needed]  Although the post-boom years represent just a small fraction of the peak levels of venture investment reached in 2000, they still represent an increase over the levels of investment from 1980 through 1995. As a percentage of GDP, venture investment was 0.058% in 1994, peaked at 1.087% (nearly 19 times the 1994 level) in 2000 and ranged from 0.164% to 0.182% in 2003 and 2004. The revival of an Internet-driven environment in 2004 through 2007 helped to revive the venture capital environment. However, as a percentage of the overall private-equity market, venture capital has still not reached its mid-1990s level, let alone its peak in 2000.[citation needed]  Venture capital funds, which were responsible for much of the fundraising volume in 2000 (the height of the dot-com bubble), raised only $25.1 billion in 2006, a 2% decline from 2005 and a significant decline from its peak.[26] The decline continued till their fortunes started to turn around in 2010 with $21.8 billion invested (not raised).[27] The industry continued to show phenomenal growth and in 2020 hit $80 billion in fresh capital.[28]  Obtaining venture capital is substantially different from raising debt or a loan. Lenders have a legal right to interest on a loan and repayment of the capital irrespective of the success or failure of a business. Venture capital is invested in exchange for an equity stake in the business. The return of the venture capitalist as a shareholder depends on the growth and profitability of the business. This return is generally earned when the venture capitalist \"exits\" by selling its shareholdings when the business is sold to another owner.[29]  Venture capitalists are typically very selective in deciding what to invest in, with a Stanford survey of venture capitalists revealing that 100 companies were considered for every company receiving financing.[30] Ventures receiving financing must demonstrate an excellent management team, a large potential market, and most importantly high growth potential, as only such opportunities are likely capable of providing financial returns and a successful exit within the required time frame (typically 8–12 years) that venture capitalists expect.[31]  Because investments are illiquid and require the extended time frame to harvest, venture capitalists are expected to carry out detailed due diligence prior to investment. Venture capitalists also are expected to nurture the companies in which they invest, in order to increase the likelihood of reaching an IPO stage when valuations are favourable. Venture capitalists typically assist at four stages in the company's development:[32]  Because there are no public exchanges listing their securities, private companies meet venture capital firms and other private-equity investors in several ways, including warm referrals from the investors' trusted sources and other business contacts; investor conferences and symposia; and summits where companies pitch directly to investor groups in face-to-face meetings, including a variant known as \"Speed Venturing\", which is akin to speed-dating for capital, where the investor decides within 10 minutes whether he wants a follow-up meeting. In addition, some new private online networks are emerging to provide additional opportunities for meeting investors.[33]  This need for high returns makes venture funding an expensive capital source for companies, and most suitable for businesses having large up-front capital requirements, which cannot be financed by cheaper alternatives such as debt. That is most commonly the case for intangible assets such as software, and other intellectual property, whose value is unproven. In turn, this explains why venture capital is most prevalent in the fast-growing technology and life sciences or biotechnology fields.[34]  If a company does have the qualities venture capitalists seek including a solid business plan, a good management team, investment and passion from the founders, a good potential to exit the investment before the end of their funding cycle, and target minimum returns in excess of 40% per year, it will find it easier to raise venture capital.[citation needed]  There are multiple stages of venture financing offered in venture capital, that roughly correspond to these stages of a company's development.[35]  In early stage and growth stage financings, venture-backed companies may also seek to take venture debt.[39]  A venture capitalist, or sometimes simply called a capitalist, is a person who makes capital investments in companies in exchange for an equity stake. The venture capitalist is often expected to bring managerial and technical expertise, as well as capital, to their investments. A venture capital fund refers to a pooled investment vehicle (in the United States, often an LP or LLC) that primarily invests the financial capital of third-party investors in enterprises that are too risky for the standard capital markets or bank loans. These funds are typically managed by a venture capital firm, which often employs individuals with technology backgrounds (scientists, researchers), business training and\/or deep industry experience.[40]  A core skill within VCs is the ability to identify novel or disruptive technologies that have the potential to generate high commercial returns at an early stage. By definition, VCs also take a role in managing entrepreneurial companies at an early stage, thus adding skills as well as capital, thereby differentiating VC from buy-out private equity, which typically invest in companies with proven revenue, and thereby potentially realizing much higher rates of returns. Inherent in realizing abnormally high rates of returns is the risk of losing all of one's investment in a given startup company. As a consequence, most venture capital investments are done in a pool format, where several investors combine their investments into one large fund that invests in many different startup companies. By investing in the pool format, the investors are spreading out their risk to many different investments instead of taking the chance of putting all of their money in one start up firm.  Venture capital firms are typically structured as partnerships, the general partners of which serve as the managers of the firm and will serve as investment advisors to the venture capital funds raised. Venture capital firms in the United States may also be structured as limited liability companies, in which case the firm's managers are known as managing members. Investors in venture capital funds are known as limited partners. This constituency comprises both high-net-worth individuals and institutions with large amounts of available capital, such as state and private pension funds, university financial endowments, foundations, insurance companies, and pooled investment vehicles, called funds of funds.[41]  Venture capitalist firms differ in their motivations[42] and approaches. There are multiple factors, and each firm is different.  Venture capital funds are generally three in types:[43]  Some of the factors that influence VC decisions include:  Within the venture capital industry, the general partners and other investment professionals of the venture capital firm are often referred to as \"venture capitalists\" or \"VCs\". Typical career backgrounds vary, but, broadly speaking, venture capitalists come from either an operational or a finance background. Venture capitalists with an operational background (operating partner) tend to be former founders or executives of companies similar to those which the partnership finances or will have served as management consultants. Venture capitalists with finance backgrounds tend to have investment banking or other corporate finance experience.  Although the titles are not entirely uniform from firm to firm, other positions at venture capital firms include:  The average maturity of most venture capital funds ranges from 10 years to 12 years, with the possibility of a few years of extensions to allow for private companies still seeking liquidity. The investing cycle for most funds is generally three to five years, after which the focus is managing and making follow-on investments in an existing portfolio.[45] This model was pioneered by successful funds in Silicon Valley through the 1980s to invest in technological trends broadly but only during their period of ascendance, and to cut exposure to management and marketing risks of any individual firm or its product.  In such a fund, the investors have a fixed commitment to the fund that is initially unfunded and subsequently \"called down\" by the venture capital fund over time as the fund makes its investments. There are substantial penalties for a limited partner (or investor) that fails to participate in a capital call.[46]  It can take anywhere from a month to several years for venture capitalists to raise money from limited partners for their fund. At the time when all of the money has been raised, the fund is said to be closed and the 10-year lifetime begins. Some funds have partial closes when one half (or some other amount) of the fund has been raised. The vintage year generally refers to the year in which the fund was closed and may serve as a means to stratify VC funds for comparison.  From an investor's point of view, funds can be: (1) traditional—where all the investors invest with equal terms; or (2) asymmetric—where different investors have different terms. Typically asymmetry is seen in cases where investors have opposing interests, such as the need to not have unrelated business taxable income in the case of public tax-exempt investors.[47]  The decision process to fund a company is elusive. One study report in the Harvard Business Review[48] states that VCs rarely use standard financial analytics.[48] First, VCs engage in a process known as \"generating deal flow,\" where they reach out to their network to source potential investments.[48] The study also reported that few VCs use any type of financial analytics when they assess deals; VCs are primarily concerned about the cash returned from the deal as a multiple of the cash invested.[48] According to 95% of the VC firms surveyed, VCs cite the founder or founding team as the most important factor in their investment decision.[48] Other factors are also considered, including intellectual property rights and the state of the economy.[49] Some argue that the most important thing a VC looks for in a company is high-growth.[50]  The funding decision process has spawned bias in the form of a large disparity between the funding received by men and minority groups, such as women and people of color.[51][52][53] In 2021, female founders only received 2% of VC funding in the United States.[54][52] Some research studies have found that VCs evaluate women differently and are less likely to fund female founders.[51]  Venture capitalists are compensated through a combination of management fees and carried interest (often referred to as a \"two and 20\" arrangement):  Because a fund may run out of capital prior to the end of its life, larger venture capital firms usually have several overlapping funds at the same time; doing so lets the larger firm keep specialists in all stages of the development of firms almost constantly engaged. Smaller firms tend to thrive or fail with their initial industry contacts; by the time the fund cashes out, an entirely new generation of technologies and people is ascending, whom the general partners may not know well, and so it is prudent to reassess and shift industries or personnel rather than attempt to simply invest more in the industry or people the partners already know.[citation needed]  Because of the strict requirements venture capitalists have for potential investments, many entrepreneurs seek seed funding from angel investors, who may be more willing to invest in highly speculative opportunities, or may have a prior relationship with the entrepreneur. Additionally, entrepreneurs may seek alternative financing, such as revenue-based financing, to avoid giving up equity ownership in the business. For entrepreneurs seeking more than just funding, startup studios can be an appealing alternative to venture capitalists, as they provide operational support and an experienced team.[59]  Furthermore, many venture capital firms will only seriously evaluate an investment in a start-up company otherwise unknown to them if the company can prove at least some of its claims about the technology and\/or market potential for its product or services. To achieve this, or even just to avoid the dilutive effects of receiving funding before such claims are proven, many start-ups seek to self-finance sweat equity until they reach a point where they can credibly approach outside capital providers such as venture capitalists or angel investors. This practice is called \"bootstrapping\".  Equity crowdfunding is emerging as an alternative to traditional venture capital. Traditional crowdfunding is an approach to raising the capital required for a new project or enterprise by appealing to large numbers of ordinary people for small donations. While such an approach has long precedents in the sphere of charity, it is receiving renewed attention from entrepreneurs, now that social media and online communities make it possible to reach out to a group of potentially interested supporters at very low cost. Some equity crowdfunding models are also being applied specifically for startup funding, such as those listed at Comparison of crowd funding services. One of the reasons to look for alternatives to venture capital is the problem of the traditional VC model. The traditional VCs are shifting their focus to later-stage investments, and return on investment of many VC funds have been low or negative.[33][60]  In Europe and India, Media for equity is a partial alternative to venture capital funding. Media for equity investors are able to supply start-ups with often significant advertising campaigns in return for equity. In Europe, an investment advisory firm offers young ventures the option to exchange equity for services investment; their aim is to guide ventures through the development stage to arrive at a significant funding, mergers and acquisition, or other exit strategy.[61]  In industries where assets can be securitized effectively because they reliably generate future revenue streams or have a good potential for resale in case of foreclosure, businesses may more cheaply be able to raise debt to finance their growth. Good examples would include asset-intensive extractive industries such as mining, or manufacturing industries. Offshore funding is provided via specialist venture capital trusts, which seek to use securitization in structuring hybrid multi-market transactions via an SPV (special purpose vehicle): a corporate entity that is designed solely for the purpose of the financing.  In addition to traditional venture capital and angel networks, groups have emerged, which allow groups of small investors or entrepreneurs themselves to compete in a privatized business plan competition where the group itself serves as the investor through a democratic process.[62]  Law firms are also increasingly acting as an intermediary between clients seeking venture capital and the firms providing it.[63]  Other forms include venture resources that seek to provide non-monetary support to launch a new venture.  Every year, there are nearly 2 million businesses created in the US, but only 600–800 get venture capital funding.[64] According to the National Venture Capital Association, 11% of private sector jobs come from venture-backed companies and venture-backed revenue accounts for 21% of US GDP.[65]  In 2020, female-founded companies raised 2.8% of capital investment from venture capital, the highest amount recorded.[66][67] Babson College's Diana Report found that the number of women partners in VC firms decreased from 10% in 1999 to 6% in 2014. The report also found that 97% of VC-funded businesses had male chief executives, and that businesses with all-male teams were more than four times as likely to receive VC funding compared to teams with at least one woman.[68]  Currently, about 3% of all venture capital is going to woman-led companies. More than 75% of VC firms in the US did not have any female venture capitalists at the time they were surveyed.[69] It was found that a greater fraction of VC firms had never had a woman represent them on the board of one of their portfolio companies. For comparison, a UC Davis study focusing on large public companies in California found 49.5% with at least one female board seat.[70]  Venture capital, as an industry, originated in the United States, and American firms have traditionally been the largest participants in venture deals with the bulk of venture capital being deployed in American companies. However, increasingly, non-US venture investment is growing, and the number and size of non-US venture capitalists have been expanding.[citation needed]  Venture capital has been used as a tool for economic development in a variety of developing regions. In many of these regions, with less developed financial sectors, venture capital plays a role in facilitating access to finance for small and medium enterprises (SMEs), which in most cases would not qualify for receiving bank loans.[citation needed]  In the year of 2008, while VC funding were still majorly dominated by U.S. money ($28.8 billion invested in over 2550 deals in 2008), compared to international fund investments ($13.4 billion invested elsewhere), there has been an average 5% growth in the venture capital deals outside the US, mainly in China and Europe.[71] Geographical differences can be significant. For instance, in the UK, 4% of British investment goes to venture capital, compared to about 33% in the U.S.[72]  VC funding has been shown to be positively related to a country's individualistic culture.[73] According to economist Jeffrey Funk however more than 90% of US startups valued over $1 billion lost money between 2019–2020 and return on investment from VC barely exceed return from public stock markets over the last 25 years.[74]  In the United States, venture capital investing reached $209.4 billion in 2022, the second-highest investment year in history.[75]  Venture capitalists invested some $29.1 billion in 3,752 deals in the U.S. through the fourth quarter of 2011, according to a report by the National Venture Capital Association. The same numbers for all of 2010 were $23.4 billion in 3,496 deals.[76]  According to a report by Dow Jones VentureSource, venture capital funding fell to $6.4 billion in the US in the first quarter of 2013, an 11.8% drop from the first quarter of 2012, and a 20.8% decline from 2011. Venture firms have added $4.2 billion into their funds this year, down from $6.3 billion in the first quarter of 2013, but up from $2.6 billion in the fourth quarter of 2012.[77]  Canadian technology companies have attracted interest from the global venture capital community partially as a result of generous tax incentive through the Scientific Research and Experimental Development (SR&ED) investment tax credit program.[citation needed] The basic incentive available to any Canadian corporation performing R&D is a refundable tax credit that is equal to 20% of \"qualifying\" R&D expenditures (labour, material, R&D contracts, and R&D equipment). An enhanced 35% refundable tax credit of available to certain (i.e. small) Canadian-controlled private corporations (CCPCs). Because the CCPC rules require a minimum of 50% Canadian ownership in the company performing R&D, foreign investors who would like to benefit from the larger 35% tax credit must accept minority position in the company, which might not be desirable. The SR&ED program does not restrict the export of any technology or intellectual property that may have been developed with the benefit of SR&ED tax incentives.[citation needed]  Canada also has a fairly unusual form of venture capital generation in its labour-sponsored venture capital corporations (LSVCC). These funds, also known as Retail Venture Capital or Labour Sponsored Investment Funds (LSIF), are generally sponsored by labor unions and offer tax breaks from government to encourage retail investors to purchase the funds. Generally, these Retail Venture Capital funds only invest in companies where the majority of employees are in Canada. However, innovative structures have been developed to permit LSVCCs to direct in Canadian subsidiaries of corporations incorporated in jurisdictions outside of Canada.[citation needed] In 2022, the Information and Communications Technology (ICT) sector closed around 50% of Canada's venture capital deals, 16% were in the Life Sciences.[78]  The Venture Capital industry in Mexico is a fast-growing sector in the country that, with the support of institutions and private funds, is estimated to reach US$100 billion invested by 2018.[79][needs update]  In Australia and New Zealand, there have been 3 waves of VC, starting with Bill Ferris who founded IVC in 1970. are more than one hundred active VC funds, syndicates, or angel investors making VC-style investments. The 2nd wave was led by Starfish & Southern Cross VC, with the latter producing the leading VC of the 3rd wave, Blackbird.[80] There was a boom in 2018, and today there are more than one hundred active VC funds, syndicates, or angel investors making VC-style investments. There have been few Nasdaq IPOs of Australian VC backed startups, with only Looksmart[81] from Bill Ferris's fund, and Quantenna[82] from Larry Marshall's Southern Cross VC, but Blackbird is expected to IPO Canva soon.  The State of Startup Funding report found that in 2021, over AUD $10 billion AUD was invested into Australian and New Zealand startups across 682 deals. This represents a 3x increase from the $3.1 billion that was invested in 2020.[83]  Some notable Australian and New Zealand startup success stories include graphic design company Canva,[84] financial services provider Airwallex, New Zealand payments provider Vend (acquired by Lightspeed), rent-to-buy company OwnHome,[85] and direct-to-consumer propositions such as Eucalyptus (a house of direct-to-consumer telehealth brands), and Lyka (a pet wellness company).[86]  In 2022, the largest Australian funds are Blackbird Ventures, Square Peg Capital, and Airtree Ventures. These three funds have more than $1 billion AUD under management across multiple funds. These funds have funding from institutional capital, including AustralianSuper and Hostplus, family offices, and sophisticated individual high-net-wealth investors.[87]  Outside of the 'Big 3', other notable institutional funds include AfterWork Ventures,[88] Artesian, Folklore Ventures, Equity Venture Partners, Our Innovation Fund, Investible, Main Sequence Ventures (the VC arm of the CSIRO), OneVentures, Proto Axiom, and Tenacious Ventures.  As the number of capital providers in the Australian and New Zealand ecosystem has grown, funds have started to specialise and innovate to differentiate themselves. For example, Tenacious Ventures is a $35 million specialised agritech fund,[89] while AfterWork Ventures is a 'community-powered fund' that has coalesced a group of 120 experienced operators from across Australia's startups and tech companies. Its community is invested in its fund, and lean into assist with sourcing and evaluating deal opportunities, as well as supporting companies post-investment.[90]  Several Australian corporates have corporate VC arms, including NAB Ventures, Reinventure (associated with Westpac), IAG Firemark Ventures, and Telstra Ventures.  Leading early-stage venture capital investors in Europe include Mark Tluszcz of Mangrove Capital Partners and Danny Rimer of Index Ventures, both of whom were named on Forbes Magazine's Midas List of the world's top dealmakers in technology venture capital in 2007.[91] In 2020, the first Italian Venture capital Fund named Primo Space was launched by Primomiglio SGR. This fund first closed €58 million out a target €80 million and is focused on Space investing.[92]  Comparing the EU market to the United States, in 2020 venture capital funding was seven times lower, the EU having less unicorns. This hampers the EU's transformation into a green and digital economy.[93][94][95]  As of 2024, tighter financial conditions have harmed venture capital funding in the European Union, which remains undeveloped in comparison to the United States.[96]  The EU lags significantly behind the US and China in venture capital investments, with the EU capturing only 5% of global venture capital compared to 52% in the US and 40% in China.[97] Venture capital funds in the EU account for just 5% of the global total, whereas those in the United States and China secure 52% and 40%, respectively.[97][98] The financing gap for EU scale-ups is significant, with companies raising 50% less capital than those in Silicon Valley. This disparity exists across industries and is unaffected by the business cycle or year of establishment.[97][99]  The European Green Deal has fostered policies that contributed to a 30% rise in venture capital specifically for greentech companies in the EU from 2021 to 2023, despite a downturn in other sectors during the same period.[97][100]  Recent years have seen a revival of the Nordic venture scene with more than €3 billion raised by VC funds in the Nordic region over the last five years. Over the past five years, a total of €2.7 billion has been invested into Nordic startups. Known Nordic early-stage venture capital funds include NorthZone (Sweden), Maki.vc (Finland) and ByFounders (Copenhagen).[101]  Many Swiss start-ups are university spin-offs, in particular from its federal institutes of technology in Lausanne and Zurich.[102] According to a study by the London School of Economics analysing 130 ETH Zurich spin-offs over 10 years, about 90% of these start-ups survived the first five critical years, resulting in an average annual IRR of more than 43%.[103] Switzerland's most active early-stage investors are The Zurich Cantonal Bank, investiere.ch, Swiss Founders Fund, as well as a number of angel investor clubs.[104] In 2022, half of the total amount of CHF 4 billion investments went to the ICT and Fintech sectors, whereas 21% was invested in Cleantech.[105]  As of March 2019, there are 130 active VC firms in Poland which have invested locally in over 750 companies, an average of 9 companies per portfolio. Since 2016, new legal institutions have been established for entities implementing investments in enterprises in the seed or startup phase. In 2018, venture capital funds invested €178M in Polish startups (0.033% of GDP). As of March 2019, total assets managed by VC companies operating in Poland are estimated at €2.6B. The total value of investments of the Polish VC market is worth €209.2M.[106]  The Bulgarian venture capital industry has been growing rapidly in the past decade. As of the beginning of 2021, there are 18 VC and growth equity firms on the local market, with the total funding available for technology startups exceeding €200M. According to BVCA – Bulgarian Private Equity and Venture Capital Association, 59 transactions of total value of €29.4 million took place in 2020.[107] Most of the venture capital investments in Bulgaria are concentrated in the seed and Series A stages. Sofia-based LAUNCHub Ventures recently launched one of the biggest funds in the region, with a target size of €70 million.[108]  South Korea has been undergoing an investment boom over the last ten years, peaking at US$10 billion in 2021. The Korean government and mega-corporations such as Kakao, Smilegate, SK, and Lotte has been behind much of the funding, backing both venture firms and accelerators, but new venture capitalists are in dire straits as they announce a 40% cut in financing in 2024.[109]  India is catching up with the West in the field of venture capital and a number of venture capital funds have a presence in the country (IVCA). In 2006, the total amount of private equity and venture capital in India reached $7.5 billion across 299 deals.[110] In the Indian market, venture capital consists of investing in equity, quasi-equity, or conditional loans in order to promote unlisted, high-risk, or high-tech firms driven by technically or professionally qualified entrepreneurs. It is also used to refer to investors \"providing seed\", \"start-up and first-stage financing\",[111] or financing companies that have demonstrated extraordinary business potential. Venture capital refers to capital investment; equity and debt; both of which carry indubitable risk. The anticipated risk is very high. The venture capital industry follows the concept of \"high risk, high return\", innovative entrepreneurship, knowledge-based ideas and human capital intensive enterprises have become common as venture capitalists invest in risky finance to encourage innovation.[112] A large portion of funding from startups in India arise from Foreign Venture Capital Funds such as Sequoia, Accel, Tiger Global, SoftBank, etc.[113]  China is also starting to develop a venture capital industry (CVCA).  Vietnam is experiencing its first foreign venture capitals, including IDG Venture Vietnam ($100 million) and DFJ Vinacapital ($35 million).[114]  Singapore is widely recognized and featured as one of the hottest places to both start up and invest, mainly due to its healthy ecosystem, its strategic location and connectedness to foreign markets.[115] With 100 deals valued at US$3.5 billion, Singapore saw a record value of PE and VC investments in 2016. The number of PE and VC investments increased substantially over the last 5 years: In 2015, Singapore recorded 81 investments with an aggregate value of US$2.2 billion while in 2014 and 2013, PE and VC deal values came to US$2.4 billion and US$0.9 billion respectively. With 53 percent, tech investments account for the majority of deal volume.  Moreover, Singapore is home to two of South-East Asia's largest unicorns. Garena is reportedly the highest-valued unicorn in the region with a US$3.5 billion price tag, while Grab is the highest-funded, having raised a total of US$1.43 billion since its incorporation in 2012.[116]  Start-ups and small businesses in Singapore receive support from policymakers and the local government fosters the role VCs play to support entrepreneurship in Singapore and the region. For instance, in 2016, Singapore's National Research Foundation (NRF) has given out grants up to around $30 million to four large local enterprises for investments in startups in the city-state. This first of its kind partnership NRF has entered into is designed to encourage these enterprises to source for new technologies and innovative business models.[117]  Currently, the rules governing VC firms are being reviewed by the Monetary Authority of Singapore (MAS) to make it easier to set up funds and increase funding opportunities for start-ups. This mainly includes simplifying and shortening the authorization process for new venture capital managers and to study whether existing incentives that have attracted traditional asset managers here will be suitable for the VC sector. A public consultation on the proposals was held in January 2017 with changes expected to be introduced by July.[118]  In recent years, Singapore's focus in venture capital investments has geared more towards more early stage, deep tech startups,[119] with the government launching SGInnovate in 2016[120] to support the development of deep tech startups. Deep tech startups aim to address significant scientific problems. Singapore's tech startup scene has grown in recent years, and the city-state ranked seventh in the latest Global Innovation Index 2022. For the first nine months of 2022, investments up to Series B rounds amounted to $5.5 billion Singapore dollars ($4 billion), an increase of 14% by volume and 45% by value, according to data from government agency Enterprise Singapore.  The Middle East and North Africa (MENA) venture capital industry is an early stage of development but growing. According to H1 2019 MENA Venture Investment Report by MAGNiTT, 238 startup investment deals have taken place in the region in the first half of 2019, totaling in $471 million in investments. Compared to 2018's H1 report, this represents an increase of 66% in total funding and 28% in number of deals.  According to the report, the UAE is the most active ecosystem in the region with 26% of the deals made in H1, followed by Egypt at 21%, and Lebanon at 13%. In terms of deals by sector, fintech remains the most active industry with 17% of the deals made, followed by e-commerce at 12%, and delivery and transport at 8%.  The report also notes that a total of 130 institutions invested in MENA-based startups in H1 2019, 30% of which were headquartered outside the MENA, demonstrating international appetite for investments in the region. 15 startup exits have been recorded in H1 2019, with Careem's $3.1 billion acquisition by Uber being the first unicorn exit in the region.[121] Other notable exits include Souq.com exit to Amazon in 2017 for $650 million.[122]  In Israel, high-tech entrepreneurship and venture capital have flourished well beyond the country's relative size. As it has very little natural resources and, historically has been forced to build its economy on knowledge-based industries, its VC industry has rapidly developed, and nowadays has about 70 active venture capital funds, of which 14 international VCs with Israeli offices, and additional 220 international funds which actively invest in Israel. In addition, as of 2010, Israel led the world in venture capital invested per capita. Israel attracted $170 per person compared to $75 in the US.[123] About two thirds of the funds invested were from foreign sources, and the rest domestic. In 2013, Wix.com joined 62 other Israeli firms on the Nasdaq.[124]  The Southern African venture capital industry is developing. The South African Government and Revenue Service is following the international trend of using tax-efficient vehicles to propel economic growth and job creation through venture capital. Section 12 J of the Income Tax Act was updated to include venture capital. Companies are allowed to use a tax-efficient structure similar to VCTs in the UK. Despite the above structure, the government needs to adjust its regulation around intellectual property, exchange control and other legislation to ensure that Venture capital succeeds.[citation needed]  Currently, there are not many venture capital funds in operation and it is a small community; however, the number of venture funds are steadily increasing with new incentives slowly coming in from government. Funds are difficult to come by and due to the limited funding, companies are more likely to receive funding if they can demonstrate initial sales or traction and the potential for significant growth. The majority of the venture capital in Sub-Saharan Africa is centered on South Africa and Kenya.[citation needed]  Entrepreneurship is a key to growth. Governments will need to ensure business friendly regulatory environments in order to help foster innovation. In 2019, venture capital startup funding grew to 1.3 billion dollars, increasing rapidly. The causes are as of yet unclear, but education is certainly a factor.[125]  Unlike public companies, information regarding an entrepreneur's business is typically confidential and proprietary. As part of the due diligence process, most venture capitalists will require significant detail with respect to a company's business plan. Entrepreneurs must remain vigilant about sharing information with venture capitalists that are investors in their competitors. Most venture capitalists treat information confidentially, but as a matter of business practice, they do not typically enter into Non Disclosure Agreements because of the potential liability issues those agreements entail. Entrepreneurs are typically well advised to protect truly proprietary intellectual property.[citation needed] Startups commonly use a data room to securely share this information with potential investors during the due diligence process.  Limited partners of venture capital firms typically have access only to limited amounts of information with respect to the individual portfolio companies in which they are invested and are typically bound by confidentiality provisions in the fund's limited partnership agreement.[citation needed]  There are several strict guidelines regulating those that deal in venture capital. Namely, they are not allowed to advertise or solicit business in any form as per the U.S. Securities and Exchange Commission guidelines.[126] "},"meta":{},"created_at":"2025-03-22T14:25:42.273677Z","updated_at":"2025-03-22T14:25:42.273677Z","inner_id":3,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":12,"annotations":[{"id":12,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.309403Z","updated_at":"2025-03-22T14:25:42.309403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"ee87bbd7-3683-43fb-bd18-3ff0e7fb70a1","import_id":null,"last_action":null,"bulk_created":false,"task":12,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"In corporate finance, capital structure refers to the mix of various forms of external funds, known as capital, used to finance a business. It consists of shareholders' equity, debt (borrowed funds), and preferred stock, and is detailed in the company's balance sheet. The larger the debt component is in relation to the other sources of capital, the greater financial leverage (or gearing, in the United Kingdom) the firm is said to have. Too much debt can increase the risk of the company and reduce its financial flexibility, which at some point creates concern among investors and results in a greater cost of capital.  Company management is responsible for establishing a capital structure for the corporation that makes optimal use of financial leverage and holds the cost of capital as low as possible.[1][2]  Capital structure is an important issue in setting rates charged to customers by regulated utilities in the United States. The utility company has the right to choose any capital structure it deems appropriate, but regulators determine an appropriate capital structure and cost of capital for ratemaking purposes.[3]  Various leverage or gearing ratios are closely watched by financial analysts to assess the amount of debt in a company's capital structure.[4][5]  The Miller and Modigliani theorem argues that the market value of a firm is unaffected by a change in its capital structure. This school of thought is generally viewed as a purely theoretical result, since it assumes a perfect market and disregards factors such as fluctuations and uncertain situations that may arise in financing a firm. In academia, much attention has been given to debating and relaxing the assumptions made by Miller and Modigliani to explain why a firm's capital structure is relevant to its value in the real world.[6]  Up to a certain point, the use of debt (such as bonds or bank loans) in a company's capital structure is beneficial. When debt is a portion of a firm's capital structure, it permits the company to achieve greater earnings per share than would be possible by issuing equity. This is because the interest paid by the firm on the debt is tax-deductible. The reduction in taxes permits more of the company's operating income to flow through to investors. The related increase in earnings per share is called financial leverage or gearing in the United Kingdom and Australia. Financial leverage can be beneficial when the business is expanding and profitable, but it is detrimental when the business enters a contraction phase. The interest on the debt must be paid regardless of the level of the company's operating income, or bankruptcy may be the result. If the firm does not prosper and profits do not meet management's expectations, too much debt (i.e., too much leverage) increases the risk that the firm may not be able to pay its creditors. At some point this makes investors apprehensive and increases the firm's cost of borrowing or issuing new equity.[7][8]  It is important that a company's management recognizes the risk inherent in taking on debt, and maintains an optimal capital structure with an appropriate balance between debt and equity.[9] An optimal capital structure is one that is consistent with minimizing the cost of debt and equity financing and maximizing the value of the firm. Internal policy decisions with respect to capital structure and debt ratios must be tempered by a recognition of how outsiders view the strength of the firm's financial position.[10] Key considerations include maintaining the firm's credit rating at a level where it can attract new external funds on reasonable terms, and maintaining a stable dividend policy and good earnings record.[11]  Once management has decided how much debt should be used in the capital structure, decisions must be made as to the appropriate mix of short-term debt and long-term debt. Increasing the percentage of short-term debt can enhance a firm's financial flexibility, since the borrower's commitment to pay interest is for a shorter period of time. But short-term debt also exposes the firm to greater refinancing risk. Therefore, as the percentage of short-term debt in a firm's capital structure increases, equity holders will expect greater returns on equity to compensate for the increased risk, according to a 2022 article in The Journal of Finance.[12]  In the event of bankruptcy, the seniority of the capital structure comes into play. A typical company has the following seniority structure listed from most senior to least:  In practice, the capital structure may be complex and include other sources of capital.  Financial analysts use some form of leverage ratio to quantify the proportion of debt and equity in a company's capital structure, and to make comparisons between companies. Using figures from the balance sheet, the debt-to-capital ratio can be calculated as shown below.[17]  The debt-to-equity ratio and capital gearing ratio are widely used for the same purpose.  Capital bearing risk includes debentures (risk is to pay interest) and preference capital (risk to pay dividend at fixed rate).[18] Capital not bearing risk includes equity.[19]  Therefore, one can also say, Capital gearing ratio = (Debentures + Preference share capital) : (shareholders' funds)[20]  Capital structure is an important issue in setting rates charged to customers by regulated utilities in the United States. Ratemaking practice in the U.S. holds that rates paid by a utility's customers should be set at a level which assures that the company can provide reliable service at reasonable cost. The cost of capital is among the costs a utility must be allowed to recover from customers, and depends on the company's capital structure. The utility company may choose whatever capital structure it deems appropriate, but regulators determine an appropriate capital structure and cost of capital for ratemaking purposes.[21]  The Modigliani–Miller theorem, proposed by Franco Modigliani and Merton Miller in 1958, forms the basis for modern academic thinking on capital structure. It is generally viewed as a purely theoretical result since it disregards many important factors in the capital structure process factors like fluctuations and uncertain situations that may occur in the course of financing a firm. The theorem states that, in a perfect market, how a firm is financed is irrelevant to its value. This result provides the base with which to examine real world reasons why capital structure is relevant, that is, a company's value is affected by the capital structure it employs. Some other reasons include bankruptcy costs, agency costs, taxes, and information asymmetry. This analysis can then be extended to look at whether there is in fact an optimal capital structure: the one which maximizes the value of the firm.[citation needed]  Consider a perfect capital market (no transaction or bankruptcy costs; perfect information); firms and individuals can borrow at the same interest rate; no taxes; and investment returns are not affected by financial uncertainty. Assuming perfections in the capital is a mirage and unattainable as suggested by Modigliani and Miller.  Modigliani and Miller made two findings under these conditions. Their first 'proposition' was that the value of a company is independent of its capital structure. Their second 'proposition' stated that the cost of equity for a leveraged firm is equal to the cost of equity for an unleveraged firm, plus an added premium for financial risk. That is, as leverage increases, risk is shifted between different investor classes, while the total firm risk is constant, and hence no extra value created.[citation needed]  Their analysis was extended to include the effect of taxes and risky debt. Under a classical tax system, the tax-deductibility of interest makes debt financing valuable; that is, the cost of capital decreases as the proportion of debt in the capital structure increases. The optimal structure would be to have virtually no equity at all, i.e. a capital structure consisting of 99.99% debt.  If capital structure is irrelevant in a perfect market, then imperfections which exist in the real world must be the cause of its relevance.[22] The theories below try to address some of these imperfections, by relaxing assumptions made in the Modigliani–Miller theorem.[23]  Trade-off theory of capital structure allows bankruptcy cost to exist as an offset to the benefit of using debt as tax shield. It states that there is an advantage to financing with debt, namely, the tax benefits of debt and that there is a cost of financing with debt the bankruptcy costs and the financial distress costs of debt.[24] This theory also refers to the idea that a company chooses how much equity finance and how much debt finance to use by considering both costs and benefits.[25] The marginal benefit of further increases in debt declines as debt increases, while the marginal cost increases, so that a firm optimizing its overall value will focus on this trade-off when choosing how much debt and equity to use for financing.[26] Empirically, this theory may explain differences in debt-to-equity ratios between industries, but it doesn't explain differences within the same industry.[27]  Pecking order theory tries to capture the costs of asymmetric information.[28] It states that companies prioritize their sources of financing (from internal financing to equity) according to the law of least effort, or of least resistance, preferring to raise equity as a financing means \"of last resort\".[29] Hence, internal financing is used first; when that is depleted, debt is issued; and when it is no longer sensible to issue any more debt, equity is issued. This theory maintains that businesses adhere to a hierarchy of financing sources and prefer internal financing when available, and debt is preferred over equity if external financing is required (equity would mean issuing shares which meant 'bringing external ownership' into the company).[30] Thus, the form of debt a firm chooses can act as a signal of its need for external finance.[31]  The pecking order theory has been popularized by Myers (1984)[32] when he argued that equity is a less preferred means to raise capital, because when managers (who are assumed to know better about true condition of the firm than investors) issue new equity, investors believe that managers think the firm is overvalued, and managers are taking advantage of the assumed over-valuation. As a result, investors may place a lower value to the new equity issuance.  The capital structure substitution theory is based on the hypothesis that company management may manipulate capital structure such that earnings per share (EPS) are maximized.[33] The model is not normative i.e. and does not state that management should maximize EPS, it simply hypothesizes they do.  The 1982 SEC rule 10b-18 allowed public companies open-market repurchases of their own stock and made it easier to manipulate capital structure.[34] This hypothesis leads to a larger number of testable predictions. First, it has been deducted[by whom?] that market average earnings yield will be in equilibrium with the market average interest rate on corporate bonds after corporate taxes, which is a reformulation of the 'Fed model'. The second prediction has been that companies with a high valuation ratio, or low earnings yield, will have little or no debt, whereas companies with low valuation ratios will be more leveraged.[35] When companies have a dynamic debt-equity target, this explains why some companies use dividends and others do not. A fourth prediction has been that there is a negative relationship in the market between companies' relative price volatilities and their leverage. This contradicts Hamada who used the work of Modigliani and Miller to derive a positive relationship between these two variables.  Three types of agency costs can help explain the relevance of capital structure.  An active area of research in finance is[when?] that which tries to translate the models above as well as others into a structured theoretical setup that is time-consistent and that has a dynamic set up similar to one that can be observed in the real world. Managerial contracts, debt contracts, equity contracts, investment returns, all have long lived, multi-period implications. Therefore, it is hard to think through what the implications of the basic models above are for the real world if they are not embedded in a dynamic structure that approximates reality.  A similar type of research is performed under the guise of credit risk research in which the modeling of the likelihood of default and its pricing is undertaken under different assumptions about investors and about the incentives of management, shareholders and debt holders. Examples of research in this area are Goldstein, Ju, Leland (1998)[38] and Hennessy and Whited (2004).[39]  In addition to firm-specific characteristics, researchers find macroeconomic conditions have a material impact on capital structure choice. Korajczyk, Lucas, and McDonald (1990) provide evidence of equity issues cluster following a run-up in the equity market.[40] Korajczyk and Levy (2003) find that target leverage is counter-cyclical for unconstrained firms, but pro-cyclical for firms that are constrained; macroeconomic conditions are significant for issue choice for firms that can time their issue choice to coincide with periods of favorable macroeconomic conditions, while constrained firms cannot.[41] Levy and Hennessy (2007) highlight that trade-offs between agency problems and risk sharing vary over the business cycle and can result in the observed patterns.[42] Others have related these patterns with asset pricing puzzles.[43]  Corporate leverage ratios are initially determined. Low relative to high leverage ratios are largely persistent despite time variation. Variation in capital structures is primarily determined by factors that remain stable for long periods of time. These stable factors are unobservable.[44]  Firms rationally invest and seek financing in a manner compatible with their growth types. As economic and market conditions improve, low growth type firms are keener to issue new debt than equity, whereas high growth type firms are least likely to issue debt and keenest to issue equity. Distinct growth types are persistent. Consistent with a generalized Myers–Majluf framework, growth type compatibility enables distinct growth types and hence specifications of market imperfection or informational environments to persist, generating capital structure persistence.[45]  A capital structure arbitrageur seeks to profit from differential pricing of various instruments issued by one corporation. Consider, for example, traditional bonds, and convertible bonds. The latter are bonds that are, under contracted-for conditions, convertible into shares of equity. The stock-option component of a convertible bond has a calculable value in itself. The value of the whole instrument should be the value of the traditional bonds plus the extra value of the option feature. If the spread (the difference between the convertible and the non-convertible bonds) grows excessively, then the capital-structure arbitrageur will bet that it will converge.[citation needed] "},"meta":{},"created_at":"2025-03-22T14:25:42.273677Z","updated_at":"2025-03-22T14:25:42.273677Z","inner_id":4,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":13,"annotations":[{"id":13,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.309403Z","updated_at":"2025-03-22T14:25:42.309403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"911d9842-bfa4-4245-b47b-300c09f6a189","import_id":null,"last_action":null,"bulk_created":false,"task":13,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Bankruptcy is a legal process through which people or other entities who cannot repay debts to creditors may seek relief from some or all of their debts. In most jurisdictions, bankruptcy is imposed by a court order, often initiated by the debtor.  Bankrupt is not the only legal status that an insolvent person may have, meaning the term bankruptcy is not a synonym for insolvency.  The word bankruptcy is derived from Italian banca rotta, literally meaning 'broken bank'. The term is often described as having originated in Renaissance Italy, where there allegedly existed the tradition of smashing a banker's bench if he defaulted on payment. However, the existence of such a ritual is doubted.[1][2]  In Ancient Greece, bankruptcy did not exist. If a man owed and he could not pay, he and his wife, children or servants were forced into \"debt slavery\" until the creditor recouped losses through their physical labour. Many city-states in ancient Greece limited debt slavery to a period of five years; debt slaves had protection of life and limb, which regular slaves did not have. However, servants of the debtor could be retained beyond that deadline by the creditor and were often forced to serve their new lord for a lifetime, usually under significantly harsher conditions. An exception to this rule was Athens, which by the laws of Solon forbade enslavement for debt; as a consequence, most Athenian slaves were foreigners (Greek or otherwise).  The Statute of Bankrupts of 1542 was the first statute under English law dealing with bankruptcy or insolvency.[3] Bankruptcy is also documented in East Asia. According to al-Maqrizi, the Yassa of Genghis Khan contained a provision that mandated the death penalty for anyone who became bankrupt three times.  A failure of a nation to meet bond repayments has been seen on many occasions. In a similar way, Philip II of Spain had to declare four state bankruptcies in 1557, 1560, 1575 and 1596. According to Kenneth S. Rogoff, \"Although the development of international capital markets was quite limited prior to 1800, we nevertheless catalog the various defaults of France, Portugal, Prussia, Spain, and the early Italian city-states. At the edge of Europe, Egypt, Russia, and Turkey have histories of chronic default as well.\"[4]  The principal focus of modern insolvency legislation and business debt restructuring practices no longer rests on the elimination of insolvent entities, but on the remodeling of the financial and organizational structure of debtors experiencing financial distress so as to permit the rehabilitation and continuation of the business.  For private households, it is important to assess the underlying problems and to minimize the risk of financial distress to recur. It has been stressed that debt advice, a supervised rehabilitation period, financial education and social help to find sources of income and to improve the management of household expenditures must be equally provided during this period of rehabilitation (Refiner et al., 2003;[missing long citation] Gerhardt, 2009;[missing long citation] Frade, 2010[missing long citation]). In most EU member states, debt discharge is conditioned by a partial payment obligation and by a number of requirements concerning the debtor's behavior. In the United States (US), discharge is conditioned to a lesser extent. The spectrum is broad in the EU, with the UK coming closest to the US system (Reifner et al., 2003;[missing long citation] Gerhardt, 2009;[missing long citation] Frade, 2010[missing long citation]). The other member states do not provide the option of a debt discharge. Spain, for example, passed a bankruptcy law (ley concurs) in 2003 which provides for debt settlement plans that can result in a reduction of the debt (maximally half of the amount) or an extension of the payment period of maximally five years (Gerhardt, 2009[missing long citation]), but it does not foresee debt discharge.[5]  In the US, it is very difficult to discharge federal or federally guaranteed student loan debt by filing bankruptcy.[6] Unlike most other debts, those student loans may be discharged only if the person seeking discharge establishes specific grounds for discharge under the Brunner test,[7] under which the court evaluates three factors:  Even if a debtor proves all three elements, a court may permit only a partial discharge of the student loan. Student loan borrowers may benefit from restructuring their payments through a Chapter 13 bankruptcy repayment plan, but few qualify for discharge of part or all of their student loan debt.[8]  Bankruptcy fraud is a white-collar crime most typically involving concealment of assets by a debtor to avoid liquidation in bankruptcy proceedings. It may include filing of false information, multiple filings in different jurisdictions, bribery, and other acts.[9]  While difficult to generalize across jurisdictions, common criminal acts under bankruptcy statutes typically involve concealment of assets, concealment or destruction of documents, conflicts of interest, fraudulent claims, false statements or declarations, and fee fixing or redistribution arrangements. Falsifications on bankruptcy forms often constitute perjury. Multiple filings are not in and of themselves criminal, but they may violate provisions of bankruptcy law. In the U.S., bankruptcy fraud statutes are particularly focused on the mental state of particular actions.[10][11] Bankruptcy fraud is a federal crime in the United States.[12]  Bankruptcy fraud should be distinguished from strategic bankruptcy, which is not a criminal act since it creates a real (not a fake) bankruptcy state. However, it may still work against the filer.  All assets must be disclosed in bankruptcy schedules whether or not the debtor believes the asset has a net value. This is because once a bankruptcy petition is filed, it is for the creditors, not the debtor, to decide whether a particular asset has value. The future ramifications of omitting assets from schedules can be quite serious for the offending debtor. In the United States, a closed bankruptcy may be reopened by motion of a creditor or the U.S. trustee if a debtor attempts to later assert ownership of such an \"unscheduled asset\" after being discharged of all debt in the bankruptcy. The trustee may then seize the asset and liquidate it to benefit the (formerly discharged) creditors. Whether or not a concealment of such an asset should also be considered for prosecution as fraud or perjury would then be at the discretion of the judge or U.S. Trustee.  In some countries, such as the United Kingdom, bankruptcy is limited to individuals; other forms of insolvency proceedings (such as liquidation and administration) are applied to companies. In the United States, bankruptcy is applied more broadly to formal insolvency proceedings. In some countries, such as in Finland, bankruptcy is limited only to companies and individuals who are insolvent are condemned to de facto indentured servitude or minimum social benefits until their debts are paid in full, with accrued interest except when the court decides to show rare clemency by accepting a debtors application for debt restructuring, in which case an individual may have the amount of remaining debt reduced or be released from the debt.[13][14]  In Argentina, the national Act \"24.522 de Concursos y Quiebras\" regulates Bankruptcy and Reorganization of individuals and companies; public entities are not included.  A person may be declared bankrupt with an application submitted to the court by the creditor or with an application to recognize his own bankruptcy. Legal and natural persons, including individual entrepreneurs, who have an indisputable payment obligation exceeding 60 days and amounting to more than one million AMD can be declared bankrupt. All creditors, including the state and municipalities, to whom the person has an obligation that meets the above-mentioned minimum criteria can submit an application to declare a person bankrupt by compulsory procedure. Basically, these obligations are derived from the legal acts of the court, transactions, the obligation of the debtor to pay taxes, duties, and other fees defined by law.  At the same time, when being declared bankrupt with a voluntary bankruptcy application, the applicant bears the obligation to prove the fact that the value of his assets is less than his assets by one million AMD or more.[15]  In Australia, bankruptcy is a status which applies to individuals and is governed by the federal Bankruptcy Act 1966.[16] Companies do not go bankrupt but rather go into liquidation or administration, which is governed by the federal Corporations Act 2001.[17]  If a person commits an act of bankruptcy, then a creditor can apply to the Federal Circuit Court or the Federal Court for a sequestration order.[18] Acts of bankruptcy are defined in the legislation, and include the failure to comply with a bankruptcy notice.[19] A bankruptcy notice can be issued where, among other cases, a person fails to pay a judgment debt of at least $5,000.[20] A person can also seek to have themselves declared bankrupt for any amount of debt by lodging a debtor's petition with the \"Official Receiver\",[21] which is the Australian Financial Security Authority (AFSA).[22]  All bankrupts must lodge a Statement of Affairs document, also known as a Bankruptcy Form, with AFSA, which includes important information about their assets and liabilities. A bankruptcy cannot be discharged until this document has been lodged.  Ordinarily, a bankruptcy lasts three years from the filing of the Statement of Affairs with AFSA.[23]  A Bankruptcy Trustee (in most cases, the Official Trustee at AFSA) is appointed to deal with all matters regarding the administration of the bankrupt estate. The Trustee's job includes notifying creditors of the estate and dealing with creditor inquiries; ensuring that the bankrupt complies with their obligations under the Bankruptcy Act; investigating the bankrupt's financial affairs; realising funds to which the estate is entitled under the Bankruptcy Act and distributing dividends to creditors if sufficient funds become available.  For the duration of their bankruptcy, all bankrupts have certain restrictions placed upon them. For example, a bankrupt must obtain the permission of their trustee to travel overseas. Failure to do so may result in the bankrupt being stopped at the airport by the Australian Federal Police. Additionally, a bankrupt is required to provide their trustee with details of income and assets. If the bankrupt does not comply with the Trustee's request to provide details of income, the trustee may have grounds to lodge an Objection to Discharge, which has the effect of extending the bankruptcy for a further three or five years depending on the type of Objection.  The realisation of funds usually comes from two main sources: the bankrupt's assets and the bankrupt's wages. There are certain assets that are protected, referred to as protected assets. These include household furniture and appliances, tools of the trade and vehicles up to a certain value. All other assets of value can be sold. If a house, including the main residence, or car is above a certain value, a third party can buy the interest from the estate in order for the bankrupt to utilise the asset. If this is not done, the interest vests in the estate and the trustee is able to take possession of the asset and sell it.  The bankrupt must pay income contributions if their income is above a certain threshold. If the bankrupt fails to pay, the trustee can ask the Official Receiver to issue a notice to garnishee the bankrupt's wages. If that is not possible, the Trustee may seek to extend the bankruptcy for a further three or five years.  Bankruptcies can be annulled, and the bankrupt released from bankruptcy, prior to the expiration of the normal three-year period if all debts are paid out in full. Sometimes a bankrupt may be able to raise enough funds to make an Offer of Composition to creditors, which would have the effect of paying the creditors some of the money they are owed. If the creditors accept the offer, the bankruptcy can be annulled after the funds are received.  After the bankruptcy is annulled or the bankrupt has been automatically discharged, the bankrupt's credit report status is shown as \"discharged bankrupt\" for some years. The maximum number of years this information can be held is subject to the retention limits under the Privacy Act. How long such information is on a credit report may be shorter, depending on the issuing company, but the report must cease to record that information based on the criteria in the Privacy Act.  In Brazil, the Bankruptcy Law (11.101\/05) governs court-ordered or out-of-court receivership and bankruptcy and only applies to public companies (publicly traded companies) with the exception of financial institutions, credit cooperatives, consortia, supplementary scheme entities, companies administering health care plans, equity companies and a few other legal entities. It does not apply to state-run companies.  Current law covers three legal proceedings. The first one is bankruptcy itself (\"Falência\"). Bankruptcy is a court-ordered liquidation procedure for an insolvent business. The final goal of bankruptcy is to liquidate company assets and pay its creditors.  The second one is Court-ordered Restructuring (Recuperação Judicial). The goal is to overcome the business crisis situation of the debtor in order to allow the continuation of the producer, the employment of workers and the interests of creditors, leading, thus, to preserving company, its corporate function and develop economic activity. It is a court procedure required by the debtor which has been in business for more than two years and requires approval by a judge.  The Extrajudicial Restructuring (Recuperação Extrajudicial) is a private negotiation that involves creditors and debtors and, as with court-ordered restructuring, also must be approved by courts.[24][non-primary source needed]  Bankruptcy, also referred to as insolvency in Canada, is governed by the Bankruptcy and Insolvency Act and is applicable to businesses and individuals. For example, Target Canada, the Canadian subsidiary of the Target Corporation, the second-largest discount retailer in the United States filed for bankruptcy on 15 January 2015, and closed all of its stores by 12 April. The office of the Superintendent of Bankruptcy, a federal agency, is responsible for ensuring that bankruptcies are administered in a fair and orderly manner by all licensed Trustees in Canada.  Trustees in bankruptcy, 1041 individuals licensed to administer insolvencies, bankruptcy and proposal estates are governed by the Bankruptcy and Insolvency Act of Canada.  Bankruptcy is filed when a person or a company becomes insolvent and cannot pay their debts as they become due and if they have at least $1,000 in debt.  In 2011, the Superintendent of Bankruptcy reported that trustees in Canada filed 127,774 insolvent estates. Consumer estates were the vast majority, with 122,999 estates.[25] The consumer portion of the 2011 volume is divided into 77,993 bankruptcies and 45,006 consumer proposals. This represented a reduction of 8.9% from 2010. Commercial estates filed by Canadian trustees in 2011 4,775 estates, 3,643 bankruptcies and 1,132 Division 1 proposals.[26] This represents a reduction of 8.6% over 2010.  Some of the duties of the trustee in bankruptcy are to:  Creditors become involved by attending creditors' meetings. The trustee calls the first meeting of creditors for the following purposes:  In Canada, a person can file a consumer proposal as an alternative to bankruptcy. A consumer proposal is a negotiated settlement between a debtor and their creditors.  A typical proposal would involve a debtor making monthly payments for a maximum of five years, with the funds distributed to their creditors. Even though most proposals call for payments of less than the full amount of the debt owing, in most cases, the creditors accept the deal—because if they do not, the next alternative may be personal bankruptcy, in which the creditors get even less money. The creditors have 45 days to accept or reject the consumer proposal. Once the proposal is accepted by both the creditors and the Court, the debtor makes the payments to the Proposal Administrator each month (or as otherwise stipulated in their proposal), and the general creditors are prevented from taking any further legal or collection action. If the proposal is rejected, the debtor is returned to his prior insolvent state and may have no alternative but to declare personal bankruptcy.  A consumer proposal can only be made by a debtor with debts to a maximum of $250,000 (not including the mortgage on their principal residence). If debts are greater than $250,000, the proposal must be filed under Division 1 of Part III of the Bankruptcy and Insolvency Act. An Administrator is required in the Consumer Proposal, and a Trustee in the Division I Proposal (these are virtually the same although the terms are not interchangeable). A Proposal Administrator is almost always a licensed trustee in bankruptcy, although the Superintendent of Bankruptcy may appoint other people to serve as administrators.  In 2006, there were 98,450 personal insolvency filings in Canada: 79,218 bankruptcies and 19,232 consumer proposals.[27]  In Canada, bankruptcy always means liquidation. There is no way for a company to emerge from bankruptcy after restructuring, as is the case in the United States with a Chapter 11 bankruptcy filing. Canada does, however, have laws that allow for businesses to restructure and emerge later with a smaller debt load and a more positive financial future. While not technically a form of bankruptcy, businesses with $5M or more in debt may make use of the Companies' Creditors Arrangement Act to halt all debt recovery efforts against the company while they formulate a plan to restructure.  The People's Republic of China legalized bankruptcy in 1986, and a revised law that was more expansive and complete was enacted in 2007.  Bankruptcy in Ireland applies only to natural persons. Other insolvency processes including liquidation and examinership are used to deal with corporate insolvency.  Irish bankruptcy law has been the subject of significant comment, from both government sources and the media, as being in need of reform. Part 7 of the Civil Law (Miscellaneous Provisions) Act 2011[28] has started this process and the government has committed to further reform.  Bankruptcy in Israel is governed by the Insolvency and Rehabilitation Law, 2018. Insolvency proceedings below ₪150,000 will be administered entirely by the Enforcement and Collection Authority. Insolvency proceedings above ₪150,000 individual debtors file the documents  will be conducted before the official receiver (the Insolvency Commissioner) and, if a creditor want to file against a debtor, he needs to open process, before the magistrate's court that hears in the district. Company bankruptcy will be conducted before District Court. Simultaneously, with the issue of the order for the commencement of insolvency proceedings, the Insolvency Commissioner shall appoint a trustee for the debtor and an audit will be carried out, in which the debtor's economic capability and his conduct will be examined (lasting approximately 12 months). At the end of this audit a payment plan is established, at the end of which the debtor will receive a discharge. The default scenario is a payment period of three years; however, the court reserves the right to increase or decrease the period depending upon the circumstances of the case. If the debtor has no proven financial ability to pay the creditors, he may be granted an immediate discharge.[29] Since 1996, Israeli personal bankruptcy law has shifted to a relatively debtor-friendly regime, not unlike the American model.[30]  In May 2016, the Parliament of India passed the Insolvency and Bankruptcy Code (IBC), updating outdated corporate insolvency laws. The IBC streamlined the process, reducing delays from a decade to 180 days, and replaced the Board for Industrial and Financial Reconstruction (BIFR) with a market-driven approach.  Dutch bankruptcy law is governed by the Dutch Bankruptcy Code (Faillissementswet). The code covers three separate legal proceedings:   Federal Law No. 127-FZ \"On Insolvency (Bankruptcy)\" dated 26 October 2002 (as amended) (the \"Bankruptcy Act\"), replacing the previous law in 1998, to better address the above problems[non sequitur] and a broader failure of the action. Russian insolvency law is intended for a wide range of borrowers: individuals and companies of all sizes, with the exception of state-owned enterprises, government agencies, political parties and religious organizations. There are also special rules for insurance companies, professional participants of the securities market, agricultural organizations and other special laws for financial institutions and companies in the natural monopolies in the energy industry. Federal Law No. 40-FZ \"On Insolvency (Bankruptcy)\" dated 25 February 1999 (as amended) (the \"Insolvency Law of Credit Institutions\") contains special provisions in relation to the opening of insolvency proceedings in relation to the credit company. Insolvency Provisions Act, credit organizations used in conjunction with the provisions of the Bankruptcy Act.  Bankruptcy law provides for the following stages of insolvency proceedings:  The main face of the bankruptcy process is the insolvency officer (trustee in bankruptcy, bankruptcy manager). At various stages of bankruptcy, he must be determined: the temporary officer in monitoring procedure, external manager in external control, the receiver or administrative officer in the economic recovery, the liquidator. During the bankruptcy trustee in bankruptcy (insolvency officer) has a decisive influence on the movement of assets (property) of the debtor – the debtor and has a key influence on the economic and legal aspects of its operations.  In Spain, people who cannot repay their home mortgages may declare bankruptcy.[31]: 219 [relevant?] Bankruptcy and foreclosure discharges the obligation to pay mortgage interest, but not mortgage principal.[31]: 219  If mortgage principal is not paid, the debtor is placed on a list of untrustworthy people.[31]: 219   Under Swiss law, bankruptcy can be a consequence of insolvency. It is a court-ordered form of debt enforcement proceedings that applies, in general, to registered commercial entities only. In a bankruptcy, all assets of the debtor are liquidated under the administration of the creditors, although the law provides for debt restructuring options similar to those under Chapter 11 of the U.S. Bankruptcy code.  In Sweden, bankruptcy (Swedish: konkurs) is a formal process that may involve a company or individual. A creditor or the company itself can apply for bankruptcy. An external bankruptcy manager takes over the company or the assets of the person, and tries to sell as much as possible. A person or a company in bankruptcy cannot access its assets (with some exceptions).  The formal bankruptcy process is rarely carried out for individuals.[32] Creditors can claim money through the Enforcement Administration anyway, and creditors do not usually benefit from the bankruptcy of individuals because there are costs of a bankruptcy manager which has priority. Unpaid debts remain after bankruptcy for individuals. People who are deeply in debt can obtain a debt arrangement procedure (Swedish: skuldsanering). On application, they obtain a payment plan under which they pay as much as they can for five years, and then all remaining debts are cancelled. Debts that derive from a ban on business operations (issued by court, commonly for tax fraud or fraudulent business practices) or owed to a crime victim as compensation for damages, are exempted from this—and, as before this process was introduced in 2006, remain lifelong.[33] Debts that have not been claimed during a 3–10 year period are cancelled. Often crime victims stop their claims after a few years since criminals often do not have job incomes and might be hard to locate, while banks make sure their claims are not cancelled. The most common reasons for personal insolvency in Sweden are illness, unemployment, divorce or company bankruptcy.  For companies, formal bankruptcy is a normal effect of insolvency, even if there is a reconstruction mechanism where the company can be given time to solve its situation, e.g. by finding an investor. The government can pay salaries to employees in insolvent companies which do not pay them, but only if the company is declared bankrupt. Therefore, it is normal that trade union do the application for bankruptcy if a supplier has not already done so.  The formal bankruptcy involves contracting a bankruptcy manager, who makes certain that assets are sold and money divided by the priority the law claims, and no other way. Banks have such a priority. After a finished bankruptcy for a company, it is terminated. The activities might continue in a new company which has bought important assets from the bankrupted company.  The United Arab Emirates Bankruptcy Law came into force on 29 December 2016,[34] and created a single law governing bankruptcy procedures, which had previously been spread across multiple sources. There are two court procedures: first, a procedure for a company that is not yet insolvent, known as a protective composition, and second, a formal bankruptcy that is split into a rescue process (similar to protective composition) or liquidation.[35]  Directors of a company can be held personally liable for its debts.[36][37]  The Bankruptcy Law does not apply to government bodies, or to companies trading in free zones such as the Dubai International Financial Centre or the Abu Dhabi Global Market, which have their own insolvency laws.[35]  Bankruptcy in the United Kingdom (in a strict legal sense) relates only to individuals (including sole proprietors) and partnerships. Companies and other corporations enter into differently named legal insolvency procedures: liquidation and administration (administration order and administrative receivership). However, the term 'bankruptcy' is often used when referring to companies in the media and in general conversation. Bankruptcy in Scotland is referred to as sequestration. To apply for bankruptcy in Scotland, an individual must have more than £1,500 of debt.  A trustee in bankruptcy must be either an Official Receiver (a civil servant) or a licensed insolvency practitioner. Current law in England and Wales derives in large part from the Insolvency Act 1986. Following the introduction of the Enterprise Act 2002, bankruptcy in England and Wales now normally lasts no longer than 12 months, and may be less if the Official Receiver files in court a certificate that investigations are complete. It was expected that the UK Government's liberalisation of the bankruptcy regime would increase the number of bankruptcy cases; initially, cases increased, as the Insolvency Service statistics appear to bear out. Since 2009, the introduction of the Debt Relief Order has resulted in a dramatic fall in bankruptcies, the latest estimates for year 2014\/15 being significantly less than 30,000 cases.[citation needed]  The UK bankruptcy law was changed in May 2000, effective from 29 May 2000.[38] Debtors may now retain occupational pensions while in bankruptcy, except in rare cases.[38]  Bankruptcy in the United States is a matter placed under federal jurisdiction by the United States Constitution (in Article 1, Section 8, Clause 4), which empowers Congress to enact \"uniform Laws on the subject of Bankruptcies throughout the United States\". Congress has enacted statutes governing bankruptcy, primarily in the form of the Bankruptcy Code, located at Title 11 of the United States Code.[40]  A debtor declares bankruptcy to obtain relief from debt, and this is normally accomplished either through a discharge of the debt or through a restructuring of the debt. When a debtor files a voluntary petition, their bankruptcy case commences.[41]  While bankruptcy cases are always filed in United States Bankruptcy Court (an adjunct to the U.S. District Courts), bankruptcy cases, particularly with respect to the validity of claims and exemptions, are often dependent upon State law.[42] A Bankruptcy Exemption defines the property a debtor may retain and preserve through bankruptcy. Certain real and personal property can be exempted on \"Schedule C\"[43] of a debtor's bankruptcy forms, and effectively be taken outside the debtor's bankruptcy estate. Bankruptcy exemptions are available only to individuals filing bankruptcy.[44]  There are two alternative systems that can be used to exempt property from a bankruptcy estate, federal exemptions[45] (available in some states but not all), and state exemptions (which vary widely between states). For example, Maryland and Virginia, which are adjoining states, have different personal exemption amounts that cannot be seized for payment of debts. This amount is the first $6,000 in property or cash in Maryland,[46] but normally only the first $5,000 in Virginia.[47] State law therefore plays a major role in many bankruptcy cases, such that there may be significant differences in the outcome of a bankruptcy case depending upon the state in which it is filed.  After a bankruptcy petition is filed, the court schedules a hearing called a 341 meeting or meeting of creditors, at which the bankruptcy trustee and creditors review the petitioner's petition and supporting schedules, question the petitioner, and can challenge exemptions they believe are improper.[48]  There are six types of bankruptcy under the Bankruptcy Code, located at Title 11 of the United States Code:  An important feature applicable to all types of bankruptcy filings is the automatic stay.[49] The automatic stay means that the mere request for bankruptcy protection automatically halts most lawsuits, repossessions, foreclosures, evictions, garnishments, attachments, utility shut-offs, and debt collection activity.  The most common types of personal bankruptcy for individuals are Chapter 7 and Chapter 13. Chapter 7, known as a \"straight bankruptcy\", involves the discharge of certain debts without repayment. Chapter 13 involves a plan of repayment of debts over a period of years. Whether a person qualifies for Chapter 7 or Chapter 13 is in part determined by income.[50][51] As many as 65% of all US consumer bankruptcy filings are Chapter 7 cases.  Before a consumer may obtain bankruptcy relief under either Chapter 7 or Chapter 13, the debtor is to undertake credit counseling with approved counseling agencies prior to filing a bankruptcy petition and to undertake education in personal financial management from approved agencies prior to being granted a discharge of debts under either Chapter 7 or Chapter 13. Some studies of the operation of the credit counseling requirement suggest that it provides little benefit to debtors who receive the counseling because the only realistic option for many is to seek relief under the Bankruptcy Code.[52]  Corporations and other business forms normally file under Chapters 7 or 11.  Often called \"straight bankruptcy\" or \"simple bankruptcy\", a Chapter 7 bankruptcy potentially allows debtors to eliminate most or all of their debts over a period of as little as three or four months. In a typical consumer bankruptcy, the only debts that survive a Chapter 7 are student loans, child support obligations, some tax bills, and criminal fines. Credit cards, pay day loans, personal loans, medical bills, and just about all other bills are discharged.  In Chapter 7, a debtor surrenders non-exempt property to a bankruptcy trustee, who then liquidates the property and distributes the proceeds to the debtor's unsecured creditors. In exchange, the debtor is entitled to a discharge of some debt. However, the debtor is not granted a discharge if guilty of certain types of inappropriate behavior (e.g., concealing records relating to financial condition) and certain debts (e.g., spousal and child support and most student loans). Some taxes are not discharged even though the debtor is generally discharged from debt. Many individuals in financial distress own only exempt property (e.g., clothes, household goods, an older car, or the tools of their trade or profession) and do not have to surrender any property to the trustee.[50] The amount of property that a debtor may exempt varies from state to state (as noted above, Virginia and Maryland have a $1,000 difference). Chapter 7 relief is available only once in any eight-year period. Generally, the rights of secured creditors to their collateral continues, even though their debt is discharged. For example, absent some arrangement by a debtor to surrender a car or \"reaffirm\" a debt, the creditor with a security interest in the debtor's car may repossess the car even if the debt to the creditor is discharged.  Ninety-one percent of US individuals who petition for relief under Chapter 7 hire an attorney to file their petitions.[53] The typical cost of an attorney is $1,170.00.[53] Alternatives to filing with an attorney are: filing pro se,[54] hiring a non-lawyer petition preparer,[55] or using online software to generate the petition.  To be eligible to file a consumer bankruptcy under Chapter 7, a debtor must qualify under a statutory means test.[56] The means test was intended to make it more difficult for a significant number of financially distressed individual debtors whose debts are primarily consumer debts to qualify for relief under Chapter 7 of the Bankruptcy Code. The \"means test\" is employed in cases where an individual with primarily consumer debts has more than the average annual income for a household of equivalent size, computed over a 180-day period prior to filing. If the individual must take the means test, their average monthly income over this 180-day period is reduced by a series of allowances for living expenses and secured debt payments in a very complex calculation that may or may not accurately reflect that individual's actual monthly budget. If the results of the means test show no disposable income (or in some cases a very small amount) then the individual qualifies for Chapter 7 relief. An individual who fails the means test will have their Chapter 7 case dismissed, or may have to convert the case to a Chapter 13 bankruptcy.  If a debtor does not qualify for relief under Chapter 7 of the Bankruptcy Code, either because of the Means Test or because Chapter 7 does not provide a permanent solution to delinquent payments for secured debts, such as mortgages or vehicle loans, the debtor may still seek relief under Chapter 13 of the Code.  Generally, a trustee sells most of the debtor's assets to pay off creditors. However, certain debtor assets will be protected to some extent by bankruptcy exemptions. These include Social Security payments, unemployment compensation, limited equity in a home, car, or truck, household goods and appliances, trade tools, and books. However, these exemptions vary from state to state.  In Chapter 11 bankruptcy, the debtor retains ownership and control of assets and is re-termed a debtor in possession (DIP).[57] The debtor in possession runs the day-to-day operations of the business while creditors and the debtor work with the Bankruptcy Court in order to negotiate and complete a plan. Upon meeting certain requirements (e.g., fairness among creditors, priority of certain creditors) creditors are permitted to vote on the proposed plan.[58] If a plan is confirmed, the debtor continues to operate and pay debts under the terms of the confirmed plan. If a specified majority of creditors do not vote to confirm a plan, additional requirements may be imposed by the court in order to confirm the plan. Debtors filing for Chapter 11 protection a second time are known informally as \"Chapter 22\" filers.[59]  In a corporate or business bankruptcy, an indebted company is typically recapitalized so that it emerges from bankruptcy with more equity and less debt, with potential for dispute over the valuation of the reorganized business.[60]  In Chapter 13, debtors retain ownership and possession of all their assets but must devote some portion of future income to repaying creditors, generally over three to five years.[61] The amount of payment and period of the repayment plan depend upon a variety of factors, including the value of the debtor's property and the amount of a debtor's income and expenses.[62] Under this chapter, the debtor can propose a repayment plan in which to pay creditors over three to five years. If the monthly income is less than the state's median income, the plan is for three years, unless the court finds just cause to extend the plan for a longer period. If the debtor's monthly income is greater than the median income for individuals in the debtor's state, the plan must generally be for five years. A plan cannot exceed the five-year limit.[62]  Relief under Chapter 13 is available only to individuals with regular income whose debts do not exceed prescribed limits.[63] If the debtor is an individual or a sole proprietor, the debtor is allowed to file for a Chapter 13 bankruptcy to repay all or part of the debts. Secured creditors may be entitled to greater payment than unsecured creditors.[64]  In contrast to Chapter 7, the debtor in Chapter 13 may keep all property, whether or not exempt. If the plan appears feasible and if the debtor complies with all the other requirements, the bankruptcy court typically confirms the plan and the debtor and creditors are bound by its terms. Creditors have no say in the formulation of the plan, other than to object to it, if appropriate, on the grounds that it does not comply with one of the Code's statutory requirements.[65] Generally, the debtor makes payments to a trustee who disburses the funds in accordance with the terms of the confirmed plan.  When the debtor completes payments pursuant to the terms of the plan, the court formally grant the debtor a discharge of the debts provided for in the plan.[62] However, if the debtor fails to make the agreed upon payments or fails to seek or gain court approval of a modified plan, a bankruptcy court will normally dismiss the case on the motion of the trustee.[66] After a dismissal, creditors may resume pursuit of state law remedies to recover the unpaid debt.  In 2004, the number of insolvencies reached record highs in many European countries. In France, company insolvencies rose by more than 4%, in Austria by more than 10%, and in Greece by more than 20%. The increase in the number of insolvencies, however, does not indicate the total financial impact of insolvencies in each country because there is no indication of the size of each case. An increase in the number of bankruptcy cases does not necessarily entail an increase in bad debt write-off rates for the economy as a whole.  Bankruptcy statistics are also a trailing indicator. There is a time delay between financial difficulties and bankruptcy. In most cases, several months or even years pass between the financial problems and the start of bankruptcy proceedings. Legal, tax, and cultural issues may further distort bankruptcy figures, especially when comparing on an international basis. Two examples:  The insolvency numbers for private individuals also do not show the whole picture. Only a fraction of heavily indebted households file for insolvency. Two of the main reasons for this are the stigma of declaring themselves insolvent and the potential business disadvantage.  Following the soar in insolvencies in the previous decade, a number of European countries, such as France, Germany, Spain and Italy, began to revamp their bankruptcy laws in 2013. They modelled these new laws on Chapter 11 of the U.S. Bankruptcy Code. Currently, the majority of insolvency cases have ended in liquidation in Europe rather than the businesses surviving the crisis. These new law models are meant to change this; lawmakers are hoping to turn bankruptcy into a chance for restructuring rather than a death sentence for the companies.[67]  EU policy aims to ensure that \"honest entrepreneurs\" are afforded a second chance at business development. A faster start-up programme for people affected by bankruptcy operating in Denmark and a scheme to support Belgian business owners and self-employed persons were highlighted in a 2008 European Commission Communication as good practice examples in this field.[68]  Technically, states do not collapse directly due to a sovereign default event itself.  However, the tumultuous events that follow may bring down the state, so in common language, states would be described as being bankrupted.  An example of this is when the Goguryeo–Sui War in 614 A.D. ended in the disintegration of Sui dynasty China within 4 years, although their enemy Goguryeo (occupying modern Korea) also seemingly entered into decline and fell some 56 years later.[69][edition needed] "},"meta":{},"created_at":"2025-03-22T14:25:42.273677Z","updated_at":"2025-03-22T14:25:42.273677Z","inner_id":5,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":14,"annotations":[{"id":14,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.309403Z","updated_at":"2025-03-22T14:25:42.309403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"d2ef14d8-f49d-4462-8eb0-4f7d04542b35","import_id":null,"last_action":null,"bulk_created":false,"task":14,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Empirical methods  Prescriptive and policy  Microeconomics is a branch of economics that studies the behavior of individuals and firms in making decisions regarding the allocation of scarce resources and the interactions among these individuals and firms.[1][2][3] Microeconomics focuses on the study of individual markets, sectors, or industries as opposed to the economy as a whole, which is studied in macroeconomics.  One goal of microeconomics is to analyze the market mechanisms that establish relative prices among goods and services and allocate limited resources among alternative uses.[4] Microeconomics shows conditions under which free markets lead to desirable allocations. It also analyzes market failure, where markets fail to produce efficient results.[5]  While microeconomics focuses on firms and individuals, macroeconomics focuses on the total of economic activity, dealing with the issues of growth, inflation, and unemployment—and with national policies relating to these issues.[2] Microeconomics also deals with the effects of economic policies (such as changing taxation levels) on microeconomic behavior and thus on the aforementioned aspects of the economy.[6] Particularly in the wake of the Lucas critique, much of modern macroeconomic theories has been built upon microfoundations—i.e., based upon basic assumptions about micro-level behavior.  Microeconomic study historically has been performed according to general equilibrium theory, developed by Léon Walras in Elements of Pure Economics (1874) and partial equilibrium theory, introduced by Alfred Marshall in Principles of Economics (1890).[7]  Microeconomic theory typically begins with the study of a single rational and utility maximizing individual. To economists, rationality means an individual possesses stable preferences that are both complete and transitive.   The technical assumption that preference relations are continuous is needed to ensure the existence of a utility function. Although microeconomic theory can continue without this assumption, it would make comparative statics impossible since there is no guarantee that the resulting utility function would be differentiable.  Microeconomic theory progresses by defining a competitive budget set which is a subset of the consumption set. It is at this point that economists make the technical assumption that preferences are locally non-satiated. Without the assumption of LNS (local non-satiation) there is no 100% guarantee but there would be a rational rise[8] in individual utility. With the necessary tools and assumptions in place the utility maximization problem (UMP) is developed.  The utility maximization problem is the heart of consumer theory. The utility maximization problem attempts to explain the action axiom by imposing rationality axioms on consumer preferences and then mathematically modeling and analyzing the consequences.[9] The utility maximization problem serves not only as the mathematical foundation of consumer theory but as a metaphysical explanation of it as well. That is, the utility maximization problem is used by economists to not only explain what or how individuals make choices but why individuals make choices as well.  The utility maximization problem is a constrained optimization problem in which an individual seeks to maximize utility subject to a budget constraint. Economists use the extreme value theorem to guarantee that a solution to the utility maximization problem exists. That is, since the budget constraint is both bounded and closed, a solution to the utility maximization problem exists. Economists call the solution to the utility maximization problem a Walrasian demand function or correspondence. [10]  The utility maximization problem has so far been developed by taking consumer tastes (i.e. consumer utility) as primitive. However, an alternative way to develop microeconomic theory is by taking consumer choice as primitive. This model of microeconomic theory is referred to as revealed preference theory.  The theory of supply and demand usually assumes that markets are perfectly competitive. This implies that there are many buyers and sellers in the market and none of them have the capacity to significantly influence prices of goods and services. In many real-life transactions, the assumption fails because some individual buyers or sellers have the ability to influence prices. Quite often, a sophisticated analysis is required to understand the demand-supply equation of a good model. However, the theory works well in situations meeting these assumptions.  Mainstream economics does not assume a priori that markets are preferable to other forms of social organization. In fact, much analysis is devoted to cases where market failures lead to resource allocation that is suboptimal and creates deadweight loss. A classic example of suboptimal resource allocation is that of a public good. In such cases, economists may attempt to find policies that avoid waste, either directly by government control, indirectly by regulation that induces market participants to act in a manner consistent with optimal welfare, or by creating \"missing markets\" to enable efficient trading where none had previously existed.  This is studied in the field of collective action and public choice theory. \"Optimal welfare\" usually takes on a Paretian norm, which is a mathematical application of the Kaldor–Hicks method. This can diverge from the Utilitarian goal of maximizing utility because it does not consider the distribution of goods between people. Market failure in positive economics (microeconomics) is limited in implications without mixing the belief of the economist and their theory.  The demand for various commodities by individuals is generally thought of as the outcome of a utility-maximizing process, with each individual trying to maximize their own utility under a budget constraint and a given consumption set.  Individuals and firms need to allocate limited resources to ensure all agents in the economy are well off. Firms decide which goods and services to produce considering low costs involving labor, materials and capital as well as potential profit margins. Consumers choose the good and services they want that will maximize their happiness taking into account their limited wealth.[11]  The government can make these allocation decisions or they can be independently made by the consumers and firms. For example, in the former Soviet Union, the government played a part in informing car manufacturers which cars to produce and which consumers will gain access to a car.[11]  Economists commonly consider themselves microeconomists or macroeconomists. The difference between microeconomics and macroeconomics likely was introduced in 1933 by the Norwegian economist Ragnar Frisch, the co-recipient of the first Nobel Memorial Prize in Economic Sciences in 1969.[12][13] However, Frisch did not actually use the word \"microeconomics\", instead drawing distinctions between \"micro-dynamic\" and \"macro-dynamic\" analysis in a way similar to how the words \"microeconomics\" and \"macroeconomics\" are used today.[12][14] The first known use of the term \"microeconomics\" in a published article was from Pieter de Wolff in 1941, who broadened the term \"micro-dynamics\" into \"microeconomics\".[13][15]  Consumer demand theory relates preferences for the consumption of both goods and services to the consumption expenditures; ultimately, this relationship between preferences and consumption expenditures is used to relate preferences to consumer demand curves. The link between personal preferences, consumption and the demand curve is one of the most closely studied relations in economics. It is a way of analyzing how consumers may achieve equilibrium between preferences and expenditures by maximizing utility subject to consumer budget constraints.  Production theory is the study of production, or the economic process of converting inputs into outputs.[16] Production uses resources to create a good or service that is suitable for use, gift-giving in a gift economy, or exchange in a market economy. This can include manufacturing, storing, shipping, and packaging. Some economists define production broadly as all economic activity other than consumption. They see every commercial activity other than the final purchase as some form of production.  The cost-of-production theory of value states that the price of an object or condition is determined by the sum of the cost of the resources that went into making it. The cost can comprise any of the factors of production (including labor, capital, or land) and taxation. Technology can be viewed either as a form of fixed capital (e.g. an industrial plant) or circulating capital (e.g. intermediate goods).  In the mathematical model for the cost of production, the short-run total cost is equal to fixed cost plus total variable cost. The fixed cost refers to the cost that is incurred regardless of how much the firm produces. The variable cost is a function of the quantity of an object being produced. The cost function can be used to characterize production through the duality theory in economics, developed mainly by Ronald Shephard (1953, 1970) and other scholars (Sickles & Zelenyuk, 2019, ch. 2).  Over a short time period (few months), most costs are fixed costs as the firm will have to pay for salaries, contracted shipment and materials used to produce various goods. Over a longer time period (2-3 years), costs can become variable. Firms can decide to reduce output, purchase fewer materials and even sell some machinery. Over 10 years, most costs become variable as workers can be laid off or new machinery can be bought to replace the old machinery [17]  Sunk costs – This is a fixed cost that has already been incurred and cannot be recovered. An example of this can be in R&D development like in the pharmaceutical industry. Hundreds of millions of dollars are spent to achieve new drug breakthroughs but this is challenging as its increasingly harder to find new breakthroughs and meet tighter regulation standards. Thus many projects are written off leading to losses of millions of dollars [18]  Opportunity cost is closely related to the idea of time constraints. One can do only one thing at a time, which means that, inevitably, one is always giving up other things. The opportunity cost of any activity is the value of the next-best alternative thing one may have done instead. Opportunity cost depends only on the value of the next-best alternative. It does not matter whether one has five alternatives or 5,000.  Opportunity costs can tell when not to do something as well as when to do something. For example, one may like waffles, but like chocolate even more. If someone offers only waffles, one would take it. But if offered waffles or chocolate, one would take the chocolate. The opportunity cost of eating waffles is sacrificing the chance to eat chocolate. Because the cost of not eating the chocolate is higher than the benefits of eating the waffles, it makes no sense to choose waffles. Of course, if one chooses chocolate, they are still faced with the opportunity cost of giving up having waffles. But one is willing to do that because the waffle's opportunity cost is lower than the benefits of the chocolate. Opportunity costs are unavoidable constraints on behavior because one has to decide what's best and give up the next-best alternative.  Microeconomics is also known as price theory to highlight the significance of prices in relation to buyer and sellers as these agents determine prices due to their individual actions.[11] Price theory is a field of economics that uses the supply and demand framework to explain and predict human behavior. It is associated with the Chicago School of Economics. Price theory studies competitive equilibrium in markets to yield testable hypotheses that can be rejected.  Price theory is not the same as microeconomics. Strategic behavior, such as the interactions among sellers in a market where they are few, is a significant part of microeconomics but is not emphasized in price theory. Price theorists focus on competition believing it to be a reasonable description of most markets that leaves room to study additional aspects of tastes and technology. As a result, price theory tends to use less game theory than microeconomics does.  Price theory focuses on how agents respond to prices, but its framework can be applied to a wide variety of socioeconomic issues that might not seem to involve prices at first glance. Price theorists have influenced several other fields including developing public choice theory and law and economics. Price theory has been applied to issues previously thought of as outside the purview of economics such as criminal justice, marriage, and addiction.  Supply and demand is an economic model of price determination in a perfectly competitive market. It concludes that in a perfectly competitive market with no externalities, per unit taxes, or price controls, the unit price for a particular good is the price at which the quantity demanded by consumers equals the quantity supplied by producers. This price results in a stable economic equilibrium.  Prices and quantities have been described as the most directly observable attributes of goods produced and exchanged in a market economy.[19] The theory of supply and demand is an organizing principle for explaining how prices coordinate the amounts produced and consumed. In microeconomics, it applies to price and output determination for a market with perfect competition, which includes the condition of no buyers or sellers large enough to have price-setting power.  For a given market of a commodity, demand is the relation of the quantity that all buyers would be prepared to purchase at each unit price of the good. Demand is often represented by a table or a graph showing price and quantity demanded (as in the figure). Demand theory describes individual consumers as rationally choosing the most preferred quantity of each good, given income, prices, tastes, etc. A term for this is \"constrained utility maximization\" (with income and wealth as the constraints on demand). Here, utility refers to the hypothesized relation of each individual consumer for ranking different commodity bundles as more or less preferred.  The law of demand states that, in general, price and quantity demanded in a given market are inversely related. That is, the higher the price of a product, the less of it people would be prepared to buy (other things unchanged). As the price of a commodity falls, consumers move toward it from relatively more expensive goods (the substitution effect). In addition, purchasing power from the price decline increases ability to buy (the income effect). Other factors can change demand; for example an increase in income will shift the demand curve for a normal good outward relative to the origin, as in the figure. All determinants are predominantly taken as constant factors of demand and supply.  Supply is the relation between the price of a good and the quantity available for sale at that price. It may be represented as a table or graph relating price and quantity supplied. Producers, for example business firms, are hypothesized to be profit maximizers, meaning that they attempt to produce and supply the amount of goods that will bring them the highest profit. Supply is typically represented as a function relating price and quantity, if other factors are unchanged.  That is, the higher the price at which the good can be sold, the more of it producers will supply, as in the figure. The higher price makes it profitable to increase production. Just as on the demand side, the position of the supply can shift, say from a change in the price of a productive input or a technical improvement. The \"Law of Supply\" states that, in general, a rise in price leads to an expansion in supply and a fall in price leads to a contraction in supply. Here as well, the determinants of supply, such as price of substitutes, cost of production, technology applied and various factors of inputs of production are all taken to be constant for a specific time period of evaluation of supply.  Market equilibrium occurs where quantity supplied equals quantity demanded, the intersection of the supply and demand curves in the figure above. At a price below equilibrium, there is a shortage of quantity supplied compared to quantity demanded. This is posited to bid the price up. At a price above equilibrium, there is a surplus of quantity supplied compared to quantity demanded. This pushes the price down. The model of supply and demand predicts that for given supply and demand curves, price and quantity will stabilize at the price that makes quantity supplied equal to quantity demanded. Similarly, demand-and-supply theory predicts a new price-quantity combination from a shift in demand (as to the figure), or in supply.  For a given quantity of a consumer good, the point on the demand curve indicates the value, or marginal utility, to consumers for that unit. It measures what the consumer would be prepared to pay for that unit.[20] The corresponding point on the supply curve measures marginal cost, the increase in total cost to the supplier for the corresponding unit of the good. The price in equilibrium is determined by supply and demand. In a perfectly competitive market, supply and demand equate marginal cost and marginal utility at equilibrium.[21]  On the supply side of the market, some factors of production are described as (relatively) variable in the short run, which affects the cost of changing output levels. Their usage rates can be changed easily, such as electrical power, raw-material inputs, and over-time and temp work. Other inputs are relatively fixed, such as plant and equipment and key personnel. In the long run, all inputs may be adjusted by management. These distinctions translate to differences in the elasticity (responsiveness) of the supply curve in the short and long runs and corresponding differences in the price-quantity change from a shift on the supply or demand side of the market.  Marginalist theory, such as above, describes the consumers as attempting to reach most-preferred positions, subject to income and wealth constraints while producers attempt to maximize profits subject to their own constraints, including demand for goods produced, technology, and the price of inputs. For the consumer, that point comes where marginal utility of a good, net of price, reaches zero, leaving no net gain from further consumption increases. Analogously, the producer compares marginal revenue (identical to price for the perfect competitor) against the marginal cost of a good, with marginal profit the difference. At the point where marginal profit reaches zero, further increases in production of the good stop. For movement to market equilibrium and for changes in equilibrium, price and quantity also change \"at the margin\": more-or-less of something, rather than necessarily all-or-nothing.  Other applications of demand and supply include the distribution of income among the factors of production, including labor and capital, through factor markets. In a competitive labor market for example the quantity of labor employed and the price of labor (the wage rate) depends on the demand for labor (from employers for production) and supply of labor (from potential workers). Labor economics examines the interaction of workers and employers through such markets to explain patterns and changes of wages and other labor income, labor mobility, and (un)employment, productivity through human capital, and related public-policy issues.[22]  Demand-and-supply analysis is used to explain the behavior of perfectly competitive markets, but as a standard of comparison it can be extended to any type of market. It can also be generalized to explain variables across the economy, for example, total output (estimated as real GDP) and the general price level, as studied in macroeconomics.[23] Tracing the qualitative and quantitative effects of variables that change supply and demand, whether in the short or long run, is a standard exercise in applied economics. Economic theory may also specify conditions such that supply and demand through the market is an efficient mechanism for allocating resources.[24]  Market structure refers to features of a market, including the number of firms in the market, the distribution of market shares between them, product uniformity across firms, how easy it is for firms to enter and exit the market, and forms of competition in the market.[25][26] A market structure can have several types of interacting market systems.   Different forms of markets are a feature of capitalism and market socialism, with advocates of state socialism often criticizing markets and aiming to substitute or replace markets with varying degrees of government-directed economic planning.   Competition acts as a regulatory mechanism for market systems, with government providing regulations where the market cannot be expected to regulate itself. Regulations help to mitigate negative externalities of goods and services when the private equilibrium of the market does not match the social equilibrium. One example of this is with regards to building codes, which if absent in a purely competition regulated market system, might result in several horrific injuries or deaths to be required before companies would begin improving structural safety, as consumers may at first not be as concerned or aware of safety issues to begin putting pressure on companies to provide them, and companies would be motivated not to provide proper safety features due to how it would cut into their profits.  The concept of \"market type\" is different from the concept of \"market structure\". Nevertheless, there are a variety of types of markets.  The different market structures produce cost curves[27] based on the type of structure present. The different curves are developed based on the costs of production, specifically the graph contains marginal cost, average total cost, average variable cost, average fixed cost, and marginal revenue, which is sometimes equal to the demand, average revenue, and price in a price-taking firm.  Perfect competition is a situation in which numerous small firms producing identical products compete against each other in a given industry. Perfect competition leads to firms producing the socially optimal output level at the minimum possible cost per unit. Firms in perfect competition are \"price takers\" (they do not have enough market power to profitably increase the price of their goods or services). A good example would be that of digital marketplaces, such as eBay, on which many different sellers sell similar products to many different buyers. Consumers in a perfect competitive market have perfect knowledge about the products that are being sold in this market.  Imperfect competition is a type of market structure showing some but not all features of competitive markets. In perfect competition, market power is not achievable due to a high level of producers causing high levels of competition. Therefore, prices are brought down to a marginal cost level. In a monopoly, market power is achieved by one firm leading to prices being higher than the marginal cost level. [28]  Between these two types of markets are firms that are neither perfectly competitive or monopolistic. Firms such as Pepsi and Coke and Sony, Nintendo and Microsoft dominate the cola and video game industry respectively. These firms are in imperfect competition [29]  Monopolistic competition is a situation in which many firms with slightly different products compete. Production costs are above what may be achieved by perfectly competitive firms, but society benefits from the product differentiation. Examples of industries with market structures similar to monopolistic competition include restaurants, cereal, clothing, shoes, and service industries in large cities.  A monopoly is a market structure in which a market or industry is dominated by a single supplier of a particular good or service. Because monopolies have no competition, they tend to sell goods and services at a higher price and produce below the socially optimal output level. However, not all monopolies are a bad thing, especially in industries where multiple firms would result in more costs than benefits (i.e. natural monopolies).[30][31]  An oligopoly is a market structure in which a market or industry is dominated by a small number of firms (oligopolists). Oligopolies can create the incentive for firms to engage in collusion and form cartels that reduce competition leading to higher prices for consumers and less overall market output.[32] Alternatively, oligopolies can be fiercely competitive and engage in flamboyant advertising campaigns.[33]  A monopsony is a market where there is only one buyer and many sellers.  A bilateral monopoly is a market consisting of both a monopoly (a single seller) and a monopsony (a single buyer).  An oligopsony is a market where there are a few buyers and many sellers.  Game theory is a major method used in mathematical economics and business for modeling competing behaviors of interacting agents. The term \"game\" here implies the study of any strategic interaction between people. Applications include a wide array of economic phenomena and approaches, such as auctions, bargaining, mergers & acquisitions pricing, fair division, duopolies, oligopolies, social network formation, agent-based computational economics, general equilibrium, mechanism design, and voting systems, and across such broad areas as experimental economics, behavioral economics, information economics, industrial organization, and political economy.  Information economics is a branch of microeconomic theory that studies how information and information systems affect an economy and economic decisions. Information has special characteristics. It is easy to create but hard to trust. It is easy to spread but hard to control. It influences many decisions. These special characteristics (as compared with other types of goods) complicate many standard economic theories.[35] The economics of information has recently become of great interest to many - possibly due to the rise of information-based companies inside the technology industry.[13] From a game theory approach, the usual constraints that agents have complete information can be loosened to further examine the consequences of having incomplete information. This gives rise to many results which are applicable to real life situations. For example, if one does loosen this assumption, then it is possible to scrutinize the actions of agents in situations of uncertainty. It is also possible to more fully understand the impacts – both positive and negative – of agents seeking out or acquiring information.[13]  Applied microeconomics includes a range of specialized areas of study, many of which draw on methods from other fields. "},"meta":{},"created_at":"2025-03-22T14:25:42.273677Z","updated_at":"2025-03-22T14:25:42.273677Z","inner_id":6,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":15,"annotations":[{"id":15,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.309403Z","updated_at":"2025-03-22T14:25:42.309403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"49863059-e473-4a28-a289-7c87f9cd2a9b","import_id":null,"last_action":null,"bulk_created":false,"task":15,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Actuarial science is the discipline that applies mathematical and statistical methods to assess risk in insurance, pension, finance, investment and other industries and professions.  Actuaries are professionals trained in this discipline. In many countries, actuaries must demonstrate their competence by passing a series of rigorous professional examinations focused in fields such as probability and predictive analysis.  Actuarial science includes a number of interrelated subjects, including mathematics, probability theory, statistics, finance, economics, financial accounting and computer science. Historically, actuarial science used deterministic models in the construction of tables and premiums. The science has gone through revolutionary changes since the 1980s due to the proliferation of high speed computers and the union of stochastic actuarial models with modern financial theory.[1]  Many universities have undergraduate and graduate degree programs in actuarial science. In 2010,[needs update] a study published by job search website CareerCast ranked actuary as the #1 job in the United States.[2] The study used five key criteria to rank jobs: environment, income, employment outlook, physical demands, and stress. In 2024, U.S. News & World Report ranked actuary as the third-best job in the business sector and the eighth-best job in STEM.[3]  Actuarial science became a formal mathematical discipline in the late 17th century with the increased demand for long-term insurance coverage such as burial, life insurance, and annuities. These long term coverages required that money be set aside to pay future benefits, such as annuity and death benefits many years into the future. This requires estimating future contingent events, such as the rates of mortality by age, as well as the development of mathematical techniques for discounting the value of funds set aside and invested. This led to the development of an important actuarial concept, referred to as the present value of a future sum. Certain aspects of the actuarial methods for discounting pension funds have come under criticism from modern financial economics.[citation needed]  Actuarial science is also applied to property, casualty, liability, and general insurance. In these forms of insurance, coverage is generally provided on a renewable period, (such as a yearly). Coverage can be cancelled at the end of the period by either party.[citation needed]  Property and casualty insurance companies tend to specialize because of the complexity and diversity of risks.[citation needed] One division is to organize around personal and commercial lines of insurance. Personal lines of insurance are for individuals and include fire, auto, homeowners, theft and umbrella coverages. Commercial lines address the insurance needs of businesses and include property, business continuation, product liability, fleet\/commercial vehicle, workers compensation, fidelity and surety, and D&O insurance. The insurance industry also provides coverage for exposures such as catastrophe, weather-related risks, earthquakes, patent infringement and other forms of corporate espionage, terrorism, and \"one-of-a-kind\" (e.g., satellite launch). Actuarial science provides data collection, measurement, estimating, forecasting, and valuation tools to provide financial and underwriting data for management to assess marketing opportunities and the nature of the risks. Actuarial science often helps to assess the overall risk from catastrophic events in relation to its underwriting capacity or surplus.[citation needed]  In the reinsurance fields, actuarial science can be used to design and price reinsurance and retrocession arrangements, and to establish reserve funds for known claims and future claims and catastrophes.[citation needed]  There is an increasing trend to recognize that actuarial skills can be applied to a range of applications outside the traditional fields of insurance, pensions, etc. One notable example is the use in some US states of actuarial models to set criminal sentencing guidelines. These models attempt to predict the chance of re-offending according to rating factors which include the type of crime, age, educational background and ethnicity of the offender.[7] However, these models have been open to criticism as providing justification for discrimination against specific ethnic groups by law enforcement personnel.  Whether this is statistically correct or a self-fulfilling correlation remains under debate.[8]  Another example is the use of actuarial models to assess the risk of sex offense recidivism. Actuarial models and associated tables, such as the MnSOST-R, Static-99, and SORAG, have been used since the late 1990s to determine the likelihood that a sex offender will re-offend and thus whether he or she should be institutionalized or set free.[9]  Traditional actuarial science and modern financial economics in the US have different practices, which is caused by different ways of calculating funding and investment strategies, and by different regulations.[citation needed]  Regulations are from the Armstrong investigation of 1905, the Glass–Steagall Act of 1932, the adoption of the Mandatory Security Valuation Reserve by the National Association of Insurance Commissioners, which cushioned market fluctuations, and the Financial Accounting Standards Board, (FASB) in the US and Canada, which regulates pensions valuations and funding.[citation needed]  Historically, much of the foundation of actuarial theory predated modern financial theory. In the early twentieth century, actuaries were developing many techniques that can be found in modern financial theory, but for various historical reasons, these developments did not achieve much recognition.[10]  As a result, actuarial science developed along a different path, becoming more reliant on assumptions, as opposed to the arbitrage-free risk-neutral valuation concepts used in modern finance. The divergence is not related to the use of historical data and statistical projections of liability cash flows, but is instead caused by the manner in which traditional actuarial methods apply market data with those numbers. For example, one traditional actuarial method suggests that changing the asset allocation mix of investments can change the value of liabilities and assets (by changing the discount rate assumption). This concept is inconsistent with financial economics.[citation needed]  The potential of modern financial economics theory to complement existing actuarial science was recognized by actuaries in the mid-twentieth century.[11] In the late 1980s and early 1990s, there was a distinct effort for actuaries to combine financial theory and stochastic methods into their established models.[12] Ideas from financial economics became increasingly influential in actuarial thinking, and actuarial science has started to embrace more sophisticated mathematical modelling of finance.[13] Today, the profession, both in practice and in the educational syllabi of many actuarial organizations, is cognizant of the need to reflect the combined approach of tables, loss models, stochastic methods, and financial theory.[14] However, assumption-dependent concepts are still widely used (such as the setting of the discount rate assumption as mentioned earlier), particularly in North America.[citation needed]  Product design adds another dimension to the debate. Financial economists argue that pension benefits are bond-like and should not be funded with equity investments without reflecting the risks of not achieving expected returns. But some pension products do reflect the risks of unexpected returns. In some cases, the pension beneficiary assumes the risk, or the employer assumes the risk. The current debate now seems to be focusing on four principles:  Essentially, financial economics state that pension assets should not be invested in equities for a variety of theoretical and practical reasons.[15]  Elementary mutual aid agreements and pensions arose in antiquity.[16] Early in the Roman empire, associations were formed to meet the expenses of burial, cremation, and monuments—precursors to burial insurance and friendly societies. A small sum was paid into a communal fund on a weekly basis, and upon the death of a member, the fund would cover the expenses of rites and burial. These societies sometimes sold shares in the building of columbāria, or burial vaults, owned by the fund—the precursor to mutual insurance companies.[17] Other early examples of mutual surety and assurance pacts can be traced back to various forms of fellowship within the Saxon clans of England and their Germanic forebears, and to Celtic society.[18] However, many of these earlier forms of surety and aid would often fail due to lack of understanding and knowledge.[19]  The 17th century was a period of advances in mathematics in Germany, France and England. At the same time there was a rapidly growing desire and need to place the valuation of personal risk on a more scientific basis. Independently of each other, compound interest was studied and probability theory emerged as a well-understood mathematical discipline. Another important advance came in 1662 from a London draper, the father of demography, John Graunt, who showed that there were predictable patterns of longevity and death in a group, or cohort, of people of the same age, despite the uncertainty of the date of death of any one individual. This study became the basis for the original life table. One could now set up an insurance scheme to provide life insurance or pensions for a group of people, and to calculate with some degree of accuracy how much each person in the group should contribute to a common fund assumed to earn a fixed rate of interest. The first person to demonstrate publicly how this could be done was Edmond Halley (of Halley's comet fame). Halley constructed his own life table, and showed how it could be used to calculate the premium amount someone of a given age should pay to purchase a life annuity.[20]  James Dodson's pioneering work on the long term insurance contracts under which the same premium is charged each year led to the formation of the Society for Equitable Assurances on Lives and Survivorship (now commonly known as Equitable Life) in London in 1762.[21] William Morgan is often considered the father of modern actuarial science for his work in the field in the 1780s and 90s. Many other life insurance companies and pension funds were created over the following 200 years. Equitable Life was the first to use the word \"actuary\" for its chief executive officer in 1762.[22] Previously, \"actuary\" meant an official who recorded the decisions, or \"acts\", of ecclesiastical courts.[19] Other companies that did not use such mathematical and scientific methods most often failed or were forced to adopt the methods pioneered by Equitable.[23]  In the 18th and 19th centuries, calculations were performed without computers. The computations of life insurance premiums and reserving requirements are rather complex, and actuaries developed techniques to make the calculations as easy as possible, for example \"commutation functions\" (essentially precalculated columns of summations over time of discounted values of survival and death probabilities).[24] Actuarial organizations were founded to support and further both actuaries and actuarial science, and to protect the public interest by promoting competency and ethical standards.[25] However, calculations remained cumbersome, and actuarial shortcuts were commonplace. Non-life actuaries followed in the footsteps of their life insurance colleagues during the 20th century. The 1920 revision for the New-York based National Council on Workmen's Compensation Insurance rates took over two months of around-the-clock work by day and night teams of actuaries.[26] In the 1930s and 1940s, the mathematical foundations for stochastic processes were developed.[27] Actuaries could now begin to estimate losses using models of random events, instead of the deterministic methods they had used in the past. The introduction and development of the computer further revolutionized the actuarial profession. From pencil-and-paper to punchcards to current high-speed devices, the modeling and forecasting ability of the actuary has rapidly improved, while still being heavily dependent on the assumptions input into the models, and actuaries needed to adjust to this new world .[28] "},"meta":{},"created_at":"2025-03-22T14:25:42.273677Z","updated_at":"2025-03-22T14:25:42.273677Z","inner_id":7,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":16,"annotations":[{"id":16,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.309403Z","updated_at":"2025-03-22T14:25:42.309403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"bbeb49ed-a176-42f4-9e7e-1fd6f9351b9a","import_id":null,"last_action":null,"bulk_created":false,"task":16,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"The shadow banking system is a term for the collection of non-bank financial intermediaries (NBFIs) that legally provide services similar to traditional commercial banks but outside normal banking regulations.[1][2] S&P Global estimates that, at end-2022, shadow banking held about $63 trillion in financial assets in major jurisdictions around the world, representing 78% of global GDP, up from $28 trillion and 68% of global GDP in 2009.[3]  Examples of NBFIs include hedge funds, insurance firms, pawn shops, cashier's check issuers, check cashing locations, payday lending, currency exchanges, and microloan organizations.[4][5] The phrase \"shadow banking\" is regarded by some as pejorative, and the term \"market-based finance\" has been proposed as an alternative.[6]   Former US Federal Reserve Chair Ben Bernanke provided the following definition in November 2013:  \"Shadow banking, as usually defined, comprises a diverse set of institutions and markets that, collectively, carry out traditional banking functions—but do so outside, or in ways only loosely linked to, the traditional system of regulated depository institutions.  Examples of important components of the shadow banking system include securitization vehicles, asset-backed commercial paper [ABCP] conduits, money market funds, markets for repurchase agreements, investment banks, and mortgage companies\"[7]  Shadow banking has grown in importance to rival traditional depository banking, and was a factor in the subprime mortgage crisis of 2007–2008 and the global recession that followed.[8][9][10][2]  Paul McCulley of investment management firm PIMCO coined the term \"shadow banking\".[9] Shadow banking is sometimes said to include entities such as hedge funds, money market funds, structured investment vehicles (SIV), \"credit investment funds, exchange-traded funds, credit hedge funds, private equity funds, securities broker-dealers, credit insurance providers, securitization and finance companies.\"[11] Still, the meaning and scope of shadow banking are disputed in academic literature.[9] In China, shadow banking activities are closely associated with commercial banks, involving trust loans, entrusted loans, undiscounted bank acceptance bills, financial products, and interbank business. These activities are often regulated by a weak level and a limited coverage of supervision compared to highly regulated on-balance sheet activities.[12]  According to Hervé Hannoun, deputy general manager of the Bank for International Settlements (BIS), investment banks and commercial banks may conduct much of their business in the shadow banking system (SBS) although most are not classified as SBS institutions themselves.[13][14] At least one financial regulatory expert has said that regulated banking organizations are the largest shadow banks.[15]  The core activities of investment banks are subject to regulation and monitoring by central banks and other government institutions – but it has been common practice for investment banks to conduct many of their transactions in ways that do not show up on their conventional balance sheet accounting and so are not visible to regulators or unsophisticated investors. For example, before the 2007–2008 financial crisis, investment banks financed mortgages through off-balance-sheet (OBS) securitizations (e.g., asset-backed commercial paper programs) and hedged risk through off-balance sheet credit default swaps. Before the 2008 financial crisis, major investment banks were subject to considerably less stringent regulation than depository banks. In 2008, investment banks Morgan Stanley and Goldman Sachs became bank holding companies, Merrill Lynch and Bear Stearns were acquired by bank holding companies, and Lehman Brothers declared bankruptcy, essentially bringing the largest investment banks into the regulated depository sphere.  The volume of transactions in the shadow banking system grew dramatically after the year 2000. Its growth was checked by the 2007–2008 financial crisis and for a short while it declined in size, both in the US and in the rest of the world.[16][17] In 2007 the Financial Stability Board estimated the size of the SBS in the U.S. to be around $25 trillion, but by 2011 estimates indicated a decrease to $24 trillion.[18] Globally, a study of the 11 largest national shadow banking systems found that they totaled $50 trillion in 2007, fell to $47 trillion in 2008, but by late 2011 had climbed to $51 trillion, just over their estimated size before the crisis. Overall, the worldwide SBS totaled about $60 (~$80.2 trillion in 2023) trillion as of late 2011.[16] In November 2012 Bloomberg reported in a Financial Stability Board report an increase of the SBS to about $67 trillion.[19] It is unclear to what extent various measures of the shadow banking system include activities of regulated banks, such as bank borrowing in the repo market and the issuance of bank-sponsored asset-backed commercial paper.  Banks by far are the largest issuers of commercial paper in the United States, for example.  As of 2013[update], academic research has suggested that the true size of the shadow banking system may have been over $100 (~$131.00 in 2023) trillion in 2012.[20] According to the Financial Stability Board the sector's size grew to $100 (~$124.00 in 2023) trillion in 2016.[21]  The shadow activities of traditional banks grew rapidly in China during 2002-2018, but the United States and Canada still top the Average Shadow Banking Index by far.[22] In 2024, the amount US financial institutions have loaned to shadow banks surpassed the $1 trillion mark.[23][24]  Shadow institutions typically do not have banking licenses; they do not take deposits as a depository bank would and therefore are not subject to the same regulations.[9]  Complex legal entities comprising the system include hedge funds, structured investment vehicles (SIV), special purpose entity conduits (SPE), money market funds, repurchase agreement (repo) markets and other non-bank financial institutions.[25] Many shadow banking entities are sponsored by banks or are affiliated with banks through their subsidiaries or parent bank holding companies.[9] The inclusion of money market funds in the definition of shadow banking has been questioned in view of their relatively simple structure and the highly regulated and unleveraged nature of these entities, which are considered safer, more liquid, and more transparent than banks.  Shadow banking institutions are typically intermediaries between investors and borrowers. For example, an institutional investor like a pension fund may be willing to lend money, while a corporation may be searching for funds to borrow. The shadow banking institution will channel funds from the investor(s) to the corporation, profiting either from fees or from the difference in interest rates between what it pays the investor(s) and what it receives from the borrower.[9]  Hervé Hannoun, Deputy General Manager of the Bank for International Settlements described the structure of this shadow banking system at the annual conference of the South East Asian Central Banks Research and Training Centre (SEACEN).[13]  \"With the development of the originate-to-distribute model, banks and other lenders are able to extend loans to borrowers and then to package those loans into ABSs, CDOs, asset-backed commercial paper (ABCP) and structured investment vehicles (SIVs). These packaged securities are then sliced into various tranches, with the highly rated tranches going to the more risk-averse investors and the subordinate tranches going to the more adventurous investors.\" This sector was worth an estimated $60 trillion in 2010, compared to prior FSB estimates of $27 trillion in 2002.[18][26] While the sector's assets declined during the 2007–2008 financial crisis, they have since returned to their pre-crisis peak[27] except in the United States where they have declined substantially.  A 2013 paper by Fiaschi et al. used a statistical analysis based on the deviation from the Zipf distribution of the sizes of the world's largest financial entities to infer that the size of the shadow banking system may have been over $100 (~$131 billion in 2023) trillion in 2012.[20][28]  There are concerns that more business may move into the shadow banking system as regulators seek to bolster the financial system by making bank rules stricter.[27]  Like regular banks, shadow banks provide credit and generally increase the liquidity of the financial sector. Yet unlike their more regulated competitors, they lack access to central bank funding or safety nets such as deposit insurance and debt guarantees.[9][27][29] In contrast to traditional banks, shadow banks do not take deposits. Instead, they rely on short-term funding provided either by asset-backed commercial paper or by the repo market, in which borrowers in substance offer collateral as security against a cash loan, through the mechanism of selling the security to a lender and agreeing to repurchase it at an agreed time in the future for an agreed price.[27] Money market funds do not rely on short-term funding; rather, they are investment pools that provide short-term funding by investing in short-term debt instruments issued by banks, corporations, state and local governments, and other borrowers.  The shadow banking sector operates across the American, European, and Chinese financial sectors,[30][31] and in perceived tax havens worldwide.[27] Shadow banks can be involved in the provision of long-term loans like mortgages, facilitating credit across the financial system by matching investors and borrowers individually or by becoming part of a chain involving numerous entities, some of which may be mainstream banks.[27] Due in part to their specialized structure, shadow banks can sometimes provide credit more cost-efficiently than traditional banks.[27] A headline study by the International Monetary Fund defines the two key functions of the shadow banking system as securitization – to create safe assets, and collateral intermediation – to help reduce counterparty risks and facilitate secured transactions.[32] In the US, before the 2007–2008 financial crisis, the shadow banking system had overtaken the regular banking system in supplying loans to various types of borrower; including businesses, home and car buyers, students and credit users.[33] As they are often less risk averse than regular banks, entities from the shadow banking system will sometimes provide loans to borrowers who might otherwise be refused credit.[27] Money market funds are considered more risk averse than regular banks and thus lack this risk characteristic.  Leverage (the means by which banks multiply and spread risk) is considered to be a key risk feature of shadow banks, as well as traditional banks. Money market funds are completely unleveraged and thus do not have this risk characteristic. Shadow banking can threaten financial stability in countries where shadow banking is prevalent.[22]  The recommendations for G20 leaders on regulating shadow banks were due to be finalised by the end of 2012. The United States and the European Union are already considering rules to increase regulation of areas like securitisation and money market funds, although the need for money market fund reforms has been questioned in the United States in light of reforms adopted by the Securities and Exchange Commission in 2010. The International Monetary Fund suggested that the two policy priorities should be to reduce spillovers from the shadow banking system to the main banking system and to reduce procyclicality and systemic risk within the shadow banking system itself.[32]  The G20 leaders meeting in Russia in September 2013, will endorse the new Financial Stability Board (FSB) global regulations for the shadow banking systems which will come into effect by 2015.[11]  Many \"shadow bank\"-like institutions and vehicles have emerged in American and European markets, between the years 2000 and 2008, and have come to play an important role in providing credit across the global financial system.[34]  In a June 2008 speech, Timothy Geithner, then president and CEO of the Federal Reserve Bank of New York, described the growing importance of what he called the \"non-bank financial system\": \"In early 2007, asset-backed commercial paper conduits, in structured investment vehicles, in auction-rate preferred securities, tender option bonds and variable rate demand notes, had a combined asset size of roughly $2.2 trillion. Assets financed overnight in triparty repo grew to $2.5 trillion. Assets held in hedge funds grew to roughly $1.8 trillion. The combined balance sheets of the then five major investment banks totaled $4 trillion. In comparison, the total assets of the top five bank holding companies in the United States at that point were just over $6 trillion, and total assets of the entire banking system were about $10 trillion.\"[35]  In 2016, Benoît Cœuré (ECB executive board member) stated that controlling shadow banking should be the focus to avoid a future financial crisis, since the banks' leverage had been lowered.[36]  S&P Global estimates that, at end-2022, shadow banking held about $63 trillion in financial assets in major global jurisdictions, representing 78% of global GDP, up from $28 trillion and 68% of global GDP in 2009.[3]  Shadow institutions are not subject to the same prudential regulations as depository banks, so that they do not have to keep as high financial reserves relative to their market exposure. Thus they can have a very high level of financial leverage, with a high ratio of debt relative to the liquid assets available to pay immediate claims. High leverage magnifies profits during boom periods and losses during downturns.  This high leverage will also not be readily apparent to investors, and shadow institutions may therefore be able to create the appearance of superior performance during boom times by simply taking greater pro-cyclical risks. Money market funds have zero leverage and thus do not pose this risk feature of shadow banks.  Shadow institutions like SIVs and conduits, typically sponsored and guaranteed by commercial banks, borrowed from investors in short-term, liquid markets (such as the money market and commercial paper markets), so that they would have to repay and borrow again from these investors at frequent intervals. On the other hand, they used the funds to lend to corporations or to invest in longer-term, less liquid (i.e. harder to sell) assets. In many cases, the long-term assets purchased were mortgage-backed securities, sometimes called \"toxic assets\" or \"legacy assets\" in the press. These assets declined significantly in value as housing prices declined and foreclosures increased during 2007–2009.  In the case of investment banks, this reliance on short-term financing required them to return frequently to investors in the capital markets to refinance their operations. When the housing market began to deteriorate and their ability to obtain funds from investors through investments such as mortgage-backed securities declined, these investment banks could not refinance themselves. Investor refusal or inability to provide funds via the short-term markets was a primary cause of the failure of Bear Stearns and Lehman Brothers during 2008.[citation needed]  From a technical standpoint, these institutions are subject to market risk, credit risk and especially liquidity risk, since their liabilities are short term while their assets are more long term and illiquid. This creates a problem, as they are not depositary institutions and do not have direct or indirect access to the support of their central bank in its role as lender of last resort. Therefore, during periods of market illiquidity, they could go bankrupt if unable to refinance their short-term liabilities. They were also highly leveraged. This meant that disruptions in credit markets would make them subject to rapid deleveraging, meaning they would have to pay off their debts by selling their long-term assets.[37] A sell off of assets could cause further price declines of those assets and further losses and selloffs. In contrast to investment banks, money market funds do not go bankrupt—they distribute their assets (which are mainly short-term) pro rata to shareholders if their net asset value falls below $.9995 per share.  Only two funds ever have failed to pay investors $1.00 per share.  The Reserve Primary Fund paid $.99 per share to its shareholders and another fund paid its shareholders $.96 per share in 1994.[citation needed]  The securitization markets frequently tapped by the shadow banking system started to close down in the spring of 2007, with the first failure of auction-rate offerings to attract bids. As excesses associated with the U.S. housing bubble became widely understood and borrower default rates rose, residential mortgage-backed securities (RMBS) deflated.  Tranched collateralized debt obligations (CDOs) lost value as default rates increased beyond the levels projected by their associated agency credit ratings. Commercial mortgage-backed securities suffered from association and from a general decline in economic activity, and the entire complex nearly shut down in the fall of 2008. More than a third of the private credit markets thus became unavailable as a source of funds.[38] In February 2009, Ben Bernanke stated that securitization markets remained effectively shut, with the exception of conforming mortgages, which could be sold to Fannie Mae and Freddie Mac.[39]  U.S. Treasury Secretary Timothy Geithner has stated that the \"combined effect of these factors was a financial system vulnerable to self-reinforcing asset price and credit cycles.\"[35]  In January 2012, the global Financial Stability Board announced its intention to further regulate the shadow banking system, in the interests of the real economy.[40]  The term \"shadow banking system\" is attributed to Paul McCulley of PIMCO, who coined it at Federal Reserve Bank of Kansas City's Economic Symposium in Jackson Hole, Wyoming in 2007 where he defined it as \"the whole alphabet soup of levered up non-bank investment conduits, vehicles, and structures.\"[41][42][43] McCulley identified the birth of the shadow banking system with the development of money market funds in the 1970s – money market accounts function largely as bank deposits, but money market funds are not regulated as banks.[44]  The concept of hidden high priority debt dates back at least 400 years to Twyne's Case and the Statute of Bankrupts (1542) in the UK, and to Clow v. Woods[45] in the U.S.  These legal cases led to the development of modern fraudulent transfer law.  The concept of credit growth by unregulated institutions, though not the term \"shadow banking system\", dates at least to 1935, when Friedrich Hayek stated:  There can be no doubt that besides the regular types of the circulating medium, such as coin, notes and bank deposits, which are generally recognised to be money or currency, and the quantity of which is regulated by some central authority or can at least be imagined to be so regulated, there exist still other forms of media of exchange which occasionally or permanently do the service of money.... The characteristic peculiarity of these forms of credit is that they spring up without being subject to any central control, but once they have come into existence their convertibility into other forms of money must be possible if a collapse of credit is to be avoided.[46] The full extent of the shadow banking system was not widely recognised until work was published in 2010 by Manmohan Singh and James Aitken of the International Monetary Fund, showing that when the role of rehypothecation was considered, in the U.S. the SBS had grown to over $10 trillion, about twice as much as previous estimates.[47][48]  During 1998, the highly leveraged and unregulated hedge fund Long-Term Capital Management failed and was bailed out by several major banks at the request of the government, which was concerned about possible damage to the broader financial system.[49]  Structured investment vehicles (SIVs) first came to public attention at the time of the Enron scandal. Since then, their use has become widespread in the financial world. In the years leading up to the crisis, the top four U.S. depository banks moved an estimated $5.2 trillion in assets and liabilities off their balance sheets into special purpose vehicles (SPEs) or similar entities. This enabled them to bypass regulatory requirements for minimum capital adequacy ratios, thereby increasing leverage and profits during the boom but increasing losses during the crisis. New accounting guidance was planned to require them to put some of these assets back onto their books during 2009, with the effect of reducing their capital ratios. One news agency estimated the amount of assets to be transferred at between $500 billion and $1 trillion. This transfer was considered as part of the stress tests performed by the government during 2009.[50]  The shadow banking system also conducts an enormous amount of trading activity in the over-the-counter (OTC) derivatives market, which grew rapidly in the decade up to the 2007–2008 financial crisis, reaching over US$650 trillion in notional contracts traded.[51] This rapid growth mainly arose from credit derivatives. In particular these included:  The market in CDS, for example, was insignificant in 2004 but rose to over $60 trillion in a few years.[51] Because credit default swaps were not regulated as insurance contracts, companies selling them were not required to maintain sufficient capital reserves to pay potential claims. Demands for settlement of hundreds of billions of dollars of credit default swaps contracts issued by AIG, the largest insurance company in the world, led to its financial collapse. Despite the prevalence and volume of this activity, it attracted little outside attention before 2007, and much of it was off the balance sheets of the contracting parties' affiliated banks. The uncertainty this created among counterparties contributed to the deterioration of credit conditions.  Since then the shadow banking system has been blamed[34] for aggravating the subprime mortgage crisis and helping to transform it into a global credit crunch.[52]  The shadow banking system has been implicated as significantly contributing to the 2007–2008 financial crisis.[53][54] In a June 2008 speech, U.S. Treasury Secretary Timothy Geithner, then President and CEO of the New York Federal Reserve Bank, placed significant blame for the freezing of credit markets on a \"run\" on the entities in the shadow banking system by their counterparties. The rapid increase of the dependency of bank and non-bank financial institutions on the use of these off-balance sheet entities to fund investment strategies had made them critical to the credit markets underpinning the financial system as a whole, despite their existence in the shadows, outside of the regulatory controls governing commercial banking activity. Furthermore, these entities were vulnerable because they borrowed short-term in liquid markets to purchase long-term, illiquid and risky assets. This meant that disruptions in credit markets would make them subject to rapid deleveraging, selling their long-term assets at depressed prices.[35]  Economist Paul Krugman described the run on the shadow banking system as the \"core of what happened\" to cause the crisis. \"As the shadow banking system expanded to rival or even surpass conventional banking in importance, politicians and government officials should have realized that they were re-creating the kind of financial vulnerability that made the Great Depression possible—and they should have responded by extending regulations and the financial safety net to cover these new institutions. Influential figures should have proclaimed a simple rule: anything that does what a bank does, anything that has to be rescued in crises the way banks are, should be regulated like a bank.\" He referred to this lack of controls as \"malign neglect.\"[55][56]  One former banking regulator has said that regulated banking organizations are the largest shadow banks and that shadow banking activities within the regulated banking system were responsible for the severity of the 2007–2008 financial crisis.[15][57]  The Chinese shadow banks, such as Sichuan Trust, have been greatly affected by the property sector crisis due to over lending and a crackdown on regulations.[22][58] "},"meta":{},"created_at":"2025-03-22T14:25:42.273677Z","updated_at":"2025-03-22T14:25:42.273677Z","inner_id":8,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":17,"annotations":[{"id":17,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.309403Z","updated_at":"2025-03-22T14:25:42.309403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"ddac51c2-e612-427d-81c3-246b6fa5fd86","import_id":null,"last_action":null,"bulk_created":false,"task":17,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Cinematography (from Ancient Greek  κίνημα (kínēma) 'movement' and  γράφειν (gráphein) 'to write, draw, paint, etc.') is the art of motion picture (and more recently, electronic video camera) photography.  Cinematographers use a lens to focus reflected light from objects into a real image that is transferred to some image sensor or light-sensitive material inside the movie camera.[1] These exposures are created sequentially and preserved for later processing and viewing as a motion picture. Capturing images with an electronic image sensor produces an electrical charge for each pixel in the image, which is electronically processed and stored in a video file for subsequent processing or display. Images captured with photographic emulsion result in a series of invisible latent images on the film stock, which are chemically \"developed\" into a visible image. The images on the film stock are projected for viewing in the same motion picture.  Cinematography finds uses in many fields of science and business, as well as for entertainment purposes and mass communication.  In the 1830s, three different solutions for moving images were invented based on the concept of revolving drums and disks, the stroboscope by Simon von Stampfer in Austria, the phenakistoscope by Joseph Plateau in Belgium, and the zoetrope by William Horner in Britain.  In 1845, Francis Ronalds invented the first successful camera able to make continuous recordings of the varying indications of meteorological and geomagnetic instruments over time. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century.[2][3][4]  William Lincoln patented a device, in 1867 that showed animated pictures called the \"wheel of life\" or \"zoopraxiscope\". In it moving drawings or photographs were watched through a slit.  On 19 June 1878, Eadweard Muybridge successfully photographed a horse named \"Sallie Gardner\" in fast motion using a series of 24 stereoscopic cameras. The cameras were arranged along a track parallel to the horse's, and each camera shutter was controlled by a trip wire triggered by the horse's hooves. They were 21 inches apart to cover the 20 feet taken by the horse stride, taking pictures at one-thousandth of a second.[5] At the end of the decade, Muybridge had adapted sequences of his photographs to a zoopraxiscope for short, primitive projected \"movies\", which were sensations on his lecture tours by 1879 or 1880.  Four years later, in 1882, French scientist Étienne-Jules Marey invented a chronophotographic gun, which was capable of taking 12 consecutive frames a second and recording all the frames of the same picture.  The late nineteenth to the early twentieth centuries brought rise to the use of film not only for entertainment purposes but for scientific exploration as well. French biologist and filmmaker Jean Painleve lobbied heavily for the use of film in the scientific field, as the new medium was more efficient in capturing and documenting the behavior, movement, and environment of microorganisms, cells, and bacteria, than the naked eye.[6] The introduction of film into scientific fields allowed for not only the viewing of \"new images and objects, such as cells and natural objects, but also the viewing of them in real time\",[6] whereas prior to the invention of moving pictures, scientists and doctors alike had to rely on hand-drawn sketches of human anatomy and its microorganisms. This posed a great inconvenience to the scientific and medical worlds. The development of film and increased usage of cameras allowed doctors and scientists to grasp a better understanding and knowledge of their projects.[citation needed]  The origins of today's cinema go back to the Lumière brothers, Auguste and Louis, who in 1895 developed a machine called the Cinematographe, which had the ability to capture and show moving images.  The early era of cinema saw rapid innovation. In the early-to-mid-20th Centrury, filmmakers discovered and applied new methods such as editing, special effects, close-ups, sound, widescreen, color films, and more. Hollywood began to emerge as the Mecca of the film industry, and many of the famous studios from that time still exist over 100 years later in the early-mid 21st Century.  The experimental film Roundhay Garden Scene, filmed by Louis Le Prince  in Roundhay, Leeds, England, on October 14, 1888, is the earliest surviving motion picture.[7] This movie was shot on paper film.[8]  An experimental film camera was developed by British inventor William Friese Greene and patented in 1889.[9] W. K. L. Dickson, working under the direction of Thomas Alva Edison, was the first to design a successful apparatus, the Kinetograph,[10] patented in 1891.[11] This camera took a series of instantaneous photographs on standard Eastman Kodak photographic emulsion coated onto a transparent celluloid strip 35 mm wide. The results of this work were first shown in public in 1893, using the viewing apparatus also designed by Dickson, the Kinetoscope. Contained within a large box, only one person at a time looking into it through a peephole could view the movie.  In the following year, Charles Francis Jenkins and his projector, the Phantoscope,[12] made a successful audience viewing while Louis and Auguste Lumière perfected the Cinématographe, an apparatus that took, printed, and projected film, in Paris in December 1895.[13] The Lumière brothers were the first to present projected, moving, photographic, pictures to a paying audience of more than one person.  In 1896, movie theaters were open in France (Paris, Lyon, Bordeaux, Nice, Marseille); Italy (Rome, Milan, Naples, Genoa, Venice, Bologna, Forlì); Brussels; and London. The chronological improvements in the medium may be listed concisely. In 1896, Edison showed his improved Vitascope projector, the first commercially successful projector in the U.S. Cooper Hewitt invented mercury lamps which made it practical to shoot films indoors without sunlight in 1905. The first animated cartoon was produced in 1906. Credits began to appear at the beginning of motion pictures in 1911. The Bell and Howell 2709 movie camera invented in 1915 allowed directors to make close-ups without physically moving the camera. By the late 1920s, most of the movies produced were sound films. Wide screen formats were first experimented within the 1950s. By the 1970s, most movies were color films. IMAX and other 70mm formats gained popularity. Wide distribution of films became commonplace, setting the ground for \"blockbusters.\" Film cinematography dominated the motion picture industry from its inception until the 2010s when digital cinematography became dominant. Film cinematography is still used by some directors, especially in specific applications or out of fondness for the format.[citation needed]  From its birth in the 1880s, movies were predominantly monochrome. Contrary to popular belief, monochrome does not always mean black-and-white; it means a movie shot in a single tone or color. Since the cost of tinted film bases was substantially higher, most movies were produced in black-and-white monochrome. Even with the advent of early color experiments, the greater expense of color meant films were mostly made in black-and-white until the 1950s, when cheaper color processes were introduced, and in some years percentage of films shot on color film surpassed 51%. By the 1960s, color became by far the dominant film stock. In the coming decades, the usage of color film greatly increased while monochrome films became scarce.  Black-and-white cinematography is a technique used in filmmaking where the images are captured and presented in shades of gray, without color. This artistic approach has a rich history and has been employed in various films throughout cinema's evolution. It is a powerful tool that allows filmmakers to emphasize contrast, texture, and lighting, enhancing the visual storytelling experience. The use of black-and-white cinematography dates back to the early days of cinema when color film was not yet available. Filmmakers relied on this technique to create visually striking and atmospheric films. Even with the advent of color film technology, black-and-white cinematography continued to be utilized for artistic and thematic purposes.  Ken Dancyger's book The Technique of Film and Video Editing: History, Theory, and Practice provides valuable insights into the historical and theoretical aspects of black-and-white cinematography. Dancyger explores how this technique has been employed throughout film history, examining its impact on storytelling, mood, and visual aesthetics. The book delves into the artistic choices and technical considerations involved in creating compelling black-and-white imagery, offering a comprehensive understanding of the technique.  Black-and-white cinematography allows filmmakers to focus on the interplay of light and shadow, emphasizing the contrast between different elements within a scene. This technique can evoke a sense of nostalgia, evoke a specific time period, or create a timeless and classic feel. By stripping away color, filmmakers can emphasize the composition, shapes, and textures within the frame, enhancing the visual impact. Notable films that have employed black-and-white cinematography include classics such as Casablanca (1942), Raging Bull (1980), and Schindler's List (1993). These films showcase the power and versatility of black-and-white cinematography in creating emotionally resonant visuals. Black-and-white cinematography remains a relevant and widely used technique in modern filmmaking. It continues to be employed by filmmakers to evoke specific moods, convey a sense of timelessness, and enhance the artistic expression of their stories.  After the advent of motion pictures, a tremendous amount of energy was invested in the production of photography in natural color.[14] The invention of the talking picture further increased the demand for the use of color photography. However, in comparison to other technological advances of the time, the arrival of color photography was a relatively slow process.[15]  Early movies were not actually color movies since they were shot monochrome and hand-colored or machine-colored afterward (such movies are referred to as colored and not color). The earliest such example is the hand-tinted Annabelle Serpentine Dance in 1895 by Edison Manufacturing Company. Machine-based tinting later became popular. Tinting continued until the advent of natural color cinematography in the 1910s. Many black-and-white movies have been colorized recently using digital tinting. This includes footage shot from both world wars, sporting events and political propaganda.[citation needed]  In 1902, Edward Raymond Turner produced the first films with a natural color process rather than using colorization techniques.[16] In 1909, Kinemacolor was first shown to the public.[17]  In 1917, the earliest version of Technicolor was introduced. Kodachrome was introduced in 1935. Eastmancolor was introduced in 1950 and became the color standard for the rest of the century.[citation needed]  In the 2010s, color films were largely superseded by color digital cinematography.[citation needed]  In digital cinematography, the movie is shot on digital media such as flash storage, as well as distributed through a digital medium such as a hard drive.  The basis for digital cameras are metal–oxide–semiconductor (MOS) image sensors.[18] The first practical semiconductor image sensor was the charge-coupled device (CCD),[19] based on MOS capacitor technology.[18] Following the commercialization of CCD sensors during the late 1970s to early 1980s, the entertainment industry slowly began transitioning to digital imaging and digital video over the next two decades.[20] The CCD was followed by the CMOS active-pixel sensor (CMOS sensor),[21] developed in the 1990s.[22][23]  Beginning in the late 1980s, Sony began marketing the concept of \"electronic cinematography\", utilizing its analog Sony HDVS professional video cameras. The effort met with very little success. However, this led to one of the earliest digitally shot feature movies, Julia and Julia (1987).[citation needed] In 1998, with the introduction of HDCAM recorders and 1920×1080 pixel digital professional video cameras based on CCD technology, the idea, now re-branded as \"digital cinematography\", began to gain traction.[citation needed]  Shot and released in 1998, The Last Broadcast is believed by some to be the first feature-length video shot and edited entirely on consumer-level digital equipment.[24] In May 1999, George Lucas challenged the supremacy of the movie-making medium of film for the first time by including footage filmed with high-definition digital cameras in Star Wars: Episode I – The Phantom Menace. In late 2013, Paramount became the first major studio to distribute movies to theaters in digital format, eliminating 35mm film entirely. Since then the demand of movies to be developed onto digital format rather than 35mm has increased significantly.[citation needed]  As digital technology improved, movie studios began increasingly shifting toward digital cinematography. Since the 2010s, digital cinematography has become the dominant form of cinematography after largely superseding film cinematography.[citation needed]  Numerous aspects contribute to the art of cinematography, including:  The first film cameras were fastened directly to the head of a tripod or other support, with only the crudest kind of leveling devices provided, in the manner of the still-camera tripod heads of the period. The earliest film cameras were thus effectively fixed during the shot, and hence the first camera movements were the result of mounting a camera on a moving vehicle. The first known of these was a film shot by a Lumière cameraman from the back platform of a train leaving Jerusalem in 1896, and by 1898, there were a number of films shot from moving trains. Although listed under the general heading of \"panoramas\" in the sales catalogues of the time, those films shot straight forward from in front of a railway engine were usually specifically referred to as \"phantom rides\".  In 1897, Robert W. Paul had the first real rotating camera head made to put on a tripod, so that he could follow the passing processions of Queen Victoria's Diamond Jubilee in one uninterrupted shot. This device had the camera mounted on a vertical axis that could be rotated by a worm gear driven by turning a crank handle, and Paul put it on general sale the next year. Shots taken using such a \"panning\" head were also referred to as \"panoramas\" in the film catalogues of the first decade of the cinema. This eventually led to the creation of a panoramic photo as well.  The standard pattern for early film studios was provided by the studio which Georges Méliès had built in 1897. This had a glass roof and three glass walls constructed after the model of large studios for still photography, and it was fitted with thin cotton cloths that could be stretched below the roof to diffuse the direct ray of the sun on sunny days. The soft overall light without real shadows that this arrangement produced, which also exists naturally on lightly overcast days, was to become the basis for film lighting in film studios for the next decade.  Black-and-white cinematography is a technique used in filmmaking where the images are captured and presented in shades of gray, without color. This artistic approach has a rich history and has been employed in various films throughout cinema's evolution. It is a powerful tool that allows filmmakers to emphasize contrast, texture, and lighting, enhancing the visual storytelling experience. The use of black-and-white cinematography dates back to the early days of cinema when color film was not yet available. Filmmakers relied on this technique to create visually striking and atmospheric films. Even with the advent of color film technology, black-and-white cinematography continued to be utilized for artistic and thematic purposes. Ken Dancyger's book The Technique of Film and Video Editing: History, Theory, and Practice provides valuable insights into the historical and theoretical aspects of black-and-white cinematography. Dancyger explores how this technique has been employed throughout film history, examining its impact on storytelling, mood, and visual aesthetics. The book delves into the artistic choices and technical considerations involved in creating compelling black-and-white imagery, offering a comprehensive understanding of the technique.  Black-and-white cinematography allows filmmakers to focus on the interplay of light and shadow, emphasizing the contrast between different elements within a scene. This technique can evoke a sense of nostalgia, evoke a specific time period, or create a timeless and classic feel. By stripping away color, filmmakers can emphasize the composition, shapes, and textures within the frame, enhancing the visual impact. Notable films that have employed black-and-white cinematography include classics such as Casablanca (1942), Raging Bull (1980), and Schindler's List (1993). These films showcase the power and versatility of black-and-white cinematography in creating emotionally resonant visuals. Black-and-white cinematography remains a relevant and widely used technique in modern filmmaking. It continues to be employed by filmmakers to evoke specific moods, convey a sense of timelessness, and enhance the artistic expression of their stories.  There are many types of Cinematography that each differ based on production purpose and process. These different types of Cinematography are similar in the sense that they all have the goal of conveying a specific emotion, mood or feeling. For each different style however they can often convey different emotions and purposes. Some examples of different types of Cinematography can be known as Realism. This style of cinematography aims to create a realistic portrayal of the world, often using natural lighting, handheld cameras, and a documentary-like approach to filming. Classic Hollywood is a style of cinematography characterized by its use of highly polished, studio-produced films with glamorous sets, bright lighting, and romanticized narratives. Film Noir is a style of cinematography that is characterized by its use of stark contrast and chiaroscuro lighting, low-key lighting, and a dark, brooding atmosphere. It often features crime, mystery, and morally ambiguous characters.  To convey mood, emotion, narrative and other factors within the shot, cinematography is implemented by using different aspects within a film.  Lighting on the scene can affect the mood of a scene or film. Darker shots with less natural light can be gloomy, scary, sad, intense. Brighter lighting can equate to a happier, exciting, more positive mood. Camera angle can affect a scene by setting perspective. It conveys how characters or the audience see something, and through what angle.  Camera angle can also play an important role by highlighting either a close up detail, or background setting. A close up angle can highlight detail on someone's face, while a wider lens can give key information that takes place in the background of a shot. Camera distance can highlight specific details that can be important to a film shot. From very far away, a group of people can all look the same, but once you zoom in very close,  the viewer is able to see differences within the population through details like facial expression and body language. Coloring is similar to lighting, in the way that it plays an important role in setting mood and emotion throughout a shot. A color like green can convey balance and peace through scenes of nature. A shot with a lot of red can express anger, intensity, passion or love. While some of these emotions might not come out intentionally while seeing color, it is a subconscious fact that color within cinematography can have a large effect. Speed is a vital element in cinematography that can be used in a variety of ways, such as the creation of action, or a sense of movement. Speed can be further used to slow down time, highlight important moments, and often times build a sense of suspense in a film. Slow motion is a technique which involves filming at a higher frame rate, then playing the footage again at a normal speed. This creates a slowed-down effect in the film, which can put emphasis on or add fluidity to a scene. On the other hand, fast motion is the opposite of slow motion, filming at a lower frame rate and then playing the film back at a normal speed. This creates a sped-up effect which can help to emphasize passage of time, or create a sense of urgency. Time lapse is when you take a series of still photographs at a regular interval over a long period of time. From here, if you play them back continuously, a sped-up effect is shown. Time lapses are used most effectively to show things like sunrises, natural movement, or growth. They are commonly used to show passage of time in a shorter sequence. Reverse motion is filming a scene normally, then playing the film in reverse. This is usually used to create uncommon\/surreal effects, and create unusual scenes. The various techniques involving speed all can add to a films intensity, vibe, show passage of time, and have many other effects. Camera movement within a film can play a role in enhancing the visual quality and impact of a film. Some aspects of camera movement that contribute to this are:  Cinematography can begin with digital image sensor or rolls of film. Advancements in film emulsion and grain structure provided a wide range of available film stocks. The selection of a film stock is one of the first decisions made in preparing a typical film production.  Aside from the film gauge selection – 8 mm (amateur), 16 mm (semi-professional), 35 mm (professional) and 65 mm (epic photography, rarely used except in special event venues) – the cinematographer has a selection of stocks in reversal (which, when developed, create a positive image) and negative formats along with a wide range of film speeds (varying sensitivity to light) from ISO 50 (slow, least sensitive to light) to 800 (very fast, extremely sensitive to light) and differing response to color (low saturation, high saturation) and contrast (varying levels between pure black (no exposure) and pure white (complete overexposure). Advancements and adjustments to nearly all gauges of film create the \"super\" formats wherein the area of the film used to capture a single frame of an image is expanded, although the physical gauge of the film remains the same. Super 8 mm, Super 16 mm, and Super 35 mm all utilize more of the overall film area for the image than their \"regular\" non-super counterparts. The larger the film gauge, the higher the overall image resolution clarity and technical quality. The techniques used by the film laboratory to process the film stock can also offer a considerable variance in the image produced. By controlling the temperature and varying the duration in which the film is soaked in the development chemicals, and by skipping certain chemical processes (or partially skipping all of them), cinematographers can achieve very different looks from a single film stock in the laboratory. Some techniques that can be used are push processing, bleach bypass, and cross processing.  Most of modern cinema uses digital cinematography and has no film stocks [citation needed], but the cameras themselves can be adjusted in ways that go far beyond the abilities of one particular film stock. They can provide varying degrees of color sensitivity, image contrast, light sensitivity and so on. One camera can achieve all the various looks of different emulsions. Digital image adjustments such as ISO and contrast are executed by estimating the same adjustments that would take place if actual film were in use, and are thus vulnerable to the camera's sensor designers perceptions of various film stocks and image adjustment parameters.  Filters, such as diffusion filters or color effect filters, are also widely used to enhance mood or dramatic effects. Most photographic filters are made up of two pieces of optical glass glued together with some form of image or light manipulation material between the glass. In the case of color filters, there is often a translucent color medium pressed between two planes of optical glass. Color filters work by blocking out certain color wavelengths of light from reaching the film. With color film, this works very intuitively wherein a blue filter will cut down on the passage of red, orange, and yellow light and create a blue tint on the film. In black-and-white photography, color filters are used somewhat counter-intuitively; for instance, a yellow filter, which cuts down on blue wavelengths of light, can be used to darken a daylight sky (by eliminating blue light from hitting the film, thus greatly underexposing the mostly blue sky) while not biasing most human flesh tone. Filters can be used in front of the lens or, in some cases, behind the lens for different effects.  Certain cinematographers, such as Christopher Doyle, are well known for their innovative use of filters; Doyle was a pioneer for increased usage of filters in movies and is highly respected throughout the cinema world.  Lenses can be attached to the camera to give a certain look, feel, or effect by focus, color, etc. As does the human eye, the camera creates perspective and spatial relations with the rest of the world. However, unlike one's eye, a cinematographer can select different lenses for different purposes. Variation in focal length is one of the chief benefits. The focal length of the lens determines the angle of view and, therefore, the field of view. Cinematographers can choose from a range of wide-angle lenses, \"normal\" lenses and long focus lenses, as well as macro lenses and other special effect lens systems such as borescope lenses. Wide-angle lenses have short focal lengths and make spatial distances more obvious. A person in the distance is shown as much smaller while someone in the front will loom large. On the other hand, long focus lenses reduce such exaggerations, depicting far-off objects as seemingly close together and flattening perspective. The differences between the perspective rendering is actually not due to the focal length by itself, but by the distance between the subjects and the camera. Therefore, the use of different focal lengths in combination with different camera to subject distances creates these different rendering. Changing the focal length only while keeping the same camera position does not affect perspective but the camera angle of view only.  A zoom lens allows a camera operator to change his focal length within a shot or quickly between setups for shots. As prime lenses offer greater optical quality and are \"faster\" (larger aperture openings, usable in less light) than zoom lenses, they are often employed in professional cinematography over zoom lenses. Certain scenes or even types of filmmaking, however, may require the use of zooms for speed or ease of use, as well as shots involving a zoom move.  As in other photography, the control of the exposed image is done in the lens with the control of the diaphragm aperture. For proper selection, the cinematographer needs that all lenses be engraved with T-stop, not f-stop so that the eventual light loss due to the glass does not affect the exposure control when setting it using the usual meters. The choice of the aperture also affects image quality (aberrations) and depth of field.  Focal length and diaphragm aperture affect the depth of field of a scene – that is, how much the background, mid-ground and foreground will be rendered in \"acceptable focus\" (only one exact plane of the image is in precise focus) on the film or video target. Depth of field (not to be confused with depth of focus) is determined by the aperture size and the focal distance. A large or deep depth of field is generated with a very small iris aperture and focusing on a point in the distance, whereas a shallow depth of field will be achieved with a large (open) iris aperture and focusing closer to the lens. Depth of field is also governed by the format size. If one considers the field of view and angle of view, the smaller the image is, the shorter the focal length should be, as to keep the same field of view. Then, the smaller the image is, the more depth of field is obtained, for the same field of view. Therefore, 70mm has less depth of field than 35mm for a given field of view, 16mm more than 35mm, and early video cameras, as well as most modern consumer level video cameras, even more depth of field than 16mm.  In Citizen Kane (1941), cinematographer Gregg Toland and director Orson Welles used tighter apertures to create every detail of the foreground and background of the sets in sharp focus. This practice is known as deep focus. Deep focus became a popular cinematographic device from the 1940s onward in Hollywood. Today, the trend is for more shallow focus. To change the plane of focus from one object or character to another within a shot is commonly known as a rack focus.  Early in the transition to digital cinematography, the inability of digital video cameras to easily achieve shallow depth of field, due to their small image sensors, was initially an issue of frustration for film makers trying to emulate the look of 35mm film. Optical adapters were devised which accomplished this by mounting a larger format lens which projected its image, at the size of the larger format, on a ground glass screen preserving the depth of field. The adapter and lens then mounted on the small format video camera which in turn focused on the ground glass screen.  Digital SLR still cameras have sensor sizes similar to that of the 35mm film frame, and thus are able to produce images with similar depth of field. The advent of video functions in these cameras sparked a revolution in digital cinematography, with more and more film makers adopting still cameras for the purpose because of the film-like qualities of their images. More recently, more and more dedicated video cameras are being equipped with larger sensors capable of 35mm film-like depth of field.  The aspect ratio of an image is the ratio of its width to its height. This can be expressed either as a ratio of 2 integers, such as 4:3, or in a decimal format, such as 1.33:1 or simply 1.33. Different ratios provide different aesthetic effects. Standards for aspect ratio have varied significantly over time.  During the silent era, aspect ratios varied widely, from square 1:1, all the way up to the extreme widescreen 4:1 Polyvision. However, from the 1910s, silent motion pictures generally settled on the ratio of 4:3 (1.33). The introduction of sound-on-film briefly narrowed the aspect ratio, to allow room for a sound stripe. In 1932, a new standard was introduced, the Academy ratio of 1.37, by means of thickening the frame line.  For years, mainstream cinematographers were limited to using the Academy ratio, but in the 1950s, thanks to the popularity of Cinerama, widescreen ratios were introduced in an effort to pull audiences back into the theater and away from their home television sets. These new widescreen formats provided cinematographers a wider frame within which to compose their images.  Many different proprietary photographic systems were invented and used in the 1950s to create widescreen movies, but one dominated film: the anamorphic process, which optically squeezes the image to photograph twice the horizontal area to the same size vertical as standard \"spherical\" lenses. The first commonly used anamorphic format was CinemaScope, which used a 2.35 aspect ratio, although it was originally 2.55. CinemaScope was used from 1953 to 1967, but due to technical flaws in the design and its ownership by Fox, several third-party companies, led by Panavision's technical improvements in the 1950s, dominated the anamorphic cine lens market. Changes to SMPTE projection standards altered the projected ratio from 2.35 to 2.39 in 1970, although this did not change anything regarding the photographic anamorphic standards; all changes in respect to the aspect ratio of anamorphic 35 mm photography are specific to camera or projector gate sizes, not the optical system. After the \"widescreen wars\" of the 1950s, the motion-picture industry settled into 1.85 as a standard for theatrical projection in the United States and the United Kingdom. This is a cropped version of 1.37. Europe and Asia opted for 1.66 at first, although 1.85 has largely permeated these markets in recent decades. Certain \"epic\" or adventure movies utilized the anamorphic 2.39 (often incorrectly denoted '2.40')  In the 1990s, with the advent of high-definition video, television engineers created the 1.78 (16:9) ratio as a mathematical compromise between the theatrical standard of 1.85 and television's 1.33, as it was not practical to produce a traditional CRT television tube with a width of 1.85. Until that change, nothing had ever been originated in 1.78. Today, this is a standard for high-definition video and for widescreen television.  Light is necessary to create an image exposure on a frame of film or on a digital target (CCD, etc.). The art of lighting for cinematography goes far beyond basic exposure, however, into the essence of visual storytelling. Lighting contributes considerably to the emotional response an audience has watching a motion picture. The increased usage of filters can greatly impact the final image and affect the lighting.  Importance of Lighting in Film Lighting in film is essential for three primary reasons: visibility, composition, and mood. Firstly, lighting ensures that the subject or scene is properly illuminated, allowing viewers to perceive the details and understand the narrative. It helps in guiding the audience's attention to specific elements within the frame, highlighting important characters or objects. Secondly, lighting contributes to the composition of a shot. Filmmakers strategically place lights to create balance, depth, and visual interest within the frame. It allows them to control the visual elements within the scene, emphasizing certain areas and de-emphasizing others. Lastly, lighting significantly impacts the mood and atmosphere of a film. By manipulating light intensity, color, and direction, filmmakers can evoke different emotions and enhance the narrative. Bright, even lighting may evoke a sense of safety and happiness, while low-key lighting with shadows can create tension, mystery, or fear. The choice of lighting style can also reflect the genre of the film, such as the high contrast lighting commonly used in film noir.  Numerous lighting techniques are employed in filmmaking to achieve desired effects. Here are some commonly used techniques: Three-Point Lighting: This classic technique involves the use of three lights: the key light, fill light, and backlight. The key light serves as the primary source, illuminating the subject from one side to create depth and dimension. The fill light reduces shadows caused by the key light, softening the overall lighting. The backlight separates the subject from the background, providing a halo effect and enhancing the sense of depth. High Key Lighting: High key lighting produces a bright, evenly lit scene, often used in comedies or light-hearted films. It minimizes shadows, creating a cheerful and upbeat atmosphere. Low Key Lighting: Low key lighting involves using a single key light or a few strategically placed lights to create strong contrasts and deep shadows. This technique is commonly used in film noir and horror genres to evoke suspense, mystery, or fear.  Filmmakers sometimes employ natural lighting to create an authentic, realistic look. This technique utilizes existing light sources, such as sunlight or practical lamps, without additional artificial lighting. It is often seen in outdoor scenes or films aiming for a naturalistic aesthetic. Color Lighting: The use of colored lights or gels can dramatically alter the mood and atmosphere of a scene. Different colors evoke different emotions and can enhance storytelling. For example, warm tones like red or orange may create a sense of warmth or passion, while cool tones like blue can convey sadness or isolation.  Cinematography can not only depict a moving subject but can use a camera, which represents the audience's viewpoint or perspective, that moves during the course of filming. This movement plays a considerable role in the emotional language of film images and the audience's emotional reaction to the action. Techniques range from the most basic movements of panning (horizontal shift in viewpoint from a fixed position; like turning your head side-to-side) and tilting (vertical shift in viewpoint from a fixed position; like tipping your head back to look at the sky or down to look at the ground) to dollying (placing the camera on a moving platform to move it closer or farther from the subject), tracking (placing the camera on a moving platform to move it to the left or right), craning (moving the camera in a vertical position; being able to lift it off the ground as well as swing it side-to-side from a fixed base position), and combinations of the above. Early cinematographers often faced problems that were not common to other graphic artists because of the element of motion.[25]  Cameras have been mounted to nearly every imaginable form of transportation. Most cameras can also be handheld, that is held in the hands of the camera operator who moves from one position to another while filming the action. Personal stabilizing platforms came into being in the late 1970s through the invention of Garrett Brown, which became known as the Steadicam. The Steadicam is a body harness and stabilization arm that connects to the camera, supporting the camera while isolating it from the operator's body movements. After the Steadicam patent expired in the early 1990s, many other companies began manufacturing their concept of the personal camera stabilizer. This invention is much more common throughout the cinematic world today. From feature-length films to the evening news, more and more networks have begun to use a personal camera stabilizer.  The first special effects in the cinema were created while the film was being shot. These came to be known as \"in-camera\" effects. Later, optical and digital effects were developed so that editors and visual effects artists could more tightly control the process by manipulating the film in post-production.  The 1896 movie The Execution of Mary Stuart shows an actor dressed as the queen placing her head on the execution block in front of a small group of bystanders in Elizabethan dress. The executioner brings his axe down, and the queen's severed head drops onto the ground. This trick was worked by stopping the camera and replacing the actor with a dummy, then restarting the camera before the axe falls. The two pieces of film were then trimmed and cemented together so that the action appeared continuous when the film was shown, thus creating an overall illusion and successfully laying the foundation for special effects.  This film was among those exported to Europe with the first Kinetoscope machines in 1895 and was seen by Georges Méliès, who was putting on magic shows in his Théâtre Robert-Houdin in Paris at the time. He took up filmmaking in 1896, and after making imitations of other films from Edison, Lumière, and Robert Paul, he made Escamotage d'un dame chez Robert-Houdin (The Vanishing Lady). This film shows a woman being made to vanish by using the same stop motion technique as the earlier Edison film. After this, Georges Méliès made many single shot films using this trick over the next couple of years.  The other basic technique for trick cinematography involves double exposure of the film in the camera, which was first done by George Albert Smith in July 1898 in the UK. Smith's The Corsican Brothers (1898) was described in the catalogue of the Warwick Trading Company, which took up the distribution of Smith's films in 1900, thus:  \"One of the twin brothers returns home from shooting in the Corsican mountains, and is visited by the ghost of the other twin. By extremely careful photography the ghost appears *quite transparent*. After indicating that he has been killed by a sword-thrust, and appealing for vengeance, he disappears. A 'vision' then appears showing the fatal duel in the snow. To the Corsican's amazement, the duel and death of his brother are vividly depicted in the vision, and overcome by his feelings, he falls to the floor just as his mother enters the room.\"  The ghost effect was done by draping the set in black velvet after the main action had been shot, and then re-exposing the negative with the actor playing the ghost going through the actions at the appropriate part. Likewise, the vision, which appeared within a circular vignette or matte, was similarly superimposed over a black area in the backdrop to the scene, rather than over a part of the set with detail in it, so that nothing appeared through the image, which seemed quite solid. Smith used this technique again in Santa Claus (1898).  Georges Méliès first used superimposition on a dark background in La Caverne maudite (The Cave of the Demons) made a couple of months later in 1898,[citation needed] and elaborated it with many superimpositions in the one shot in Un Homme de têtes (The Four Troublesome Heads). He created further variations in subsequent films.  Motion picture images are presented to an audience at a constant speed. In the theater it is 24 frames per second, in NTSC (US) Television it is 30 frames per second (29.97 to be exact), in PAL (Europe) television it is 25 frames per second. This speed of presentation does not vary.  However, by varying the speed at which the image is captured, various effects can be created knowing that the faster or slower recorded image will be played at a constant speed. Giving the cinematographer even more freedom for creativity and expression to be made.  For instance, time-lapse photography is created by exposing an image at an extremely slow rate. If a cinematographer sets a camera to expose one frame every minute for four hours, and then that footage is projected at 24 frames per second, a four-hour event will take 10 seconds to present, and one can present the events of a whole day (24 hours) in just one minute.  The inverse of this, if an image is captured at speeds above that at which they will be presented, the effect is to greatly slow down (slow motion) the image. If a cinematographer shoots a person diving into a pool at 96 frames per second, and that image is played back at 24 frames per second, the presentation will take 4 times as long as the actual event. Extreme slow motion, capturing many thousands of frames per second can present things normally invisible to the human eye, such as bullets in flight and shockwaves travelling through media, a potentially powerful cinematographic technique.  In motion pictures, the manipulation of time and space is a considerable contributing factor to the narrative storytelling tools. Film editing plays a much stronger role in this manipulation, but frame rate selection in the photography of the original action is also a contributing factor to altering time. For example, Charlie Chaplin's Modern Times was shot at \"silent speed\" (18 fps) but projected at \"sound speed\" (24 fps), which makes the slapstick action appear even more frenetic.  Speed ramping, or simply \"ramping\", is a process whereby the capture frame rate of the camera changes over time. For example, if in the course of 10 seconds of capture, the capture frame rate is adjusted from 60 frames per second to 24 frames per second, when played back at the standard movie rate of 24 frames per second, a unique time-manipulation effect is achieved. For example, someone pushing a door open and walking out into the street would appear to start off in slow-motion, but in a few seconds later within the same shot, the person would appear to walk in \"realtime\" (normal speed). The opposite speed-ramping is done in The Matrix when Neo re-enters the Matrix for the first time to see the Oracle. As he comes out of the warehouse \"load-point\", the camera zooms into Neo at normal speed but as it gets closer to Neo's face, time seems to slow down, foreshadowing the manipulation of time itself within the Matrix later in the movie.  G. A. Smith initiated the technique of reverse motion and also improved the quality of self-motivating images. This he did by repeating the action a second time while filming it with an inverted camera and then joining the tail of the second negative to that of the first. The first films using this were Tipsy, Topsy, Turvy, and The Awkward Sign Painter, the latter which showed a sign painter lettering a sign, and then the painting on the sign vanishing under the painter's brush. The earliest surviving example of this technique is Smith's The House That Jack Built, made before September 1901. Here, a small boy is shown knocking down a castle just constructed by a little girl out of children's building blocks. A title then appears, saying \"Reversed\", and the action is repeated in reverse so that the castle re-erects itself under his blows.  Cecil Hepworth improved upon this technique by printing the negative of the forward motion backward, frame by frame, so that in the production of the print the original action was exactly reversed. Hepworth made The Bathers in 1900 in which bathers who have undressed and jumped into the water appear to spring backward out of it, and have their clothe magically fly back onto their bodies.  The use of different camera speeds also appeared around 1900. Robert Paul's On a Runaway Motor Car through Piccadilly Circus (1899), had the camera turn so slowly that when the film was projected at the usual 16 frames per second, the scenery appeared to be passing at great speed. Cecil Hepworth used the opposite effect in The Indian Chief and the Seidlitz powder (1901), in which a naïve Red Indian eats a lot of the fizzy stomach medicine, causing his stomach to expand and then he then leaps around balloon-like. This was done by cranking the camera faster than the normal 16 frames per second giving the first \"slow motion\" effect.  In descending order of seniority, the following staff is involved:  In the film industry, the cinematographer is responsible for the technical aspects of the images (lighting, lens choices, composition, exposure, filtration, film selection), but works closely with the director to ensure that the artistic aesthetics are supporting the director's vision of the story being told. The cinematographers are the heads of the camera, grip and lighting crew on a set, and for this reason, they are often called directors of photography or DPs. The American Society of Cinematographers defines cinematography as a creative and interpretive process that culminates in the authorship of an original work of art rather than the simple recording of a physical event. Cinematography is not a subcategory of photography. Rather, photography is but one craft that the cinematographer uses in addition to other physical, organizational, managerial, interpretive. and image-manipulating techniques to effect one coherent process.[26] In British tradition, if the DOP actually operates the camera him\/herself they are called the cinematographer. On smaller productions, it is common for one person to perform all these functions alone. The career progression usually involves climbing up the ladder from seconding, firsting, eventually to operating the camera.  Directors of photography make many interpretive decisions during the course of their work, from pre-production to post-production, all of which affect the overall feel and look of the motion picture. Many of these decisions are similar to what a photographer needs to note when taking a picture: the cinematographer controls the film choice itself (from a range of available stocks with varying sensitivities to light and color), the selection of lens focal lengths, aperture exposure and focus. Cinematography, however, has a temporal aspect (see persistence of vision), unlike still photography, which is purely a single still image. It is also bulkier and more strenuous to deal with movie cameras, and it involves a more complex array of choices. As such a cinematographer often needs to work cooperatively with more people than does a photographer, who could frequently function as a single person. As a result, the cinematographer's job also includes personnel management and logistical organization. Given the in-depth knowledge, a cinematographer requires not only of his or her own craft but also that of other personnel, formal tuition in analogue or digital filmmaking can be advantageous.[27] "},"meta":{},"created_at":"2025-03-22T14:25:42.273677Z","updated_at":"2025-03-22T14:25:42.273677Z","inner_id":9,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":18,"annotations":[{"id":18,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.309403Z","updated_at":"2025-03-22T14:25:42.309403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"db6d7786-7499-4d5b-b980-4d0c568d4267","import_id":null,"last_action":null,"bulk_created":false,"task":18,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  A hedge fund is a pooled investment fund that holds liquid assets and that makes use of complex trading and risk management techniques to aim to improve investment performance and insulate returns from market risk. Among these portfolio techniques are short selling and the use of  leverage and derivative instruments.[1] In the United States, financial regulations require that hedge funds be marketed only to institutional investors and high-net-worth individuals.  Hedge funds are considered alternative investments. Their ability to use leverage and more complex investment techniques distinguishes them from regulated investment funds available to the retail market, commonly known as mutual funds and ETFs. They are also considered distinct from private equity funds and other similar closed-end funds as hedge funds generally invest in relatively liquid assets and are usually open-ended. This means they typically allow investors to invest and withdraw capital periodically based on the fund's net asset value, whereas private-equity funds generally invest in illiquid assets and return capital only after a number of years.[2][3] Other than a fund's regulatory status, there are no formal or fixed definitions of fund types, and so there are different views of what can constitute a \"hedge fund\".  Although hedge funds are not subject to the many restrictions applicable to regulated funds, regulations were passed in the United States and Europe following the financial crisis of 2007–2008 with the intention of increasing government oversight of hedge funds and eliminating certain regulatory gaps.[4] While most modern hedge funds are able to employ a wide variety of financial instruments and risk management techniques,[5] they can be very different from each other with respect to their strategies, risks, volatility and expected return profile. It is common for hedge fund investment strategies to aim to achieve a positive return on investment regardless of whether markets are rising or falling (\"absolute return\"). Hedge funds can be considered risky investments; the expected returns of some hedge fund strategies are less volatile than those of retail funds with high exposure to stock markets because of the use of hedging techniques. Research in 2015 showed that hedge fund activism can have significant real effects on target firms, including improvements in productivity and efficient reallocation of corporate assets. Moreover, these interventions often lead to increased labor productivity, although the benefits may not fully accrue to workers in terms of increased wages or work hours.[6]  A hedge fund usually pays its investment manager a management fee (typically, 2% per annum of the net asset value of the fund) and a performance fee (typically, 20% of the increase in the fund's net asset value during a year).[1] Hedge funds have existed for many decades and have become increasingly popular. They have now grown to be a substantial portion of the asset management industry,[7] with assets totaling around $3.8 trillion as of 2021.[8]  The word \"hedge\", meaning a line of bushes around the perimeter of a field, has long been used as a metaphor for placing limits on risk.[9] Early hedge funds sought to hedge specific investments against general market fluctuations by shorting other, similar assets.[10]: 4  Nowadays, however, many different investment strategies are used, many of which do not \"hedge\" risk.[10]: 16–34 [11]  During the US bull market of the 1920s, there were numerous private investment vehicles available to wealthy investors. Of that period, the best known today is the Graham-Newman Partnership, founded by Benjamin Graham and his long-time business partner Jerry Newman.[12] This was cited by Warren Buffett in a 2006 letter to the Museum of American Finance as an early hedge fund,[13] and based on other comments from Buffett, Janet Tavakoli deems Graham's investment firm the first hedge fund.[14]  The sociologist Alfred W. Jones is credited with coining the phrase \"hedged fund\"[15][16] and is credited with creating the first hedge fund structure in 1949.[17] Jones referred to his fund as being \"hedged\", a term then commonly used on Wall Street to describe the management of investment risk due to changes in the financial markets.[18] Jones also developed the popular 2-and-20 structure of hedge funds, in which hedge funds charged investors a management fee of 2% on total assets and a 20% fee on realized gains.[19]  In the 1970s, hedge funds specialized in a single strategy with most fund managers following the long\/short equity model. Many hedge funds closed during the recession of 1969–1970 and the 1973–1974 stock market crash due to heavy losses. They received renewed attention in the late 1980s.[16]  During the 1990s, the number of hedge funds increased significantly with the 1990s stock market rise,[15] the aligned-interest compensation structure (i.e., common financial interests), and the promise of above average returns[20] as likely causes. Over the next decade, hedge fund strategies expanded to include credit arbitrage, distressed debt, fixed income, quantitative, and multi-strategy.[16] US institutional investors, such as pension and endowment funds, began allocating greater portions of their portfolios to hedge funds.[21][22]  During the first decade of the 21st century, hedge funds gained popularity worldwide, and, by 2008, the worldwide hedge fund industry held an estimated US$1.93 trillion in assets under management (AUM).[23][24] However, the financial crisis of 2007–2008 caused many hedge funds to restrict investor withdrawals and their popularity and AUM totals declined.[25] AUM totals rebounded and in April 2011 were estimated at almost $2 trillion.[26][27] As of February 2011[update], 61% of worldwide investment in hedge funds came from institutional sources.[28]  In June 2011, the hedge fund management firms with the greatest AUM were Bridgewater Associates (US$58.9 billion), Man Group (US$39.2 billion), Paulson & Co. (US$35.1 billion), Brevan Howard (US$31 billion), and Och-Ziff (US$29.4 billion).[29] Bridgewater Associates had $70 billion in assets under management as of March 2012[update].[30][31] At the end of that year, the 241 largest hedge fund firms in the United States collectively held $1.335 trillion.[32] In April 2012, the hedge fund industry reached a record high of US$2.13 trillion total assets under management.[33] In the middle of the 2010s, the hedge fund industry experienced a general decline in the \"old guard\" fund managers. Dan Loeb called it a \"hedge fund killing field\" due to the classic long\/short falling out of favor because of unprecedented easing by central banks. The US stock market correlation became untenable to short sellers.[34] The hedge fund industry today has reached a state of maturity that is consolidating around the larger, more established firms such as Citadel, Elliot, Millennium, Bridgewater, and others. The rate of new fund start ups is now outpaced by fund closings.[35]  In July 2017, hedge funds recorded their eighth consecutive monthly gain in returns with assets under management rising to a record $3.1 trillion.[36]  Hedge fund strategies are generally classified among four major categories: global macro, directional, event-driven, and relative value (arbitrage).[59] Strategies within these categories each entail characteristic risk and return profiles. A fund may employ a single strategy or multiple strategies for flexibility, risk management, or diversification.[60] The hedge fund's prospectus, also known as an offering memorandum, offers potential investors information about key aspects of the fund, including the fund's investment strategy, investment type, and leverage limit.[61]  The elements contributing to a hedge fund strategy include the hedge fund's approach to the market, the particular instrument use, the market sector the fund specializes in (e.g., healthcare), the method used to select investments, and the amount of diversification within the fund. There are a variety of market approaches to different asset classes, including equity, fixed income, commodity, and currency. Instruments used include equities, fixed income, futures, options, and swaps. Strategies can be divided into those in which investments can be selected by managers, known as \"discretionary\/qualitative\", or those in which investments are selected using a computerized system, known as \"systematic\/quantitative\".[62] The amount of diversification within the fund can vary; funds may be multi-strategy, multi-fund, multi-market, multi-manager, or a combination.  Sometimes hedge fund strategies are described as \"absolute return\" and are classified as either \"market neutral\" or \"directional\". Market neutral funds have less correlation to overall market performance by \"neutralizing\" the effect of market swings whereas directional funds utilize trends and inconsistencies in the market and have greater exposure to the market's fluctuations.[60][63]  Hedge funds using a global macro investing strategy take large positions in share, bond, or currency markets in anticipation of global macroeconomic events in order to generate a risk-adjusted return.[63] Global macro fund managers use macroeconomic (\"big picture\") analysis based on global market events and trends to identify opportunities for investment that would profit from anticipated price movements. While global macro strategies have a large amount of flexibility (due to their ability to use leverage to take large positions in diverse investments in multiple markets), the timing of the implementation of the strategies is important in order to generate attractive, risk-adjusted returns.[64] Global macro is often categorized as a directional investment strategy.[63]  Global macro strategies can be divided into discretionary and systematic approaches. Discretionary trading is carried out by investment managers who identify and select investments, whereas systematic trading is based on mathematical models and executed by software with limited human involvement beyond the programming and updating of the software. These strategies can also be divided into trend or counter-trend approaches depending on whether the fund attempts to profit from following market trend (long or short-term) or attempts to anticipate and profit from reversals in trends.[62]  Within global macro strategies, there are further sub-strategies including \"systematic diversified\", in which the fund trades in diversified markets, or sector specialists such as \"systematic currency\", in which the fund trades in foreign exchange markets or any other sector specialisation.[65]: 348  Other sub-strategies include those employed by commodity trading advisors (CTAs), where the fund trades in futures (or options) in commodity markets or in swaps.[66] This is also known as a \"managed future fund\".[63] CTAs trade in commodities (such as gold) and financial instruments, including stock indices. They also take both long and short positions, allowing them to make profit in both market upswings and downswings.[67] Most global macro managers tends to be a CTA from a regulatory perspective and the main divide is between systematic and discretionary strategies. A classification framework for CTA\/Macro Strategies can be found in the reference.[68]  Directional investment strategies use market movements, trends, or inconsistencies when picking stocks across a variety of markets. Computer models can be used, or fund managers will identify and select investments. These types of strategies have a greater exposure to the fluctuations of the overall market than do market neutral strategies.[60][63] Directional hedge fund strategies include US and international long\/short equity hedge funds, where long equity positions are hedged with short sales of equities or equity index options.  Within directional strategies, there are a number of sub-strategies. \"Emerging markets\" funds focus on emerging markets such as China and India,[65]: 351  whereas \"sector funds\" specialize in specific areas including technology, healthcare, biotechnology, pharmaceuticals, energy, and basic materials. Funds using a \"fundamental growth\" strategy invest in companies with more earnings growth than the overall stock market or relevant sector, while funds using a \"fundamental value\" strategy invest in undervalued companies.[65]: 344  Funds that use quantitative and financial signal processing techniques for equity trading are described as using a \"quantitative directional\" strategy.[65]: 345  Funds using a \"short bias\" strategy take advantage of declining equity prices using short positions.[69]  Event-driven strategies concern situations in which the underlying investment opportunity and risk are associated with an event.[70] An event-driven investment strategy finds investment opportunities in corporate transactional events such as consolidations, acquisitions, recapitalizations, bankruptcies, and liquidations. Managers employing such a strategy capitalize on valuation inconsistencies in the market before or after such events, and take a position based on the predicted movement of the security or securities in question. Large institutional investors such as hedge funds are more likely to pursue event-driven investing strategies than traditional equity investors because they have the expertise and resources to analyze corporate transactional events for investment opportunities.[64][71][72]  Corporate transactional events generally fit into three categories: distressed securities, risk arbitrage, and special situations.[64] Distressed securities include such events as restructurings, recapitalizations, and bankruptcies.[64] A distressed securities investment strategy involves investing in the bonds or loans of companies facing bankruptcy or severe financial distress, when these bonds or loans are being traded at a discount to their value. Hedge fund managers pursuing the distressed debt investment strategy aim to capitalize on depressed bond prices. Hedge funds purchasing distressed debt may prevent those companies from going bankrupt, as such an acquisition deters foreclosure by banks.[63] While event-driven investing, in general, tends to thrive during a bull market, distressed investing works best during a bear market.[72]  Risk arbitrage or merger arbitrage includes such events as mergers, acquisitions, liquidations, and hostile takeovers.[64] Risk arbitrage typically involves buying and selling the stocks of two or more merging companies to take advantage of market discrepancies between acquisition price and stock price. The risk element arises from the possibility that the merger or acquisition will not go ahead as planned; hedge fund managers will use research and analysis to determine if the event will take place.[72][73]  Special situations are events that impact the value of a company's stock, including the restructuring of a company or corporate transactions including spin-offs, share buy backs, security issuance\/repurchase, asset sales, or other catalyst-oriented situations. To take advantage of special situations the hedge fund manager must identify an upcoming event that will increase or decrease the value of the company's equity and equity-related instruments.[74]  Other event-driven strategies include credit arbitrage strategies, which focus on corporate fixed income securities; an activist strategy, where the fund takes large positions in companies and uses the ownership to participate in the management; a strategy based on predicting the final approval of new pharmaceutical drugs; and legal catalyst strategy, which specializes in companies involved in major lawsuits.  Relative value arbitrage strategies take advantage of relative discrepancies in price between securities. The price discrepancy can occur due to mispricing of securities compared to related securities, the underlying security or the market overall. Hedge fund managers can use various types of analysis to identify price discrepancies in securities, including mathematical, technical, or fundamental techniques.[75] Relative value is often used as a synonym for market neutral, as strategies in this category typically have very little or no directional market exposure to the market as a whole.[76] Other relative value sub-strategies include:  In addition to those strategies within the four main categories, there are several strategies that do not entirely fit into these categories.  For an investor who already holds large quantities of equities and bonds, investment in hedge funds may provide diversification and reduce the overall portfolio risk.[77] Managers of hedge funds often aim to produce returns that are relatively uncorrelated with market indices and are consistent with investors' desired level of risk.[78][79] While hedging can reduce some risks of an investment it usually increases others, such as operational risk and model risk, so overall risk is reduced but cannot be eliminated. According to a report by the Hennessee Group, hedge funds were approximately one-third less volatile than the S&P 500 between 1993 and 2010.[80]  Investors in hedge funds are, in most countries, required to be qualified investors who are assumed to be aware of the investment risks, and accept these risks because of the potential returns relative to those risks. Fund managers may employ extensive risk management strategies in order to protect the fund and investors. According to the Financial Times, \"big hedge funds have some of the most sophisticated and exacting risk management practices anywhere in asset management.\"[78] Hedge fund managers that hold a large number of investment positions for short periods are likely to have a particularly comprehensive risk management system in place, and it has become usual for funds to have independent risk officers who assess and manage risks but are not otherwise involved in trading.[81] A variety of different measurement techniques and models are used to estimate risk according to the fund's leverage, liquidity, and investment strategy.[79][82] Non-normality of returns, volatility clustering and trends are not always accounted for by conventional risk measurement methodologies and so in addition to value at risk and similar measurements, funds may use integrated measures such as drawdowns.[83]  In addition to assessing the market-related risks that may arise from an investment, investors commonly employ operational due diligence to assess the risk that error or fraud at a hedge fund might result in a loss to the investor. Considerations will include the organization and management of operations at the hedge fund manager, whether the investment strategy is likely to be sustainable, and the fund's ability to develop as a company.[84]  Since hedge funds are private entities and have few public disclosure requirements, this is sometimes perceived as a lack of transparency.[85] Another common perception of hedge funds is that their managers are not subject to as much regulatory oversight and\/or registration requirements as other financial investment managers, and more prone to manager-specific idiosyncratic risks such as style drifts, faulty operations, or fraud.[86] New regulations introduced in the US and the EU as of 2010 required hedge fund managers to report more information, leading to greater transparency.[87] In addition, investors, particularly institutional investors, are encouraging further developments in hedge fund risk management, both through internal practices and external regulatory requirements.[78] The increasing influence of institutional investors has led to greater transparency: hedge funds increasingly provide information to investors including valuation methodology, positions, and leverage exposure.[88]  Hedge funds share many of the same types of risk as other investment classes, including liquidity risk and manager risk.[86] Liquidity refers to the degree to which an asset can be bought and sold or converted to cash; similar to private-equity funds, hedge funds employ a lock-up period during which an investor cannot remove money.[63][89] Manager risk refers to those risks which arise from the management of funds. As well as specific risks such as style drift, which refers to a fund manager \"drifting\" away from an area of specific expertise, manager risk factors include valuation risk, capacity risk, concentration risk, and leverage risk.[85] Valuation risk refers to the concern that the net asset value (NAV) of investments may be inaccurate;[90] capacity risk can arise from placing too much money into one particular strategy, which may lead to fund performance deterioration;[91] and concentration risk may arise if a fund has too much exposure to a particular investment, sector, trading strategy, or group of correlated funds.[92] These risks may be managed through defined controls over conflict of interest,[90] restrictions on allocation of funds,[91] and set exposure limits for strategies.[92]  Many investment funds use leverage, the practice of borrowing money, trading on margin, or using derivatives to obtain market exposure in excess of that provided by investors' capital. Although leverage can increase potential returns, the opportunity for larger gains is weighed against the possibility of greater losses.[89] Hedge funds employing leverage are likely to engage in extensive risk management practices.[81][85] In comparison with investment banks, hedge fund leverage is relatively low; according to a National Bureau of Economic Research working paper, the average leverage for investment banks is 14.2, compared to between 1.5 and 2.5 for hedge funds.[93]  Some types of funds, including hedge funds, are perceived as having a greater appetite for risk, with the intention of maximizing returns,[89] subject to the risk tolerance of investors and the fund manager. Managers will have an additional incentive to increase risk oversight when their own capital is invested in the fund.[81]  Hedge fund management firms typically charge their funds both a management fee and a performance fee.  Management fees are calculated as a percentage of the fund's net asset value and typically range from 1% to 4% per annum, with 2% being standard.[94][95][96] They are usually expressed as an annual percentage, but calculated and paid monthly or quarterly. Management fees for hedge funds are designed to cover the operating costs of the manager, whereas the performance fee provides the manager's profits. However, due to economies of scale the management fee from larger funds can generate a significant part of a manager's profits, and as a result some fees have been criticized by some public pension funds, such as CalPERS, for being too high.[97]  The performance fee is typically 20% of the fund's profits during any year, though performance fees range between 10% and 50%. Performance fees are intended to provide an incentive for a manager to generate profits.[98][99] Performance fees have been criticized by Warren Buffett, who believes that because hedge funds share only the profits and not the losses, such fees create an incentive for high-risk investment management. Performance fee rates have fallen since the start of the credit crunch.[100]  Almost all hedge fund performance fees include a \"high water mark\" (or \"loss carryforward provision\"), which means that the performance fee only applies to net profits (i.e., profits after losses in previous years have been recovered). This prevents managers from receiving fees for volatile performance, though a manager will sometimes close a fund that has suffered serious losses and start a new fund, rather than attempt to recover the losses over a number of years without a performance fee.[101]  Some performance fees include a \"hurdle\", so that a fee is only paid on the fund's performance in excess of a benchmark rate (e.g., LIBOR) or a fixed percentage.[102] The hurdle is usually tied to a benchmark rate such as Libor or the one-year Treasury bill rate plus a spread.[103] A \"soft\" hurdle means the performance fee is calculated on all the fund's returns if the hurdle rate is cleared. A \"hard\" hurdle is calculated only on returns above the hurdle rate.[104] By example the manager sets a hurdle rate equal to 5%, and the fund return 15%, incentive fees would only apply to the 10% above the hurdle rate.[103] A hurdle is intended to ensure that a manager is only rewarded if the fund generates returns in excess of the returns that the investor would have received if they had invested their money elsewhere.  Some hedge funds charge a redemption fee (or withdrawal fee) for early withdrawals during a specified period of time (typically a year), or when withdrawals exceed a predetermined percentage of the original investment.[105] The purpose of the fee is to discourage short-term investing, reduce turnover, and deter withdrawals after periods of poor performance. Unlike management fees and performance fees, redemption fees are usually kept by the fund and redistributed to all investors.  Hedge fund management firms are often owned by their portfolio managers, who are therefore entitled to any profits that the business makes. As management fees are intended to cover the firm's operating costs, performance fees (and any excess management fees) are generally distributed to the firm's owners as profits. Funds do not tend to report compensation, and so published lists of the amounts earned by top managers tend to be estimates based on factors such as the fees charged by their funds and the capital they are thought to have invested in them.[106] Many managers have accumulated large stakes in their own funds and so top hedge fund managers can earn extraordinary amounts of money, perhaps up to $4 billion in a good year.[107][108]  Earnings at the very top are higher than in any other sector of the financial industry,[109] and collectively the top 25 hedge fund managers regularly earn more than all 500 of the chief executives in the S&P 500.[110] Most hedge fund managers are remunerated much less, however, and if performance fees are not earned then small managers at least are unlikely to be paid significant amounts.[109]  In 2011, the top manager earned $3 billion, the tenth earned $210 million, and the 30th earned $80 million.[111] In 2011, the average earnings for the 25 highest-compensated hedge fund managers in the United States was $576 million[112] while the mean total compensation for all hedge fund investment professionals was $690,786 and the median was $312,329. The same figures for hedge fund CEOs were $1,037,151 and $600,000, and for chief investment officers were $1,039,974 and $300,000, respectively.[113]  Of the 1,226 people on the Forbes World's Billionaires List for 2012,[114] 36 of the financiers listed \"derived significant chunks\" of their wealth from hedge fund management.[115] Among the richest 1,000 people in the United Kingdom, 54 were hedge fund managers, according to the Sunday Times Rich List for 2012.[116]  A portfolio manager risks losing his past compensation if he or she engages in insider trading. In Morgan Stanley v. Skowron, 989 F. Supp. 2d 356 (S.D.N.Y. 2013), applying New York's faithless servant doctrine, the court held that a hedge fund's portfolio manager engaging in insider trading in violation of his company's code of conduct, which also required him to report his misconduct, must repay his employer the full $31 million his employer paid him as compensation during his period of faithlessness.[117][118][119][120] The court called the insider trading the \"ultimate abuse of a portfolio manager's position\".[118] The judge also wrote: \"In addition to exposing Morgan Stanley to government investigations and direct financial losses, Skowron's behavior damaged the firm's reputation, a valuable corporate asset.\"[118]  A hedge fund is an investment vehicle that is most often structured as an offshore corporation, limited partnership, or limited liability company.[121] The fund is managed by an investment manager in the form of an organization or company that is legally and financially distinct from the hedge fund and its portfolio of assets.[122][123] Many investment managers utilize service providers for operational support.[124] Service providers include prime brokers, banks, administrators, distributors, and accounting firms.  Prime brokers clear trades and provide leverage and short-term financing.[125][126] They are usually divisions of large investment banks.[127] The prime broker acts as a counterparty to derivative contracts, and lends securities for particular investment strategies, such as long\/short equities and convertible bond arbitrage.[128][129] It can provide custodial services for the fund's assets, and trade execution and clearing services for the hedge fund manager.[130]  Hedge fund administrators are typically responsible for valuation services, and often operations, and accounting.  Calculation of the net asset value (\"NAV\") by the administrator, including the pricing of securities at current market value and calculation of the fund's income and expense accruals, is a core administrator task, because it is the price at which investors buy and sell shares in the fund.[131] The accurate and timely calculation of NAV by the administrator is vital.[131][132] The case of Anwar v. Fairfield Greenwich (SDNY 2015) is the major case relating to fund administrator liability for failure to handle its NAV-related obligations properly.[133][134] There, the hedge fund administrator and other defendants settled in 2016 by paying the Anwar investor plaintiffs $235 million.[133][134]  Administrator back office support allows fund managers to concentrate on trades.[135] Administrators also process subscriptions and redemptions and perform various shareholder services.[136][137] Hedge funds in the United States are not required to appoint an administrator and all of these functions can be performed by an investment manager.[138] A number of conflict of interest situations may arise in this arrangement, particularly in the calculation of a fund's net asset value.[139] Most funds employ external auditors, thereby arguably offering a greater degree of transparency.[138]  An auditor is an independent accounting firm used to perform a complete audit the fund's financial statements. The year-end audit is performed in accordance with the standard accounting practices enforced within the country in which the fund it established, typically US GAAP or the International Financial Reporting Standards (IFRS).[140] The auditor may verify the fund's NAV and assets under management (AUM).[141][142] Some auditors only provide \"NAV lite\" services, meaning that the valuation is based on prices received from the manager rather than an independent assessment.[143]  A distributor is an underwriter, broker, dealer, or other person who participates in the distribution of securities.[144] The distributor is also responsible for marketing the fund to potential investors. Many hedge funds do not have distributors, and in such cases, the investment manager will be responsible for the distribution of securities and marketing, though many funds also use placement agents and broker-dealers for distribution.[145][146]  The legal structure of a specific hedge fund, in particular its domicile and the type of legal entity in use, is usually determined by the tax expectations of the fund's investors. Regulatory considerations will also play a role. Many hedge funds are established in offshore financial centers to avoid adverse tax consequences for its foreign and tax-exempt investors.[147][148] Offshore funds that invest in the US typically pay withholding taxes on certain types of investment income, but not US capital gains tax. However, the fund's investors are subject to tax in their own jurisdictions on any increase in the value of their investments.[149][150] This tax treatment promotes cross-border investments by limiting the potential for multiple jurisdictions to layer taxes on investors.[151]  US tax-exempt investors (such as pension plans and endowments) invest primarily in offshore hedge funds to preserve their tax exempt status and avoid unrelated business taxable income.[150] The investment manager, usually based in a major financial center, pays tax on its management fees per the tax laws of the state and country where it is located.[152] In 2011, half of the existing hedge funds were registered offshore and half onshore. The Cayman Islands was the leading location for offshore funds, accounting for 34% of the total number of global hedge funds. The US had 24%, Luxembourg 10%, Ireland 7%, the British Virgin Islands 6%, and Bermuda had 3%.[153]  Hedge funds take advantage of a tax loopole called carried interest to get around paying too much in taxes by fancy legalistic maneouvres on their part.[154]  Deutsche Bank and Barclays created special options accounts for hedge fund clients in the banks' names and claimed to own the assets, when in fact the hedge fund clients had full control of the assets and reaped the profits. The hedge funds would then execute trades – many of them a few seconds in duration – but wait until just after a year had passed to exercise the options, allowing them to report the profits at a lower long-term capital gains tax rate. The US Senate Permanent Subcommittee on Investigations chaired by Carl Levin issued a 2014 report that found that from 1998 and 2013, hedge funds avoided billions of dollars in taxes by using basket options. The Internal Revenue Service began investigating Renaissance Technologies[155] in 2009, and Levin criticized the IRS for taking six years to investigate the company. Using basket options Renaissance avoided \"more than $6 billion in taxes over more than a decade\".[156]  These banks and hedge funds involved in this case used dubious structured financial products in a giant game of 'let's pretend,' costing the Treasury billions and bypassing safeguards that protect the economy from excessive bank lending for stock speculation. A dozen other hedge funds along with Renaissance Technologies used Deutsche Bank's and Barclays' basket options.[156] Renaissance argued that basket options were \"extremely important because they gave the hedge fund the ability to increase its returns by borrowing more and to protect against model and programming failures\".[156] In July 2015, the United States Internal Revenue claimed hedge funds used basket options \"to bypass taxes on short-term trades\". These basket options will now be labeled as listed transactions that must be declared on tax returns, and a failure to do would result in a penalty.[156]  In contrast to the funds themselves, investment managers are primarily located onshore. The United States remains the largest center of investment with US-based funds managing around 70% of global assets at the end of 2011.[153] As of April 2012, there were approximately 3,990 investment advisers managing one or more private hedge funds registered with the Securities and Exchange Commission.[157] New York City and the Gold Coast area of Connecticut are the leading locations for US hedge fund managers.[158]  London was Europe's leading center for hedge fund managers, but since the Brexit referendum some formerly London-based hedge funds have relocated to other European financial centers such as Frankfurt, Luxembourg, Paris, and Dublin, while some other hedge funds have moved their European offices back to New York City.[159][160] Before Brexit, according to EuroHedge data, around 800 funds located in the UK had managed 85% of European-based hedge fund assets in 2011.[153] Interest in hedge funds in Asia has increased significantly since 2003, especially in Japan, Hong Kong, and Singapore.[161] After Brexit, Europe and the US remain the leading locations for the management of Asian hedge fund assets.[153]  Hedge fund legal structures vary depending on location and the investor(s). US hedge funds aimed at US-based, taxable investors are generally structured as limited partnerships or limited liability companies. Limited partnerships and other flow-through taxation structures assure that investors in hedge funds are not subject to both entity-level and personal-level taxation.[130] A hedge fund structured as a limited partnership must have a general partner. The general partner may be an individual or a corporation. The general partner serves as the manager of the limited partnership, and has unlimited liability.[125][162] The limited partners serve as the fund's investors, and have no responsibility for management or investment decisions. Their liability is limited to the amount of money they invest for partnership interests.[162][163] As an alternative to a limited partnership arrangement, U.S. domestic hedge funds may be structured as limited liability companies, with members acting as corporate shareholders and enjoying protection from individual liability.[164]  By contrast, offshore corporate funds are usually used for non-US investors, and when they are domiciled in an applicable offshore tax haven, no entity-level tax is imposed.[147] Many managers of offshore funds permit the participation of tax-exempt US investors, such as pensions funds, institutional endowments, and charitable trusts.[162] As an alternative legal structure, offshore funds may be formed as an open-ended unit trust using an unincorporated mutual fund structure.[165] Japanese investors prefer to invest in unit trusts, such as those available in the Cayman Islands.[166]  The investment manager who organizes the hedge fund may retain an interest in the fund, either as the general partner of a limited partnership or as the holder of \"founder shares\" in a corporate fund.[167] For offshore funds structured as corporate entities, the fund may appoint a board of directors. The board's primary role is to provide a layer of oversight while representing the interests of the shareholders.[168] However, in practice board members may lack sufficient expertise to be effective in performing those duties. The board may include both affiliated directors who are employees of the fund and independent directors whose relationship to the fund is limited.[168]  A side pocket is a mechanism whereby a fund compartmentalizes assets that are relatively illiquid or difficult to value reliably.[172] When an investment is side-pocketed, its value is calculated separately from the value of the fund's main portfolio.[173] Because side pockets are used to hold illiquid investments, investors do not have the standard redemption rights with respect to the side pocket investment that they do with respect to the fund's main portfolio.[173] Profits or losses from the investment are allocated on a pro rata basis only to those who are investors at the time the investment is placed into the side pocket and are not shared with new investors.[173][174] Funds typically carry side pocket assets \"at cost\" for purposes of calculating management fees and reporting net asset values. This allows fund managers to avoid attempting a valuation of the underlying investments, which may not always have a readily available market value.[174]  Side pockets were widely used by hedge funds during the financial crisis of 2007–2008 amidst a flood of withdrawal requests. Side pockets allowed fund managers to lay away illiquid securities until market liquidity improved, a move that could reduce losses. However, as the practice restricts investors' ability to redeem their investments it is often unpopular and many have alleged that it has been abused or applied unfairly.[175][176] The SEC also has expressed concern about aggressive use of side pockets and has sanctioned certain fund managers for inappropriate use of them.[1]  Hedge funds must abide by the national, federal, and state regulatory laws in their respective locations. The U.S. regulations and restrictions that apply to hedge funds differ from those that apply to its mutual funds.[177] Mutual funds, unlike hedge funds and other private funds, are subject to the Investment Company Act of 1940, which is a highly detailed and extensive regulatory regime.[178] According to a report by the International Organization of Securities Commissions, the most common form of regulation pertains to restrictions on financial advisers and hedge fund managers in an effort to minimize client fraud. On the other hand, U.S. hedge funds are exempt from many of the standard registration and reporting requirements because they only accept accredited investors.[63] In 2010, regulations were enacted in the US and European Union which introduced additional hedge fund reporting requirements. These included the U.S.'s Dodd-Frank Wall Street Reform Act[4] and European Alternative Investment Fund Managers Directive.[179]  In 2007, in an effort to engage in self-regulation, 14 leading hedge fund managers developed a voluntary set of international standards in best practice and known as the Hedge Fund Standards they were designed to create a \"framework of transparency, integrity and good governance\" in the hedge fund industry.[180] The Hedge Fund Standards Board was set up to prompt and maintain these standards going forward, and by 2016 it had approximately 200 hedge fund managers and institutional investors with a value of US$3tn investment endorsing the standards.[181] The Managed Funds Association is a US-based trade association, while the Alternative Investment Management Association is the primarily European counterpart.[182]  Hedge funds within the US are subject to regulatory, reporting, and record-keeping requirements.[183] Many hedge funds also fall under the jurisdiction of the Commodity Futures Trading Commission, and are subject to rules and provisions of the 1922 Commodity Exchange Act, which prohibits fraud and manipulation.[184] The Securities Act of 1933 required companies to file a registration statement with the SEC to comply with its private placement rules before offering their securities to the public,[185] and most traditional hedge funds in the United States are offered effectively as private placement offerings.[186] The Securities Exchange Act of 1934 required a fund with more than 499 investors to register with the SEC.[187][188][189] The Investment Advisers Act of 1940 contained anti-fraud provisions that regulated hedge fund managers and advisers, created limits for the number and types of investors, and prohibited public offerings. The Act also exempted hedge funds from mandatory registration with the SEC[63][190][191] when selling to accredited investors with a minimum of US$5 million in investment assets. Companies and institutional investors with at least US$25 million in investment assets also qualified.[192]  In December 2004, the SEC began requiring hedge fund advisers, managing more than US$25 million and with more than 14 investors, to register with the SEC under the Investment Advisers Act.[193] The SEC stated that it was adopting a \"risk-based approach\" to monitoring hedge funds as part of its evolving regulatory regime for the burgeoning industry.[194] The new rule was controversial, with two Commissioners dissenting,[195] and was later challenged in court by a hedge fund manager. In June 2006, the U.S. Court of Appeals for the District of Columbia overturned the rule and sent it back to the agency to be reviewed.[196] In response to the court decision, in 2007 the SEC adopted Rule 206(4)-8, which unlike the earlier-challenged rule, \"does not impose additional filing, reporting or disclosure obligations\" but does potentially increase \"the risk of enforcement action\" for negligent or fraudulent activity.[197] Hedge fund managers with at least US$100 million in assets under management are required to file publicly quarterly reports disclosing ownership of registered equity securities and are subject to public disclosure if they own more than 5% of the class of any registered equity security.[188] Registered advisers must report their business practices and disciplinary history to the SEC and to their investors. They are required to have written compliance policies, a chief compliance officer, and their records and practices may be examined by the SEC.[183]  The U.S.'s Dodd-Frank Wall Street Reform Act was passed in July 2010[4][87] and requires SEC registration of advisers who manage private funds with more than US$150 million in assets.[198][199] Registered managers must file Form ADV with the SEC, as well as information regarding their assets under management and trading positions.[200] Previously, advisers with fewer than 15 clients were exempt, although many hedge fund advisers voluntarily registered with the SEC to satisfy institutional investors.[201] Under Dodd-Frank, investment advisers with less than US$100 million in assets under management became subject to state regulation.[198] This increased the number of hedge funds under state supervision.[202] Overseas advisers who managed more than US$25 million were also required to register with the SEC.[203] The Act requires hedge funds to provide information about their trades and portfolios to regulators including the newly created Financial Stability Oversight Council.[202] In this regard, most hedge funds and other private funds, including private-equity funds, must file Form PF with the SEC, which is an extensive reporting form with substantial data on the funds' activities and positions.[1] Under the \"Volcker Rule\", regulators are also required to implement regulations for banks, their affiliates, and holding companies to limit their relationships with hedge funds and to prohibit these organizations from proprietary trading, and to limit their investment in, and sponsorship of, hedge funds.[202][204][205]  Within the European Union (EU), hedge funds are primarily regulated through their managers.[63] In the United Kingdom, where 80% of Europe's hedge funds are based,[206] hedge fund managers are required to be authorised and regulated by the Financial Conduct Authority (FCA).[179] Each country has its own specific restrictions on hedge fund activities, including controls on use of derivatives in Portugal, and limits on leverage in France.[63]  In the EU, managers are subject to the EU's Directive on Alternative Investment Fund Managers (AIFMD). According to the EU, the aim of the directive is to provide greater monitoring and control of alternative investment funds.[207] AIFMD requires all EU hedge fund managers to register with national regulatory authorities[208] and to disclose more information, on a more frequent basis. It also directs hedge fund managers to hold larger amounts of capital. AIFMD also introduced a \"passport\" for hedge funds authorised in one EU country to operate throughout the EU.[87][179] The scope of AIFMD is broad and encompasses managers located within the EU as well as non-EU managers that market their funds to European investors.[87] An aspect of AIFMD which challenges established practices in the hedge funds sector is the potential restriction of remuneration through bonus deferrals and clawback provisions.[209]  Some hedge funds are established in offshore centres, such as the Cayman Islands, Dublin, Luxembourg, Singapore[210] the British Virgin Islands, and Bermuda, which have different regulations[211] concerning non-accredited investors, client confidentiality, and fund manager independence.[4][179]  In South Africa, investment fund managers must be approved by, and register with, the Financial Services Board (FSB).[212]  Performance statistics for individual hedge funds are difficult to obtain, as the funds have historically not been required to report their performance to a central repository, and restrictions against public offerings and advertisement have led many managers to refuse to provide performance information publicly. However, summaries of individual hedge fund performance are occasionally available in industry journals[213][214] and databases.[215]  One estimate is that the average hedge fund returned 11.4% per year,[216] representing a 6.7% return above overall market performance before fees, based on performance data from 8,400 hedge funds.[63] Another estimate is that between January 2000 and December 2009 hedge funds outperformed other investments and were substantially less volatile, with stocks falling an average of 2.62% per year over the decade and hedge funds rising an average of 6.54% per year; this was an unusually volatile period with both the 2001-2002 dot-com bubble and a recession beginning mid 2007.[217] However, more recent data show that hedge fund performance declined and underperformed the market from about 2009 to 2016.[218]  Hedge funds performance is measured by comparing their returns to an estimate of their risk.[219] Common measures are the Sharpe ratio,[220] Treynor measure and Jensen's alpha.[221] These measures work best when returns follow normal distributions without autocorrelation, and these assumptions are often not met in practice.[222]  New performance measures have been introduced that attempt to address some of theoretical concerns with traditional indicators, including: modified Sharpe ratios;[222][223] the Omega ratio introduced by Keating and Shadwick in 2002;[224] Alternative Investments Risk Adjusted Performance (AIRAP) published by Sharma in 2004;[225] and Kappa developed by Kaplan and Knowles in 2004.[226]  There is a debate over whether alpha (the manager's skill element in performance) has been diluted by the expansion of the hedge fund industry. Two reasons are given. First, the increase in traded volume may have been reducing the market anomalies that are a source of hedge fund performance. Second, the remuneration model is attracting more managers, which may dilute the talent available in the industry.[227][228]  Indices play a central and unambiguous role in traditional asset markets, where they are widely accepted as representative of their underlying portfolios. Equity and debt index fund products provide investable access to most developed markets in these asset classes.  Hedge fund indices are more problematic. The typical hedge fund is not traded on exchange, will accept investments only at the discretion of the manager, and does not have an obligation to publish returns. Despite these challenges, Non-investable, Investable, and Clone indices have been developed.  Non-investable indices are indicative in nature and aim to represent the performance of some database of hedge funds using some measure such as mean, median, or weighted mean from a hedge fund database. The databases have diverse selection criteria and methods of construction, and no single database captures all funds. This leads to significant differences in reported performance between different indices.  Although they aim to be representative, non-investable indices suffer from a lengthy and largely unavoidable list of biases. Funds' participation in a database is voluntary, leading to self-selection bias because those funds that choose to report may not be typical of funds as a whole. For example, some do not report because of poor results or because they have already reached their target size and do not wish to raise further money.  The short lifetimes of many hedge funds mean that there are many new entrants and many departures each year, which raises the problem of survivorship bias. If we examine only funds that have survived to the present, we will overestimate past returns because many of the worst-performing funds have not survived, and the observed association between fund youth and fund performance suggests that this bias may be substantial.  When a fund is added to a database for the first time, all or part of its historical data is recorded ex-post in the database. It is likely that funds only publish their results when they are favorable, so that the average performances displayed by the funds during their incubation period are inflated. This is known as \"instant history bias\" or \"backfill bias\".  Investable indices are an attempt to reduce these problems by ensuring that the return of the index is available to shareholders. To create an investable index, the index provider selects funds and develops structured products or derivative instruments that deliver the performance of the index. When investors buy these products the index provider makes the investments in the underlying funds, making an investable index similar in some ways to a fund of hedge funds portfolio.  To make the index investable, hedge funds must agree to accept investments on the terms given by the constructor. To make the index liquid, these terms must include provisions for redemptions that some managers may consider too onerous to be acceptable. This means that investable indices do not represent the total universe of hedge funds. Most seriously, they under-represent more successful managers, who typically refuse to accept such investment protocols.  The most recent addition to the field approaches the problem in a different manner. Instead of reflecting the performance of actual hedge funds, they take a statistical approach to the analysis of historic hedge fund returns and use this to construct a model of how hedge fund returns respond to the movements of various investable financial assets. This model is then used to construct an investable portfolio of those assets. This makes the index investable, and in principle, they can be as representative as the hedge fund database from which they were constructed. However, these clone indices rely on a statistical modelling process. Such indices have too short a history to state whether this approach will be considered successful.  In March 2017, HFR – a hedge fund research data and service provider – reported that there were more hedge-fund closures in 2016 than during the 2009 recession. According to the report, several large public pension funds pulled their investments in hedge funds, because the funds' subpar performance as a group did not merit the high fees they charged.  Despite the hedge fund industry topping $3 trillion for the first time ever in 2016, the number of new hedge funds launched fell short of levels before the financial crisis of 2007–2008. There were 729 hedge fund launches in 2016, fewer than the 784 opened in 2009, and dramatically fewer than the 968 launches in 2015.[229]  Systemic risk refers to the risk of instability across the entire financial system, as opposed to within a single company. Such risk may arise following a destabilizing event or events affecting a group of financial institutions linked through investment activity.[230] Organizations such as the European Central Bank have charged that hedge funds pose systemic risks to the financial sector,[231][232] and following the failure of hedge fund Long-Term Capital Management (LTCM) in 1998 there was widespread concern about the potential for systemic risk if a hedge fund failure led to the failure of its counterparties. (As it happens, no financial assistance was provided to LTCM by the US Federal Reserve, so there was no direct cost to US taxpayers,[233] but a large bailout had to be mounted by a number of financial institutions.)  However, these claims are widely disputed by the financial industry,[234] who typically regard hedge funds as \"small enough to fail\", since most are relatively small in terms of the assets they manage and operate with low leverage, thereby limiting the potential harm to the economic system should one of them fail.[216][235] Formal analysis of hedge fund leverage before and during the financial crisis of 2007–2008 suggests that hedge fund leverage is both fairly modest and counter-cyclical to the market leverage of investment banks and the larger financial sector.[93] Hedge fund leverage decreased prior to the financial crisis, even while the leverage of other financial intermediaries continued to increase.[93] Hedge funds fail regularly, and numerous hedge funds failed during the financial crisis.[236] In testimony to the US House Financial Services Committee in 2009, Ben Bernanke, the Federal Reserve Board Chairman said he \"would not think that any hedge fund or private-equity fund would become a systemically critical firm individually\".[237]  This does leave the possibility that hedge funds collectively might contribute to systemic risk if they exhibit herd or self-coordinating behavior,[238] perhaps because many hedge funds make losses in similar trades. This coupled with the extensive use of leverage could lead to forced liquidations in a crisis.  Hedge funds are also closely connected to their prime brokers, typically investment banks, which could contribute to their instability in a crisis, though this works both ways and failing counterparty banks can freeze hedge funds assets, as Lehman Brothers did in 2008.[239]  An August 2012 survey by the Financial Services Authority concluded that risks were limited and had reduced as a result, inter alia, of larger margins being required by counterparty banks, but might change rapidly according to market conditions. In stressed market conditions, investors might suddenly withdraw large sums, resulting in forced asset sales. This might cause liquidity and pricing problems if it occurred across a number of funds or in one large highly leveraged fund.[240]  Hedge funds are structured to avoid most direct regulation (although their managers may be regulated), and are not required to publicly disclose their investment activities, except to the extent that investors generally are subject to disclosure requirements. This is in contrast to a regulated mutual fund or exchange-traded fund, which will typically have to meet regulatory requirements for disclosure. An investor in a hedge fund usually has direct access to the investment adviser of the fund, and may enjoy more personalized reporting than investors in retail investment funds. This may include detailed discussions of risks assumed and significant positions. However, this high level of disclosure is not available to non-investors, contributing to hedge funds' reputation for secrecy, while some hedge funds have very limited transparency even to investors.[241]  Funds may choose to report some information in the interest of recruiting additional investors. Much of the data available in consolidated databases is self-reported and unverified.[242] A study was done on two major databases containing hedge fund data. The study noted that 465 common funds had significant differences in reported information (e.g., returns, inception date, net assets value, incentive fee, management fee, investment styles, etc.) and that 5% of return numbers and 5% of NAV numbers were dramatically different.[243] With these limitations, investors have to do their own research, which may cost on the scale of US$50,000 for a fund that is not well-established.[244]  A lack of verification of financial documents by investors or by independent auditors has, in some cases, assisted in fraud.[245] In the mid-2000s, Kirk Wright of International Management Associates was accused of mail fraud and other securities violations[246][247] which allegedly defrauded clients of close to US$180 million.[248] In December 2008, Bernard Madoff was arrested for running a US$50 billion Ponzi scheme[249] that closely resembled a hedge fund and was incorrectly[250] described as one.[251][252][253] Several feeder hedge funds, of which the largest was Fairfield Sentry, channeled money to it. Following the Madoff case, the SEC adopted reforms in December 2009 that subjected hedge funds to an audit requirement.[254]  The process of matching hedge funds to investors has traditionally been fairly opaque, with investments often driven by personal connections or recommendations of portfolio managers.[255] Many funds disclose their holdings, strategy, and historic performance relative to market indices, giving investors some idea of how their money is being allocated, although individual holdings are often not disclosed.[256] Investors are often drawn to hedge funds by the possibility of realizing significant returns, or hedging against volatility in the market. The complexity and fees associated with hedge funds are causing some to exit the market – CalPERS, the largest pension fund in the US, announced plans to completely divest from hedge funds in 2014.[257] Some services are attempting to improve matching between hedge funds and investors: HedgeZ is designed to allow investors to easily search and sort through funds;[258] iMatchative aims to match investors to funds through algorithms that factor in an investor's goals and behavioral profile, in hopes of helping funds and investors understand the how their perceptions and motivations drive investment decisions.[259]  In June 2006, prompted by a letter from Gary J. Aguirre, the U.S. Senate Judiciary Committee began an investigation into the links between hedge funds and independent analysts. Aguirre was fired from his job with the SEC when, as lead investigator of insider trading allegations against Pequot Capital Management, he tried to interview John Mack, then being considered for chief executive officer at Morgan Stanley.[260] The Judiciary Committee and the US Senate Finance Committee issued a scathing report in 2007, which found that Aguirre had been illegally fired in reprisal[261] for his pursuit of Mack, and in 2009 the SEC was forced to re-open its case against Pequot. Pequot settled with the SEC for US$28 million, and Arthur J. Samberg, chief investment officer of Pequot, was barred from working as an investment advisor.[262] Pequot closed its doors under the pressure of investigations.[263]  The systemic practice of hedge funds submitting periodic electronic questionnaires to stock analysts as a part of market research was reported by The New York Times in July 2012. According to the report, one motivation for the questionnaires was to obtain subjective information not available to the public and possible early notice of trading recommendations that could produce short-term market movements.[264]  According to modern portfolio theory, rational investors will seek to hold portfolios that are mean\/variance efficient (that is, portfolios that offer the highest level of return per unit of risk). One of the attractive features of hedge funds (in particular market neutral and similar funds) is that they sometimes have a modest correlation with traditional assets such as equities. This means that hedge funds have a potentially quite valuable role in investment portfolios as diversifiers, reducing overall portfolio risk.[102]  However, there are at least three reasons why one might not wish to allocate a high proportion of assets into hedge funds. These reasons are:  Several studies have suggested that hedge funds are sufficiently diversifying to merit inclusion in investor portfolios, but this is disputed for example by Mark Kritzman who performed a mean-variance optimization calculation on an opportunity set that consisted of a stock index fund, a bond index fund, and ten hypothetical hedge funds.[265][266] The optimizer found that a mean-variance efficient portfolio did not contain any allocation to hedge funds, largely because of the impact of performance fees. To demonstrate this, Kritzman repeated the optimization using an assumption that the hedge funds took no performance fees. The result from this second optimization was an allocation of 74% to hedge funds.  Hedge funds tend to perform poorly during equity bear markets, just when an investor needs part of their portfolio to add value.[102] For example, in January–September 2008, the Credit Suisse\/Tremont Hedge Fund Index returned -9.87%.[267] According to the same index series, even \"dedicated short bias\" funds returned −6.08% in September 2008, when Lehman Brothers collapsed. "},"meta":{},"created_at":"2025-03-22T14:25:42.273677Z","updated_at":"2025-03-22T14:25:42.273677Z","inner_id":10,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":19,"annotations":[{"id":19,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"3a89cb4a-75a6-4ef4-8d88-f68db29f0540","import_id":null,"last_action":null,"bulk_created":false,"task":19,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  A film producer is a person who oversees film production.[1] Either employed by a production company or working independently, producers plan and coordinate various aspects of film production, such as selecting the script, coordinating writing, directing, editing, and arranging financing.[2]  The producer is responsible for finding and selecting promising material for development.[2] Unless the film is based on an existing script, the producer hires a screenwriter and oversees the script's development.[3] These activities culminate with the pitch, led by the producer, to secure the financial backing that enables production to begin. If all succeeds, the project is \"greenlit\".  The producer supervises the pre-production, principal photography and post-production stages of filmmaking. A producer hires a director for the film, as well as other key crew members. Whereas the director makes the creative decisions during the production, the producer typically manages logistics and business operations, though some directors also produce their own films. The producer must ensure the film is delivered on time and within budget, and in the later stages before release, will oversee the marketing and distribution of the film.[4]  Producers cannot always supervise all of the production. In this case, the primary producer or executive producer may hire and delegate work to associate producers, assistant producers, line producers, or unit production managers.[5]  During this stage of the production process, producers bring together people like the film director, cinematographer, and production designer.[6] Unless the film is to be based on an original script, the producer must find an appropriate screenwriter.[7][8] If an existing script is considered flawed, the producer can order a new version or decide to hire a script doctor.[9][10][11] The producer also gives final approval when hiring the film director, cast members, and other staff.[12][13] In some cases, producers also have the last word when it comes to casting questions.[14] A producer will also approve locations, the studio hire, the final shooting script, the production schedule, and the budget. Spending more time and money in pre-production can reduce budget waste and delays during the production stage.[6]  During production, the producer's job is to ensure the film remains on schedule and under budget.[4] To this end, they must remain in constant contact with directors and other key creative team members.[6][15][16]  Producers cannot always personally supervise all parts of their production but will instead delegate tasks as needed. For example, some producers run a company that also deals with film distribution.[15][16] Also, the cast and film crew often work at different times and places, and certain films even require a second unit.  Even after shooting for a film is complete, the producers can still demand that additional scenes be filmed. In the case of a negative test screening, producers may even demand an alternative film ending. For example, when the audience reacted negatively to Rambo's death in the test screening of the film First Blood, the producers requested a new ending be filmed.[17] Producers also oversee the film's sales, marketing, and distribution rights, often working with third-party specialist firms.[4]  Different types of producers and their roles within the industry today include:  An executive producer oversees all other producers under a specific project and ensures that the entire project remains on track. They are also usually in charge of managing the film's finances and all other business aspects.[1][18] On a television series an executive producer is often a writer and given credit in a creative capacity. In a feature film or movie, the executive producer is often the person directly funding the project or is directly responsible for bringing in investors for funding. In television, it is becoming more and more common to split this role into two for creative projects. These are the executive producer and the showrunner. A showrunner, in this context, is the most senior creative, working on writing and producing their vision; they are effectively the same as the producer; overseeing, arranging, managing, and beginning every aspect of production. Whereas the executive producer focuses more on budgeting and predicting the views of the higher authorities in the wider company; trying to ground the showrunner's vision to tangible limits. A co-executive producer is someone whose input is considered as valuable as that of the executive producer, despite having a junior or unofficial role.[citation needed]  A line producer manages the staff and the day-to-day operations and oversees each physical aspect involved in making a film or television program. The line producer can be credited as \"produced by\" in certain cases.[1][18]  A supervising producer supervises the creative process of screenplay development and often aids in script rewrites. They can also fulfill the executive producer's role of overseeing other producers.[1]  Within the production process, a producer can oversee, arrange, manage, and begin every aspect of production. They are typically involved in every stage of the overall production process.[1][18]  A co-producer is a member of a team of producers that perform all of the functions and roles that a single producer would in a given project.[1]  A coordinating producer coordinates the work\/role of multiple producers trying to achieve a shared result.[1]  The associate or assistant producer helps the producer during the production process. They can sometimes be involved in coordinating others' jobs, such as creating peoples' schedules and hiring the main talent.[1][18]  A segment producer produces one or more specific segments of a multi-segment film or television production.[1]  A field producer helps the producer by overseeing all of the production outside the studio in specific film locations.[18]  Considered executive employees in regard to the Fair Labor Standards Act of 1938 in the United States, producers represent the management team of production and are charged by the studios to enforce the provisions of the union contracts negotiated by the Alliance of Motion Picture and Television Producers (AMPTP) with the below-the-line employees. Founded in 1924 by the U.S. Trade Association as the Association of Motion Picture Producers,[19] the AMPTP was initially responsible for negotiating labor contracts. Still, during the mid-1930s, it took over all contract negotiation responsibilities previously controlled by the Academy of Motion Picture Arts and Sciences.[19] Today, the AMPTP negotiates with various industry associations when dealing with union contracts, including the International Alliance of Theatrical Stage Employees (IATSE), the Directors Guild of America (DGA), and the Screen Actors Guild - American Federation of Television and Radio Artists (SAG-AFTRA).[20] In 2012, the AMPTP negotiated over eighty industry-wide union agreements on behalf of 350 studios and independent production companies. Since 1982, the AMPTP has been responsible for negotiating these union agreements and is now considered the official contract negotiation representative for everyone within the film and television industry.[21]  While individual producers are responsible for negotiating deals with the studios distributing their films, the Producers Guild of America offers guidance to protect and promote the interests of producers and the production team in film, television, and new media, offering the framework to provide health insurance and pension benefits, and assists in establishing safe working conditions and vetting the validity of screen credits.[22]  In December 2021, global unions filed a report titled Demanding Dignity Behind the Scenes to attempt to end the \"long hours culture\" of the television and film industry, citing in part that abuses increased in 2021 as the industry attempted to recover lost time due to the COVID-19 pandemic. The unions supporting the report make up over 20 million television, film, and arts workers worldwide.[23]  Many producers begin in a college, university, or film school. Film schools and many universities offer courses covering film production knowledge, with some courses specially designed for future film producers.[24][25] These courses focus on key topics like pitching, script development, script assessment, shooting schedule design, and budgeting.[26][2][27][28] Students can also expect practical training on post-production.[29] Training at a top-producing school is one of the most efficient ways a student can gain industry credibility.[30]  While education is one way to begin a career as a film producer, experience is also usually required to land a job. Internships are a way to gain experience while in school and give students a foundation to build a career. Many internships are paid, which enables students to earn money while gaining hands-on skills from industry professionals.[31][32] Through internships, students can network within the film industry, which is an important way to make necessary industry connections. Once an internship is over, the next step will typically be to land a junior position, such as a production assistant.[30]  Pay can vary based on the producer's role and the filming location. In the United States, the salary can start between $20,000 and $70,000, even doubling when working in Los Angeles.[33] As of 2022, the average annual salary for a producer in the U.S. is listed as $70,180 per year, with an estimated range from $43,000 to $150,000.[34] When examining more than 15,000 producers in the Los Angeles metropolitan area, the average annual salary is $138,640.[35] Producers can also have an agreement to take a percentage of a movie's sales.[35]  There is no average workday for film producers since their tasks change from day to day. A producer's work hours are often irregular and can consist of long days with the possibility of working nights and weekends.[36] "},"meta":{},"created_at":"2025-03-22T14:25:42.273677Z","updated_at":"2025-03-22T14:25:42.273677Z","inner_id":11,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":20,"annotations":[{"id":20,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"79b06a8e-1f11-4ecf-911a-263e552c1192","import_id":null,"last_action":null,"bulk_created":false,"task":20,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Public finance refers to the monetary resources available to governments and also to the study of finance within government and role of the government in the economy.[1] Within academic settings, public finance is a widely studied subject in many branches of political science, political economy and public economics. Research assesses the government revenue and government expenditure of the public authorities and the adjustment of one or the other to achieve desirable effects and avoid undesirable ones.[2] The purview of public finance is considered to be threefold, consisting of governmental effects on:[3]  American public policy advisor and economist Jonathan Gruber put forth a framework to assess the broad field of public finance in 2010:[4]  One of the more traditional subfields of economics, public finance emphasizes the function and role of government in the economy. A region's inhabitants established a formal or informal entity known as the government to carry out a variety of tasks, including providing for social requirements like education and healthcare as well as protecting the populace's private property from outside threats.  The proper role of government provides a starting point for the analysis of public finance. In theory, under certain circumstances, private markets will allocate goods and services among individuals efficiently (in the sense that no waste occurs and that individual tastes are matching with the economy's productive abilities). If private markets were able to provide efficient outcomes and if the distribution of income were socially acceptable, then there would be little or no scope for government. In many cases, however, conditions for private market efficiency are violated. For example, if many people can enjoy the same good (the moment that good was produced and sold, it starts to give its utility to every one for free) at the same time (non-rival, non-excludable consumption), then private markets may supply too little of that good. National defense is one example of non-rival consumption, or of a public good.[8]  \"Market failure\" occurs when private markets do not allocate goods or services efficiently. The existence of market failure provides an efficiency-based rationale for collective or governmental provision of goods and services.[9] Externalities, public goods, informational advantages, strong economies of scale, and network effects can cause market failures. Public provision via a government or a voluntary association, however, is subject to other inefficiencies, termed \"government failure.\"  Under broad assumptions, government decisions about the efficient scope and level of activities can be efficiently separated from decisions about the design of taxation systems (Diamond-Mirrlees separation). In this view, public sector programs should be designed to maximize social benefits minus costs (cost-benefit analysis), and then revenues needed to pay for those expenditures should be raised through a taxation system that creates the fewest efficiency losses caused by distortion of economic activity as possible. In practice, government budgeting or public budgeting is substantially more complicated and often results in inefficient practices.   Government can pay for spending by borrowing (for example, with government bonds), although borrowing is a method of distributing tax burdens through time rather than a replacement for taxes. A deficit is the difference between government spending and revenues. The accumulation of deficits over time is the total public debt. Deficit finance allows governments to smooth tax burdens over time and gives governments an important fiscal policy tool. Deficits can also narrow the options of successor governments. There is also a difference between public and private finance, in public finance the source of income is indirect, e.g., various taxes (specific taxes, value added taxes), but in private finance sources of income is direct.[10]  Although public finance only began to be viewed as a body of knowledge no more than a century and a half ago, there is evidence of principles common to public finance as early as the bible with discussions of Sunday-trade, slavery regulations, and compassion for the poor. Public finance, although not explicitly named, is often the subject of much of political philosophy.  These concepts can be seen in ancient greece as well, although it was split into two categories there: on one hand the government was to provide for a theater in every city and works of art in the country side. On the other hand, the government was to provide financing for war. Unemployment in ancient Greece was virtually non-existent as Greek economic rule equated heavily to slavery. Greek economic development as per the governmental duties extended to growth, equity, and employment.  The Romans later popularized systemic bodies of law. They guaranteed freedom of contract and property, as well as reasonable price and value. They also developed a well-maintained system of roads and colonies which led to one of the first real tax systems. Their system was tbased on two types of taxes: tributa and vectigalia. The former included the land tax and a poll tax, while the latter was made up of another poll tax, an inheritance tax, a sales tax, and a postage tax. Other taxes depended entirely on the city and were usually temporary. These taxes were used among other things to fund the military, establish trade routes, and fund the cursus publicum. Each region had a set amount to pay which would be collected by aristocrats. Who paid taxes was determined by local officials. The Romans employed a regressive tax system wherein the lower income levels paid higher taxes and the wealthier enjoyed reduced taxation.[11]  During feudalism lacking communication led to issues with pre-existing tax systems. Taxation was organized based on what \"men spend\" in hopes of encouraging investment and savings. Since the government was meant to take care of those who would otherwise turn to charity or crime by means of an allowance provided by a public tax, it is one of the first concepts of what could be considered a negative income tax. Additionally, in England at the time, the main taxes paid were land taxes, a tax that was collected in order to pay for mercenaries. The first mention of a tax in Anglo-Saxon England dates back to the 7th century where it's specified that fines resulting from judicial cases should be paid to the king. Later something known as food rent was introduced, wherein regions would pay a certain amount of their foodstuffs to the king periodically.  This food rent was not too dissimilar from the taxes imposed on serfs in Russia in the Middle Ages wherein they were to pay most of their produce and goods to the local lord. In 1550 serfs were instructed to pay another tax called za povoz, which was imposed on those who refused to deliver the harvest from their fields to their master. Later in the eighteenth and nineteenth century lords began having to pay a per capita tax for each of their peasants and were responsible for their well-being during times of famine.  Toward this time, public finance and interest in how governments were to utilize the money earned from taxes as well as how to provide for their state became increasingly common.  The laissez-faire approach first became popular toward the middle of the 17th century, popularized especially by Charles Davenant. The laissez-faire attitude was especially common with Physiocrats in France (as opposed to the classical school in Britain). They maintained a \"laissez-faire, laisser-passer\" attitude, with one of the central ideas being that the government's central role should be to guarantee private property, and the maintenance of one single tax, namely the produit net, which encompassed the farmer's surplus.  Adam Smith also advocated for the laissez-faire attitude, but also claimed that the government would need to take a more proactive role in protection, justice, and public works. He first proposed the idea of a public good, as he believed that a good could provide a value to society as whole that would exceed the value it would provide to only one individual. Adam Smith also maintained that a government should maintain a properly regulated money flow and banking system, patents as well as copyrights,  and provide public education and transport. For him public projects always needed to yield a profit that would be greater to society than the individual. One of the most pivotal works on taxation, Adam Smith's Canons of Taxation gave further criteria for taxation, namely equality, certainty, convenience, and economy.  Following Adam Smith, several economists expanded on his ideas, or transformed them as in the case of Thomas Robert Malthus, who believed that tax-financed public works would be most effective, so long as it created greater demand for labor and commodities.  Public finance as a field began becoming more well-known and independently recognized around this time, with John Ramsay McCulloch writing many pivotal works in the field.[12]  Collection of sufficient resources from the economy in an appropriate manner along with allocating and use of these resources efficiently and effectively constitute good financial management. Resource generation, resource allocation, and expenditure management (resource utilization) are the essential components of a public financial management system.  The following subdivisions form the subject matter of public finance.  Economists classify government expenditures into three main types. Government purchases of goods and services for current use are classed as government consumption. Government purchases of goods and services intended to create future benefits – such as infrastructure investment or research spending – are classed as government investment. Government expenditures that are not purchases of goods and services, and instead just represent transfers of money – such as social security payments – are called transfer payments.[13]  Government operations are those activities involved in the running of a state or a functional equivalent of a state (for example, tribes, secessionist movements or revolutionary movements) for the purpose of producing value for the citizens.  Government operations have the power to make, and the authority to enforce rules and laws within a civil, corporate, religious, academic, or other organization or group.[14]  Government expenditures are financed primarily in three ways:  How a government chooses to finance its activities can have important effects on the distribution of income and wealth (income redistribution) and on the efficiency of markets (effect of taxes on market prices and efficiency). The issue of how taxes affect income distribution is closely related to tax incidence, which examines the distribution of tax burdens after market adjustments are taken into account.  Public finance research also analyzes effects of the various types of taxes and types of borrowing as well as administrative concerns, such as tax enforcement.  Taxation is the central part of modern public finance. Its significance arises not only from the fact that it is by far the most important of all revenues but also because of the gravity of the problems created by the present day tax burden.[15] The main objective of taxation is raising revenue. A high level of taxation is necessary in a welfare State to fulfill its obligations. Taxation is used as an instrument of attaining certain social objectives, i.e., as a means of redistribution of wealth and thereby reducing inequalities. Taxation in a modern government is thus needed not merely to raise the revenue required to meet its expenditure on administration and social services, but also to reduce the inequalities of income and wealth. Taxation might also be needed to draw away money that would otherwise go into consumption and cause inflation to rise.[16]  A tax is a financial charge or other levy imposed on an individual or a legal entity by a state or a functional equivalent of a state (for example, tribes, secessionist movements or revolutionary movements). Taxes could also be imposed by a subnational entity. Taxes consist of direct tax or indirect tax, and may be paid in money or as corvée labor. A tax may be defined as a \"pecuniary burden laid upon individuals or property to support the government [ . . .] a payment exacted by legislative authority.\"[17]  A tax \"is not a voluntary payment or donation, but an enforced contribution, exacted pursuant to legislative authority\" and is \"any contribution imposed by government [ . . .] whether under the name of toll, tribute, tallage, gabel, impost, duty, custom, excise, subsidy, aid, supply, or other name.\"[18]  Governments, like any other legal entity, can take out loans, issue bonds, and make financial investments. Government debt (also known as public debt or national debt) is money (or credit) owed by any level of government; either central or federal government, municipal government, or local government. Some local governments issue bonds based on their taxing authority, such as tax increment bonds or revenue bonds.  As the government represents the people, government debt can be seen as an indirect debt of the taxpayers. Government debt can be categorized as internal debt, owed to lenders within the country, and external debt, owed to foreign lenders. Governments usually borrow by issuing securities such as government bonds and bills. Less creditworthy countries sometimes borrow directly from commercial banks or international institutions such as the International Monetary Fund or the World Bank.  Most government budgets are calculated on a cash basis, meaning that revenues are recognized when collected and outlays are recognized when paid. Some consider all government liabilities, including future pension payments and payments for goods and services the government has contracted for but not yet paid, as government debt. This approach is called accrual accounting, meaning that obligations are recognized when they are acquired, or accrued, rather than when they are paid. This constitutes public debt.  Seigniorage is the net revenue derived from the issuing of currency. It arises from the difference between the face value of a coin or banknote and the cost of producing, distributing and eventually retiring it from circulation. Seigniorage is an important source of revenue for some national banks, although it provides a very small proportion of revenue for advanced industrial countries.[19]  Public finance in centrally planned economies has differed in fundamental ways from that in market economies. Some state-owned enterprises generated profits that helped finance government activities.. In various mixed economies, the revenue generated by state-owned enterprises is used for various state endeavors; typically the revenue generated by state and government agencies.  Macroeconomic data to support public finance economics are generally referred to as fiscal or government finance statistics (GFS). The Government Finance Statistics Manual 2001 (GFSM 2001) is the internationally accepted methodology for compiling fiscal data.  It is consistent with regionally accepted methodologies such as the European System of Accounts 1995 and consistent with the methodology of the System of National Accounts (SNA1993) and broadly in line with its most recent update, the SNA2008.  The size of governments, their institutional composition and complexity, their ability to carry out large and sophisticated operations, and their impact on the other sectors of the economy warrant a well-articulated system to measure government economic operations.  The GFSM 2001 addresses the institutional complexity of government by defining various levels of government. The main focus of the GFSM 2001 is the general government sector defined as the group of entities capable of implementing public policy through the provision of primarily non market  goods and services and the redistribution of income and wealth, with both activities supported mainly by compulsory levies on other sectors. The GFSM 2001 disaggregates the general government into subsectors: central government, state government, and local government (See Figure 1). The concept of general government does not include public corporations. The general government plus the public corporations comprise the public sector (See Figure 2).  The general government sector of a nation includes all non-private sector institutions, organisations and activities. The general government sector, by convention, includes all the public corporations that are not able to cover at least 50% of their costs by sales, and, therefore, are considered non-market producers.[20]  In the European System of Accounts,[21] the sector \"general government\" has been defined as containing:  Therefore, the main functions of general government units are :  The general government sector, in the European System of Accounts, has four sub-sectors:  \"Central government\"[22] consists of all administrative departments of the state and other central agencies whose responsibilities cover the whole economic territory of a country, except for the administration of social security funds.  \"State government\"[23] is defined as the separate institutional units that exercise some government functions below those units at central government level and above those units at local government level, excluding the administration of social security funds.  \"Local government\"[24] consists of all types of public administration whose responsibility covers only a local part of the economic territory, apart from local agencies of social security funds.  \"Social security fund\"[25] is a central, state or local institutional unit whose main activity is to provide social benefits. It fulfils the two following criteria:  The GFSM 2001 framework is similar to the financial accounting of businesses. For example, it recommends that governments produce a full set of financial statements including the statement of government operations (akin to the income statement), the balance sheet, and a cash flow statement. Two other similarities between the GFSM 2001 and business financial accounting are the recommended use of accrual accounting as the basis of recording and the presentations of stocks of assets and liabilities at market value. It is an improvement on the prior methodology – Government Finance Statistics Manual 1986 – based on cash flows and without a balance sheet statement.  The GFSM 2001 recommends standard tables including standard fiscal indicators that meet a broad group of users including policy makers, researchers, and investors in sovereign debt. Government finance statistics should offer data for topics such as the fiscal architecture, the measurement of the efficiency and effectiveness of government expenditures, the economics of taxation, and the structure of public financing. The GFSM 2001 provides a blueprint for the compilation, recording, and presentation of revenues, expenditures, stocks of assets, and stocks of liabilities. The GFSM 2001 also defines some indicators of effectiveness in government's expenditures, for example the compensation of employees as a percentage of expense. The GFSM 2001 includes a functional classification of expense as defined by the Classification of Functions of Government (COFOG) .  This functional classification allows policy makers to analyze expenditures on categories such as health, education, social protection, and environmental protection. The financial statements can provide investors with the necessary information to assess the capacity of a government to service and repay its debt, a key element determining sovereign risk, and risk premia. Like the risk of default of a private corporation, sovereign risk is a function of the level of debt, its ratio to liquid assets, revenues and expenditures, the expected growth and volatility of these revenues and expenditures, and the cost of servicing the debt. The government's financial statements contain the relevant information for this analysis.  The government's balance sheet presents the level of the debt; that is the government's liabilities. The memorandum items of the balance sheet provide additional information on the debt including its maturity and whether it is owed to domestic or external residents. The balance sheet also presents a disaggregated classification of financial and non-financial assets.  These data help estimate the resources a government can potentially access to repay its debt. The statement of operations  (\"income statement\") contains the revenue and expense accounts of the government. The revenue accounts are divided into subaccounts, including the different types of taxes, social contributions, dividends from the public sector, and royalties from natural resources. Finally, the interest expense account is one of the necessary inputs to estimate the cost of servicing the debt.  GFS can be accessible through several sources. The International Monetary Fund publishes GFS in two publications: International Financial Statistics and the Government Finance Statistics Yearbook. The World Bank gathers information on external debt. On a regional level, the Organization for Economic Co-operation and Development (Dibidami ) compiles general government account data for its members, and Eurostat, following a methodology compatible with the GFSM 2001, compiles GFS for the members of the European Union.  Social equality is the equivalent treatment of and opportunity for members of different groups within society regardless of individual distinctions of race, ethnicity, gender, age, social class, socioeconomic status, sexual orientation, or other characteristics or circumstances.[26]  Social fairness includes the equal access of the various groups forming society to the financial resources and opportunities in all areas. This concept is ensuring that every individual, despite their socioeconomic condition, race, gender, and other qualities, get equal opportunities to benefit from public services that relate to health, education, and social welfare.  The core tenets of promoting social equality through public finance include:[27]  To achieve social equality, governments employ a variety of strategies, including:  The challenges of a political nature and budget constraints are among many, which could prevent integration of social equality into public finance. There has to be constant review and fine-tuning of the policies with full commitment to fairness in the distribution of public finance.  These principles and strategies might very well make public finance one of the strongest allies for social equality—one where everyone has, under any circumstance, fair chances for success and participation in the benefits provided by society.  Social equity is the fair, just and equitable management of all institutions serving the public directly or by contract; and the fair and equitable distribution of public services, and implementation of public policy; and the commitment to promote fairness, justice and equity in the formation of public policy.[29]  Social equity in public finance underlies the principles and practices that work toward fair distribution of public resources, especially geared towards reducing inequalities between different social groups. This idea is very relevant for designing and implementing public policies, notably towards issues such as taxation, budget allocations, and public spending.  Social equity requires formulating fiscal policies that are not only fair but also inclusive across all groups of society. In more cases than not, this calls for a progressive tax system, where people of higher income bring in a higher percentage of their incomes, which helps in redistributing the resources toward communities that are under-served.  This underpins the importance for local governments to consider social equity in their strategic planning and budgeting process. This achieved through techniques such as the \"veil of ignorance,\" which would be applied to make sure that policy makers design systems without any bias based on their personal characteristics, such as race, income, or place of residence. Under this experiment, decision-makers would ideally create systems that they would deem fair regardless of their status in society.[30]  In practice, successful involvement of social equity in public finance often requires a focus on specific demographic groups most affected by disparities, such as those differentiated by race, socio-economic status, or geographic location. Local governments may, in this way, bring equity by being able to customize public services and even distribution of resources to those groups, effectively dealing with the systemic inequalities.  Achieving social equity is challenged by limited resources, political resistance, and economic disparities, respectively. Overcoming them calls for transparent policymaking, antidiscrimination laws, and region-specific policies. Success, on the other hand, calls for involvement from the stakeholders and effective governance that takes into account long-term planning and sustainability.[31]  Clear goals have to be set with regard to measuring social equity and also relevant metrics. Common measures include accessibility to quality education, health outcomes from healthcare, and walkability of neighborhoods. Local governments can measure through such indicators the efficiency of their efforts and performance in the allocation of resources from the budget to take care of the social disparities.[32]  In so doing, public finance may appear as a very strong lever that guarantees social equity based on the ability of all members of the community to gain respective fair access to the opportunities required for their well-being and success. "},"meta":{},"created_at":"2025-03-22T14:25:42.273677Z","updated_at":"2025-03-22T14:25:42.273677Z","inner_id":12,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":21,"annotations":[{"id":21,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"add84871-4f48-46b4-9cd6-f8dbcfbc8f97","import_id":null,"last_action":null,"bulk_created":false,"task":21,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A film genre is a stylistic or thematic category for motion pictures based on similarities either in the narrative elements, aesthetic approach, or the emotional response to the film.[2]  Drawing heavily from the theories of literary-genre criticism, film genres are usually delineated by \"conventions, iconography, settings, narratives, characters and actors\".[3] One can also classify films by the tone, theme\/topic, mood, format, target audience, or budget.[4] These characteristics are most evident in genre films, which are \"commercial feature films [that], through repetition and variation, tell familiar stories with familiar characters and familiar situations\" in a given genre.[5]  A film's genre will influence the use of filmmaking styles and techniques, such as the use of flashbacks and low-key lighting in film noir; tight framing in horror films; or fonts that look like rough-hewn logs for the titles of Western films.[6] In addition, genres have associated film scoring conventions, such as lush string orchestras for romantic melodramas or electronic music for science fiction films.[6] Genre also affects how films are broadcast on television, advertised, and organized in video rental stores.[5]  Alan Williams distinguishes three main genre categories: narrative, avant-garde, and documentary.[7]  With the proliferation of particular genres, film subgenres can also emerge: the legal drama, for example, is a sub-genre of drama that includes courtroom- and trial-focused films. Subgenres are often a mixture of two separate genres; genres can also merge with seemingly unrelated ones to form hybrid genres, where popular combinations include the romantic comedy and the action comedy film. Broader examples include the docufiction and docudrama, which merge the basic categories of fiction and non-fiction (documentary).[8]  Genres are not fixed; they change and evolve over time, and some genres may largely disappear (for example, the melodrama).[4] Not only does genre refer to a type of film or its category, a key role is also played by the expectations of an audience about a film, as well as institutional discourses that create generic structures.[4]  Characteristics of particular genres are most evident in genre films, which are \"commercial feature films [that], through repetition and variation, tell familiar stories with familiar characters and familiar situations\" in a given genre.[5]  Drawing heavily from the theories of literary-genre criticism, film genres are usually delineated by conventions, iconography, narratives, formats, characters, and actors, all of which can vary according to the genre.[3] In terms of standard or \"stock\" characters, those in film noir, for example, include the femme fatale[9] and the \"hardboiled\" detective; while those in Westerns, stock characters include the schoolmarm and the gunslinger. Regarding actors, some may acquire a reputation linked to a single genre, such as John Wayne (the Western) or Fred Astaire (the musical).[10] Some genres have been characterized or known to use particular formats, which refers to the way in which films are shot (e.g., 35 mm, 16 mm or 8 mm) or the manner of presentation (e.g., anamorphic widescreen).[4]  Genres can also be classified by more inherent characteristics (usually implied in their names), such as settings, theme\/topic, mood, target audience, or budget\/type of production.[4]  Screenwriters, in particular, often organize their stories by genre, focusing their attention on three specific aspects: atmosphere, character, and story.[11] A film's atmosphere includes costumes, props, locations, and the visceral experiences created for the audience.[12] Aspects of character include archetypes, stock characters, and the goals and motivations of the central characters.[13] Some story considerations for screenwriters, as they relate to genre, include theme, tent-pole scenes, and how the rhythm of characters' perspective shift from scene to scene.[14]  From the earliest days of cinema in the 19th century the term \"genre\" (already in use in English with reference to works of art or literary production from at least 1770[26]) was used[by whom?] to organize films according to type.[27] By the 1950s André Bazin was discussing the concept of \"genre\" by using the Western film as an example; during this era, there was a debate over auteur theory versus genre.[4] In the late 1960s the concept of genre became a significant part of film theory.[4]  Film genres draw on genres from other forms; Western novels existed before the Western film, and musical theatre pre-dated film musicals.[28] The perceived genre of a film can change over time; for example, in the 21st century The Great Train Robbery (1903) classes as a key early Western film, but when released, marketing promoted it \"for its relation to the then-popular genres of the chase film, the railroad film and the crime film\".[29] A key reason that the early Hollywood industrial system from the 1920s to the 1950s favoured genre films is that in \"Hollywood's industrial mode of production, genre movies are dependable products\" to market to audiences – they were easy to produce and it was easy for audiences to understand a genre film.[30] In the 1920s to 1950s, genre films had clear conventions and iconography, such as the heavy coats worn by gangsters in films like Little Caesar (1931).[31] The conventions in genre films enable filmmakers to generate them in an industrial, assembly-line fashion, an approach which can be seen in the James Bond spy-films, which all use a formula of \"lots of action, fancy gadgets, beautiful woman and colourful villains\", even though the actors, directors and screenwriters change.[31]  Films are rarely purely from one genre, which is in keeping with the cinema's diverse and derivative origins, it being a blend of \"vaudeville, music-hall, theatre, photography\" and novels.[4] American film historian Janet Staiger states that the genre of a film can be defined in four ways. The \"idealist method\" judges films by predetermined standards. The \"empirical method\" identifies the genre of a film by comparing it to a list of films already deemed to fall within a certain genre. The a priori method uses common generic elements which are identified in advance. The \"social conventions\" method of identifying the genre of a film is based on the accepted cultural consensus within society.[32] Martin Loop contends that Hollywood films are not pure genres because most Hollywood movies blend the love-oriented plot of the romance genre with other genres.[32] Jim Colins claims that since the 1980s, Hollywood films have been influenced by the trend towards \"ironic hybridization\", in which directors combine elements from different genres, as with the Western\/science fiction mix in Back to the Future Part III.[32]  Many films cross into multiple genres. Susan Hayward states that spy films often cross genre boundaries with thriller films.[4] Some genre films take genre elements from one genre and place them into the conventions of a second genre, such as with The Band Wagon (1953), which adds film noir and detective film elements into \"The Girl Hunt\" ballet.[31] In the 1970s New Hollywood era, there was so much parodying of genres that it can be hard to assign genres to some films from this era, such as Mel Brooks' comedy-Western Blazing Saddles (1974) or the private eye parody The Long Goodbye (1973).[4] Other films from this era bend genres so much that it is challenging to put them in a genre category, such as Roman Polanski's Chinatown (1974) and William Friedkin's The French Connection (1971).[4]  Film theorist Robert Stam challenged whether genres really exist, or whether they are merely made up by critics. Stam has questioned whether \"genres [are] really 'out there' in the world or are they really the construction of analysts?\". As well, he has asked whether there is a \"... finite taxonomy of genres or are they in principle infinite?\" and whether genres are \"...timeless essences ephemeral, time-bound entities? Are genres culture-bound or trans-cultural?\". Stam has also asked whether genre analysis should aim at being descriptive or prescriptive. While some genres are based on story content (the war film), other are borrowed from literature (comedy, melodrama) or from other media (the musical). Some are performer-based (Fred Astaire and Ginger Rogers films) or budget-based (blockbusters, low-budget film), while others are based on artistic status (the art film), racial identity (race films), location (the Western), or sexual orientation (\"New Queer Cinema\").[33]  Many genres have built-in audiences and corresponding publications that support them, such as magazines and websites. For example, horror films have a well-established fanbase that reads horror magazines such as Fangoria. Films that are difficult to categorize into a genre are often less successful. As such, film genres are also useful in the areas of marketing, film criticism and the analysis of consumption. Hollywood story consultant John Truby states that \"...you have to know how to transcend the forms [genres] so you can give the audience a sense of originality and surprise\".[34]  Some screenwriters use genre as a means of determining what kind of plot or content to put into a screenplay. They may study films of specific genres to find examples. This is a way that some screenwriters are able to copy elements of successful movies and pass them off in a new screenplay. It is likely that such screenplays fall short in originality. As Truby says, \"Writers know enough to write a genre script but they haven't twisted the story beats of that genre in such a way that it gives an original face to it\".[35]  Cinema technologies are associated with genres. Huge widescreens helped Western films to create an expansive setting of the open plains and desert. Science fiction and fantasy films are associated with special effects, notably computer generated imagery (e.g., the Harry Potter films).[4]  In 2017, screenwriter Eric R. Williams published a system for screenwriters to conceptualize narrative film genres based on audience expectations.[36] The system was based upon the structure biologists use to analyze living beings. Williams wrote a companion book detailing his taxonomy, which claims to be able to identify all feature length narrative films with seven categorizations: film type, super genre, macro-genre, micro-genre, voice, and pathway.[37]  Because genres are easier to recognize than to define, academics agree they cannot be identified in a rigid way.[38] Furthermore, different countries and cultures define genres in different ways. A typical example are war movies. In United States, they are mostly related to ones with large U.S. involvement such as World wars and Vietnam, whereas in other countries, movies related to wars in other historical periods are considered war movies.  Film genres may appear to be readily categorizable from the setting of the film. Nevertheless, films with the same settings can be very different, due to the use of different themes or moods. For example, while both The Battle of Midway and All Quiet on the Western Front are set in a wartime context and might be classified as belonging to the war film genre, the first examines the themes of honor, sacrifice, and valour, and the second is an anti-war film which emphasizes the pain and horror of war. While there is an argument that film noir movies could be deemed to be set in an urban setting, in cheap hotels and underworld bars, many classic noirs take place mainly in small towns, suburbia, rural areas, or on the open road.[39]  The editors of filmsite.org argue that animation, pornographic film, documentary film, silent film and so on are non-genre-based film categories.[40]  Linda Williams argues that horror, melodrama, and pornography all fall into the category of \"body genres\" since they are each designed to elicit physical reactions on the part of viewers. Horror is designed to elicit spine-chilling, white-knuckled, eye-bulging terror; melodramas are designed to make viewers cry after seeing the misfortunes of the onscreen characters; and pornography is designed to elicit sexual arousal.[41] This approach can be extended: comedies make people laugh, tear-jerkers make people cry, feel-good films lift people's spirits and inspiration films provide hope for viewers.  Eric R. Williams (no relation to Linda Williams) argues that all narrative feature-length films can be categorized as one of eleven \"super genres\" (action, crime, fantasy, horror, romance, science fiction, slice of life, sports, thriller, war and Western).[11] Williams contends that labels such as comedy or drama are more broad than the category of super genre, and therefore fall into a category he calls \"film type\".[36] Similarly, Williams explains that labels such as animation and musical are more specific to storytelling technique and therefore fall into his category of \"voice\".[42] For example, according to Williams, a film like Blazing Saddles could be categorized as a comedy (type) Western (super-genre) musical (voice), while Anomalisa is a drama (type) Slice of Life (super-genre) animation (voice). Williams has created a seven-tiered categorization for narrative feature films called the Screenwriters Taxonomy.[37]  A genre movie is a film that follows some or all of the conventions of a particular genre, whether or not it was intentional when the movie was produced.[43]  In order to understand the creation and context of each film genre, we must look at its popularity in the context of its place in history. For example, the 1970s Blaxploitation films have been called an attempt to \"undermine the rise of Afro-American's Black consciousness movement\" of that era.[4] In William Park's analysis of film noir, he states that we must view and interpret film for its message with the context of history within our minds; he states that this is how film can truly be understood by its audience.[44] Film genres such as film noir and Western film reflect values of the time period. While film noir combines German expressionist filming strategies with post World War II ideals; Western films focused on the ideal of the early 20th century. Films such as the musical were created as a form of entertainment during the Great Depression allowing its viewers an escape during tough times. So when watching and analyzing film genres we must remember to remember its true intentions aside from its entertainment value.  Over time, a genre can change through stages: the classic genre era; the parody of the classics; the period where filmmakers deny that their films are part of a certain genre; and finally a critique of the entire genre.[4] This pattern can be seen with the Western film. In the earliest, classic Westerns, there was a clear hero who protected society from lawless villains who lived in the wilderness and came into civilization to commit crimes.[4] However, in revisionist Westerns of the 1970s, the protagonist becomes an antihero who lives in the wilderness to get away from a civilization that is depicted as corrupt, with the villains now integrated into society. Another example of a genre changing over time is the popularity of the neo-noir films in the early 2000s (Mulholland Drive (2001), The Man Who Wasn't There (2001) and Far from Heaven (2002); are these film noir parodies, a repetition of noir genre tropes, or a re-examination of the noir genre?[4]  This is also important to remember when looking at films in the future. As viewers watch a film they are conscious of societal influence with the film itself. In order to understand its true intentions, we must identify its intended audience and what narrative of our current society, as well as it comments to the past in relation with today's society. This enables viewers to understand the evolution of film genres as time and history morphs or views and ideals of the entertainment industry. "},"meta":{},"created_at":"2025-03-22T14:25:42.274677Z","updated_at":"2025-03-22T14:25:42.274677Z","inner_id":13,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":22,"annotations":[{"id":22,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"82ca1017-079d-42ea-87c3-462522769735","import_id":null,"last_action":null,"bulk_created":false,"task":22,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  The Academy Awards, commonly known as the Oscars, are awards for artistic and technical merit in film.[1][2] They are presented annually by the Academy of Motion Picture Arts and Sciences (AMPAS) in the United States in recognition of excellence in cinematic achievements as assessed by the Academy's voting membership.[3] The Oscars are widely considered to be the most prestigious awards in the film industry.[4]  The major award categories, known as the Academy Awards of Merit,[5] are presented during a live-televised Hollywood ceremony in February or March. It is the oldest worldwide entertainment awards ceremony.[1] The 1st Academy Awards were held in 1929.[6] The second ceremony, in 1930, was the first one broadcast by radio. The 1953 ceremony was the first one televised.[1] It is the oldest of the four major annual American entertainment awards. Its counterparts—the Emmy Awards for television, the Tony Awards for theater, and the Grammy Awards for music—are modeled after the Academy Awards.[7]  The Oscar statuette depicts a knight, rendered in the Art Deco style.[8]  Oppenheimer  Anora  The first Academy Awards presentation was held on May 16, 1929, at a private dinner function at the Hollywood Roosevelt Hotel, with an audience of about 270 people.[9]  The post-awards party was held at the Mayfair Hotel.[1] The cost of guest tickets for that night's ceremony was $5 (equivalent to $92 in 2024). Fifteen statuettes were awarded, honoring artists, directors, and other participants in the film-making industry of the time, for their works during the 1927–28 period. The ceremony ran for 15 minutes.  For this first ceremony, winners were announced to the media three months earlier.[10] For the second ceremony in 1930, and the rest of the first decade, the results were given to newspapers for publication at 11:00 pm on the night of the awards.[1] In 1940, the Los Angeles Times announced the winners before the ceremony began. As a result, in 1941 the Academy started using a sealed envelope to reveal the names of the winners.[1]  The term \"Oscar\" is a registered trademark of the AMPAS.  The first Best Actor awarded was Emil Jannings, for his performances in The Last Command and The Way of All Flesh. As he had to return to Europe before the ceremony, the Academy agreed to give him the prize early, making him the first Academy Award recipient. For the first Awards, winners were recognized for multiple films during the qualifying period; Jannings received the award for two films in which he starred, and Janet Gaynor won the first Best Actress award for performances in three films. Beginning with the second ceremony, performers received separate nominations for individual films; no performer has received multiple nominations in the same category since the 3rd Academy Awards.  For the first five ceremonies, the eligibility period ran from August 1 to July 31. The 6th Academy Awards' eligibility ran from August 1, 1932, to December 31, 1933, and as of the 7th Academy Awards, subsequent eligibility periods have matched the calendar year (with the exception of the 93rd Academy Awards, which, due to the COVID-19 pandemic, extended the eligibility period to February 28, 2021).[1]  Best Foreign Language Film, now known as Best International Feature Film, was introduced at the 20th Academy Awards as a special award, and became a competitive category at the 29th Academy Awards.[11]  The 74th Academy Awards, held in 2002, presented the first Academy Award for Best Animated Feature.[12]  Since 1973, all Academy Awards ceremonies, except for 2021, have ended with the Academy Award for Best Picture. Traditionally, the previous year's winners for Best Actor and Best Supporting Actor present the awards for Best Actress and Best Supporting Actress, respectively, and vice versa. In 2009, this model was replaced by each acting award being introduced by five previous winners, each of whom introduces one of the nominated performances, referred to as the \"Fab 5\" presenters format.[13] The Fab 5 model returned in 2024 after a 15-year hiatus.  On February 9, 2020, Parasite became the first foreign-language film to win Best Picture at the 92nd Academy Awards.[14]  The 93rd Academy Awards ceremony, honoring the best films of 2020 and early 2021, was held on April 25, 2021, after it was postponed from its original February 28, 2021, schedule due to the impact of the COVID-19 pandemic on cinema. As with the two previous ceremonies, there was no host. The ceremony was broadcast on ABC. It took place at the Dolby Theatre in Los Angeles, California for the 19th consecutive year, with satellite locations at Union Station also in Los Angeles.[15] Because of the virus impact on films and TV industries, Academy president David Rubin and CEO Dawn Hudson announced that for the 2021 Oscar Ceremony, streaming films with a previously planned theatrical release were eligible.[16] The theatrical requirement was reinstated starting with the 95th Academy Awards.[17]  The Oscar statuette, officially the Academy Award of Merit,[18] is given to winners of each year's awards. Made of gold-plated bronze on a black metal base, it is 13.5 in (34.3 cm) tall, weighs 8.5 lb (3.9 kg) and depicts a knight rendered in Art Deco style holding a sword standing on a reel of film with five spokes. The five spokes represent the original branches of the Academy: Actors, Writers, Directors, Producers, and Technicians.[19]  Sculptor George Stanley, who also did the Muse Fountain at the Hollywood Bowl, sculpted Cedric Gibbons' design. The statuettes presented at the initial ceremonies were gold-plated solid bronze. Within a few years, the bronze was abandoned in favor of Britannia metal, a pewter-like alloy that is then plated in copper, nickel silver, and finally, 24-karat gold.[18] Due to a metal shortage during World War II, Oscars were made of painted plaster for three years. Following the war, the Academy invited recipients to redeem the plaster figures for gold-plated metal ones.[20]  The only addition to the Oscar since it was created is a minor streamlining of the base. The original Oscar mold was cast in 1928 at the C.W. Shumway & Sons Foundry in Batavia, Illinois, which also contributed to casting the molds for the Vince Lombardi Trophy and Emmy Award statuettes. During the 1970s, the Oscar statues were cast in Crystal Lake, Illinois.[21] From 1983 to 2015,[22] approximately 50 Oscars in a tin alloy with gold plating were made each year in Chicago by Illinois manufacturer R.S. Owens & Company.[23] It would take between three and four weeks to manufacture 50 statuettes.[24]  In 2016, the Academy returned to bronze as the core metal of the statuettes, handing manufacturing duties to Walden, New York-based Polich Tallix Fine Art Foundry, now owned and operated by UAP Urban Art Projects.[25][26] While based on a digital scan of an original 1929 Oscar, the statuettes retain their modern-era dimensions and black pedestal. Cast in liquid bronze from 3D-printed ceramic molds and polished, they are then electroplated in 24-karat gold by Brooklyn, New York-based Epner Technology. The time required to produce 50 such statuettes is roughly three months.[27] R.S. Owens is expected to continue producing other awards for the Academy, and service existing Oscars that need replating.[28]  The origin of the nickname of the trophy has been disputed, as multiple people have taken credit for naming the trophy \"Oscar\".  Margaret Herrick, librarian and president of the Academy, may have said she named it after her supposed uncle Oscar in 1931.[a] The only corroboration was a 1938 clipping from the Los Angeles Examiner, in which Herrick told a story of her and her husband joking with each other using the phrase, \"How's your uncle Oscar\".[29]  Bette Davis, in her 1962 autobiography, claimed she named it in 1936 after her first husband, Harmon Oscar Nelson, of whom the statue's rear end reminded her.[29][30] But the term had been in use at least two years before. In a 1974 biography written by Whitney Stine with commentary from Davis, Davis wrote \"I relinquish once and for all any claim that I was the one—so, Academy of Motion Picture Arts and Sciences, the honor is all yours\".[29][31]  Columnist Sidney Skolsky wrote in his 1970 memoir that he came up with the term in 1934 under pressure for a deadline, mocking Vaudeville comedians who asked \"Will you have a cigar, Oscar?\" The Academy credits Skolsky with \"the first confirmed newspaper reference\" to Oscar in his column on March 16, 1934, which was written about that year's 6th Academy Awards.[32] But in the newspaper clipping that Skolsky referred to, he wrote that these statues are called 'Oscars', meaning that the name was already in use.[29]  Bruce Davis, a former executive director of the Academy, credited Eleanore Lilleberg, a secretary at the Academy when the award was first introduced, for the nickname. She had overseen the pre-ceremony handling of the awards. Davis credits Lilleberg because he found in an autobiography of Einar Lilleberg, Eleanore's brother, that Einar had referenced a Norwegian army veteran named Oscar whom the two knew in Chicago, whom Einar described as having always \"stood straight and tall\".[29][33] He asserts credit \"should almost certainly belong to\" Lilleberg.[33]  In 2021, Brazilian researcher Dr. Waldemar Dalenogare Neto found the probable first public mention of the name \"Oscar\", in journalist Relman Morin's \"Cinematters\" column in the Los Angeles Evening Record on December 5, 1933. Since the awards didn't take place that year, he said: \"What's happened to the annual Academy banquet? As a rule, the banquet and the awarding of \"Oscar\", the bronze statuette given for best performances, is all over long before this\". This information changes the version of Sidney Skolsky as the first to publicly mention the name.[34]  To prevent information identifying the Oscar winners from leaking ahead of the ceremony, Oscar statuettes presented at the ceremony have blank baseplates. Until 2010, winners returned their statuettes to the Academy and had to wait several weeks to have their names inscribed on their respective Oscars. Since 2010, winners have had the option of having engraved nameplates applied to their statuettes at an inscription-processing station at the Governor's Ball, a party held immediately after the Oscar ceremony. The R.S. Owens company has engraved nameplates made before the ceremony, bearing the name of every potential winner. The nameplates for the non-winning nominees are later recycled.[35][36]  Before 1950, Oscar statuettes were, and remain, the property of the recipient.[37] Since then the statuettes have been legally encumbered by the requirement that the statuette be first offered for sale back to the Academy for $1. If a winner refuses to agree to this stipulation, then the Academy keeps the statuette. Academy Awards predating this agreement have been sold in public auctions and private deals for six-figure sums.[38]  In 1989, Michael Todd's grandson tried to sell Todd's Best Picture Oscar for his 1956 production of Around the World in 80 Days to a movie prop collector. The Academy earned enforcement of its statuette contract by gaining a permanent injunction against the sale.  In 1992, Harold Russell consigned his 1946 Oscar for Best Supporting Actor for The Best Years of Our Lives to auction to raise money for his wife's medical expenses. Though his decision caused controversy, the first Oscar ever to be sold passed to a private collector on August 6, 1992, for $60,500 (equivalent to $135,562 in 2024). Russell defended his action, saying, \"I don't know why anybody would be critical. My wife's health is much more important than sentimental reasons. The movie will be here, even if Oscar isn't\".[39]  In December 2011, Orson Welles' 1941 Oscar for Citizen Kane (Academy Award for Best Original Screenplay) was put up for auction, after his heirs won a 2004 court decision contending that Welles did not sign any agreement to return the statue to the Academy.[40] On December 20, 2011, it sold in an online auction for $861,542 (equivalent to $1,204,247 in 2024).[41]  Some buyers have subsequently returned the statuettes to the Academy, which keeps them in its treasury.[38]  In addition to the Academy Award of Merit (Oscar award), there are nine honorary (non-competitive) awards presented by the Academy from time to time (except for the Academy Honorary Award, the Technical Achievement Award, and the Student Academy Awards, which are presented annually):[42]  The Academy also awards Nicholl Fellowships in Screenwriting.  From 2004 to 2020, the Academy Award nomination results were announced to the public in mid-January. Prior to that, the results were announced in early February. In 2021, the nominees were announced in March. In 2022, the nominees were announced in early February for the first time since 2003.  The Academy of Motion Picture Arts and Sciences (AMPAS), a professional honorary organization, is composed of 9,905 voting members as of 2024[update].[43][44]  Academy membership is divided into different branches, with each representing a different discipline in film production. As of 2024[update], actors constitute the largest bloc, numbering 1,258 (12.7% of the voting body).[44] Votes have been certified by the auditing firm PricewaterhouseCoopers, and its predecessor Price Waterhouse, since the 7th Academy Awards in 1935.[45][46][47] In May 2011, the Academy sent a letter advising its then-6,000 or so voting members that an online system for Oscar voting would be implemented in 2013, replacing mailed paper ballots.[48]  All AMPAS members must be invited to join by the Board of Governors, on behalf of Academy Branch Executive Committees. Membership eligibility may be achieved by a competitive nomination, or an existing member may submit a name, based on other significant contributions to the field of motion pictures.  New membership proposals are considered annually. The Academy does not publicly disclose its membership, although as recently as 2007 press releases have announced the names of those who have been invited to join.[49]  In 2012, the results of a study conducted by the Los Angeles Times were published describing the demographic breakdown of approximately 88% of AMPAS' voting membership. Of the 5,100+ active voters confirmed, 94% were Caucasian, 77% were male, and 54% were found to be over the age of 60. 33% of voting members are former nominees (14%) and winners (19%).[50] In 2016, the Academy launched an initiative to expand its membership and increase diversity. In 2024, voting membership stood at 9,905.[44]  According to Rules 2 and 3 of the official Academy Awards Rules, a film must open in the previous calendar year, from midnight at the start of January 1 to midnight at the end of December 31, in Los Angeles County, California, and play for seven consecutive days, to qualify, except for the Best International Feature Film, Best Documentary Feature, and awards in short film categories. The film must be shown at least three times on each day of its qualifying run, with at least one of the daily showings starting between 6 pm and 10 pm local time.[51][52]  For example, the 2009 Best Picture winner, The Hurt Locker, was originally first released in 2008, but did not qualify for the 2008 awards, as it did not play its Oscar-qualifying run in Los Angeles until mid-2009, thus qualifying for the 2009 awards. Foreign films must include English subtitles. Each country can submit only one film for consideration in the International Feature Film category per year.[53]  Rule 2 states that a film must be feature-length, defined as a minimum of 40 minutes, except for short-subject awards. It must exist either on a 35 mm or 70 mm film print, or in 24 frame\/s or 48 frame\/s progressive scan digital cinema format, with a minimum projector resolution of 2,048 by 1,080 pixels.[54] Since the 90th Academy Awards, presented in 2018, multi-part and limited series have been ineligible for the Best Documentary Feature award. This followed the win of O.J.: Made in America, an eight-hour presentation that was screened in a limited release before being broadcast in five parts on ABC and ESPN, in that category in 2017. The Academy's announcement of the new rule made no direct mention of that film.[33]  The Best International Feature Film award does not require a U.S. release. It requires the film to be submitted as its country's official selection.  The Best Documentary Feature award requires either week-long releases in both Los Angeles County and any of the five boroughs of New York City during the previous calendar year,[b] or a qualifying award at a competitive film festival from the Documentary Feature Qualifying Festival list, regardless of any public exhibition or distribution, or submission in the International Feature Film category as its country's official selection. The qualifying theatrical runs must meet the same requirements as those for non-documentary films regarding numbers and times of screenings. A film must have been reviewed by a critic from The New York Times, Time Out New York, the Los Angeles Times, or LA Weekly.[56]  Producers must submit an Official Screen Credits online form before the deadline. If it is not submitted by the defined deadline, the film will be ineligible for Academy Awards in any year. The form includes the production credits for all related categories.  Awards in short film categories (Best Documentary Short Subject, Best Animated Short Film, and Best Live Action Short Film) have different eligibility rules from most other competitive awards. First, the qualifying period for release does not coincide with a calendar year, instead covering one year starting on October 1, and ending on September 30 of the calendar year before the ceremony. Second, there are multiple methods of qualification. The main method is a week-long theatrical release in either New York City or Los Angeles County during the eligibility period. Films also can qualify by winning specified awards at one of several competitive film festivals designated by the Academy, also without regard to prior public distribution.[56][57]  A film that is selected as a gold, silver, or bronze medal winner in an appropriate category of the immediately previous Student Academy Awards is also eligible (Documentary category for that award, and Animation, Narrative, Alternative, or International for the other awards). The requirements for the qualifying theatrical run are also different from those for other awards. Only one screening per day is required. For the Documentary award, the screening must start between noon and 10 pm local time. For other awards, no specific start time is required, but the film must appear in regular theater listings with dates and screening times.[56][57]  In late December, ballots and lists of eligible films are sent to the membership. For most categories, members from each of the branches vote to determine the nominees only in their respective categories, i.e. only directors vote for directors, writers for writers, actors for actors, etc. In the special case of Best Picture, all voting members are eligible to select the nominees. A number of branches are only eligible to vote in Best Picture during nomination voting; this includes a producers' branch, as Best Picture is awarded to a film's producer(s), and other branches which have no corresponding award.[44] In all major categories, a variant of the single transferable vote is used, with each member casting a ballot with up to five nominees (ten for Best Picture) ranked preferentially.[58][59][60] In certain categories, including International Feature Film, Documentary and Animated Feature, nominees are selected by special screening committees made up of members from all branches.  In most categories, the winner is selected from among the nominees by plurality voting of all members.[58][60] Since 2009, the Best Picture winner has been chosen by instant-runoff voting.[60][61] Since 2013, re-weighted range voting has been used to select the nominees for the Best Visual Effects.[62]  Film companies will spend as much as several million dollars on marketing to awards voters for a film in the running for Best Picture, in attempts to improve chances of receiving Oscars and other film awards conferred in Oscar season. The Academy enforces rules to limit overt campaigning by its members to try to eliminate excesses and prevent the process from becoming undignified. It has an awards czar on staff who advises members on allowed practices and levies penalties on offenders.[63] For example, a producer of the 2009 Best Picture nominee The Hurt Locker was disqualified as a producer in the category when he contacted associates urging them to vote for his film and not another that was seen as the front-runner. The Hurt Locker eventually won.  The Academy Screening Room or Academy Digital Screening Room is a secure streaming platform which allows voting members of the Academy to view all eligible films (except, initially, those in the International category) in one place. It was introduced in 2019, for the 2020 Oscars. DVD screeners and Academy in-person screenings were still provided. For films to be included on the platform, the North American distributor must pay $12,500, including a watermarking fee, and a digital copy of the film to be prepared for streaming by the Academy. The platform can be accessed via Apple TV and Roku players.[64][65] The watermarking process involved several video security firms, creating a forensic watermark and restricting the ability to take screenshots or screen recordings.[66]  In 2021, for the 2022 Oscars, the Academy banned all physical screeners and in-person screenings, restricting official membership viewing to the Academy Screening Room. Films eligible in the Documentary and International categories were made available in different sections of the platform. Distributors can also pay an extra fee to add video featurettes to promote their films on the platform.[67] The in-person screenings were said to be cancelled because of the COVID-19 pandemic.[68] Eligible films do not have to be added to the platform, but the Academy advertises them to voting members when they are.[67]  The major awards are presented at a live televised ceremony, commonly in late February or early March following the relevant calendar year, and six weeks after the announcement of the nominees. It is the culmination of the film awards season, which usually begins during November or December of the previous year. This is an elaborate extravaganza, with the invited guests walking up the red carpet in the creations of the most prominent fashion designers of the day. Black tie dress is the most common outfit for men. Fashion may dictate not wearing a bow tie, and musical performers are sometimes not required to adhere to this. The artists who recorded the nominees for Best Original Song quite often perform those songs live at the awards ceremony, and the fact that they are performing is often used to promote the television broadcast.  The Academy Awards is the world's longest-running awards show televised live from the United States to all time zones in North America and worldwide, and gathers billions of viewers elsewhere throughout the world.[69] The Oscars were first televised in 1953 by NBC, which continued to broadcast the event until 1960, when ABC took over, televising the festivities, including the first color broadcast of the event in 1966, to 1970. NBC regained the rights for five years (1971–75), then ABC resumed broadcast duties in 1976 and its current contract with the Academy runs through 2028.[70]  The Academy has produced condensed versions of the ceremony for broadcast in international markets, especially those outside of the Americas, in more desirable local timeslots. The ceremony was broadcast live internationally for the first time via satellite since 1970, but only two South American countries, Chile and Brazil, purchased the rights to air the broadcast. By that time, the television rights to the Academy Awards had been sold in 50 countries. In 1980, the rights were sold to 60 countries, and by 1984, the television rights to the Academy Awards were licensed in 76 countries.  In 2004, the ceremonies were moved up from late March\/early April to late February, to help disrupt and shorten the intense lobbying and ad campaigns associated with Oscar season in the film industry. Another reason was because of the growing television ratings success coinciding with the NCAA division I men's basketball tournament, which would cut into the Academy Awards audience. In 1976 and 1977, ABC's regained Oscars were moved from Tuesday to Monday and went directly opposite the national championship game on NBC. The earlier date is also to the advantage of ABC, as it now usually occurs during the highly profitable and important February sweeps period.[71]  Some years, the ceremony is moved into the first Sunday of March to avoid a clash with the Winter Olympic Games. Another reason for the move to late February and early March is to avoid the awards ceremony occurring so close to the religious holidays of Passover and Easter, which for decades had been a grievance from members and the general public.[71] Advertising is somewhat restricted, as traditionally no film studios or competitors of official Academy Award sponsors may advertise during the telecast. As of 2020, the production of the Academy Awards telecast held the distinction of winning one the highest number of Emmys in history, with 54 wins and 280 nominations overall.[72]  After many years of being held on Mondays at 6:00 p.m. Pacific\/9:00 pm Eastern, since the 1999 ceremony, it was moved to Sundays at 5:30 pm PT\/8:30 pm ET.[73] The reasons given for the move were that more viewers would tune in on Sundays, that Los Angeles rush-hour traffic jams could be avoided, and an earlier start time would allow viewers on the East Coast to go to bed earlier.[74] For many years the film industry opposed a Sunday broadcast because it would cut into the weekend box office.[75]  In 2010, the Academy contemplated moving the ceremony even further back into January, citing television viewers' fatigue with the film industry's long awards season. However, such an accelerated schedule would dramatically decrease the voting period for its members, to the point where some voters would only have time to view the contending films streamed on their computers, as opposed to traditionally receiving the films and ballots in the mail. Additionally, a January ceremony on Sunday would clash with National Football League (NFL) playoff games.[76] In 2018, the Academy announced that the ceremony would be moved from late February to mid-February beginning with the 92nd Academy Awards in 2020.[77] In 2024, the ceremony was moved to an even earlier start time of 4:00 pm PT\/7:00 p.m. ET, the apparent impetus being the ability for ABC to air a half-hour of primetime programming as a lead-out program at 7:30 p.m. PT\/10:30 p.m. ET.[78]  Originally scheduled for April 8, 1968, the 40th Academy Awards ceremony was postponed for two days, because of the assassination of Dr. Martin Luther King, Jr. On March 30, 1981, the 53rd Academy Awards was postponed for one day, after the attempted assassination of President Ronald Reagan and others in Washington, D.C.[79]  In 1993, an In Memoriam segment was introduced,[80] honoring those who had made a significant contribution to cinema who had died in the preceding 12 months, a selection compiled by a small committee of Academy members.[81] This segment has drawn criticism over the years for the omission of some names. Criticism was also levied for many years regarding another aspect, with the segment having a \"popularity contest\" feel as the audience varied their applause to those who had died by the subject's cultural impact. The applause has since been muted during the telecast, and the audience is discouraged from clapping during the segment and giving silent reflection instead. This segment was later followed by a commercial break.  In terms of broadcast length, the ceremony generally averages three and a half hours. The first Oscars, in 1929, lasted 15 minutes. At the other end of the spectrum, the 2002 ceremony lasted four hours and twenty-three minutes.[82][83] In 2010, the organizers of the Academy Awards announced winners' acceptance speeches must not run past 45 seconds. This, according to organizer Bill Mechanic, was to ensure the elimination of what he termed \"the single most hated thing on the show\"—overly long and embarrassing displays of emotion.[84] In 2016, in a further effort to streamline speeches, winners' dedications were displayed on an on-screen ticker.[85]  During the 2018 ceremony, host Jimmy Kimmel acknowledged how long the ceremony had become, by announcing that he would give a brand-new jet ski to whoever gave the shortest speech of the night, a reward won by Mark Bridges when accepting his Best Costume Design award for Phantom Thread.[86] The Wall Street Journal analyzed the average minutes spent across the 2014–2018 telecasts as follows: 14 on song performances; 25 on the hosts' speeches; 38 on prerecorded clips; and 78 on the awards themselves, broken into 24 on the introduction and announcement, 24 on winners walking to the stage, and 30 on their acceptance speeches.[87]  Although still dominant in ratings, the viewership of the Academy Awards has steadily dropped. The 88th Academy Awards were the lowest-rated in the past eight years (although with increases in male and 18–49 viewership), while the show itself also faced mixed reception. Following the show, Variety reported that ABC was, in negotiating an extension to its contract to broadcast the Oscars, seeking to have more creative control over the broadcast itself. Currently and nominally, AMPAS is responsible for most aspects of the telecast, including the choice of production staff and hosting, although ABC is allowed to have some input on their decisions.[88] In August 2016, AMPAS extended its contract with ABC to 2028: the contract neither contains any notable changes nor gives ABC any further creative control over the telecast.[89]  Historically, the telecast's viewership is higher when box-office hits are favored to win the Best Picture award. More than 57.25 million viewers tuned to the telecast for the 70th Academy Awards in 1998, the year of Titanic, which generated a box office haul during its initial 1997–98 run of $600.8 million in the US, a box-office record that would remain unsurpassed for years.[91] The 76th Academy Awards ceremony, in which The Lord of the Rings: The Return of the King (pre-telecast box office earnings of $368 million) received 11 Awards, including Best Picture, drew 43.56 million viewers.[92] The most-watched ceremony based on Nielsen ratings to date, was the 42nd Academy Awards (Best Picture Midnight Cowboy), which drew a 43.4% household rating on April 7, 1970.[93] Hoping to reinvigorate the pre-show and ratings, the 2023 Oscars organizers hired members of the Met Gala creative team.[94]  By contrast, ceremonies honoring films that have not performed well at the box office tend to show weaker ratings, despite how much critical acclaim those films have received. The 78th Academy Awards, which awarded a low-budget independent film (Crash with a pre-Oscar gross of $53.4 million) generated an audience of 38.64 million with a household rating of 22.91%.[95] In 2008, the 80th Academy Awards telecast was watched by 31.76 million viewers on average with an 18.66% household rating, the lowest-rated and least-watched ceremony at the time, in spite of celebrating 80 years of the Academy Awards.[96] The Best Picture winner of that particular ceremony was another independent film (this time, the Coen brothers's No Country for Old Men).  Whereas the 92nd Academy Awards drew an average of 23.6 million viewers,[97] the 93rd Academy Awards drew an even lower viewership of 10.4 million,[98] the lowest viewership recorded by Nielsen since it started recording audience totals in 1974.[99] The 94th and 95th editions drew 16.6 and 18.7 million viewers, respectively, still below the audience of the 92nd edition.[100][101]  The Academy Film Archive holds copies of every Academy Awards ceremony since the 1949 Oscars, as well as material on many prior ceremonies, along with ancillary material related to more recent shows. Copies are held in a variety of film, video and digital formats.[102]  In 1929, the first Academy Awards were presented at a banquet dinner at the Hollywood Roosevelt Hotel. From 1930 to 1943, the ceremony alternated between two venues: the Ambassador Hotel on Wilshire Boulevard and the Biltmore Hotel in downtown Los Angeles.  Grauman's Chinese Theatre in Hollywood then hosted the awards from 1944 to 1946, followed by the Shrine Auditorium in Los Angeles from 1947 to 1948. The 21st Academy Awards in 1949 were held at the Academy Award Theatre at what had been the Academy's headquarters on Melrose Avenue in Hollywood.[103]  From 1950 to 1960, the awards were presented at Hollywood's Pantages Theatre. With the advent of television, the awards from 1953 to 1957 took place simultaneously in Hollywood and New York, first at the NBC International Theatre (1953) and then at the NBC Century Theatre, after which the ceremony took place solely in Los Angeles. In 1961, the Oscars moved to the Santa Monica Civic Auditorium in Santa Monica, California. In 1969, the Academy moved the ceremonies back to Downtown Los Angeles, to the Dorothy Chandler Pavilion at the Los Angeles County Music Center. In the late 1990s and early 2000s, the ceremony returned to the Shrine Auditorium.  In 2002, Hollywood's Dolby Theatre, previously known as the Kodak Theatre, became the presentation's current venue.[104]  In the first year of the awards, the Best Directing award was split into two categories, Drama and Comedy. At times, the Best Original Score award has also been split into separate categories, Drama and Comedy\/Musical. From the 1930s to the 1960s, the Art Direction (now Production Design), Cinematography, and Costume Design awards were split into two categories (black-and-white films and color films). Prior to 2012, the Production Design award was called Art Direction, while the Makeup and Hairstyling award was called Makeup. Prior to 2020, the International Feature Film award was called Foreign Language Film.  In August 2018, the Academy announced that several categories would not be televised live, but recorded during commercial breaks and aired later in the ceremony.[106] Following dissent from Academy members, they announced that they would air all 24 categories live. This followed several proposals, among them, the introduction of a Popular Film category, that the Academy had announced but did not implement.[107]  The Board of Governors meets each year and considers new award categories. To date, the following categories have been proposed:  The Special Academy Awards are voted on by special committees, rather than by the Academy membership as a whole. They are not always presented on an annual basis.  Due to the positive exposure and prestige of the Academy Awards, many studios spend around 25 million dollars and hire publicists specifically to promote their films during what is typically called the \"Oscar season\".[115] This has generated accusations of the Academy Awards being influenced more by marketing and lobbying than by quality. William Friedkin, an Academy Award-winning film director and former producer of the ceremony, expressed this sentiment at a conference in New York in 2009, describing it as \"the greatest promotion scheme that any industry ever devised for itself\".[116]  Tim Dirks, editor of AMC's Filmsite, has written of the Academy Awards:  Unfortunately, the critical worth, artistic vision, cultural influence and innovative qualities of many films are not given the same voting weight. Especially since the 1980s, moneymaking \"formula-made\" blockbusters with glossy production values have often been crowd-pleasing titans (and Best Picture winners), but they haven't necessarily been great films with depth or critical acclaim by any measure.[117] A recent technique that has been claimed to be used during the Oscar season is the whisper campaign. These campaigns are intended to spread negative perceptions of other films nominated and are believed to be perpetrated by those who were involved in creating the film. Examples of whisper campaigns include the allegations against Zero Dark Thirty suggesting that it justifies torture and the claim that Lincoln distorts history.[118]  Typical criticism of the Academy Awards for Best Picture is that among the winners and nominees there is an over-representation of romantic historical epics, biographical dramas, romantic dramedies and family melodramas, most of which are released in the U.S. in the last three months of the calendar year. The Oscars have been infamously known for selecting specific genres of films to be awarded. The term \"Oscar bait\" was coined to describe such films. This has led, at times, to more specific criticisms that the Academy is disconnected from the audience, e.g., by favoring \"Oscar bait\" over audience favorites or favoring historical melodramas over critically acclaimed films that depict current life issues.[119]  Despite the success of The Dark Knight, the film did not receive a Best Picture nomination at the 81st Academy Awards. This decision received substantial criticism and was described as a \"snub\" by many publications.[120][121][122] The backlash to the decision was such that, for the 82nd Academy Awards awards in 2010, the Academy increased the limit for Best Picture nominees from five to ten, a change known as \"The Dark Knight Rule\".[122][123][124][125]  The Academy Awards have long received criticism over its lack of diversity among the nominees.[126][127][128] This criticism is based on the statistics from every Academy Awards since 1929, which show that only 6.4% of Academy Award nominees have been non-white and since 1991, 11.2% of nominees have been non-white, with the rate of winners being even more polarizing.[129] For a variety of reasons, including marketability and historical bans on interracial couples, a number of high-profile Oscars have been given to yellowface portrayals, as well as performances of Asian characters rewritten for white characters.[130][131] It took until 2023 for an Asian woman to win an Academy Award for Best Actress, when Michelle Yeoh received the award for her performance in Everything Everywhere All at Once. The 88th awards ceremony became the target of a boycott, popularized on social media with the hashtag #OscarsSoWhite, based on activists' perception that its all-white acting nominee list reflected bias.[132] In response, the Academy initiated \"historic\" changes in membership by 2020.[133][134] Some media critics claim the Academy's efforts to address its purported racial, gender and national biases are merely distractions.[135][136][137][138] By contrast, the Golden Globe Awards already have multiple winners of Asian descent in leading actress categories.[139] Some question whether the Academy's definition of \"merit\" is just or empowering for non-Americans.[140]  The Academy's Representation and Inclusion Standards have been criticized for excluding Jews as a distinct underrepresented class.[141]  The Academy has no rules for how to categorize whether a performance is leading or supporting, and it is up to the discretion of the studios whether a given performance is submitted for either Best Actor\/Actress or Best Supporting Actor\/Actress. This has led to situations where a film has two or more co-leads, and one of these is submitted in a supporting category to avoid the two leads competing against each other, and to increase the film's chances of winning. This practice has been derisively called \"category fraud\".[142][143]  For example, Rooney Mara was nominated for Best Supporting Actress for Carol (2015), despite her having a comparable amount of screentime to Cate Blanchett, who was nominated for Best Actress. Another example is Once Upon a Time in Hollywood (2019), where Brad Pitt was nominated for and won Best Supporting Actor, even though he played an equally important role to Best Actor nominee Leonardo DiCaprio. In both these cases, critics argued that the studios behind the films had placed someone who was actually a leading actor or actress into the supporting categories to avoid them competing against their co-lead.[142][143]  Acting prizes in certain years have been criticized for not recognizing superior performances so much as being awarded for personal popularity,[144] to make up for a \"snub\" for a work that proved in time to be more popular or renowned than the one awarded (a 'make-up Oscar'),[145] or as a \"career honor\" to recognize a distinguished nominee's entire body of work (a \"legacy Oscar\").[146][147]  Following the 91st Academy Awards in February 2019 in which the Netflix-broadcast film Roma had been nominated for ten awards including the Best Picture category, Steven Spielberg and other members of the Academy discussed changing the requirements through the Board of Governors for films as to exclude those from Netflix and other media streaming services. Spielberg had been concerned that Netflix as a film production and distribution studio could spend much more than for typical Oscar-winning films and have much wider and earlier distribution than for other Best Picture-nominated films, while still being able to meet the minimal theatrical-run status to qualify for an Oscar.[148]  The United States Department of Justice, having heard of this potential rule change, wrote a letter to the Academy in March 2019, cautioning them that placing additional restrictions on films that originate from streaming media services without proper justification could raise anti-trust concerns against the Academy.[149] Following its April 2019 board meeting, the Academy Board of Governors agreed to retain the current rules that allow for streaming media films to be eligible for Oscars as long as they enjoy limited theatrical runs.[150]  During the 94th Academy Awards on March 27, 2022, Chris Rock joked about Jada Pinkett Smith's shaved head[151] with a G.I. Jane reference. Will Smith walked onstage and slapped Rock across the face, then returned to his seat and told Rock, twice, to \"Keep my wife's name out [of] your fucking mouth!\"[152][153][154] While later accepting the Best Actor award for King Richard, Smith apologized to the Academy and the other nominees, but not to Rock.[155][156][157] Rock decided not to press charges against Smith.[158]  On April 8, 2022, the Academy made an announcement via a letter sent by president David Rubin and CEO Dawn Hudson informing the public that Will Smith had received a ten-year ban from attending the Oscars as a result of the incident.[159]  Some winners critical of the Academy Awards have boycotted the ceremonies and refused to accept their Oscars. The first to do so was screenwriter Dudley Nichols (Best Writing in 1935 for The Informer). Nichols boycotted the 8th Academy Awards ceremony because of conflicts between the Academy and the Writers' Guild.[160] Nichols eventually accepted the 1935 award three years later, at the 1938 ceremony. Nichols was nominated for three further Academy Awards during his career.  George C. Scott became the second person to refuse his award (Best Actor in 1970 for Patton) at the 43rd Academy Awards ceremony. Scott described it as a \"meat parade\", saying, \"I don't want any part of it\".[161][162][163]  The third person to refuse the award was Marlon Brando, who refused his award (Best Actor for 1972's The Godfather), citing the film industry's discrimination against and mistreatment of Native Americans. At the 45th Academy Awards ceremony, Brando asked actress and civil rights activist Sacheen Littlefeather to read a 15-page speech in his place, detailing his criticisms, for which there was booing and cheering by the audience.[160][164] In 2022, Littlefeather was accused by her sisters of misrepresenting her ancestry as Native American.[165][166][167][168][169]  Seven films have had nominations revoked before the official award ceremony:[170]  One film was disqualified after winning the award, and had the winner return the Oscar:  One film had its nomination revoked after the award ceremony when it had not won the Oscar:  At the 94th Academy Awards in 2022, the award for the Best Animated Feature was presented by three actresses who portrayed Disney princess characters in live-action remakes of their respective animated films: Lily James (Cinderella), Naomi Scott (Aladdin), and Halle Bailey (The Little Mermaid). While introducing the category, Bailey stated that animated films are \"formative experiences as kids who watch them,\" as James put it, \"So many kids watch these movies over and over, over and over again.\" Scott added: \"I see some parents who know exactly what we're talking about.\"[171] The remarks were heavily criticized by animation enthusiasts and those working in the industry as infantilizing the medium and perpetuating the stigma that animated works are strictly for children, especially since the industry was credited with sustaining the flow of Hollywood content and revenue during the height of the COVID-19 pandemic. Phil Lord, co-producer of one of the nominated films, The Mitchells vs. the Machines, tweeted that it was \"super cool to position animation as something that kids watch and adults have to endure.\" The film's official social media accounts responded to the joke with an image reading: \"Animation is cinema.\"[172][173] A week later, Lord and his producing partner Christopher Miller wrote a guest column in Variety criticizing the Academy for the joke and how Hollywood has treated animation, writing that \"no one set out to diminish animated films, but it's high time we set out to elevate them.\" They also suggested to the Academy that the category should be presented by a filmmaker who respects the art of animation as cinema.[174]  Adding to the controversy was that the award for Best Animated Short Film (the nominees for which were mostly made up of shorts not aimed at children) was one of the eight categories that were not presented during the live broadcast.[175] The winner for the Best Animated Short award was The Windshield Wiper, a multilingual Spanish-American film which is adult animated, while another nominee in three categories: Best Animated Feature, Best Documentary Feature Film, and Best International Feature Film, was Flee, a PG-13 rated animated documentary about an Afghan refugee. Alberto Mielgo, director of The Windshield Wiper, later gave an acceptance speech for the Oscar: \"Animation is an art that includes every single art that you can imagine. Animation for adults is a fact. It's happening. Let's call it cinema. I'm very honored because this is just the beginning of what we can do with animation.\"[175] Some speculations suggested that the speech played a role in the decision not to broadcast the award.[176]  Another factor is that numerous animated films have been made for mature audiences or with ranges of PG-13 or more, with a few of them—The Triplets of Belleville, Persepolis, Chico and Rita, The Wind Rises, Anomalisa, My Life as a Courgette, The Breadwinner, Loving Vincent, Isle of Dogs, I Lost My Body, Flee, and Memoir of a Snail—having been nominated in this category, with The Boy and the Heron being the first adult animated film (in this case, PG-13-rated) to win in the 96th Academy Awards.[177][178]  These comments came as #NewDeal4Animation, a movement of animation workers demanding equal pay, treatment and recognition alongside their contemporaries working in live-action, was picking up momentum during negotiations for a new contract between The Animation Guild, IATSE Local 839\/SAG-AFTRA and the Alliance of Motion Picture and Television Producers,[179] and the presentation is being used to rally the movement.  During the 96th Academy Awards in 2024, host Jimmy Kimmel said: \"Please raise your hand if you let your kid fill out this part of the ballot.\" These remarks would again prompt backlash, with Christopher Miller, producer of that year's nominated Spider-Man: Across the Spider-Verse, tweeting out that the joke was \"tired and lazy\".[180] The PG-13-rated The Boy and the Heron would subsequently win the award.  The following events are closely associated with the annual Academy Awards:  It has become a tradition to give out gift bags to the presenters and performers at the Oscars. In recent years, these gifts have been extended to award nominees and winners.[185] The value of each of these gift bags can reach into the tens of thousands of dollars. In 2014, the value was reported to be as high as $80,000.[186] The value has risen to the point where the U.S. Internal Revenue Service issued a statement regarding the gifts and their taxable status.[187]  Oscar gift bags have included vacation packages to Hawaii and Mexico and Japan, a private dinner party for the recipient and friends at a restaurant, videophones, a four-night stay at a hotel, watches, bracelets, spa treatments, bottles of vodka, maple salad dressing, weight-loss gummie candy and up to $25,000 worth of cosmetic treatments and rejuvenation procedures such as lip fillers and chemical peels from New York City facial plastic surgeon Konstantin Vasyukevich.[185][188][189][190][191] Some of the gifts have even had a \"risque\" element to them; in 2014, the adult products retailer Adam & Eve had a \"Secret Room Gifting Suite\". Celebrities visiting the gifting suite included Judith Hoag, Carolyn Hennesy, Kate Linder, Chris Mulkey, Jim O'Heir and John Salley.[192]  From 2006 onwards, results are Live+SD; all previous years are live viewing.[90]  The following nominees received at least 10 nominations:    The following winners received at least 5 awards (including non-competitive):    The following nominees received at least 5 nominations:    The following winners received at least 2 awards:    The following nominees received at least 5 nominations:    The following winners received at least 3 awards (including non-competitive):   "},"meta":{},"created_at":"2025-03-22T14:25:42.274677Z","updated_at":"2025-03-22T14:25:42.274677Z","inner_id":14,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":23,"annotations":[{"id":23,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"91d40a92-0ffc-4062-a437-0517a2286ce2","import_id":null,"last_action":null,"bulk_created":false,"task":23,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A credit rating agency (CRA, also called a ratings service) is a company that assigns credit ratings, which rate a debtor's ability to pay back debt by making timely principal and interest payments and the likelihood of default. An agency may rate the creditworthiness of issuers of debt obligations, of debt instruments,[1] and in some cases, of the servicers of the underlying debt,[2] but not of individual consumers.  Other forms of a rating agency include environmental, social and corporate governance (ESG) rating agencies and the Chinese Social Credit System.  The debt instruments rated by CRAs include government bonds, corporate bonds, CDs, municipal bonds, preferred stock, and collateralized securities, such as mortgage-backed securities and collateralized debt obligations.[3]  The issuers of the obligations or securities may be companies, special purpose entities, state or local governments, non-profit organizations, or sovereign nations.[3] A credit rating facilitates the trading of securities on international markets. It affects the interest rate that a security pays out, with higher ratings leading to lower interest rates. Individual consumers are rated for creditworthiness not by credit rating agencies but by credit bureaus (also called consumer reporting agencies or credit reference agencies), which issue credit scores.  The value of credit ratings for securities has been widely questioned. Hundreds of billions of securities that were given the agencies' highest ratings were downgraded to junk during the financial crisis of 2007–08.[4][5][6] Rating downgrades during the European sovereign debt crisis of 2010–12 were blamed by EU officials for accelerating the crisis.[3]  Credit rating is a highly concentrated industry, with the \"Big Three\" credit rating agencies controlling approximately 95% of the ratings business.[3] Moody's Investors Service and Standard & Poor's (S&P) together control 80% of the global market, and Fitch Ratings controls a further 15%. They are externalized sell-side functions for the marketing of securities.  When the United States began to expand to the west and other parts of the country, so did the distance of businesses to their customers. When businesses were close to those who purchased goods or services from them, it was easy for the merchants to extend credit to them, due to their proximity and the fact that merchants knew their customers personally and knew whether or not they would be able to pay them back. As trading distances increased, merchants no longer personally knew their customers and became wary of extending credit to people who they did not know in fear of them not being able to pay them back. Business owners' hesitation to extend credit to new customers led to the birth of the credit reporting industry.[7]  Mercantile credit agencies—the precursors of today's rating agencies—were established in the wake of the financial crisis of 1837. These agencies rated the ability of merchants to pay their debts and consolidated these ratings in published guides.[8] The first such agency was established in 1841 by Lewis Tappan in New York City.[8][9] It was subsequently acquired by Robert Dun, who published its first ratings guide in 1859. Another early agency, John Bradstreet, formed in 1849 and published a ratings guide in 1857.[8]  Credit rating agencies originated in the United States in the early 1900s, when ratings began to be applied to securities, specifically those related to the railroad bond market.[8] In the United States, the construction of extensive railroad systems had led to the development of corporate bond issues to finance them, and therefore a bond market several times larger than in other countries. The bond markets in the Netherlands and Britain had been established longer but tended to be small, and revolved around sovereign governments that were trusted to honor their debts.[10] Companies were founded to provide investors with financial information on the growing railroad industry, including Henry Varnum Poor's publishing company, which produced a publication compiling financial data about the railroad and canal industries.[11] Following the 1907 financial crisis, demand rose for such independent market information, in particular for independent analyses of bond creditworthiness.[12] In 1909, financial analyst John Moody issued a publication focused solely on railroad bonds.[12][13][14] His ratings became the first to be published widely in an accessible format,[10][12][15] and his company was the first to charge subscription fees to investors.[14]  In 1913, the ratings publication by Moody's underwent two significant changes: it expanded its focus to include industrial firms and utilities, and it began to use a letter-rating system. For the first time, public securities were rated using a system borrowed from the mercantile credit rating agencies, using letters to indicate their creditworthiness.[16] In the next few years, antecedents of the \"Big Three\" credit rating agencies were established. Poor's Publishing Company began issuing ratings in 1916, Standard Statistics Company in 1922,[12] and the Fitch Publishing Company in 1924.[13]  In the United States, the rating industry grew and consolidated rapidly following the passage of the Glass-Steagall act of 1933 and the separation of the securities business from banking.[17] As the market grew beyond that of traditional investment banking institutions, new investors again called for increased transparency, leading to the passage of new, mandatory disclosure laws for issuers, and the creation of the Securities and Exchange Commission (SEC).[10] In 1936, regulation was introduced to prohibit banks from investing in bonds determined by \"recognized rating manuals\" (the forerunners of credit rating agencies) to be \"speculative investment securities\" (\"junk bonds\", in modern terminology). US banks were permitted to hold only \"investment grade\" bonds, and it was the ratings of Fitch, Moody's, Poor's, and Standard that legally determined which bonds were which. State insurance regulators approved similar requirements in the following decades.[13]  From 1930 to 1980, the bonds and ratings of them were primarily relegated to American municipalities and American blue chip industrial firms.[18] International \"sovereign bond\" rating shrivelled during the Great Depression to a handful of the most creditworthy countries,[19] after a number of defaults of bonds issued by governments such as Germany's.[18]  In the late 1960s and 1970s, ratings were extended to commercial paper and bank deposits. Also during that time, major agencies changed their business model by beginning to charge bond issuers as well as investors.[12] The reasons for this change included a growing free rider problem related to the increasing availability of inexpensive photocopy machines[20] and the increased complexity of the financial markets.[21]  The rating agencies added levels of gradation to their rating systems. In 1973, Fitch added plus and minus symbols to its existing letter-rating system. The following year, Standard and Poor's did the same, and Moody's began using numbers for the same purpose in 1982.[8]  The end of the Bretton Woods system in 1971 led to the liberalization of financial regulations and the global expansion of capital markets in the 1970s and 1980s.[12] In 1975, SEC rules began explicitly referencing credit ratings.[22] For example, the commission changed its minimum capital requirements for broker-dealers, allowing smaller reserves for higher-rated bonds; the rating would be done by \"nationally recognized statistical ratings organizations\" (NRSROs). This referred to the \"Big Three\",[23] but in time ten agencies (later six, due to consolidation) were identified by the SEC as NRSROs.[13][24]  Rating agencies also grew in size and profitability as the number of issuers accessing the debt markets grew exponentially, both in the United States and abroad.[25] By 2009 the worldwide bond market (total debt outstanding) reached an estimated $82.2 trillion, in 2009 dollars.[26]  Two economic trends of the 1980s and 90s that brought significant expansion for the global capital market were[12]  More debt securities meant more business for the Big Three agencies, which many investors depended on to judge the securities of the capital market.[14] US government regulators also depended on the rating agencies; they allowed pension funds and money market funds to purchase only securities rated above certain levels.[29]  A market for low-rated, high-yield \"junk\" bonds blossomed in the late 1970s, expanding securities financing to firms other than a few large, established blue chip corporations.[28] Rating agencies also began to apply their ratings beyond bonds to counterparty risks, the performance risk of mortgage servicers, and the price volatility of mutual funds and mortgage-backed securities.[8] Ratings were increasingly used in most developed countries' financial markets and in the \"emerging markets\" of the developing world. Moody's and S&P opened offices Europe, Japan, and particularly emerging markets.[12] Non-American agencies also developed outside of the United States. Along with the largest US raters, one British, two Canadian and three Japanese firms were listed among the world's \"most influential\" rating agencies in the early 1990s by the Financial Times publication Credit Ratings International.[30]  Structured finance was another growth area of growth. The \"financial engineering\" of the new \"private-label\" asset-backed securities—such as subprime mortgage-backed securities (MBS), collateralized debt obligations (CDO), \"CDO-Squared\", and \"synthetic CDOs\"—made them \"harder to understand and to price\" and became a profit center for rating agencies.[31] By 2006, Moody's earned $881 million in revenue from structured finance.[32] By December 2008, there were over $11 trillion structured finance debt securities outstanding in the US bond market.[33]  The Big Three issued 97%–98% of all credit ratings in the United States[34] and roughly 95% worldwide,[35] giving them considerable pricing power.[36] This and credit market expansion brought them profit margins of around 50% from 2004 through 2009.[37][38]  As the influence and profitability of CRAs expanded, so did scrutiny and concern about their performance and alleged illegal practices.[39] In 1996 the US Department of Justice launched an investigation into possible improper pressuring of issuers by Moody's in order to win business.[40][41] Agencies were subjected to dozens of lawsuits by investors complaining of inaccurate ratings[42][43] following the collapse of Enron,[12] and especially after the US subprime mortgage crisis and subsequent financial crisis of 2007–2008.[44][45] During that debacle, 73%—over $800 billion worth[46]—of all mortgage-backed securities that one credit rating agency (Moody's) had rated triple-A in 2006 were downgraded to junk status two years later.[46][47] In July 2008, SIFMA formed a global task force with members drawn from a cross-section of the financial services industry, including asset managers, underwriters, and issuers, and provided industry input to lawmakers and regulators in Europe and Asia, and was designated by the U.S. President's Working Group on Financial Markets as the private-sector group to provide the PWG with industry recommendations on credit rating matters. It published the \"Recommendations of the Securities Industry and Financial Markets Association Credit Rating Agency Task Force\", which included a dozen recommendations to change the credit rating agency process.[48][49][50]   Downgrades of European and US sovereign debt were also criticized. In August 2011, S&P downgraded the long-held triple-A rating of US securities.[3] Since the spring of 2010,  one or more of the Big Three relegated Greece, Portugal, and Ireland to \"junk\" status—a move that many EU officials say has accelerated a burgeoning European sovereign-debt crisis. In January 2012, amid continued eurozone instability, S&P downgraded nine eurozone countries, stripping France and Austria of their triple-A ratings.[3] Credit rating agencies assess the relative credit risk of specific debt securities or structured finance instruments and borrowing entities (issuers of debt),[51] and in some cases the creditworthiness of governments and their securities.[52][53] By serving as information intermediaries, CRAs theoretically reduce information costs, increase the pool of potential borrowers, and promote liquid markets.[54][55][56] These functions may increase the supply of available risk capital in the market and promote economic growth.[51][56]  Credit rating agencies provide assessments about the creditworthiness of bonds issued by corporations, governments, and packagers of asset-backed securities.[57][58] In market practice, a significant bond issuance generally has a rating from one or two of the Big Three agencies.[59]  CRAs theoretically provide investors with an independent evaluation and assessment of debt securities' creditworthiness.[60] However, in recent decades the paying customers of CRAs have primarily not been buyers of securities but their issuers, raising the issue of conflict of interest (see below).[61]  In addition, rating agencies have been liable—at least in US courts—for any losses incurred by the inaccuracy of their ratings only if it is proven that they knew the ratings were false or exhibited \"reckless disregard for the truth\".[12][62][63] Otherwise, ratings are simply an expression of the agencies' informed opinions,[64] protected as \"free speech\" under the First Amendment.[65][66] As one rating agency disclaimer read:   The ratings ... are and must be construed solely as, statements of opinion and not statements of fact or recommendations to purchase, sell, or hold any securities.[67]  Under an amendment to the 2010 Dodd-Frank Act, this protection has been removed, but how the law will be implemented remains to be determined by rules made by the SEC and decisions by courts.[68][69][70][71]  To determine a bond's rating, a credit rating agency analyzes the accounts of the issuer and the legal agreements attached to the bond[72][73] to produce what is effectively a forecast of the bond's chance of default, expected loss, or a similar metric.[72] The metrics vary somewhat between the agencies. S&P's ratings reflect default probability, while ratings by Moody's reflect expected investor losses in the case of default.[74][75] For corporate obligations, Fitch's ratings incorporate a measure of investor loss in the event of default, but its ratings on structured, project, and public finance obligations narrowly measure default risk.[76] The process and criteria for rating a convertible bond are similar, although different enough that bonds and convertible bonds issued by the same entity may still receive different ratings.[77] Some bank loans may receive ratings to assist in wider syndication and attract institutional investors.[73]  The relative risks—the rating grades—are usually expressed through some variation of an alphabetical combination of lower- and uppercase letters, with either plus or minus signs or numbers added to further fine-tune the rating.[78][79]  Fitch and S&P use (from the most creditworthy to the least) AAA, AA, A, and BBB for investment-grade long-term credit risk and BB, CCC, CC, C, and D for \"speculative\" long-term credit risk. Moody's long-term designators are Aaa, Aa, A, and Baa for investment grade and Ba, B, Caa, Ca, and C for speculative grade.[78][79] Fitch and S&P use pluses and minuses (e.g., AA+ and AA−), and Moody's uses numbers (e.g., Aa1 and Aa3) to add further gradations.[78][79]  Agencies do not attach a hard number of probability of default to each grade, preferring descriptive definitions, such as \"the obligor's capacity to meet its financial commitment on the obligation is extremely strong\", (from a Standard and Poor's definition of a AAA-rated bond) or \"less vulnerable to non-payment than other speculative issues\" (for a BB-rated bond).[85] However, some studies have estimated the average risk and reward of bonds by rating. One study by Moody's[83][84] claimed that over a \"5-year time horizon\", bonds that were given its highest rating (Aaa) had a \"cumulative default rate\" of just 0.18%, the next highest (Aa2) 0.28%, the next (Baa2) 2.11%, 8.82% for the next (Ba2), and 31.24% for the lowest it studied (B2). (See \"Default rate\" in \"Estimated spreads and default rates by rating grade\" table to right.) Over a longer time horizon, it stated, \"the order is by and large, but not exactly, preserved\".[86]  Another study in the Journal of Finance calculated the additional interest rate or \"spread\" that corporate bonds pay over that of \"riskless\" US Treasury bonds, according to the bonds rating. (See \"Basis point spread\" in the table to right.) Looking at rated bonds from 1973 through 1989, the authors found a AAA-rated bond paid only 43 \"basis points\" (or 43\/100ths of a percentage point) more than a Treasury bond (so that it would yield 3.43% if the Treasury bond yielded 3.00%). A CCC-rated \"junk\" (or speculative) bond, on the other hand, paid over 4% more than a Treasury bond on average (7.04% if the Treasury bond yielded 3.00%) over that period.[23][81]  The market also follows the benefits from ratings that result from government regulations (see below), which often prohibit financial institutions from purchasing securities rated below a certain level. For example, in the United States, in accordance with two 1989 regulations, pension funds are prohibited from investing in asset-backed securities rated below A,[87] and savings and loan associations from investing in securities rated below BBB.[88]  CRAs provide \"surveillance\" (ongoing review of securities after their initial rating) and may change a security's rating if they feel its creditworthiness has changed. CRAs typically signal in advance their intention to consider rating changes.[78][89] Fitch, Moody's, and S&P all use negative \"outlook\" notifications to indicate the potential for a downgrade within the next two years (one year in the case of speculative-grade credits). Negative \"watch\" notifications are used to indicate that a downgrade is likely within the next 90 days.[78][89]  Critics maintain that this rating, outlooking, and watching of securities has not worked nearly as smoothly as agencies suggest. They point to near-defaults, defaults, and financial disasters not detected by the rating agencies' post-issuance surveillance, or ratings of troubled debt securities not downgraded until just before (or even after) bankruptcy.[90] These include the 1970 Penn Central bankruptcy, the 1975 New York City fiscal crisis, the 1994 Orange County default, the Asian and Russian financial crises, the 1998 collapse of the Long-Term Capital Management hedge fund, the 2001 Enron and WorldCom bankruptcies, and especially the 2007–8 subprime mortgage crisis.[90][91][92][93][94]  In the 2001 Enron accounting scandal, the company's ratings remained at investment grade until four days before bankruptcy—though Enron's stock had been in sharp decline for several months[95][96]—when \"the outlines of its fraudulent practices\" were first revealed.[97] Critics complained that \"not a single analyst at either Moody's or S&P lost his job as a result of missing the Enron fraud\" and \"management stayed the same\".[98] During the subprime crisis, when hundreds of billion of dollars' worth[46] of triple-A-rated mortgage-backed securities were abruptly downgraded from triple-A to \"junk\" status within two years of issue,[47] the CRAs' ratings were characterized by critics as \"catastrophically misleading\"[99] and \"provided little or no value\".[69][100] Ratings of preferred stocks also fared poorly. Despite over a year of rising mortgage delinquencies,[101] Moody's continued to rate Freddie Mac's preferred stock triple-A until mid-2008, when it was downgraded to one tick above the junk bond level.[102][103] Some empirical studies have also found that rather than a downgrade lowering the market price and raising the interest rates of corporate bonds, the cause and effect are reversed. Expanding yield spreads (i.e., declining value and quality) of corporate bonds precedes downgrades by agencies, suggesting it is the market that alerts the CRAs of trouble and not vice versa.[104][105]  In February 2018, an investigation by the Australian Securities and Investments Commission found a serious lack of detail and rigour in many of the ratings issued by agencies. ASIC examined six agencies, including the Australian arms of Fitch, Moody's and S&P Global Ratings (the other agencies were Best Asia-Pacific, Australia Ratings and Equifax Australia). It said agencies had often paid lip service to compliance. In one case, an agency had issued an annual compliance report only a single page in length, with scant discussion of methodology. In another case, a chief executive officer of a company had signed off on a report as though a board member. Also, overseas staff of ratings agencies had assigned credit ratings despite lacking the necessary accreditation.[106][107][108]  Defenders of credit rating agencies complain of the market's lack of appreciation. Argues Robert Clow, \"When a company or sovereign nation pays its debt on time, the market barely takes momentary notice ... but let a country or corporation unexpectedly miss a payment or threaten default, and bondholders, lawyers and even regulators are quick to rush the field to protest the credit analyst's lapse.\"[109] Others say that bonds assigned a low credit rating by rating agencies have been shown to default more frequently than bonds that receive a high credit rating, suggesting that ratings still serve as a useful indicator of credit risk.[68]  A number of explanations of the rating agencies' inaccurate ratings and forecasts have been offered, especially in the wake of the subprime crisis:[92][94]  Conversely, the complaint has been made that agencies have too much power over issuers and that downgrades can even force troubled companies into bankruptcy. The lowering of a credit score by a CRA can create a vicious cycle and a self-fulfilling prophecy: not only do interest rates on securities rise, but other contracts with financial institutions may also be affected adversely, causing an increase in financing costs and an ensuing decrease in creditworthiness. Large loans to companies often contain a clause that makes the loan due in full if the company's credit rating is lowered beyond a certain point (usually from investment grade to \"speculative\"). The purpose of these \"ratings triggers\" is to ensure that the loan-making bank is able to lay claim to a weak company's assets before the company declares bankruptcy and a receiver is appointed to divide up the claims against the company. The effect of such ratings triggers, however, can be devastating: under a worst-case scenario, once the company's debt is downgraded by a CRA, the company's loans become due in full; if the company is incapable of paying all of these loans in full at once, it is forced into bankruptcy (a so-called death spiral). These ratings triggers were instrumental in the collapse of Enron. Since that time, major agencies have put extra effort into detecting them and discouraging their use, and the US SEC requires that public companies in the United States disclose their existence.  The 2010 Dodd–Frank Wall Street Reform and Consumer Protection Act[116] mandated improvements to the regulation of credit rating agencies and addressed several issues relating to the accuracy of credit ratings specifically.[68][71] Under Dodd-Frank rules, agencies must publicly disclose how their ratings have performed over time and must provide additional information in their analyses so investors can make better decisions.[68][71] An amendment to the act also specifies that ratings are not protected by the First Amendment as free speech but are \"fundamentally commercial in character and should be subject to the same standards of liability and oversight as apply to auditors, securities analysts and investment bankers.\"[69][68] Implementation of this amendment has proven difficult due to conflict between the SEC and the rating agencies.[71][70] The Economist magazine credits the free speech defence at least in part for the fact that \"41 legal actions targeting S&P have been dropped or dismissed\" since the crisis.[117]  In the European Union, there is no specific legislation governing contracts between issuers and credit rating agencies.[70] General rules of contract law apply in full, although it is difficult to hold agencies liable for breach of contract.[70] In 2012, an Australian federal court held Standard & Poor's liable for inaccurate ratings.[70]  Credit rating agencies play a key role in structured financial transactions such as asset-backed securities (ABS), residential mortgage-backed securities (RMBS), commercial mortgage-backed securities (CMBS), collateralized debt obligations (CDOs), \"synthetic CDOs\", or derivatives.[118]  Credit ratings for structured finance instruments may be distinguished from ratings for other debt securities in several important ways.[119]  Aside from investors mentioned above—who are subject to ratings-based constraints in buying securities—some investors simply prefer that a structured finance product be rated by a credit rating agency.[125] And not all structured finance products receive a credit rating agency rating.[125] Ratings for complicated or risky CDOs are unusual and some issuers create structured products relying solely on internal analytics to assess credit risk.[125]  The Financial Crisis Inquiry Commission[129] has described the Big Three rating agencies as \"key players in the process\" of mortgage securitization,[31] providing reassurance of the soundness of the securities to money manager investors with \"no history in the mortgage business\".[130]  Credit rating agencies began issuing ratings for mortgage-backed securities (MBS) in the mid-1970s. In subsequent years, the ratings were applied to securities backed by other types of assets.[125] During the first years of the twenty-first century, demand for highly rated fixed income securities was high.[131]  Growth was particularly strong and profitable in the structured finance industry during the 2001-2006 subprime mortgage boom, and business with finance industry accounted for almost all of the revenue growth at at least one of the CRAs (Moody's).[132]  From 2000 to 2007, Moody's rated nearly 45,000 mortgage-related securities as triple-A. In contrast only six (private sector) companies in the United States were given that top rating.[133]  Rating agencies were even more important in rating collateralized debt obligations (CDOs). These securities mortgage\/asset backed security tranches lower in the \"waterfall\" of repayment that could not be rated triple-A, but for whom buyers had to be found or the rest of the pool of mortgages and other assets could not be securitized. Rating agencies solved the problem by rating 70%[134] to 80%[135]  of the CDO tranches triple-A. Still another innovative structured product most of whose tranches were also given high ratings was the \"synthetic CDO\". Cheaper and easier to create than ordinary \"cash\" CDOs, they paid insurance premium-like payments from credit default swap \"insurance\", instead of interest and principal payments from house mortgages. If the insured or \"referenced\" CDOs defaulted, investors lost their investment, which was paid out much like an insurance claim.[136]  However, when it was discovered that the mortgages had been sold to buyers who could not pay them, massive numbers of securities were downgraded, the securitization \"seized up\" and the Great Recession ensued.[137][138]  Critics blamed this underestimation of the risk of the securities on the conflict between two interests the CRAs have—rating securities accurately, and serving their customers, the security issuers[139] who need high ratings to sell to investors subject to ratings-based constraints, such as pension funds and life insurance companies.[123][124] While this conflict had existed for years, the combination of CRA focus on market share and earnings growth,[140]  the importance of structured finance to CRA profits,[141] and pressure from issuers who began to 'shop around' for the best ratings brought the conflict to a head between 2000 and 2007.[142][143][144][145]  A small number of arrangers of structured finance products—primarily investment banks—drive a large amount of business to the ratings agencies, and thus have a much greater potential to exert undue influence on a rating agency than a single corporate debt issuer.[146]  A 2013 Swiss Finance Institute study of structured debt ratings from S&P, Moody's, and Fitch found that agencies provide better ratings for the structured products of issuers that provide them with more overall bilateral rating business.[147][148] This effect was found to be particularly pronounced in the run-up to the subprime mortgage crisis.[147][148] Alternative accounts of the agencies' inaccurate ratings before the crisis downplay the conflict of interest factor and focus instead on the agencies' overconfidence in rating securities, which stemmed from faith in their methodologies and past successes with subprime securitizations.[149][150]  As a result of the 2007–2008 financial crisis, various legal requirements were introduced to increase the transparency of structured finance ratings. The European Union now requires credit rating agencies to use an additional symbol with ratings for structured finance instruments in order to distinguish them from other rating categories.[119]  Credit rating agencies also issue credit ratings for sovereign borrowers, including national governments, states, municipalities, and sovereign-supported international entities.[151] Sovereign borrowers are the largest debt borrowers in many financial markets.[151] Governments from both advanced economies and emerging markets borrow money by issuing government bonds and selling them to private investors, either overseas or domestically.[152] Governments from emerging and developing markets may also choose to borrow from other government and international organizations, such as the World Bank and the International Monetary Fund.[152]  Sovereign credit ratings represent an assessment by a rating agency of a sovereign's ability and willingness to repay its debt.[153] The rating methodologies used to assess sovereign credit ratings are broadly similar to those used for corporate credit ratings, although the borrower's willingness to repay receives extra emphasis since national governments may be eligible for debt immunity under international law, thus complicating repayment obligations.[151][152] In addition, credit assessments reflect not only the long-term perceived default risk, but also short- or immediate-term political and economic developments.[154] Differences in sovereign ratings between agencies may reflect varying qualitative evaluations of the investment environment.[155]  National governments may solicit credit ratings to generate investor interest and improve access to the international capital markets.[154][155] Developing countries often depend on strong sovereign credit ratings to access funding in international bond markets.[154] Once ratings for a sovereign have been initiated, the rating agency will continue to monitor for relevant developments and adjust its credit opinion accordingly.[154]  A 2010 International Monetary Fund study concluded that ratings were a reasonably good indicator of sovereign default risk.[54][156] However, credit rating agencies were criticized for failing to predict the 1997 Asian financial crisis and for downgrading countries in the midst of that turmoil.[153] Similar criticisms emerged after recent credit downgrades to Greece, Ireland, Portugal, and Spain,[153] although credit ratings agencies had begun to downgrade peripheral Eurozone countries well before the Eurozone crisis began.[156]  As part of the Sarbanes–Oxley Act of 2002, Congress ordered the U.S. SEC to develop a report, titled \"Report on the Role and Function of Credit Rating Agencies in the Operation of the Securities Markets\"[157] detailing how credit ratings are used in U.S. regulation and the policy issues this use raises. Partly as a result of this report, in June 2003, the SEC published a \"concept release\" called \"Rating Agencies and the Use of Credit Ratings under the Federal Securities Laws\"[158] that sought public comment on many of the issues raised in its report.  Public comments on this concept release have also been published on the SEC's website.[159]  In December 2004, the International Organization of Securities Commissions (IOSCO) published a Code of Conduct[160] for CRAs that, among other things, is designed to address the types of conflicts of interest that CRAs face. All of the major CRAs have agreed to sign on to this Code of Conduct and it has been praised by regulators ranging from the European Commission to the US SEC.[citation needed]  Regulatory authorities and legislative bodies in the United States and other jurisdictions rely on credit rating agencies' assessments of a broad range of debt issuers, and thereby attach a regulatory function to their ratings.[161][162] This regulatory role is a derivative function in that the agencies do not publish ratings for that purpose.[161] Governing bodies at both the national and international level have woven credit ratings into  minimum capital requirements for banks, allowable investment alternatives for many institutional investors, and similar restrictive regulations for insurance companies and other financial market participants.[163][164]  The use of credit ratings by regulatory agencies is not a new phenomenon.[161] In the 1930s, regulators in the United States used credit rating agency ratings to prohibit banks from investing in bonds that were deemed to be below investment grade.[92]  In the following decades, state regulators outlined a similar role for agency ratings in restricting insurance company investments.[92][161] From 1975 to 2006, the U.S. Securities and Exchange Commission (SEC) recognized the largest and most credible agencies as Nationally Recognized Statistical Rating Organizations, and relied on such agencies exclusively for distinguishing between grades of creditworthiness in various regulations under federal securities laws.[161][165] The Credit Rating Agency Reform Act of 2006 created a voluntary registration system for CRAs that met a certain minimum criteria, and provided the SEC with broader oversight authority.[166]  The practice of using credit rating agency ratings for regulatory purposes has since expanded globally.[161] Today, financial market regulations in many countries contain extensive references to ratings.[161][167] The Basel III accord, a global bank capital standardization effort, relies on credit ratings to calculate minimum capital standards and minimum liquidity ratios.[161]  The extensive use of credit ratings for regulatory purposes can have a number of unintended effects.[161]  Because regulated market participants must follow minimum investment grade provisions, ratings changes across the investment\/non-investment grade boundary may lead to strong market price fluctuations and potentially cause systemic reactions.[161] The regulatory function granted to credit rating agencies may also adversely affect their original market information function of providing credit opinions.[161][168]  Against this background and in the wake of criticism of credit rating agencies following the subprime mortgage crisis, legislators in the United States and other jurisdictions have commenced to reduce rating reliance in laws and regulations.[92][169] The 2010 Dodd–Frank Act  removes statutory references to credit rating agencies, and calls for federal regulators to review and modify existing regulations to avoid relying on credit ratings as the sole assessment of creditworthiness.[170][171][172]  Credit rating is a highly concentrated industry, with the \"Big Three\" credit rating agencies controlling approximately 95% of the ratings business.[3] Moody's Investors Service and Standard & Poor's (S&P) together control 80% of the global market, and Fitch Ratings controls a further 15%.[76][173][3][174]  As of December 2012,[update] S&P is the largest of the three, with 1.2 million outstanding ratings and 1,416 analysts and supervisors;[173][175] Moody's has 1 million outstanding ratings and 1,252 analysts and supervisors;[173][175] and Fitch is the smallest, with approximately 350,000 outstanding ratings, and is sometimes used as an alternative to S&P and Moody's.[173][176]  The three largest agencies are not the only sources of credit information.[57] Many smaller rating agencies also exist, mostly serving non-US markets.[177] All of the large securities firms have internal fixed income analysts who offer information about the risk and volatility of securities to their clients.[57] And specialized risk consultants working in a variety of fields offer credit models and default estimates.[57]  Market share concentration is not a new development in the credit rating industry. Since the establishment of the first agency in 1909, there have never been more than four credit rating agencies with significant market share.[178] Even the Financial crisis of 2007–08—where the performance of the three rating agencies was dubbed \"horrendous\" by The Economist magazine[179]—led to a drop in the share of the three by just one percent—from 98 to 97%.[180]  The reason for the concentrated market structure is disputed. One widely cited opinion is that the Big Three's historical reputation within the financial industry creates a high barrier of entry for new entrants.[178] Following the enactment of the Credit Rating Agency Reform Act of 2006 in the US, seven additional rating agencies attained recognition from the SEC as nationally recognized statistical rating organization (NRSROs).[181][182] While these other agencies remain niche players,[183] some have gained market share following the Financial crisis of 2007–08,[184] and in October 2012 several announced plans to join together and create a new organization called the Universal Credit Rating Group.[185]  The European Union has considered setting up a state-supported EU-based agency.[186] In November 2013, credit ratings organizations from five countries (CPR of Portugal, CARE Rating of India, GCR of South Africa, MARC of Malaysia, and SR Rating of Brazil) joint ventured to launch ARC Ratings, a new global agency touted as an alternative to the \"Big Three\".[187]  In addition to \"the Big Three\" of Moody's, Standard & Poor's, and Fitch Ratings, other agencies and rating companies include:  Infomerics Credit Rating Nepal Limited (Nepal), Acuité Ratings & Research Limited (India), A. M. Best (U.S.), Agusto & Co. (Nigeria), ARC Ratings (UK) & (EU), DataPro Limited (Nigeria), Capital Intelligence Ratings Limited (CIR) (Cyprus), CareEdge Ratings (India), China Lianhe Credit Rating Co., Ltd. (China), CSCI Pengyuan Credit Ratings Co., Ltd (China), China Chengxin Credit Rating Group (China), Credit Rating Agency Ltd (Zambia), Credit Rating Information and Services Limited[188][189] (Bangladesh), CSPI Ratings (Hong Kong), CTRISKS (Hong Kong),[190] DBRS (Canada), Dun & Bradstreet (U.S.), Egan-Jones Rating Company (U.S.), Global Credit Ratings Co. (South Africa), HR Ratings de México, S.A. de C.V. (Mexico), The Pakistan Credit Rating Agency Limited (PACRA) (Pakistan), ICRA Limited (India), Japan Credit Rating Agency[191] (Japan), JCR VIS Credit Rating Company Ltd (Pakistan), Kroll Bond Rating Agency (U.S.), Levin and Goldstein (Zambia), Lianhe Ratings Global (Hong Kong), modeFinance (Italy), Morningstar, Inc. (U.S.), Muros Ratings (Russia, alternative rating company), Public Sector Credit Solutions (U.S., not-for profit rating provider), Rapid Ratings International (U.S.), SMERA Gradings & Ratings Private Limited (India), Universal Credit Rating Group (Hong Kong), Veda (Australia, previously known as Baycorp Advantage), Wikirating (Switzerland, alternative rating organization), Feller Rate Clasificadora de Riesgo (Chile), Humphreys Ltd (Chile, previously known as Moody's partner in Chile), National University of Singapore's Credit Research Initiative[192] (Singapore, non-profit rating provider), Spread Research (independent credit research and rating agency, France), INC Rating (Poland), Scope Ratings GmbH (Germany).[193]  Credit rating agencies generate revenue from a variety of activities related to the production and distribution of credit ratings.[194] The sources of the revenue are generally the issuer of the securities or the investor. Most agencies operate under one or a combination of business models: the subscription model and the issuer-pays model.[52] However, agencies may offer additional services using a combination of business models.[194][195]  Under the subscription model, the credit rating agency does not make its ratings freely available to the market, so investors pay a subscription fee for access to ratings.[52][196] This revenue provides the main source of agency income, although agencies may also provide other types of services.[52][197] Under the issuer-pays model, agencies charge issuers a fee for providing credit rating assessments.[52] This revenue stream allows issuer-pays credit rating agencies to make their ratings freely available to the broader market, especially via the Internet.[198][199]  The subscription approach was the prevailing business model until the early 1970s, when Moody's, Fitch, and finally Standard & Poor's adopted the issuer-pays model.[52][196] Several factors contributed to this transition, including increased investor demand for credit ratings, and widespread use of information sharing technology—such as fax machines and photocopiers—which allowed investors to freely share agencies' reports and undermined demand for subscriptions.[200] Today, eight of the nine nationally recognized statistical rating organizations (NRSRO) use the issuer-pays model, only Egan-Jones maintains an investor subscription service.[198] Smaller, regional credit rating agencies may use either model. For example, China's oldest rating agency, Chengxin Credit Management Co., uses the issuer-pays model. The Universal Credit Ratings Group, formed by Beijing-based Dagong Global Credit Rating, Egan-Jones of the U.S. and Russia's RusRatings, uses the investor-pays model,[201][202][203] while Dagong Europe Credit Rating, the other joint-venture of Dagong Global Credit Rating, uses the issuer-pays model.  Critics argue that the issuer-pays model creates a potential conflict of interest because the agencies are paid by the organizations whose debt they rate.[204] However, the subscription model is also seen to have disadvantages, as it restricts the ratings' availability to paying investors.[198][199] Issuer-pays CRAs have argued that subscription-models can also be subject to conflicts of interest due to pressures from investors with strong preferences on product ratings.[205] In 2010 Lace Financials, a subscriber-pays agency later acquired by Kroll Ratings, was fined by the SEC for violating securities rules to the benefit of its largest subscriber.[206]  A 2009 World Bank report proposed a \"hybrid\" approach in which issuers who pay for ratings are required to seek additional scores from subscriber-based third parties.[207] Other proposed alternatives include a \"public-sector\" model in which national governments fund the rating costs, and an \"exchange-pays\" model, in which stock and bond exchanges pay for the ratings.[208][209] Crowd-sourced, collaborative models such as Wikirating have been suggested as an alternative to both the subscription and issuer-pays models, although it is a recent development as of the 2010, and not yet widely used.[210][211]  Agencies are sometimes accused of being oligopolists,[212]  because barriers to market entry are high and rating agency business is itself reputation-based (and the finance industry pays little attention to a rating that is not widely recognized). In 2003, the US SEC submitted a report to Congress detailing plans to launch an investigation into the anti-competitive practices of credit rating agencies and issues including conflicts of interest.[213]  Think tanks such as the World Pensions Council (WPC) have argued that the Basel II\/III \"capital adequacy\" norms favored at first essentially by the central banks of France, Germany and Switzerland (while the US and the UK were rather lukewarm) have unduly encouraged the use of ready-made opinions produced by oligopolistic rating agencies[214][215]  Of the large agencies, only Moody's is a separate, publicly held corporation that discloses its financial results without dilution by non-ratings businesses, and its high profit margins (which at times have been greater than 50 percent of gross margin) can be construed as consistent with the type of returns one might expect in an industry which has high barriers to entry.[216] Celebrated investor Warren Buffett described the company as \"a natural duopoly\", with  \"incredible\" pricing power, when asked by the Financial Crisis Inquiry Commission about his ownership of 15% of the company.[217][218]  According to professor Frank Partnoy, the regulation of CRAs by the SEC and Federal Reserve Bank has eliminated competition between CRAs and practically forced market participants to use the services of the three big agencies, Standard and Poor's, Moody's and Fitch.[219]  SEC Commissioner Kathleen Casey has said that these CRAs have acted much like Fannie Mae, Freddie Mac and other companies that dominate the market because of government actions. When the CRAs gave ratings that were \"catastrophically misleading, the large rating agencies enjoyed their most profitable years ever during the past decade.\"[219]  To solve this problem, Ms. Casey (and others such as NYU professor Lawrence White[220]) have proposed removing the NRSRO rules completely.[219] Professor Frank Partnoy suggests that the regulators use the results of the credit risk swap markets rather than the ratings of NRSROs.[219]  The CRAs have made competing suggestions that would, instead, add further regulations that would make market entrance even more expensive than it is now.[220] "},"meta":{},"created_at":"2025-03-22T14:25:42.274677Z","updated_at":"2025-03-22T14:25:42.274677Z","inner_id":15,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":24,"annotations":[{"id":24,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"31383d08-17f3-4dfc-8853-31fb25bac223","import_id":null,"last_action":null,"bulk_created":false,"task":24,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Heterodox  In economics, deflation is a decrease in the general price level of goods and services.[1] Deflation occurs when the inflation rate falls below 0% (a negative inflation rate). Inflation reduces the value of currency over time, but deflation increases it. This allows more goods and services to be bought than before with the same amount of currency. Deflation is distinct from disinflation, a slowdown in the inflation rate; i.e., when inflation declines to a lower rate but is still positive.[2]  Economists generally believe that a sudden deflationary shock is a problem in a modern economy because it increases the real value of debt, especially if the deflation is unexpected. Deflation may also aggravate recessions and lead to a deflationary spiral (see later section).[3][4][5][6][7][8][9]  Some economists argue that prolonged deflationary periods are related to the underlying technological progress in an economy, because as productivity increases (TFP), the cost of goods decreases.[10]  Deflation usually happens when supply is high (when excess production occurs), when demand is low (when consumption decreases), or when the money supply decreases (sometimes in response to a contraction created from careless investment or a credit crunch) or because of a net capital outflow from the economy.[11] It can also occur when there is too much competition and too little market concentration.[12][better source needed]  In the IS–LM model (investment and saving equilibrium – liquidity preference and money supply equilibrium model),[13][14][15] deflation is caused by a shift in the supply and demand curve for goods and services.[citation needed] This in turn can be caused by an increase in supply, a fall in demand, or both.  When prices are falling, consumers have an incentive to delay purchases and consumption until prices fall further, which in turn reduces overall economic activity. When purchases are delayed, productive capacity is idled and investment falls, leading to further reductions in aggregate demand. This is the deflationary spiral. The way to reverse this quickly would be to introduce an economic stimulus. The government could increase productive spending on things like infrastructure or the central bank could start expanding the money supply.[15]  Deflation is also related to risk aversion, where investors and buyers will start hoarding money because its value is now increasing over time.[16] This can produce a liquidity trap or it may lead to shortages that entice investments yielding more jobs and commodity production. A central bank cannot, normally, charge negative interest for money, and even charging zero interest often produces less stimulative effect than slightly higher rates of interest. In a closed economy, this is because charging zero interest also means having zero return on government securities, or even negative return on short maturities. In an open economy, it creates a carry trade and devalues the currency. A devalued currency produces higher prices for imports without necessarily stimulating exports to a like degree.  Deflation is the natural condition of economies when the supply of money is fixed, or does not grow as quickly as population and the economy. When this happens, the available amount of hard currency per person falls, in effect making money more scarce, and consequently, the purchasing power of each unit of currency increases. Deflation also occurs when improvements in production efficiency lower the overall price of goods. Competition in the marketplace often prompts those producers to apply at least some portion of these cost savings into reducing the asking price for their goods. When this happens, consumers pay less for those goods, and consequently, deflation has occurred, since purchasing power has increased.  Rising productivity and reduced transportation cost created structural deflation during the accelerated productivity era from 1870 to 1900, but there was mild inflation for about a decade before the establishment of the Federal Reserve in 1913.[17] There was inflation during World War I, but deflation returned again after the war and during the 1930s depression. Most nations abandoned the gold standard in the 1930s so that there is less reason to expect deflation, aside from the collapse of speculative asset classes, under a fiat monetary system with low productivity growth.  In mainstream economics, deflation may be caused by a combination of the supply and demand for goods and the supply and demand for money, specifically the supply of money going down and the supply of goods going up. Historic episodes of deflation have often been associated with the supply of goods going up (due to increased productivity) without an increase in the supply of money, or (as with the Great Depression and possibly Japan in the early 1990s) the demand for goods going down combined with a decrease in the money supply. Studies of the Great Depression by Ben Bernanke have indicated that, in response to decreased demand, the Federal Reserve of the time decreased the money supply, hence contributing to deflation.  Causes include, on the demand side:  And on the supply side:  Growth deflation is an enduring decrease in the real cost of goods and services as the result of technological progress, accompanied by competitive price cuts, resulting in an increase in aggregate demand.[18]  A structural deflation existed from the 1870s until the cycle upswing that started in 1895. The deflation was caused by the decrease in the production and distribution costs of goods. It resulted in competitive price cuts when markets were oversupplied. The mild inflation after 1895 was attributed to the increase in gold supply that had been occurring for decades.[19] There was a sharp rise in prices during World War I, but deflation returned at the war's end.  By contrast, under a fiat monetary system, there was high productivity growth from the end of World War II until the 1960s, but no deflation.[20]  Historically not all episodes of deflation correspond with periods of poor economic growth.[21]  Productivity and deflation are discussed in a 1940 study by the Brookings Institution that gives productivity by major US industries from 1919 to 1939, along with real and nominal wages. Persistent deflation was clearly understood as being the result of the enormous gains in productivity of the period.[22] By the late 1920s, most goods were over supplied, which contributed to high unemployment during the Great Depression.[23]  Bank credit deflation is a decrease in the bank credit supply due to bank failures or increased perceived risk of defaults by private entities or a contraction of the money supply by the central bank.[24]  Debt deflation is a complicated phenomenon associated with the end of long-term credit cycles. It was proposed as a theory by Irving Fisher (1933) to explain the deflation of the Great Depression.[25]  From a monetarist perspective, deflation is caused primarily by a reduction in the velocity of money or the amount of money supply per person.  A historical analysis of money velocity and monetary base shows an inverse correlation: for a given percentage decrease in the monetary base the result is a nearly equal percentage increase in money velocity.[16] This is to be expected because monetary base (MB), velocity of base money (VB), price level (P) and real output (Y) are related by definition: MBVB = PY.[26] However, the monetary base is a much narrower definition of money than M2 money supply. Additionally, the velocity of the monetary base is interest-rate sensitive, the highest velocity being at the highest interest rates.[16]  In the early history of the United States, there was no national currency and an insufficient supply of coinage.[27] Banknotes were the majority of the money in circulation. During financial crises, many banks failed and their notes became worthless. Also, banknotes were discounted relative to gold and silver, the discount depended on the financial strength of the bank.[28]  In recent years changes in the money supply have historically taken a long time to show up in the price level, with a rule of thumb lag of at least 18 months. More recently Alan Greenspan cited the time lag as taking between 12 and 13 quarters.[29][full citation needed] Bonds, equities and commodities have been suggested as reservoirs for buffering changes in the money supply.[30]  In modern credit-based economies, deflation may be caused by the central bank initiating higher interest rates (i.e., to \"control\" inflation), thereby possibly popping an asset bubble. In a credit-based economy, a slow-down or fall in lending leads to less money in circulation, with a further sharp fall in money supply as confidence reduces and velocity weakens, with a consequent sharp fall-off in demand for employment or goods. The fall in demand causes a fall in prices as a supply glut develops. This becomes a deflationary spiral when prices fall below the costs of financing production, or repaying debt levels incurred at the prior price level. Businesses, unable to make enough profit no matter how low they set prices, are then liquidated. Banks get assets that have fallen dramatically in value since their mortgage loan was made, and if they sell those assets, they further glut supply, which only exacerbates the situation. To slow or halt the deflationary spiral, banks will often withhold collecting on non-performing loans (as in Japan, and most recently America and Spain). This is often no more than a stop-gap measure, because they must then restrict credit, since they do not have money to lend, which further reduces demand, and so on.  In the early economic history of the United States, cycles of inflation and deflation correlated with capital flows between regions, with money being loaned from the financial center in the Northeast to the commodity producing regions of the (mid)-West and South. In a procyclical manner, prices of commodities rose when capital was flowing in, that is, when banks were willing to lend, and fell in the depression years of 1818 and 1839 when banks called in loans.[31] Also, there was no national paper currency at the time and there was a scarcity of coins. Most money circulated as banknotes, which typically sold at a discount according to distance from the issuing bank and the bank's perceived financial strength.  When banks failed their notes were redeemed for bank reserves, which often did not result in payment at par value, and sometimes the notes became worthless. Notes of weak surviving banks traded at steep discounts.[27][28] During the Great Depression, people who owed money to a bank whose deposits had been frozen would sometimes buy bank books (deposits of other people at the bank) at a discount and use them to pay off their debt at par value.[32]  Deflation occurred periodically in the U.S. during the 19th century (the most important exception was during the Civil War). This deflation was at times caused by technological progress that created significant economic growth, but at other times it was triggered by financial crises – notably the Panic of 1837 which caused deflation through 1844, and the Panic of 1873 which triggered the Long Depression that lasted until 1879.[17][28][31] These deflationary periods preceded the establishment of the U.S. Federal Reserve System and its active management of monetary matters. Episodes of deflation have been rare and brief since the Federal Reserve was created (a notable exception being the Great Depression) while U.S. economic progress has been unprecedented.  A financial crisis in England in 1818 caused banks to call in loans and curtail new lending, draining specie out of the U.S.[citation needed] The Bank of the United States also reduced its lending. Prices for cotton and tobacco fell. The price of agricultural commodities also was pressured by a return of normal harvests following 1816, the year without a summer, that caused large scale famine and high agricultural prices.[33]  There were several causes of the deflation of the severe depression of 1839–1843, which included an oversupply of agricultural commodities (importantly cotton) as new cropland came into production following large federal land sales a few years earlier, banks requiring payment in gold or silver, the failure of several banks, default by several states on their bonds and British banks cutting back on specie flow to the U.S.[31][34]  This cycle has been traced out on a broad scale during the Great Depression. Partly because of overcapacity and market saturation and partly as a result of the Smoot–Hawley Tariff Act, international trade contracted sharply, severely reducing demand for goods, thereby idling a great deal of capacity, and setting off a string of bank failures.[23] A similar situation in Japan, beginning with the stock and real estate market collapse in the early 1990s, was arrested by the Japanese government preventing the collapse of most banks and taking over direct control of several in the worst condition.  The United States had no national paper money until 1862 (greenbacks used to fund the Civil War), but these notes were discounted to gold until 1877. There was also a shortage of U.S. minted coins. Foreign coins, such as Mexican silver, were commonly used.[27] At times banknotes were as much as 80% of currency in circulation before the Civil War. In the financial crises of 1818–19 and 1837–1841, many banks failed, leaving their money to be redeemed below par value from reserves. Sometimes the notes became worthless, and the notes of weak surviving banks were heavily discounted.[28] The Jackson administration opened branch mints, which over time increased the supply of coins. Following the 1848 finding of gold in the Sierra Nevada, enough gold came to market to devalue gold relative to silver. To equalize the value of the two metals in coinage, the US mint slightly reduced the silver content of new coinage in 1853.[27]  When structural deflation appeared in the years following 1870, a common explanation given by various government inquiry committees was a scarcity of gold and silver, although they usually mentioned the changes in industry and trade we now call productivity. However, David A. Wells (1890) notes that the U.S. money supply during the period 1879-1889 actually rose 60%, the increase being in gold and silver, which rose against the percentage of national bank and legal tender notes. Furthermore, Wells argued that the deflation only lowered the cost of goods that benefited from recent improved methods of manufacturing and transportation. Goods produced by craftsmen did not decrease in price, nor did many services, and the cost of labor actually increased. Also, deflation did not occur in countries that did not have modern manufacturing, transportation and communications.[17]  By the end of the 19th century, deflation ended and turned to mild inflation. William Stanley Jevons predicted rising gold supply would cause inflation decades before it actually did. Irving Fisher blamed the worldwide inflation of the pre-WWI years on rising gold supply.[35]  In economies with an unstable currency, barter and other alternate currency arrangements such as dollarization are common, and therefore when the 'official' money becomes scarce (or unusually unreliable), commerce can still continue (e.g., most recently in Zimbabwe). Since in such economies the central government is often unable, even if it were willing, to adequately control the internal economy, there is no pressing need for individuals to acquire official currency except to pay for imported goods.  If a country pegs its currency to one of another country that features a higher productivity growth or a more favourable unit cost development, it must – to maintain its competitiveness – either become equally more productive or lower its factor prices (e.g., wages). Cutting factor prices fosters deflation. Monetary unions have a similar effect to currency pegs.  Some believe that, in the absence of large amounts of debt, deflation would be a welcome effect because the lowering of prices increases purchasing power.[36] However, while an increase in the purchasing power of one's money benefits some, it amplifies the sting of debt for others: after a period of deflation, the payments to service a debt represent a larger amount of purchasing power than they did when the debt was first incurred. Consequently, deflation can be thought of as an effective increase in a loan's interest rate. If, as during the Great Depression in the United States, deflation averages 10% per year, even an interest-free loan is unattractive as it must be repaid with money worth 10% more each year.  Under normal conditions, most central banks, such as the Federal Reserve, implement policy by setting a target for a short-term interest rate –  the overnight federal funds rate in the U.S. –  and enforcing that target by buying and selling securities in open capital markets. When the short-term interest rate hits zero, the central bank can no longer ease policy by lowering its usual interest-rate target. With interest rates near zero, debt relief becomes an increasingly important tool in managing deflation.  In recent times, as loan terms have grown in length and loan financing (or leveraging) is common among many types of investments, the costs of deflation to borrowers has grown larger.  Deflation can discourage private investment, because there is reduced expectations on future profits when future prices are lower. Consequently, with reduced private investments, spiraling deflation can cause a collapse in aggregate demand. Without the \"hidden risk of inflation\", it may become more prudent for institutions to hold on to money, and not to spend or invest it (burying money). They are therefore rewarded by saving and holding money. This \"hoarding\" behavior is seen as undesirable by most economists.[citation needed] Friedrich Hayek, a libertarian Austrian-school economist, wrote that:  It is agreed that hoarding money, whether in cash or in idle balances, is deflationary in its effects. No one thinks that deflation is in itself desirable. Deflation causes a transfer of wealth from borrowers and holders of illiquid assets to the benefit of savers and of holders of liquid assets and currency, and because confused price signals cause malinvestment in the form of underinvestment. In this sense, its effects are the opposite of inflation, the effect of which is to transfer wealth from currency holders and lenders (savers) and to borrowers, including governments, and cause overinvestment. Whereas inflation encourages short term consumption and can similarly overstimulate investment in projects that may not be worthwhile in real terms (for example, the dot-com and housing bubbles), deflation reduces investment even when there is a real-world demand not being met. In modern economies, deflation is usually associated with economic depression, as occurred in the Great Depression and the Long Depression. Deflation was present during most economic depressions in US history.[38][better source needed]  A deflationary spiral is a situation where decreases in the price level lead to lower production, which in turn leads to lower wages and demand, which leads to further decreases in the price level.[39][40] Since reductions in general price level are called deflation, a deflationary spiral occurs when reductions in price lead to a vicious circle, where a problem exacerbates its own cause.[41] In science, this effect is also known as a positive feedback loop. Another economic example of this situation in economics is the bank run.  The Great Depression was regarded by some as a deflationary spiral.[42] A deflationary spiral is the modern macroeconomic version of the general glut controversy of the 19th century. Another related idea is Irving Fisher's theory that excess debt can cause a continuing deflation.  During severe deflation, targeting an interest rate (the usual method of determining how much currency to create) may be ineffective, because even lowering the short-term interest rate to zero may result in a real interest rate which is too high to attract credit-worthy borrowers. In the 21st-century, negative interest rates have been tried, but it cannot be too negative, since people might withdraw cash from bank accounts if they have a negative interest rate. Thus the central bank must directly set a target for the quantity of money (called \"quantitative easing\") and may use extraordinary methods to increase the supply of money, e.g. purchasing financial assets of a type not usually used by the central bank as reserves (such as mortgage-backed securities). Before he was Chairman of the United States Federal Reserve, Ben Bernanke claimed in 2002, \"sufficient injections of money will ultimately always reverse a deflation\",[43] although Japan's deflationary spiral was not broken by the amount of quantitative easing provided by the Bank of Japan.  Until the 1930s, it was commonly believed by economists that deflation would cure itself. As prices decreased, demand would naturally increase, and the economic system would correct itself without outside intervention.  This view was challenged in the 1930s during the Great Depression. Keynesian economists argued that the economic system was not self-correcting with respect to deflation and that governments and central banks had to take active measures to boost demand through tax cuts or increases in government spending. Reserve requirements from the central bank were high compared to recent times. So were it not for redemption of currency for gold (in accordance with the gold standard), the central bank could have effectively increased money supply by simply reducing the reserve requirements and through open market operations (e.g., buying treasury bonds for cash) to offset the reduction of money supply in the private sectors due to the collapse of credit (credit is a form of money).  With the rise of monetarist ideas, the focus in fighting deflation was put on expanding demand by lowering interest rates (i.e., reducing the \"cost\" of money). This view has received criticism in light of the failure of accommodative policies in both Japan and the US to spur demand after stock market shocks in the early 1990s and in 2000–2002, respectively. Austrian economists worry about the inflationary impact of monetary policies on asset prices. Sustained low real rates can cause higher asset prices and excessive debt accumulation. Therefore, lowering rates may prove to be only a temporary palliative, aggravating an eventual debt deflation crisis.  When the central bank has lowered nominal interest rates to zero, it can no longer further stimulate demand by lowering interest rates. This is the famous liquidity trap. When deflation takes hold, it requires \"special arrangements\" to lend money at a zero nominal rate of interest (which could still be a very high real rate of interest, due to the negative inflation rate) in order to artificially increase the money supply.  Although the values of capital assets are often casually said to deflate when they decline, this usage is not consistent with the usual definition of deflation; a more accurate description for a decrease in the value of a capital asset is economic depreciation. Another term, the accounting conventions of depreciation are standards to determine a decrease in values of capital assets when market values are not readily available or practical.  The inflation rate of Greece was negative during three years from 2013 to 2015. The same applies to Bulgaria, Cyprus, Spain, and Slovakia from 2014 to 2016. Greece, Cyprus, Spain, and Slovakia are members of the European monetary union. The Bulgarian currency, the lev, is pegged to the Euro with a fixed exchange rate. In the entire European Union and the Eurozone, a disinflationary development was to be observed in the years 2011 to 2015.  Table: Harmonised index of consumer prices. Annual average rate of change (%) (HICP inflation rate).[44] Negative values are highlighted in colour.  Following the Asian financial crisis in late 1997, Hong Kong experienced a long period of deflation which did not end until the fourth quarter of 2004.[45]  Many East Asian currencies devalued following the crisis. The Hong Kong dollar, however, was pegged to the U.S. dollar, leading to an adjustment instead by a deflation of consumer prices. The situation was worsened by the increasingly cheap exports from mainland China, and \"weak consumer confidence\" in Hong Kong. This deflation was accompanied by an economic slump that was more severe and prolonged than those of the surrounding countries that devalued their currencies in the wake of the Asian financial crisis.[46][47]  In February 2009, Ireland's Central Statistics Office announced that during January 2009, the country experienced deflation, with prices falling by 0.1% from the same time in 2008. This was the first time deflation has hit the Irish economy since 1960. Overall consumer prices decreased by 1.7% in the month.[48]  Brian Lenihan, Ireland's Minister for Finance, mentioned deflation in an interview with RTÉ Radio. According to RTÉ's account,[49] \"Minister for Finance Brian Lenihan has said that deflation must be taken into account when Budget cuts in child benefit, public sector pay and professional fees are being considered. Mr Lenihan said month-on-month there has been a 6.6% decline in the cost of living this year.\"  This interview is notable in that the deflation referred to is not discernibly regarded negatively by the Minister in the interview. The Minister mentions the deflation as an item of data helpful to the arguments for a cut in certain benefits. The alleged economic harm caused by deflation is not alluded to or mentioned by this member of government. This is a notable example of deflation in the modern era being discussed by a senior financial Minister without any mention of how it might be avoided, or whether it should be.[50][original research?]  Deflation started in the early 1990s.[40] The Bank of Japan and the government tried to eliminate it by reducing interest rates and \"quantitative easing,\" but did not create a sustained increase in broad money and deflation persisted. In July 2006, the zero-rate policy was ended.  Systemic reasons for deflation in Japan can be said to include:  In November 2009, Japan returned to deflation, according to The Wall Street Journal. Bloomberg L.P. reports that consumer prices fell in October 2009 by a near-record 2.2%.[69] It was not until 2014 that new economic policies laid out by Prime Minister Shinzo Abe finally allowed for significant levels of inflation to return.[70] However, the COVID-19 recession once again led to deflation in 2020, with consumer good prices quickly falling, prompting heavy government stimulus worth over 20% of GDP.[71][72][73] As a result, it is likely that deflation will remain as a long-term economic issue for Japan.[74]  During World War I the British pound sterling was removed from the gold standard. The motivation for this policy change was to finance World War I; one of the results was inflation, and a rise in the gold price, along with the corresponding drop in international exchange rates for the pound. When the pound was returned to the gold standard after the war it was done on the basis of the pre-war gold price, which, since it was higher than equivalent price in gold, required prices to fall to realign with the higher target value of the pound.  The UK experienced deflation of approximately 10% in 1921, 14% in 1922, and 3 to 5% in the early 1930s.[75]  There have been four significant periods of deflation in the United States.  The first and most severe was during the depression in 1818–1821 when prices of agricultural commodities declined by almost 50%. A credit contraction caused by a financial crisis in England drained specie out of the U.S. The Bank of the United States also contracted its lending. The price of agricultural commodities fell by almost 50% from the high in 1815 to the low in 1821, and did not recover until the late 1830s, although to a significantly lower price level. Most damaging was the price of cotton, the U.S.'s main export. Food crop prices, which had been high because of the famine of 1816 that was caused by the year without a summer, fell after the return of normal harvests in 1818. Improved transportation, mainly from turnpikes, and to a minor extent the introduction of steamboats, significantly lowered transportation costs.[28]  The second was the depression of the late 1830s to 1843, following the Panic of 1837, when the currency in the United States contracted by about 34% with prices falling by 33%. The magnitude of this contraction is only matched by the Great Depression.[76] (See: § Historical examples of credit deflation.) This \"deflation\" satisfies both definitions, that of a decrease in prices and a decrease in the available quantity of money. Despite the deflation and depression, GDP rose 16% from 1839 to 1843.[76]  The third was after the Civil War, sometimes called The Great Deflation. It was possibly spurred by return to a gold standard, retiring paper money printed during the Civil War:  The Great Sag of 1873–96 could be near the top of the list. Its scope was global. It featured cost-cutting and productivity-enhancing technologies. It flummoxed the experts with its persistence, and it resisted attempts by politicians to understand it, let alone reverse it. It delivered a generation's worth of rising bond prices, as well as the usual losses to unwary creditors via defaults and early calls. Between 1875 and 1896, according to Milton Friedman, prices fell in the United States by 1.7% a year, and in Britain by 0.8% a year.  (Note: David A. Wells (1890) gives an account of the period and discusses the great advances in productivity which Wells argues were the cause of the deflation. The productivity gains matched the deflation.[78] Murray Rothbard (2002) gives a similar account.[79])  The fourth was in 1930–1933 when the rate of deflation was approximately 10 percent\/year, part of the United States' slide into the Great Depression, where banks failed and unemployment peaked at 25%.  The deflation of the Great Depression occurred partly because there was an enormous contraction of credit (money), bankruptcies creating an environment where cash was in frantic demand, and when the Federal Reserve was supposed to accommodate that demand, it instead contracted the money supply by 30% in enforcement of its new real bills doctrine, so banks failed one by one (because they were unable to meet the sudden demand for cash – see Bank run). From the standpoint of the Fisher equation (see above), there was a simultaneous drop both in money supply (credit) and the velocity of money which was so profound that price deflation took hold despite the increases in money supply spurred by the Federal Reserve.  Throughout the history of the United States, inflation has approached zero and dipped below for short periods of time. This was quite common in the 19th century, and in the 20th century until the permanent abandonment of the gold standard for the Bretton Woods system in 1948. In the past 60 years, the United States has experienced deflation only two times; in 2009 with the Great Recession and in 2015, when the CPI barely broke below 0% at −0.1%.[80]  Some economists believe the United States may have experienced deflation as part of the financial crisis of 2007–2008; compare the theory of debt deflation. Consumer prices dropped 1 percent in October 2008. This was the largest one-month fall in prices in the U.S. since at least 1947. That record was again broken in November 2008 with a 1.7% decline. In response, the Federal Reserve decided to continue cutting interest rates, down to a near-zero range as of December 16, 2008.[81]  In late 2008 and early 2009, some economists feared the U.S. would enter a deflationary spiral. Economist Nouriel Roubini predicted that the United States would enter a deflationary recession, and coined the term \"stag-deflation\" to describe it.[82] It was the opposite of stagflation, which was the main fear during the spring and summer of 2008. The United States then began experiencing measurable deflation, steadily decreasing from the first measured deflation of −0.38% in March, to July's deflation rate of −2.10%. On the wage front, in October 2009, the state of Colorado announced that its state minimum wage, which was indexed to inflation, was set to be cut, which would be the first time a state had cut its minimum wage since 1938.[83] "},"meta":{},"created_at":"2025-03-22T14:25:42.274677Z","updated_at":"2025-03-22T14:25:42.274677Z","inner_id":16,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":25,"annotations":[{"id":25,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"1e1f8e1a-53ca-4823-b797-f3994b5dec82","import_id":null,"last_action":null,"bulk_created":false,"task":25,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A mutual fund is an investment fund that pools money from many investors to purchase securities. The term is typically used in the United States, Canada, and India, while similar structures across the globe include the SICAV in Europe ('investment company with variable capital'), and the open-ended investment company (OEIC) in the UK.  Mutual funds are often classified by their principal investments: money market funds, bond or fixed income funds, stock or equity funds, or hybrid funds.[1] Funds may also be categorized as index funds, which are passively managed funds that track the performance of an index, such as a stock market index or bond market index, or actively managed funds, which seek to outperform stock market indices but generally charge higher fees. The primary structures of mutual funds are open-end funds, closed-end funds, and unit investment trusts. Over long durations, passively managed funds consistently outperform actively managed funds.[2][3][4]  Open-end funds are purchased from or sold to the issuer at the net asset value of each share as of the close of the trading day in which the order was placed, as long as the order was placed within a specified period before the close of trading. They can be traded directly with the issuer.[5]  Mutual funds have advantages and disadvantages compared to direct investing in individual securities. The advantages of mutual funds include economies of scale, diversification, liquidity, and professional management.[6] As with other types of investment, investing in mutual funds involves various fees and expenses.  Mutual funds are regulated by governmental bodies and are required to publish information including performance, comparisons of performance to benchmarks, fees charged, and securities held. A single mutual fund may have several share classes, for which larger investors pay lower fees.  Hedge funds and exchange-traded funds are not typically referred to as mutual funds, and each is targeted at different investors, with hedge funds being available only to high-net-worth individuals.[7]  At the end of 2020, open-end mutual fund assets worldwide were $63.1 trillion.[8] The countries with the largest mutual fund industries are:  At the end of 2019, 23% of household financial assets were invested in mutual funds. Mutual funds accounted for approximately 50% of the assets in individual retirement accounts, 401(k)s and other similar retirement plans.[8]  Luxembourg and Ireland are the primary jurisdictions for the registration of UCITS funds. These funds may be sold throughout the European Union and in other countries that have adopted mutual recognition regimes.  The first modern investment funds, the precursor of mutual funds, were established in the Dutch Republic. In response to the financial crisis of 1772–1773, Amsterdam-based businessman Abraham (or Adriaan) van Ketwich formed a trust named Eendragt Maakt Magt (\"unity creates strength\"). His aim was to provide small investors with an opportunity to diversify.[9][10]  The first investment trust in the UK, the Scottish American Investment Trust formed in 1873, is considered the \"most obvious progenitor\" to the mutual fund, according to Diana B. Henriques.[11]  One of the earliest investment companies in the U.S. similar to a modern mutual fund was the Boston Personal Property Trust that was founded in 1893; however, its original intent was as a workaround to Massachusetts law restricting corporate real estate holdings rather than investing.[12] Early U.S. funds were generally closed-end funds with a fixed number of shares that often traded at prices above the portfolio net asset value.[13] The first open-end mutual fund with redeemable shares was established on March 21, 1924, as the Massachusetts Investors Trust, which is still in existence today and managed by MFS Investment Management.[14][15]  In the U.S., there were nearly six times as many closed-end funds as mutual funds in 1929.[16]  After the Wall Street Crash of 1929, the United States Congress passed a series of acts regulating the securities markets in general and mutual funds in particular.   These new regulations encouraged the development of open-end mutual funds (as opposed to closed-end funds).[17]  In 1936, U.S. mutual fund industry was nearly half as large as closed-end investment trusts. But mutual funds had grown to twice as large as closed-end funds by 1947; growth would accelerate to ten times as much by 1959. In terms of dollar amounts, mutual funds in the U.S. totaled $2 billion in value in 1950 and about $17 billion in 1960.[18] The introduction of money market funds in the high-interest rate environment of the late 1970s boosted industry growth dramatically.   The first retail index funds appeared in the early 1970s, aiming to capture average market returns rather than doing detailed company-by-company analysis as earlier funds had done. Rex Sinquefield offered the first S&P 500 index fund to the general public starting in 1973, while employed at American National Bank of Chicago.[19][20] Sinquefield's fund had $12 billion in assets after its first seven years.[21] John \"Mac\" McQuown also began an index fund in 1973, though it was part of a large pension fund managed by Wells Fargo and not open to the general public.[19] Batterymarch Financial, a small Boston firm then employing Jeremy Grantham, also offered index funds beginning in 1973 but it was such a revolutionary concept they did not have paying customers for over a year.[19] John Bogle was another early pioneer of index funds with the First Index Investment Trust, formed in 1976 by The Vanguard Group; it is now called the \"Vanguard 500 Index Fund\" and is one of the largest mutual funds.[19]  Beginning the 1980s, the mutual fund industry began a period of growth.[8] According to Robert Pozen and Theresa Hamacher, growth was the result of three factors:  The 2003 mutual fund scandal involved unequal treatment of fund shareholders whereby some fund management companies allowed favored investors to engage in prohibited late trading or market timing. The scandal was uncovered by former New York Attorney General Eliot Spitzer and led to an increase in regulation.  In a 2007 study about German mutual funds, Johannes Gomolka and Ralf Jasny found statistical evidence of illegal time zone arbitrage in trading of German mutual funds.[23] Though reported to regulators, BaFin never commented on these results.  Like other types of investment funds, mutual funds have advantages and disadvantages compared to alternative structures or investing directly in individual securities. According to Robert Pozen and Theresa Hamacher, these are:  Mutual funds have disadvantages as well, which include:  In the United States, the principal laws governing mutual funds are:  Mutual funds are overseen by a board of directors if organized as a corporation, or by a board of trustees, if organized as a trust. The Board must ensure that the fund is managed in the interests of the fund's investors. The board hires the fund manager and other service providers to the fund.  The sponsor or fund management company often referred to as the fund manager, trades (buys and sells) the fund's investments in accordance with the fund's investment objective. Funds that are managed by the same company under the same brand are known as a fund family or fund complex. A fund manager must be a registered investment adviser.  In the European Union, funds are governed by laws and regulations established by their home country. However, the European Union has established a mutual recognition regime that allows funds regulated in one country to be sold in all other countries in the European Union, if they comply with certain requirements. The directive establishing this regime is the Undertakings for Collective Investment in Transferable Securities Directive 2009, and funds that comply with its requirements are known as UCITS funds.  Regulation of mutual funds in Canada is primarily governed by National Instrument 81-102 \"Mutual Funds\", which is implemented separately in each province or territory. The Canadian Securities Administrator works to harmonize regulation across Canada.[27]  In the Hong Kong market mutual funds are regulated by two authorities:  In Taiwan, mutual funds are regulated by the Financial Supervisory Commission (FSC).[30]  Mutual funds in India are regulated by Securities and Exchange Board of India, the regulator of the securities and commodity market owned by the Government of India,[31] under the SEBI (Mutual Funds) regulations of 1996. The functional aspect of Mutual Funds industry comes under the purview of AMFI, a trade association of all fund houses. Formed in August 1995, the body undertook the Mutual Funds Sahi hai campaign in March 2017 for promoting investor awareness on mutual funds in India.[32]  There are three primary structures of mutual funds: open-end funds, unit investment trusts, and closed-end funds. Exchange-traded funds (ETFs) are open-end funds or unit investment trusts that trade on an exchange.  Open-end mutual funds must be willing to buy back (\"redeem\") their shares from their investors at the net asset value (NAV) computed that day based upon the prices of the securities owned by the fund. In the United States, open-end funds must be willing to buy back shares at the end of every business day. In other jurisdictions, open-end funds may only be required to buy back shares at longer intervals. For example, UCITS funds in Europe are only required to accept redemptions twice each month (though most UCITS accept redemptions daily).  Most open-end funds also sell shares to the public every business day; these shares are priced at NAV.  Open-end funds are often referred to simply as \"mutual funds\".  In the United States at the end of 2019, there were 7,945 open-end mutual funds with combined assets of $21.3 trillion, accounting for 83% of the U.S. industry.[8]  Unit investment trusts (UITs) are issued to the public only once when they are created. UITs generally have a limited life span, established at creation. Investors can redeem shares directly with the fund at any time (similar to an open-end fund) or wait to redeem them upon the trust's termination. Less commonly, they can sell their shares in the open market.  Unlike other types of mutual funds, unit investment trusts do not have a professional investment manager. Their portfolio of securities is established at the creation of the UIT.  In the United States, at the end of 2019, there were 4,571 UITs with combined assets of less than $0.1 trillion.[8]  Closed-end funds generally issue shares to the public only once, when they are created through an initial public offering. Their shares are then publicly listed. Investors who want to sell their shares must sell their shares to another investor in the market; they cannot sell their shares back to the fund. The price that investors receive for their shares may be significantly different from NAV; it may be at a \"premium\" to NAV (i.e., higher than NAV) or, more commonly, at a \"discount\" to NAV (i.e., lower than NAV).  In the United States, at the end of 2019, there were 500 closed-end mutual funds with combined assets of $0.28 trillion.[8]  Mutual funds may be classified by their principal investments, as described in the prospectus and investment objective. The four main categories of funds are money market funds, bond or fixed-income funds, stock or equity funds, and hybrid funds. Within these categories, funds may be sub-classified by investment objective, investment approach, or specific focus.  The types of securities that a particular fund may invest in are set forth in the fund's prospectus, a legal document that describes the fund's investment objective, investment approach and permitted investments. The investment objective describes the type of income that the fund seeks. For example, a capital appreciation fund generally looks to earn most of its returns from increases in the prices of the securities it holds, rather than from dividend or interest income. The investment approach describes the criteria that the fund manager uses to select investments for the fund.  Bond, stock, and hybrid funds may be classified as either index (or passively-managed) funds or actively managed funds.  Alternative investments which incorporate advanced techniques such as hedging known as \"liquid alternatives\".  Money market funds invest in money market instruments, which are fixed income securities with a very short time to maturity and high credit quality. Investors often use money market funds as a substitute for bank savings accounts, though money market funds are not insured by the government, unlike bank savings accounts.  In the United States, money market funds sold to retail investors and those investing in government securities may maintain a stable net asset value of $1 per share, when they comply with certain conditions. Money market funds sold to institutional investors that invest in non-government securities must compute a net asset value based on the value of the securities held in the funds.  In the United States, at the end of 2019, assets in money market funds were $3.6 trillion, representing 14% of the industry.[8]  Bond funds invest in fixed income or debt securities. Bond funds can be sub-classified according to:  In the United States, at the end of 2019, assets in bond funds (of all types) were $5.7 trillion, representing 22% of the industry.[8]  Stock or equity funds invest in common stocks. Stock funds may focus on a particular area of the stock market, such as   In the United States, at the end of 2019, assets in stock funds (of all types) were $15.0 trillion, representing 58% of the industry.[8]  Funds which invest in a relatively small number of stocks are known as \"focus funds\".  Hybrid funds invest in both bonds and stocks or in convertible securities.  Balanced funds, asset allocation funds, convertible bond funds,[33] target date or target-risk funds, and lifecycle or lifestyle funds are all types of hybrid funds.  The performance of hybrid funds can be explained by a combination of stock factors (e.g., Fama–French three-factor model), bond factors (e.g., excess returns of a Government bond index), option factors (e.g., implied stock-market volatility), and fund factors (e.g., the net supply of convertible bonds).[34]  Hybrid funds may be structured as fund of funds, meaning that they invest by buying shares in other mutual funds that invest in securities. Many funds of funds invest in affiliated funds (meaning mutual funds managed by the same fund sponsor), although some invest in unaffiliated funds (i.e., managed by other fund sponsors) or some combination of the two.  In the United States, at the end of 2019, assets in hybrid funds were $1.6 trillion, representing 6% of the industry.[8]  Funds may invest in commodities or other investments.  Investors in a mutual fund pay the fund's expenses. Some of these expenses reduce the value of an investor's account; others are paid by the fund and reduce net asset value.  These expenses fall into five categories:  The management fee is paid by the fund to the management company or sponsor that organizes the fund, provides the portfolio management or investment advisory services, and normally lends its brand name to the fund. The fund manager may also provide other administrative services. The management fee often has breakpoints, which means that it declines in percentage as the invested amount (in either the specific fund or in the fund family as a whole) increases. The fund's board reviews the management fee annually. Fund shareholders must vote on any proposed increase, but the fund manager or sponsor can agree to waive some or all of the management fees in order to lower the fund's expense ratio.  Index funds generally charge a lower management fee than actively-managed funds.  When these expenses are charged separately, distribution charges pay for marketing, distribution of the fund's shares, and services to investors. There are three types of distribution charges.  Distribution charges generally vary for each share class.  A mutual fund pays expenses related to buying or selling the securities in its portfolio. These expenses may include brokerage commissions. These costs are normally positively correlated with turnover.  Shareholders may be required to pay fees for certain transactions, such as buying or selling shares of the fund. A fund may charge a fee for maintaining an individual retirement account for an investor.  Some funds charge redemption fees when an investor sells fund shares shortly after buying them (usually defined as within 30, 60, or 90 days of purchase). Redemption fees are computed as a percentage of the sale amount. Shareholder transaction fees are not part of the expense ratio.  A mutual fund may pay for other services including:  The fund manager or sponsor may agree to subsidize some of these charges.  The expense ratio equals recurring fees and expenses charged to the fund during the year divided by average net assets. The management fee and fund services charges are ordinarily included in the expense ratio. Front-end and back-end loads, securities transaction fees, and shareholder transaction fees are normally excluded.  To facilitate comparisons of expenses, regulators generally require that funds use the same formula to compute the expense ratio and publish the results.  In the United States, a fund that calls itself \"no-load\" cannot charge a front-end load or back-end load under any circumstances and cannot charge a distribution and services fee greater than 0.25% of fund assets.  Critics of the fund industry argue that the expenses for many mutual funds are too high. They believe that the market for mutual funds is not as competitive as it should be and that there are often many hidden fees so that it can be difficult for investors to understand and minimize the fees that they pay. They argue that the most effective way for investors to raise the returns they earn from mutual funds is to invest in funds with low expense ratios.  Fund managers counter that fees are determined by a highly competitive market and, therefore, reflect the value that investors attribute to the service provided. They also note that fees are clearly disclosed.  Mutual funds in the United States are required to report the average annual compounded rates of return for one-, five- and ten-year periods using the following formula:[35]  Where:  A fund's net asset value (NAV) equals the current market value of a fund's holdings minus the fund's liabilities (this figure may also be referred to as the fund's \"net assets\"). It is usually expressed as a per-share amount, computed by dividing net assets by the number of fund shares outstanding. Funds must compute their net asset value according to the rules set forth in their prospectuses. Most compute their NAV at the end of each business day.  Valuing the securities held in a fund's portfolio is often the most difficult part of calculating net asset value. The fund's board typically oversees security valuation.  A single mutual fund may give investors a choice of different combinations of front-end loads, back-end loads and distribution and services fee, by offering several different types of shares, known as share classes. All of them invest in the same portfolio of securities, but each has different expenses and, therefore, different net asset values and different performance results. Some of these share classes may be available only to certain types of investors.  Typical share classes for funds sold through brokers or other intermediaries in the United States are:  No-load funds in the United States often have two classes of shares:  Neither class of shares typically charges a front-end or back-end load.  Portfolio turnover is a measure of the volume of a fund's securities trading. It is expressed as a percentage of the average market value of the portfolio's long-term securities. Turnover is the lesser of a fund's purchases or sales during a given year divided by average long-term securities market value for the same period. If the period is less than a year, turnover is generally annualized. "},"meta":{},"created_at":"2025-03-22T14:25:42.274677Z","updated_at":"2025-03-22T14:25:42.274677Z","inner_id":17,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":26,"annotations":[{"id":26,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"d6c075b7-598e-4666-9089-54b81a7ca303","import_id":null,"last_action":null,"bulk_created":false,"task":26,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Heterodox  In macroeconomics, money supply (or money stock) refers to the total volume of money held by the public at a particular point in time. There are several ways to define \"money\", but standard measures usually include currency in circulation (i.e. physical cash) and demand deposits (depositors' easily accessed assets on the books of financial institutions).[1][2] Money supply data is recorded and published, usually by the national statistical agency or the central bank of the country. Empirical money supply measures are usually named M1, M2, M3, etc., according to how wide a definition of money they embrace. The precise definitions vary from country to country, in part depending on national financial institutional traditions.   Even for narrow aggregates like M1, by far the largest part of the money supply consists of deposits in commercial banks, whereas currency (banknotes and coins) issued by central banks only makes up a small part of the total money supply in modern economies. The public's demand for currency and bank deposits and commercial banks' supply of loans are consequently important determinants of money supply changes. As these decisions are influenced by central banks' monetary policy, not least their setting of interest rates, the money supply is ultimately determined by complex interactions between non-banks, commercial banks and central banks.   According to the quantity theory supported by the monetarist school of thought, there is a tight causal connection between growth in the money supply and inflation. In particular during the 1970s and 1980s this idea was influential, and several major central banks during that period attempted to control the money supply closely, following a monetary policy target of increasing the money supply stably. However, the strategy was generally found to be impractical because money demand turned out to be too unstable for the strategy to work as intended.   Consequently, the money supply has lost its central role in monetary policy, and central banks today generally do not try to control the money supply. Instead they focus on adjusting interest rates, in developed countries normally as part of a direct inflation target which leaves little room for a special emphasis on the money supply. Money supply measures may still play a role in monetary policy, however, as one of many economic indicators that central bankers monitor to judge likely future movements in central variables like employment and inflation.  There are several standard measures of the money supply,[4] classified along a spectrum or continuum between narrow and broad monetary aggregates. Narrow measures include only the most liquid assets: those most easily used to spend (currency, checkable deposits). Broader measures add less liquid types of assets (certificates of deposit, etc.).  This continuum corresponds to the way that different types of money are more or less controlled by monetary policy. Narrow measures include those more directly affected and controlled by monetary policy, whereas broader measures are less closely related to monetary-policy actions.[5]  The different types of money are typically classified as \"M\"s. The \"M\"s usually range from M0 (narrowest) to M3 (and M4 in some countries[6]) (broadest), but which \"M\"s, if any, are actually focused on in central bank communications depends on the particular institution. A typical layout for each of the \"M\"s is as follows for the United States:  Both central banks and commercial banks play a role in the process of money creation. In short, in the fractional-reserve banking system used throughout the world, money can be subdivided into two types:[17][18][19]  In the money supply statistics, central bank money is MB while the commercial bank money is divided up into the M1–M3 components, where it makes up the non-M0 component.   By far the largest part of the money used by individuals and firms to execute economic actions are commercial bank money, i.e. deposits issued by banks and other financial institutions. In the United Kingdom, deposit money outweighs the central bank issued currency by a factor of more than 30 to 1. In the United States, where the country's currency has a special international role being used in many transactions around the world, legally as well as illegally, the ratio is still more than 8 to 1.[20] Commercial banks create money whenever they make a loan and simultaneously create a matching deposit in the borrower's bank account. In return, money is destroyed when the borrower pays back the principal on the loan.[21] Movements in the money supply therefore to a large extent depend on the decisions of commercial banks to supply loans and consequently deposits, and the public's behavior in demanding currency as well as bank deposits.[20] These decisions are influenced by the monetary policy of central banks, so that money supply is ultimately created by complex interactions between banks, non-banks and central banks.[22]  Even though central banks today rarely try to control the amount of money in circulation, their policies still impact the actions of both commercial banks and their customers. When setting the interest rate on central bank reserves, interest rates on bank loans are affected, which in turn affects their demand. Central banks may also affect the money supply more directly by engaging in various open market operations.[21] They can increase the money supply by purchasing government securities, such as government bonds or treasury bills. This increases the liquidity in the banking system by converting the illiquid securities of commercial banks into liquid deposits at the central bank. This also causes the price of such securities to rise due to the increased demand, and interest rates to fall. In contrast, when the central bank \"tightens\" the money supply, it sells securities on the open market, drawing liquid funds out of the banking system. The prices of such securities fall as supply is increased, and interest rates rise.[23]  In some economics textbooks, the supply-demand equilibrium in the markets for money and reserves is represented by a simple so-called money multiplier relationship between the monetary base of the central bank and the resulting money supply including commercial bank deposits. This is a short-hand simplification which disregards several other factors determining commercial banks' reserve-to-deposit ratios and the public's money demand.[20][21][24][self-published source?]  The Hong Kong Basic Law and the Sino-British Joint Declaration provides that Hong Kong retains full autonomy with respect to currency issuance. Currency in Hong Kong is issued by the government and three local banks under the supervision of the territory's de facto central bank, the Hong Kong Monetary Authority. Bank notes are printed by Hong Kong Note Printing.  A bank can issue a Hong Kong dollar only if it has the equivalent exchange in US dollars on deposit. The currency board system ensures that Hong Kong's entire monetary base is backed with US dollars at the linked exchange rate. The resources for the backing are kept in Hong Kong's exchange fund, which is among the largest official reserves in the world. Hong Kong also has huge deposits of US dollars, with official foreign currency reserves of 331.3 billion USD as of September 2014[update].[25]  Hong Kong's exchange rate regime has changed over time.  The Bank of Japan defines the monetary aggregates as:[26]  The European Central Bank's definition of euro area monetary aggregates:[27]  There are just two official UK measures. M0 is referred to as the \"wide monetary base\" or \"narrow money\" and M4 is referred to as \"broad money\" or simply \"the money supply\".  There are several different definitions of money supply to reflect the differing stores of money. Owing to the nature of bank deposits, especially time-restricted savings account deposits, M4 represents the most illiquid measure of money. M0, by contrast, is the most liquid measure of the money supply.  The United States Federal Reserve published data on three monetary aggregates until 2006, when it ceased publication of M3 data[14] and only published data on M1 and M2. M1 consists of money commonly used for payment, basically currency in circulation and checking account balances; and M2 includes M1 plus balances that generally are similar to transaction accounts and that, for the most part, can be converted fairly readily to M1 with little or no loss of principal. The M2 measure is thought to be held primarily by households. Prior to its discontinuation, M3 comprised M2 plus certain accounts that are held by entities other than individuals and are issued by banks and thrift institutions to augment M2-type balances in meeting credit demands, as well as balances in money market mutual funds held by institutional investors. The aggregates have had different roles in monetary policy as their reliability as guides has changed. The principal components are:[32]  Prior to 2020, savings accounts were counted as M2 and not part of M1 as they were not considered \"transaction accounts\" by the Fed. (There was a limit of six transactions per cycle that could be carried out in a savings account without incurring a penalty.) On March 15, 2020, the Federal Reserve eliminated reserve requirements for all depository institutions and rendered the regulatory distinction between reservable \"transaction accounts\" and nonreservable \"savings deposits\" unnecessary. On April 24, 2020, the Board removed this regulatory distinction by deleting the six-per-month transfer limit on savings deposits. From this point on, savings account deposits were included in M1.[9]  Although the Treasury can and does hold cash and a special deposit account at the Fed (TGA account), these assets do not count in any of the aggregates. So in essence, money paid in taxes paid to the Federal Government (Treasury) is excluded from the money supply. To counter this, the government created the Treasury Tax and Loan (TT&L) program in which any receipts above a certain threshold are redeposited in private banks. The idea is that tax receipts won't decrease the amount of reserves in the banking system. The TT&L accounts, while demand deposits, do not count toward M1 or any other aggregate either.  When the Federal Reserve announced in 2005 that they would cease publishing M3 statistics in March 2006, they explained that M3 did not convey any additional information about economic activity compared to M2, and thus, \"has not played a role in the monetary policy process for many years.\" Therefore, the costs to collect M3 data outweighed the benefits the data provided.[14] Some politicians have spoken out against the Federal Reserve's decision to cease publishing M3 statistics and have urged the U.S. Congress to take steps requiring the Federal Reserve to do so. Congressman Ron Paul (R-TX) claimed that \"M3 is the best description of how quickly the Fed is creating new money and credit. Common sense tells us that a government central bank creating new money out of thin air depreciates the value of each dollar in circulation.\"[34] Some of the data used to calculate M3 are still collected and published on a regular basis.[14] Current alternate sources of M3 data are available from the private sector.[35]  In the United States, a bank's reserves consist of U.S. currency held by the bank (also known as \"vault cash\"[36]) plus the bank's balances in Federal Reserve accounts.[37][38] For this purpose, cash on hand and balances in Federal Reserve (\"Fed\") accounts are interchangeable (both are obligations of the Fed). Reserves may come from any source, including the federal funds market, deposits by the public, and borrowing from the Fed itself.[39]  As of April 2013, the monetary base was $3 trillion[40] and M2, the broadest measure of money supply, was $10.5 trillion.[41]  The Reserve Bank of Australia defines the monetary aggregates as:[42]  The Reserve Bank of New Zealand defines the monetary aggregates as:[44]  The Reserve Bank of India defines the monetary aggregates as:[45]  The importance which has historically been attached to the money supply in the monetary policy of central banks is due to the suggestion that movements in money may determine important economic variables like prices (and hence inflation), output and employment. Indeed, two prominent analytical frameworks in the 20th century both built on this premise: the Keynesian IS-LM model and the monetarist quantity theory of money.[20]  The IS-LM model was introduced by John Hicks in 1937 to describe Keynesian macroeconomic theory. Between the 1940s and mid-1970s, it was the leading framework of macroeconomic analysis[47] and is still today an important conceptual introductory tool in many macroeconomics textbooks.[48] In the traditional version of this model it is assumed that the central bank conducts monetary policy by increasing or decreasing the money supply, which affects interest rates and consequently investment, aggregate demand and output.  In light of the fact that modern central banks have generally ceased to target the money supply as an explicit policy variable,[49] in some more recent macroeconomic textbooks the IS-LM model has been modified to incorporate the fact that rather than manipulating the money supply, central banks tend to conduct their policies by setting policy interest rates more directly.[23]  According to the quantity theory of money, inflation is caused by movements in the supply of money and hence can be controlled by the central bank if the bank controls the money supply. The theory builds upon Irving Fisher's equation of exchange from 1911:[50]  where  In practice, macroeconomists almost always use real GDP to define Q, omitting the role of all other transactions.[51] Either way, the equation in itself is an identity which is true by definition rather than describing economic behavior. That is, velocity is defined by the values of the other three variables. Unlike the other terms, the velocity of money has no independent measure and can only be estimated by dividing PQ by M. Adherents of the quantity theory of money assume that the velocity of money is stable and predictable, being determined mostly by financial institutions. If that assumption is valid, then changes in M can be used to predict changes in PQ.[52] If not, then a model of V is required in order for the equation of exchange to be useful as a macroeconomics model or as a predictor of prices.  Most macroeconomists replace the equation of exchange with equations for the demand for money which describe more regular economic behavior. However, predictability (or the lack thereof) of the velocity of money is equivalent to predictability (or the lack thereof) of the demand for money (since in equilibrium real money demand is simply ⁠Q\/V⁠).  There is some empirical evidence of a direct relationship between the growth of the money supply and long-term price inflation, at least for rapid increases in the amount of money in the economy.[53] The quantity theory was a cornerstone for the monetarists and in particular Milton Friedman, who together with Anna Schwartz in 1963 in a pioneering work documented the relationship between money and inflation in the United States during the period 1867–1960.[20] During the 1970s and 1980s the monetarist ideas were increasingly influential, and major central banks like the Federal Reserve, the Bank of England and the German Bundesbank officially followed a monetary policy objective of increasing the money supply in a stable way.[51]  Starting in the mid-1970s and increasingly over the next decades, the empirical correlation between fluctuations in the money supply and changes in income or prices broke down, and there appeared clear evidence that money demand (or, equivalently, velocity) was unstable, at least in the short and medium run, which is the time horizon that is relevant to monetary policy. This made a money target less useful for central banks and led to the decline of money supply as a tool of monetary policy. Instead central banks generally switched to steering interest rates directly, allowing money supply to fluctuate to accommodate fluctuations in money demand.[20] Concurrently, most central banks in developed countries implemented direct inflation targeting as the foundation of their monetary policy,[54] which leaves little room for a special emphasis on the money supply. In the United States, the strategy of targeting the money supply was tried under Federal Reserve chairman Paul Volcker from 1979, but was found to be impractical and later given up.[55] According to Benjamin Friedman, the number of central banks that actively seek to influence money supply as an element of their monetary policy is shrinking to zero.[20]  Even though today central banks generally do not try to determine the money supply, monitoring money supply data may still play a role in the preparation of monetary policy as part of a wide array of financial and economic data that policymakers review.[56] Developments in money supply may contain information of the behavior of commercial banks and of the general economic stance which is useful for judging future movements in, say, employment and inflation.[57] Also in this respect, however, money supply data have a mixed record. In the United States, for instance, the Conference Board Leading Economic Index originally included a real money supply (M2) component as one of its 10 leading indicators, but removed it from the index in 2012 after having ascertained that it had performed poorly as a leading indicator since 1989.[58] "},"meta":{},"created_at":"2025-03-22T14:25:42.274677Z","updated_at":"2025-03-22T14:25:42.274677Z","inner_id":18,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":27,"annotations":[{"id":27,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"24c745da-7508-44fb-bef3-247cddfbc35c","import_id":null,"last_action":null,"bulk_created":false,"task":27,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  British cinema has significantly influenced the global film industry since the 19th century.  The oldest known surviving film in the world, Roundhay Garden Scene (1888), was shot in England by French inventor Louis Le Prince. Early colour films were also pioneered in the UK. Film production reached an all-time high in 1936,[6] but the \"golden age\" of British cinema is usually thought to have occurred in the 1940s, which saw the release of the most critically acclaimed works by filmmakers such as David Lean, Michael Powell, and Carol Reed.[7][8][9]  Many British actors have accrued critical success and worldwide recognition, including Patrick Stewart, Julie Andrews, Michael Caine, Joan Collins, Sean Connery, Benedict Cumberbatch, Daniel Craig, Daniel Day-Lewis, Judi Dench, Olivia de Havilland, Audrey Hepburn, Anthony Hopkins, Glynis Johns, Vivien Leigh, Ian Mckellen, Peter O'Toole, Gary Oldman, Laurence Olivier, John Gielgud, Maggie Smith, Joan Plowright, Emma Thompson, and Kate Winslet.[10][11][12][13][14][15] Some of the films with the largest ever box office profits have been made in the United Kingdom, including Harry Potter and James Bond, the fourth and fifth highest-grossing film franchises of all time.[16]  The identity of British cinema, particularly in relation to the cinema of the United States, has been the subject of debate. Its history has often been affected by its attempts to compete with the United States; the career of producer Alexander Korda was marked by this objective, which the Rank Organisation also attempted to do in the 1940s, as well as Goldcrest in the 1980s. British filmmakers such as Alfred Hitchcock, Christopher Nolan, and Ridley Scott achieved success primarily through their work in the United States,[17] as did British performers such as Charlie Chaplin and Cary Grant.[18]  In 2009, British films grossed around $2 billion worldwide and achieved a market share of around 7% globally and 17% in the United Kingdom.[19] UK box office earnings totalled £1.1 billion in 2012,[20] with 172.5 million admissions.[21] The British Film Institute has produced a poll ranking what it considers to be the 100 greatest British films of all time.[22] The annual BAFTA Awards hosted by the British Academy of Film and Television Arts are considered to be the British equivalent of the Academy Awards.[23]  The world's first moving picture was shot in Leeds by Louis Le Prince in 1888[24][25] and the first moving pictures developed on celluloid film were made in Hyde Park, London in 1889 by British inventor William Friese Greene,[26] who patented the process in 1890.  The first people to build and run a working 35 mm camera in Britain were Robert W. Paul and Birt Acres. They made the first British film Incident at Clovelly Cottage in February 1895, shortly before falling out over the camera's patent. Soon several British film companies had opened to meet the demand for new films, such as Mitchell and Kenyon in Blackburn. The Lumière brothers first brought their show to London in 1896. In 1898, American producer Charles Urban expanded the London-based Warwick Trading Company to produce British films, mostly documentary and news.  Although the earliest British films were of everyday events, the early 20th century saw the appearance of narrative shorts, mainly comedies and melodramas. The early films were often melodramatic in tone, and there was a distinct preference for story lines already known to the audience, in particular, adaptations of Shakespeare plays and Dickens novels.  In 1898, Gaumont-British Picture Corp. was founded as a subsidiary of the French Gaumont Film Company, constructing Lime Grove Studios in West London in 1915 in the first building built in Britain solely for film production. Also in 1898, Hepworth Studios was founded in Lambeth, South London by Cecil Hepworth, the Bamforths began producing films in Yorkshire, and William Haggar began producing films in Wales.  Directed by Walter R. Booth in 1901, Scrooge, or, Marley's Ghost is the earliest film adaptation of Charles Dickens's festive novella A Christmas Carol.[27] Booth's The Hand of the Artist (1906) has been described as the first British animated film.[28][29]  In 1902, Ealing Studios was founded by Will Barker. It has become the oldest continuously operating film studio in the world.  In 1902, the earliest colour film in the world was made; capturing everyday events. In 2012, it was found by the National Science and Media Museum in Bradford after lying forgotten in an old tin for 110 years. The previous title for earliest colour film, using Urban's inferior Kinemacolor process, was thought to date from 1909. The re-discovered films were made by pioneer Edward Raymond Turner from London who patented his process on 22 March 1899.[30]  In 1909, Urban formed the Natural Color Kinematograph Company, which produced early colour films using his patented Kinemacolor process. This was later challenged in court by Greene, causing the company to go out of business in 1914.[31]  In 1903, Cecil Hepworth and Percy Stow directed Alice in Wonderland, the first film adaptation of Lewis Carroll's children's book Alice's Adventures in Wonderland.[32] Also in 1903, Frank Mottershaw of Sheffield produced the film A Daring Daylight Robbery, which launched the chase genre.  In 1911, the Ideal Film Company was founded in Soho, London, distributing almost 400 films by 1934, and producing 80.  In 1913, stage director Maurice Elvey began directing British films, becoming Britain's most prolific film director, with almost 200 by 1957.  In 1914, Elstree Studios was founded, and acquired in 1928 by German-born Ludwig Blattner, who invented a magnetic steel tape recording system that was adopted by the BBC in 1930.  In 1915, the Kinematograph Renters’ Society of Great Britain and Ireland was formed to represent the film distribution companies. It is the oldest film trade body in the world. It was known as the Society of Film Distributors until it changed its name again to the Film Distributors’ Association (FDA).[33]  In 1920, Gaumont opened Islington Studios, where Alfred Hitchcock got his start, selling out to Gainsborough Pictures in 1927. Also in 1920 Cricklewood Studios was founded by Sir Oswald Stoll, becoming Britain's largest film studio, known for Fu Manchu and Sherlock Holmes film series.  In 1920, the short-lived company Minerva Films was founded in London by the actor Leslie Howard (also producer and director) and his friend and story editor Adrian Brunel. Some of their early films include four written by A. A. Milne including The Bump, starring C. Aubrey Smith; Twice Two; Five Pound Reward; and Bookworms.[34]  By the mid-1920s the British film industry was losing out to heavy competition from the United States, which was helped by its much larger home market – in 1914 25% of films shown in the UK were British, but by 1926 this had fallen to 5%.[35] A slump in 1924 caused many British film studios to close,[citation needed] resulting in the passage of the Cinematograph Films Act 1927 to boost local production, requiring that cinemas show a certain percentage of British films. The act was technically a success, with audiences for British films becoming larger than the quota required, but it had the effect of creating a market for poor quality, low cost films, made to satisfy the quota. The \"quota quickies\", as they became known, are often blamed by historians for holding back the development of the industry. However, some British film makers, such as Michael Powell, learnt their craft making such films. The act was modified with the Cinematograph Films Act 1938 assisted the British film industry by specifying only films made by and shot in Great Britain would be included in the quota, an act that severely reduced Canadian and Australian film production.  The biggest star of the silent era, English comedian Charlie Chaplin, was Hollywood-based.[36]  Scottish solicitor John Maxwell founded British International Pictures (BIP) in 1927. Based at the former British National Pictures Studios in Elstree, the facilities original owners, including producer-director Herbert Wilcox, had run into financial difficulties.[37] One of the company's early films, Alfred Hitchcock's Blackmail (1929), is often regarded as the first British sound feature.[38][39] It was a part-talkie with a synchronised score and sound effects. Earlier in 1929, the first all-talking British feature, The Clue of the New Pin was released. It was based on a novel by Edgar Wallace, starring Donald Calthrop, Benita Home and Fred Raines, which was made by British Lion at their Beaconsfield Studios. John Maxwell's BIP became the Associated British Picture Corporation (ABPC) in 1933.[40] ABPC's studios in Elstree came to be known as the \"porridge factory\", according to Lou Alexander, \"for reasons more likely to do with the quantity of films that the company turned out, than their quality\".[41] Elstree (strictly speaking almost all the studios were in neighbouring Borehamwood) became the centre of the British film industry, with six film complexes over the years all in close proximity to each other.[42]  By 1927, the largest cinema chains in the United Kingdom consisted of around 20 cinemas but the following year Gaumont-British expanded significantly to become the largest, controlling 180 cinemas by 1928 and up to 300 by 1929. Maxwell formed ABC Cinemas in 1927 which became a subsidiary of BIP and went on to become one of the largest in the country, together with Odeon Cinemas, founded by Oscar Deutsch, who opened his first cinema in 1928. By 1937, these three chains controlled almost a quarter of all cinemas in the country. A booking by one of these chains was indispensable for the success of any British film.[35]  With the advent of sound films, many foreign actors were in less demand, with English received pronunciation commonly used; for example, the voice of Czech actress Anny Ondra in Blackmail was substituted by an off-camera Joan Barry during Ondra's scenes.  Starting with John Grierson's Drifters (also 1929), the period saw the emergence of the school of realist Documentary Film Movement, from 1933 associated with the GPO Film Unit. It was Grierson who coined the term \"documentary\" to describe a non-fiction film, and he produced the movement's most celebrated early films, Night Mail (1936), written and directed by Basil Wright and Harry Watt, and incorporating the poem by W. H. Auden towards the end of the short.  Music halls also proved influential in comedy films of this period, and a number of popular personalities emerged, including George Formby, Gracie Fields, Jessie Matthews and Will Hay. These stars often made several films a year, and their productions remained important for morale purposes during World War II.  Many of the British films with larger budgets during the 1930s were produced by London Films, founded by Hungarian emigre Alexander Korda. The success of The Private Life of Henry VIII (1933), made at British and Dominions Elstree Studios, persuaded United Artists and The Prudential to invest in Korda's Denham Film Studios, which opened in May 1936, but both investors suffered losses as a result.[43] Korda's films before the war included Things to Come, Rembrandt (both 1936) and Knight Without Armour (1937), as well as the early Technicolour films The Drum (1938) and The Four Feathers (1939). These had followed closely on from Wings of the Morning (1937), the UK's first three-strip Technicolour feature film, made by the local offshoot of 20th Century Fox. Although some of Korda's films indulged in \"unrelenting pro-Empire flag waving\", those featuring Sabu turned him into \"a huge international star\";[44] \"for many years\" he had the highest profile of any actor of Indian origin.[45] Paul Robeson was also cast in leading roles when \"there were hardly any opportunities\" for African Americans \"to play challenging roles\" in their own country's productions.[46]  In 1933, the British Film Institute was established as the lead organisation for film in the UK.[47] They set up the National Film Library in 1935 (now known as the BFI National Archive), with Ernest Lindgren as its curator.  In 1934, J. Arthur Rank became a co-founder of British National Films Company and they helped create Pinewood Studios, which opened in 1936. Also in 1936, Rank took over General Film Distributors and in 1937, Rank founded The Rank Organisation. In 1938, General Film Distributors became affiliated with Odeon Cinemas.  Rising expenditure and over-optimistic expectations of expansion into the American market caused a financial crisis in 1937,[48] after an all-time high of 192 films were released in 1936. Of the 640 British production companies registered between 1925 and 1936, only 20 were still active in 1937. Moreover, the 1927 Films Act was up for renewal. The replacement Cinematograph Films Act 1938 provided incentives, via a \"quality test\", for UK companies to make fewer films, but of higher quality, and to eliminate the \"quota quickies\". Influenced by world politics, it encouraged American investment and imports. One result was the creation of MGM-British, an English subsidiary of the largest American studio, which produced four films before the war, including Goodbye, Mr. Chips (1939).  The new venture was initially based at Denham Studios. Korda himself lost control of the facility in 1939 to the Rank Organisation.[49] Circumstances forced Korda's The Thief of Bagdad (1940), a spectacular fantasy film, to be completed in California, where Korda continued his film career during the war.  By now contracted to Gaumont British, Alfred Hitchcock had settled on the thriller genre by the mid-1930s with The Man Who Knew Too Much (1934), The 39 Steps (1935) and The Lady Vanishes (1938). Lauded in Britain where he was dubbed \"Alfred the Great\" by Picturegoer magazine, Hitchcock's reputation was beginning to develop overseas, with a New York Times feature writer asserting; \"Three unique and valuable institutions the British have that we in America have not. Magna Carta, the Tower Bridge and Alfred Hitchcock, the greatest director of screen melodramas in the world.\"[50] Hitchcock was then signed up to a seven-year contract by Selznick and moved to Hollywood.  \"The idea of a nation of devoted cinema-goers is inextricably linked with the number of classic films released during the war years. This was British cinema’s ‘golden age’, a period in which filmmakers such as Humphrey Jennings, David Lean, Powell and Pressburger, and Carol Reed came to the fore.\"[51]  Published in The Times on 5 September 1939, two days after Britain declared war on Germany, George Bernard Shaw’s letter protested against a government order to close all places of entertainment, including cinemas. ‘What agent of Chancellor Hitler is it who has suggested that we should all cower in darkness and terror “for the duration”?’. Within two weeks of the order cinemas in the provinces were reopened, followed by central London within a month.[51] In 1940, cinema admissions figures rose, to just over 1 billion for the year, and they continued rising to over 1.5 billion in 1943, 1944 and 1945.[51]  Humphrey Jennings began his career as a documentary film maker just before the war, in some cases working in collaboration with co-directors. London Can Take It (with Harry Wat, 1940) detailed the Blitz while Listen to Britain (with Stewart McAllister, 1942) looked at the home front.[52] The Crown Film Unit,[52] part of the Ministry of Information took over the responsibilities of the GPO Film Unit in 1940. Paul Rotha and Alberto Cavalcanti were colleagues of Jennings. British films began to make use of documentary techniques; Cavalcanti joined Ealing for Went the Day Well? (1942),  Many other films helped to shape the popular image of the nation at war. Among the best known of these films are In Which We Serve (1942), We Dive at Dawn (1943), Millions Like Us (1943) and The Way Ahead (1944). The war years also saw the emergence of The Archers partnership between director Michael Powell and the Hungarian-born writer-producer Emeric Pressburger with films such as The Life and Death of Colonel Blimp (1943) and A Canterbury Tale (1944).  Two Cities Films, an independent production company releasing their films through a Rank subsidiary, also made some important films, including the Noël Coward and David Lean collaborations This Happy Breed (1944) and Blithe Spirit (1945) as well as Laurence Olivier's Henry V (1944). By this time, Gainsborough Studios were releasing their series of critically derided but immensely popular period melodramas, including The Man in Grey (1943) and The Wicked Lady (1945). New stars, such as Margaret Lockwood and James Mason, emerged in the Gainsborough films.  Towards the end of the 1940s, the Rank Organisation became the dominant force behind British film-making, having acquired a number of British studios and the Gaumont chain (in 1941) to add to its Odeon Cinemas. Rank's serious financial crisis in 1949, a substantial loss and debt, resulted in the contraction of its film production.[53] In practice, Rank maintained an industry duopoly with ABPC (later absorbed by EMI) for many years.  For the moment, the industry hit new heights of creativity in the immediate post-war years. Among the most significant films produced during this period were David Lean's Brief Encounter (1945) and his Dickens adaptations Great Expectations (1946) and Oliver Twist (1948), Ken Annakin's comedy Miranda (1948) starring Glynis Johns, Carol Reed's thrillers Odd Man Out (1947) and The Third Man (1949), and Powell and Pressburger's A Matter of Life and Death (1946), Black Narcissus (1947) and The Red Shoes (1948), the most commercially successful film of its year in the United States. Laurence Olivier's Hamlet (also 1948), was the first non-American film to win the Academy Award for Best Picture. Ealing Studios (financially backed by Rank) began to produce their most celebrated comedies, with three of the best remembered films, Whisky Galore (1948), Kind Hearts and Coronets and Passport to Pimlico (both 1949), being on release almost simultaneously. Their portmanteau horror film Dead of Night (1945) is also particularly highly regarded.  Under the Import Duties Act 1932, HM Treasury levied a 75% tariff on all film imports on 6 August 1947 which became known as Dalton Duty (after Hugh Dalton then the Chancellor of the Exchequer). The tax came into effect on 8 August, applying to all imported films, of which the overwhelming majority came from the United States; American film studio revenues from the UK had been in excess of US$68 million in 1946. The following day, 9 August, the Motion Picture Association of America announced that no further films would be supplied to British cinemas until further notice. The Dalton Duty was ended on 3 May 1948 with the American studios again exported films to the UK though the Marshall Plan prohibited US film companies from taking foreign exchange out of the nations their films played in.[54]  Following the Cinematograph Film Production (Special Loans) Act 1949, the National Film Finance Corporation (NFFC) was established as a British film funding agency.  The Eady Levy, named after Sir Wilfred Eady was a tax on box office receipts in the United Kingdom in order to support the British Film industry. It was established in 1950 coming into effect in 1957. A direct governmental payment to British-based producers would have qualified as a subsidy under the terms of the General Agreement on Tariffs and Trade, and would have led to objections from American film producers. An indirect levy did not qualify as a subsidy, and so was a suitable way of providing additional funding for the UK film industry whilst avoiding criticism from abroad.  In 1951, the National Film Theatre was initially opened in a temporary building at the Festival of Britain. It moved to its present location on the South Bank in London for the first London Film Festival on 16 October 1957 run by the BFI.[55]  During the 1950s, the British industry began to concentrate on popular comedies and World War II dramas aimed more squarely at the domestic audience. The war films were often based on true stories and made in a similar low-key style to their wartime predecessors. They helped to make stars of actors like John Mills, Jack Hawkins and Kenneth More. Some of the most successful included The Cruel Sea (1953), The Dam Busters (1954), The Colditz Story (1955) and Reach for the Sky (1956).  The Rank Organisation produced some comedy successes, such as Genevieve (1953). The writer\/director\/producer team of twin brothers John and Roy Boulting also produced a series of successful satires on British life and institutions, beginning with Private's Progress (1956), and continuing with (among others) Brothers in Law (1957), Carlton-Browne of the F.O. (1958), and I'm All Right Jack (1959). Starring in School for Scoundrels (1960), the British Film Institute thought Terry-Thomas was \"outstanding as a classic British bounder\".[56]  Popular comedy series included the \"Doctor\" series, beginning with Doctor in the House (1954). The series originally starred Dirk Bogarde, probably the British industry's most popular star of the 1950s, though later films had Michael Craig and Leslie Phillips in leading roles. The Carry On series began in 1958 with regular instalments appearing for the next twenty years. The Italian director-producer Mario Zampi also made a number of successful black comedies, including Laughter in Paradise (1951), The Naked Truth (1957) and Too Many Crooks (1958). Ealing Studios had continued its run of successful comedies, including The Lavender Hill Mob (1951) and The Ladykillers (1955), but the company ceased production in 1958, after the studios had already been bought by the BBC.  Less restrictive censorship towards the end of the 1950s encouraged film producer Hammer Films to embark on their series of commercially successful horror films. Beginning with adaptations of Nigel Kneale's BBC science fiction serials The Quatermass Experiment (1955) and Quatermass II (1957), Hammer quickly graduated to The Curse of Frankenstein (1957) and Dracula (1958), both deceptively lavish and the first gothic horror films in colour. The studio turned out numerous sequels and variants, with English actors Peter Cushing and Christopher Lee being the most regular leads. Peeping Tom (1960), a now highly regarded thriller, with horror elements, set in the contemporary period, was badly received by the critics at the time, and effectively finished the career of Michael Powell, its director.  The British New Wave film makers attempted to produce social realist films (see also 'kitchen sink realism') attempted in commercial feature films released between around 1959 and 1963 to convey narratives about a wider spectrum of people in Britain than the country's earlier films had done. These individuals, principally Karel Reisz, Lindsay Anderson and Tony Richardson, were also involved in the short lived Oxford film journal Sequence and the \"Free Cinema\" documentary film movement. The 1956 statement of Free Cinema, the name was coined by Anderson, asserted: \"No film can be too personal. The image speaks. Sounds amplifies and comments. Size is irrelevant. Perfection is not an aim. An attitude means a style. A style means an attitude.\" Anderson, in particular, was dismissive of the commercial film industry. Their documentary films included Anderson's Every Day Except Christmas, among several sponsored by Ford of Britain, and Richardson's Momma Don't Allow. Another member of this group, John Schlesinger, made documentaries for the BBC's Monitor arts series.  Together with future James Bond co-producer Harry Saltzman, dramatist John Osborne and Tony Richardson established the company Woodfall Films to produce their early feature films. These included adaptations of Richardson's stage productions of Osborne's Look Back in Anger (1959), with Richard Burton, and The Entertainer (1960) with Laurence Olivier, both from Osborne's own screenplays. Such films as Reisz's Saturday Night and Sunday Morning (also 1960), Richardson's A Taste of Honey (1961), Schlesinger's A Kind of Loving (1962) and Billy Liar (1963), and Anderson's This Sporting Life (1963) are often associated with a new openness about working-class life or previously taboo issues.  The team of Basil Dearden and Michael Relph, from an earlier generation, \"probe[d] into the social issues that now confronted social stability and the establishment of the promised peacetime consensus\".[57] Pool of London (1950).[58] and Sapphire (1959) were early attempts to create narratives about racial tensions and an emerging multi-cultural Britain.[59] Dearden and Relph's Victim (1961), was about the blackmail of homosexuals. Influenced by the Wolfenden report of four years earlier, which advocated the decriminalising of homosexual sexual activity, this was \"the first British film to deal explicitly with homosexuality\".[60] Unlike the New Wave film makers though, critical responses to Dearden's and Relph's work have not generally been positive.[57][61]  As the 1960s progressed, American studios returned to financially supporting British films, especially those that capitalised on the \"swinging London\" image propagated by Time magazine in 1966. Films like Darling, The Knack ...and How to Get It (both 1965), Alfie and Georgy Girl (both 1966), all explored this phenomenon. Blowup (also 1966), and later Women in Love (1969), showed female and then male full-frontal nudity on screen in mainstream British films for the first time.  At the same time, film producers Harry Saltzman and Albert R. Broccoli combined sex with exotic locations, casual violence and self-referential humour in the phenomenally successful James Bond series with Sean Connery in the leading role. The first film Dr. No (1962) was a sleeper hit in the UK and the second, From Russia with Love (1963), a hit worldwide. By the time of the third film, Goldfinger (1964), the series had become a global phenomenon, reaching its commercial peak with Thunderball the following year. The series' success led to a spy film boom with many Bond imitations. Bond co-producer Saltzman also instigated a rival series of more realistic spy films based on the novels of Len Deighton. Michael Caine starred as bespectacled spy Harry Palmer in The Ipcress File (1965), and two sequels in the next few years. Other more downbeat espionage films were adapted from John le Carré novels, such as The Spy Who Came In from the Cold (1965) and The Deadly Affair (1966).  American directors were regularly working in London throughout the decade, but several became permanent residents in the UK. Blacklisted in America, Joseph Losey had a significant influence on British cinema in the 1960s, particularly with his collaborations with playwright Harold Pinter and leading man Dirk Bogarde, including The Servant (1963) and Accident (1967). Voluntary exiles Richard Lester and Stanley Kubrick were also active in the UK. Lester had major hits with The Beatles film A Hard Day's Night (1964) and The Knack ...and How to Get It (1965) and Kubrick with Dr. Strangelove (1963) and 2001: A Space Odyssey (1968). While Kubrick settled in Hertfordshire in the early 1960s and would remain in England for the rest of his career, these two films retained a strong American influence. Other films of this era involved prominent filmmakers from elsewhere in Europe, Repulsion (1965) and Blowup (1966) were the first English language films of the Polish director Roman Polanski and the Italian Michelangelo Antonioni respectively.  Historical films as diverse as Lawrence of Arabia (1962), Tom Jones (1963), and A Man for All Seasons (1966) benefited from the investment of American studios. Major films like Becket (1964), Khartoum (1966) and The Charge of the Light Brigade (1968) were regularly mounted, while smaller-scale films, including Accident (1967), were big critical successes. Four of the decade's Academy Award winners for best picture were British productions, including six Oscars for the film musical Oliver! (1968), based on the Charles Dickens novel Oliver Twist.  After directing several contributions to the BBC's Wednesday Play anthology series, Ken Loach began his feature film career with the social realist Poor Cow (1967) and Kes (1969). Meanwhile, the controversy around Peter Watkins The War Game (1965), which won the Best Documentary Film Oscar in 1967, but had been suppressed by the BBC who had commissioned it, would ultimately lead Watkins to work exclusively outside Britain.  American studios cut back on British productions, and in many cases withdrew from financing them altogether. Films financed by American interests were still being made, including Billy Wilder's The Private Life of Sherlock Holmes (1970), but for a time funds became hard to come by.  More relaxed censorship also brought several controversial films, including Nicolas Roeg and Donald Cammell's Performance, Ken Russell's The Devils (1971), Sam Peckinpah's Straw Dogs (1971), and Stanley Kubrick's A Clockwork Orange (1971) starring Malcolm McDowell as the leader of a gang of thugs in a dystopian future Britain.[62]  Other films during the early 1970s included the Edwardian drama The Go-Between (1971), which won the Palme d'Or at the Cannes Film Festival, Nicolas Roeg's Venice-set supernatural thriller Don't Look Now (1973) and Mike Hodges' gangster drama Get Carter (1971) starring Michael Caine. Alfred Hitchcock returned to Britain to shoot Frenzy (1972), Other productions such as Richard Attenborough's Young Winston (1972) and A Bridge Too Far (1977) met with mixed commercial success. The British horror film cycle associated with Hammer Film Productions, Amicus and Tigon drew to a close, despite attempts by Hammer to spice up the formula with added nudity and gore. Although some attempts were made to broaden the range of British horror films, such as with The Wicker Man (1973), these films made little impact at the box office, In 1976, British Lion, who produced The Wicker Man, were finally absorbed into the film division of EMI, who had taken over ABPC in 1969. The duopoly in British cinema exhibition, via Rank and now EMI, continued.  In the early 1970s, the government reduced its funding of the National Film Finance Corporation so the NFFC started to operate as a consortium, including with banks, which led to them using more commercial criteria for funding British films rather than focusing on quality or new talent, moving to fund films based on TV shows such as Up Pompeii (1971).[63]  Some other British producers, including Hammer, turned to television for inspiration, and big screen versions of popular sitcoms like On the Buses (1971) and Steptoe and Son (1972) proved successful with domestic audiences, the former had greater domestic box office returns in its year than the Bond film, Diamonds Are Forever and in 1973, an established British actor Roger Moore was cast as Bond in, Live and Let Die, it was a commercial success and Moore would continue the role for the next 12 years. Low-budget British sex comedies included the Confessions of ... series starring Robin Askwith, beginning with Confessions of a Window Cleaner (1974). More elevated comedy films came from the Monty Python team, also from television. Their two most successful films were Monty Python and the Holy Grail (1975) and Monty Python's Life of Brian (1979), the latter a major commercial success, probably at least in part due to the controversy at the time surrounding its subject.  Some American productions did return to the major British studios in 1977–79, including the original Star Wars (1977) at Elstree Studios, Superman (1978) at Pinewood, and Alien (1979) at Shepperton. Successful adaptations were made in the decade of the Agatha Christie novels Murder on the Orient Express (1974) and Death on the Nile (1978). The entry of Lew Grade's company ITC into film production in the latter half of the decade brought only a few box office successes and an unsustainable number of failures  In 1980, only 31 British films were made,[6] a 50% decline from the previous year and the lowest number since 1914, and production fell again in 1981 to 24 films.[6] The industry suffered further blows from falling cinema attendances, which reached a record low of 54 million in 1984, and the elimination of the 1957 Eady Levy, a tax concession, in the same year. The concession had made it possible for an overseas based film company to write off a large amount of its production costs by filming in the UK – this was what attracted a succession of big-budget American productions to British studios in the 1970s.[citation needed] These factors led to significant changes in the industry, with the profitability of British films now \"increasingly reliant on secondary markets such as video and television, and Channel 4 ... [became] a crucial part of the funding equation.\"[64]  With the removal of the levy, multiplex cinemas were introduced to the United Kingdom with the opening of a ten-screen cinema by AMC Cinemas at The Point in Milton Keynes in 1985 and the number of screens in the UK increased by around 500 over the decade leading to increased attendances of almost 100 million by the end of the decade.[65][66]  The 1980s soon saw a renewed optimism, led by smaller independent production companies such as Goldcrest, HandMade Films and Merchant Ivory Productions.  Handmade Films, which was partly owned by George Harrison, was originally formed to take over the production of Monty Python's Life of Brian, after EMI's Bernard Delfont (Lew Grade's brother) had pulled out. Handmade also bought and released the gangster drama The Long Good Friday (1980), produced by a Lew Grade subsidiary, after its original backers became cautious. Members of the Python team were involved in other comedies during the decade, including Terry Gilliam's fantasy films Time Bandits (1981) and Brazil (1985), the black comedy Withnail & I (1987), and John Cleese's hit A Fish Called Wanda (1988), while Michael Palin starred in A Private Function (1984), from Alan Bennett's first screenplay for the cinema screen.[67]  Goldcrest producer David Puttnam has been described as \"the nearest thing to a mogul that British cinema has had in the last quarter of the 20th century.\"[68] Under Puttnam, a generation of British directors emerged making popular films with international distribution. Some of the talent backed by Puttnam — Hugh Hudson, Ridley Scott, Alan Parker, and Adrian Lyne — had shot commercials; Puttnam himself had begun his career in the advertising industry. When Hudson's Chariots of Fire (1981) won 4 Academy Awards in 1982, including Best Picture, its writer Colin Welland declared \"the British are coming!\".[69] When Gandhi (1982), another Goldcrest film, picked up a Best Picture Oscar, it looked as if he was right.  It prompted a cycle of period films – some with a large budget for a British film, such as David Lean's final film A Passage to India (1984), alongside the lower-budget Merchant Ivory adaptations of the works of E. M. Forster, such as A Room with a View (1986). But further attempts to make 'big' productions for the US market ended in failure, with Goldcrest losing its independence after Revolution (1985) and Absolute Beginners (1986) were commercial and critical flops. Another Goldcrest film, Roland Joffé's The Mission (also 1986), won the 1986 Palme d'Or, but did not go into profit either. Joffé's earlier The Killing Fields (1984) had been both a critical and financial success. These were Joffé's first two feature films and were amongst those produced by Puttnam.  Mainly outside the commercial sector, film makers from the new commonwealth countries had begun to emerge during the 1970s. Horace Ové's Pressure (1975) had been funded by the British Film Institute as was A Private Enterprise (1974), these being the first Black British and Asian British films, respectively. The 1980s however saw a wave of new talent, with films such as Franco Rosso's Babylon (1980), Menelik Shabazz's Burning an Illusion (1981) and Po-Chih Leong's Ping Pong (1986; one of the first films about Britain's Chinese community). Many of these films were assisted by the newly formed Channel 4, which had an official remit to provide for \"minority audiences.\" Commercial success was first achieved with My Beautiful Laundrette (1985). Dealing with racial and gay issues, it was developed from Hanif Kureishi's first film script. My Beautiful Laundrette features Daniel Day-Lewis in a leading role. Day-Lewis and other young British actors who were becoming stars, such as Gary Oldman, Colin Firth, Tim Roth and Rupert Everett, were dubbed the Brit Pack.[70]  With the involvement of Channel 4 in film production, talents from television moved into feature films with Stephen Frears (My Beautiful Laundrette) and Mike Newell with Dance with a Stranger (1985). John Boorman, who had been working in the US, was encouraged back to the UK to make Hope and Glory (1987). Channel Four also became a major sponsor of the British Film Institute's Production Board, which backed three of Britain's most critically acclaimed filmmakers: Derek Jarman (The Last of England, 1987), Terence Davies (Distant Voices, Still Lives, 1988), and Peter Greenaway; the latter of whom gained surprising commercial success with The Draughtsman's Contract (1982) and The Cook, the Thief, His Wife & Her Lover (1989). Stephen Woolley's company Palace Pictures also produced some successful films, including Neil Jordan's The Company of Wolves (1984) and Mona Lisa (1986), before collapsing amid a series of unsuccessful films. Amongst the other British films of the decade were Bill Forsyth's Gregory's Girl (1981) and Local Hero (1983), Lewis Gilbert's Educating Rita (1983), Peter Yates' The Dresser (1983) and Kenneth Branagh's directorial debut, Henry V (1989).  Compared to the 1980s, investment in film production rose dramatically. In 1989, annual investment was a meagre £104 million. By 1996, this figure had soared to £741 million.[71] Nevertheless, the dependence on finance from television broadcasters such as the BBC and Channel 4 meant that budgets were often low and indigenous production was very fragmented: the film industry mostly relied on Hollywood inward investment. According to critic Neil Watson, it was hoped that the £90 million apportioned by the new National Lottery into three franchises (The Film Consortium, Pathé Pictures, and DNA) would fill the gap, but \"corporate and equity finance for the UK film production industry continues to be thin on the ground and most production companies operating in the sector remain hopelessly under-capitalised.\"[72]  These problems were mostly compensated by PolyGram Filmed Entertainment, a film studio whose British subsidiary Working Title Films released a Richard Curtis-scripted comedy Four Weddings and a Funeral (1994). It grossed $244 million worldwide and introduced Hugh Grant to global fame, led to renewed interest and investment in British films, and set a pattern for British-set romantic comedies, including Sliding Doors (1998) and Notting Hill (1999). Other Working Titles films included Bean (1997), Elizabeth (1998) and Captain Corelli's Mandolin (2001). PFE was eventually sold and merged with Universal Pictures in 1999, the hopes and expectations of \"building a British-based company which could compete with Hollywood in its home market [had] eventually collapsed.\"[73]  Tax incentives allowed American producers to increasingly invest in UK-based film production throughout the 1990s, including films such as Interview with the Vampire (1994), Mission: Impossible (1996), Saving Private Ryan (1998), Star Wars: Episode I – The Phantom Menace (1999) and The Mummy (1999). Miramax also distributed Neil Jordan's acclaimed thriller The Crying Game (1992), which was generally ignored on its initial release in the UK, but was a considerable success in the United States. The same company also enjoyed some success releasing the BBC period drama Enchanted April (1992) and The Wings of the Dove (1997).  Among the more successful British films were the Merchant Ivory productions Howards End (1992) and The Remains of the Day (1993), Richard Attenborough's Shadowlands (1993), and Kenneth Branagh's Shakespeare adaptations. The Madness of King George (1994) proved there was still a market for British costume dramas, and other period films followed, including Sense and Sensibility (1995), Restoration (1995), Emma (1996), Mrs. Brown (1997), Basil (1998), Shakespeare in Love (1998) and Topsy-Turvy (1999).  After a six-year hiatus for legal reasons the James Bond films returned to production with the 17th Bond film, GoldenEye. With their traditional home Pinewood Studios fully booked, a new studio was created for the film in a former Rolls-Royce aero-engine factory at Leavesden in Hertfordshire.[74]  Mike Leigh emerged as a significant figure in British cinema in the 1990s, with a series of films financed by Channel 4 about working and middle class life in modern England, including Life Is Sweet (1991), Naked (1993) and his biggest hit Secrets & Lies (1996), which won the Palme d'Or at Cannes.  Other new talents to emerge during the decade included the writer-director-producer team of John Hodge, Danny Boyle and Andrew Macdonald responsible for Shallow Grave (1994) and Trainspotting (1996). The latter film generated interested in other \"regional\" productions, including the Scottish films Small Faces (1996), Ratcatcher (1999) and My Name Is Joe (1998).  The first decade of the 21st century was a relatively successful one for the British film industry. Many British films found a wide international audience due to funding from BBC Films, Film 4 and the UK Film Council, and some independent production companies, such as Working Title, secured financing and distribution deals with major American studios. Working Title scored three major international successes, all starring Hugh Grant and Colin Firth, with the romantic comedies Bridget Jones's Diary (2001), which grossed $254 million worldwide; the sequel Bridget Jones: The Edge of Reason, which earned $228 million; and Richard Curtis's directorial debut Love Actually (2003), which grossed $239 million. The most successful of all, Phyllida Lloyd's Mamma Mia! (2008), grossed $601 million.  The new decade saw a major new film series in the Harry Potter films, beginning with Harry Potter and the Philosopher's Stone in 2001. David Heyman's company Heyday Films has produced seven sequels, with the final title released in two parts – Harry Potter and the Deathly Hallows – Part 1 in 2010 and Harry Potter and the Deathly Hallows – Part 2 in 2011. All were filmed at Leavesden Studios in England.[75]  Aardman Animations' Nick Park, the creator of Wallace and Gromit and the Creature Comforts series, produced his first feature-length film, Chicken Run in 2000. Co-directed with Peter Lord, the film was a major success worldwide and one of the most successful British films of its year. Park's follow up, Wallace & Gromit: The Curse of the Were-Rabbit was another worldwide hit: it grossed $56 million at the US box office and £32 million in the UK. It also won the 2005 Academy Award for Best Animated Feature.  However it was usually through domestically funded features throughout the decade that British directors and films won awards at the top international film festivals. In 2003, Michael Winterbottom won the Golden Bear at the Berlin Film Festival for In This World. In 2004, Mike Leigh directed Vera Drake, an account of a housewife who leads a double life as an abortion provider in 1950s London. The film won the Golden Lion at the Venice Film Festival. In 2006 Stephen Frears directed The Queen based on the events surrounding the death of Princess Diana, which won the Best Actress prize at the Venice Film Festival and Academy Awards and the BAFTA for Best Film. In 2006, Ken Loach won the Palme d'Or at the Cannes Film Festival with his account of the struggle for Irish Independence in The Wind That Shakes the Barley. Joe Wright's adaptation of the Ian McEwan novel Atonement was nominated for 7 Academy Awards, including Best Film and won the Golden Globe and BAFTA for Best Film. Slumdog Millionaire was filmed entirely in Mumbai with a mostly Indian cast, though with a British director (Danny Boyle), producer (Christian Colson), screenwriter (Simon Beaufoy) and star (Dev Patel)—the film was all-British financed via Film4 and Celador. It has received worldwide critical acclaim. It has won four Golden Globes, seven BAFTA Awards and eight Academy Awards, including Best Director and Best Film. The King's Speech, which tells the story of King George VI's attempts to overcome his speech impediment, was directed by Tom Hooper and filmed almost entirely in London. It received four Academy Awards (including Best Film, Best Director, Best Actor and Best Screenplay) in 2011.  The start of the 21st century saw Asian British cinema assert itself at the box office, starting with East Is East (1999) and continuing with Bend It Like Beckham (2002). Other notable British Asian films from this period include My Son the Fanatic (1997), Ae Fond Kiss... (2004), Mischief Night (2006), Yasmin (2004) and Four Lions (2010). Some argue it has brought more flexible attitudes towards casting Black and Asian British actors, with Robbie Gee and Naomie Harris take leading roles in Underworld and 28 Days Later respectively.  2005 saw the emergence of The British Urban Film Festival, a timely addition to the film festival calendar, which recognised the influence of urban and black films on UK audiences and consequently began to showcase a growing profile of films in a genre previously not otherwise regularly seen in the capital's cinemas. Then, in 2006, Kidulthood, a film depicting a group of teenagers growing up on the streets of West London, had a limited release. This was successfully followed up with a sequel Adulthood (2008) that was written and directed by actor Noel Clarke. The success of Kidulthood and Adulthood led to the release of several other films in the 2000s and 2010s such as Bullet Boy (2004), Life and Lyrics (2006), The Intent (2016), its sequel The Intent 2: The Come Up (2018), Blue Story and Rocks (both 2019), all of starred Black-British actors.  Like the 1960s, this decade saw plenty of British films directed by imported talent. The American Woody Allen shot Match Point (2005)[76][77] and three later films in London. The Mexican director Alfonso Cuarón helmed Harry Potter and the Prisoner of Azkaban (2004) and Children of Men (2006); New Zealand filmmaker Jane Campion made Bright Star (2009), a film set in 19th century London; Danish director Nicolas Winding Refn made Bronson (2008), a biopic about the English criminal Michael Gordon Peterson; the Spanish filmmaker Juan Carlos Fresnadillo directed 28 Weeks Later (2007), a sequel to a British horror film; and two John le Carré adaptations were also directed by foreigners—The Constant Gardener by the Brazilian Fernando Meirelles and Tinker Tailor Soldier Spy by the Swedish Tomas Alfredson. The decade also saw English actor Daniel Craig became the new James Bond with Casino Royale, the 21st entry in the official Eon Productions series.  Despite increasing competition from film studios in Australia and Eastern Europe, British studios such as Pinewood, Shepperton and Leavesden remained successful in hosting major productions, including Finding Neverland, Closer, Batman Begins, Charlie and the Chocolate Factory, United 93, The Phantom of the Opera, Sweeney Todd, Fantastic Mr. Fox, Robin Hood, X-Men: First Class, Hugo and War Horse.  In February 2007, the UK became home to Europe's first DCI-compliant fully digital multiplex cinemas with the launch of Odeon Hatfield and Odeon Surrey Quays (in London), with a total of 18 digital screens.  In November 2010, Warner Bros. completed the acquisition of Leavesden Film Studios, becoming the first Hollywood studio since the 1940s to have a permanent base in the UK, and announced plans to invest £100 million in the site.[78][79]  A study by the British Film Institute published in December 2013 found that of the 613 tracked British films released between 2003 and 2010 only 7% made a profit. Films with low budgets, those that cost below £500,000 to produce, were even less likely to gain a return on outlay. Of these films, only 3.1% went into the black. At the top end of budgets for the British industry, under a fifth of films that cost £10million went into profit.[80]  On 26 July 2010 it was announced that the UK Film Council, which was the main body responsible for the development of promotion of British cinema during the 2000s, would be abolished, with many of the abolished body's functions being taken over by the British Film Institute. Actors and professionals, including James McAvoy, Emily Blunt, Pete Postlethwaite, Damian Lewis, Timothy Spall, Daniel Barber and Ian Holm, campaigned against the council's abolition.[82][83] The move also led American actor and director Clint Eastwood (who had filmed Hereafter in London) to write to the British Chancellor of the Exchequer George Osborne in August 2010 to protest the decision to close the council. Eastwood warned Osborne that the closure could result in fewer foreign production companies choosing to work in the UK.[84][85] A grass-roots online campaign was launched[86] and a petition established by supporters of the council.  Countering this, a few professionals, including Michael Winner and Julian Fellowes, supported the Government's decision.[87][88][89] A number of other organisations responded positively.  At the closure of the UK Film Council on 31 March 2011, The Guardian reported that \"The UKFC's entire annual budget was a reported £3m, while the cost of closing it down and restructuring is estimated to have been almost four times that amount.\"[90] One of the UKFC's last films, The King's Speech, is estimated to have cost $15m to make and grossed $235m, besides winning several Academy Awards. UKFC invested $1.6m for a 34% share of net profits, a valuable stake that will pass to the British Film Institute.[91]  In June 2012, Warner opened the re-developed Leavesden studio for business.[93] The most commercially successful British directors in recent years are Paul Greengrass, Mike Newell, Christopher Nolan, Ridley Scott and David Yates.[94]  In January 2012, at Pinewood Studios to visit film-related businesses, UK Prime Minister David Cameron said that his government had bold ambitions for the film industry: \"Our role, and that of the BFI, should be to support the sector in becoming even more dynamic and entrepreneurial, helping UK producers to make commercially successful pictures that rival the quality and impact of the best international productions. Just as the British Film Commission has played a crucial role in attracting the biggest and best international studios to produce their films here, so we must incentivise UK producers to chase new markets both here and overseas.\"[95]  The film industry remains an important earner for the British economy. According to a UK Film Council press release of 20 January 2011, £1.115 billion was spent on UK film production during 2010. A 2014 survey suggested that British-made films were generally more highly rated than Hollywood productions, especially when considering low-budget UK productions.  In November 2022, director Danny Boyle expressed a negative sentiment of the British film industry in recent years, stating that \"I am not sure we are great filmmakers, to be absolutely honest. As a nation, our two artforms are theatre, in a middle-class sense, and pop music, because we are extraordinary at it.\"[96]  The BFI's published figures reported £6.27 billion spent on film and high-end television production in 2022, with domestic UK film spend at £173.6 million. While the total spend was at a record high for the UK, the independent UK filmmaking spend decreased by 31% since 2021.[97]  The UK film industry was affected by the 2023 SAG-AFTRA strike with 80% of behind-the-scenes workers surveyed stating that their jobs had been affected.[98]  Although it had been funding British experimental films as early as 1952, the British Film Institute's foundation of a production board in 1964—and a substantial increase in public funding from 1971 onwards—enabled it to become a dominant force in developing British art cinema in the 1970s and 80s: from the first of Bill Douglas's Trilogy My Childhood (1972), and of Terence Davies' Trilogy Childhood (1978), via Peter Greenaway's earliest films (including the surprising commercial success of The Draughtsman's Contract (1982)) and Derek Jarman's championing of the New Queer Cinema. The first full-length feature produced under the BFI's new scheme was Kevin Brownlow and Andrew Mollo's Winstanley (1975), while others included Moon Over the Alley (1975), Requiem for a Village (1975), the openly avant-garde Central Bazaar (1973), Pressure (1975) and A Private Enterprise (1974) – the last two being, respectively, the first British Black and Asian features.  The release of Derek Jarman's Jubilee (1978) marked the beginning of a successful period of UK art cinema, continuing into the 1980s with filmmakers like Sally Potter and Ken McMullen, and producers like Stewart Richards, with success at the Cannes Film Festival and the Academy Awards. Unlike the previous generation of British film makers who had broken into directing and production after careers in the theatre or on television, the Art Cinema Directors were mostly the products of Art Schools. Many of these filmmakers were championed in their early career by the London Film Makers Cooperative and their work was the subject of detailed theoretical analysis in the journal Screen Education. Peter Greenaway was an early pioneer of the use of computer generated imagery blended with filmed footage and was also one of the first directors to film entirely on high definition video for a cinema release.  With the launch of Channel 4 and its Film on Four commissioning strand, Art Cinema was promoted to a wider audience. However, the Channel had a sharp change in its commissioning policy in the early 1990s and Greenaway and others were forced to seek European co-production financing.  In the 1970s and 1980s, British studios established a reputation for great special effects in films such as Superman (1978), Alien (1979), and Batman (1989). Some of this reputation was founded on the core of talent brought together for the filming of 2001: A Space Odyssey (1968) who subsequently worked together on series and feature films for Gerry Anderson. Thanks to the Bristol-based Aardman Animations, the UK is still recognised as a world leader in the use of stop-motion animation.  British special effects technicians and production designers are known for creating visual effects at a far lower cost than their counterparts in the US, as seen in Time Bandits (1981) and Brazil (1985). This reputation has continued through the 1990s and into the 21st century with films such as the James Bond series, Gladiator (2000) and the Harry Potter franchise.  From the 1990s to the present day, there has been a progressive movement from traditional film opticals to an integrated digital film environment, with special effects, cutting, colour grading, and other post-production tasks all sharing the same all-digital infrastructure. The London-based visual effects company Framestore, with Tim Webber the visual effects supervisor, have worked on some of the most technically and artistically challenging projects, including, The Dark Knight (2008) and Gravity (2013), with new techniques involved in Gravity realized by Webber and the Framestore team taking three years to complete.[99]  The availability of high-speed internet has made the British film industry capable of working closely with U.S. studios as part of globally distributed productions. As of 2005, this trend is expected to continue with moves towards (currently experimental) digital distribution and projection as mainstream technologies. The British film This Is Not a Love Song (2003) was the first to be streamed live on the Internet at the same time as its cinema premiere. "},"meta":{},"created_at":"2025-03-22T14:25:42.274677Z","updated_at":"2025-03-22T14:25:42.274677Z","inner_id":19,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":28,"annotations":[{"id":28,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"7a3e3ab8-6de3-4ede-b955-3ed7761d9e9c","import_id":null,"last_action":null,"bulk_created":false,"task":28,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"The main applications of financial engineering[1][2] are to:  Financial engineering is a multidisciplinary field involving financial theory, methods of engineering, tools of mathematics and the practice of programming.[3] It has also been defined as the application of technical methods, especially from mathematical finance and computational finance, in the practice of finance.[4]  Financial engineering plays a key role in a bank's customer-driven derivatives business[5]  — delivering bespoke OTC-contracts and \"exotics\", and implementing various structured products —  which encompasses quantitative modelling, quantitative programming and risk managing financial products in compliance with the regulations and Basel capital\/liquidity requirements.  An older use of the term \"financial engineering\" that is less common today is aggressive restructuring of corporate balance sheets.[citation needed]  Mathematical finance is the application of mathematics to finance.[6] Computational finance and mathematical finance are both subfields of financial engineering.[citation needed] Computational finance is a field in computer science and deals with the data and algorithms that arise in financial modeling.  Financial engineering draws on tools from applied mathematics, computer science, statistics and economic theory.[7] In the broadest sense, anyone who uses technical tools in finance could be called a financial engineer, for example any computer programmer in a bank or any statistician in a government economic bureau.[8] However, most practitioners restrict the term to someone educated in the full range of tools of modern finance and whose work is informed by financial theory.[9] It is sometimes restricted even further, to cover only those originating new financial products and strategies.[6]  Despite its name, financial engineering does not belong to any of the fields in traditional professional engineering even though many financial engineers have studied engineering beforehand and many universities offering a postgraduate degree in this field require applicants to have a background in engineering as well.[10][11] In the United States, the Accreditation Board for Engineering and Technology (ABET) does not accredit financial engineering degrees.[12] In the United States, financial engineering programs are accredited by the International Association of Quantitative Finance.[13]  Quantitative analyst (\"Quant\") is a broad term that covers any person who uses math for practical purposes, including financial engineers. Quant is often taken to mean \"financial quant\", in which case it is similar to financial engineer.[14] The difference is that it is possible to be a theoretical quant, or a quant in only one specialized niche in finance, while \"financial engineer\" usually implies a practitioner with broad expertise.[15]  \"Rocket scientist\" (aerospace engineer) is an older term, first coined in the development of rockets in WWII (Wernher von Braun), and later, the NASA space program; it was adapted by the first generation of financial quants who arrived on Wall Street in the late 1970s and early 1980s.[16] While basically synonymous with financial engineer, it implies adventurousness and fondness for disruptive innovation.[17] Financial \"rocket scientists\" were usually trained in applied mathematics, statistics or finance and spent their entire careers in risk-taking.[18] They were not hired for their mathematical talents, they either worked for themselves or applied mathematical techniques to traditional financial jobs.[9][17] The later generation of financial engineers were more likely to have PhDs in mathematics, physics, electrical and computer engineering, and often started their careers in academics or non-financial fields.[19][20]  One of the prominent critics of financial engineering is Nassim Taleb, a professor of financial engineering at Polytechnic Institute of New York University[21] who argues that it replaces common sense and leads to disaster.  A series of economic collapses has led many governments to argue a return to \"real\" engineering from financial engineering. A gentler criticism came from Emanuel Derman[22] who heads a financial engineering degree program at Columbia University. He blames over-reliance on models for financial problems; see Financial Modelers' Manifesto.  Many other authors have identified specific problems in financial engineering that caused catastrophes:   The financial innovation often associated with financial engineers was mocked by former chairman of the Federal Reserve Paul Volcker in 2009 when he said it was a code word for risky securities, that brought no benefits to society. For most people, he said, the advent of the ATM was more crucial than any asset-backed bond.[31]  The first Master of Financial Engineering degree programs were set up in the early 1990s.  The number and size of programs has grown rapidly, to the extent that some now use the term \"financial engineer\" to refer to a graduate in the field.[7]  The financial engineering program at New York University Polytechnic School of Engineering was the first curriculum to be certified by the International Association of Financial Engineers.[32][33] The number, and variation, of these programs has grown over the decades subsequent (see Master of Quantitative Finance § History); and lately includes undergraduate study, as well as designations such as the Certificate in Quantitative Finance. "},"meta":{},"created_at":"2025-03-22T14:25:42.274677Z","updated_at":"2025-03-22T14:25:42.274677Z","inner_id":20,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":29,"annotations":[{"id":29,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"cb9598fb-5782-40f1-9661-cec463674cba","import_id":null,"last_action":null,"bulk_created":false,"task":29,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"    The cinema of the United States, primarily associated with major film studios collectively referred to as Hollywood, has significantly influenced the global film industry since the early 20th century.  Classical Hollywood cinema, a filmmaking style developed in the 1910s, continues to shape many American films today. While French filmmakers Auguste and Louis Lumière are often credited with modern cinema's origins,[5] American filmmaking quickly rose to global dominance. As of 2017, more than 600 English-language films were released annually in the U.S., making it the fourth-largest producer of films, trailing only India, Japan, and China.[6] Although the United Kingdom, Canada, Australia, and New Zealand also produce English-language films, they are not directly part of the Hollywood system. Due to this global reach, Hollywood is frequently regarded as a transnational cinema[7] with some films released in multiple language versions, such as Spanish and French.  Contemporary Hollywood frequently outsources production to countries including the United Kingdom, Canada, Australia, and New Zealand. The five major film studios—Universal Pictures, Paramount Pictures, Warner Bros., Walt Disney Studios, and Sony Pictures—are media conglomerates that dominate U.S. box office revenue and have produced some of the most commercially successful film and television programs worldwide.[8][9]  In 1894, the world's first commercial motion-picture exhibition was held in New York City using Thomas Edison's kinetoscope[10] and kinetograph.[11] In the following decades, the production of silent films greatly expanded. New studios formed, migrated to California, and began to create longer films. The United States produced the world's first sync-sound musical film, The Jazz Singer in 1927,[12] and was at the forefront of sound-film development in the following decades.  Since the early 20th century, the U.S. film industry has primarily been based in and around the thirty-mile zone, centered in the Hollywood neighborhood of Los Angeles County, California. The director D. W. Griffith was central to the development of a film grammar. Orson Welles's Citizen Kane (1941) is frequently cited in critics' polls as the greatest film of all time.[13] Hollywood is widely regarded as the oldest hub of the film industry, where most of the earliest studios and production companies originated, and is the birthplace of numerous cinematic genres.[14]  The earliest recorded instance of motion capture was Eadweard Muybridge’s series of photographs depicting a running horse, which he took in Palo Alto, California using a set of still cameras placed in a row. Muybridge's accomplishment led inventors everywhere to attempt to make similar devices. In the United States, Thomas Edison was among the first to produce such a device, the kinetoscope and kinetograph.[15][16]  The history of cinema in the United States can trace its roots to the East Coast, where, at one time, Fort Lee, New Jersey, was the motion-picture capital of America. The American film industry began at the end of the 19th century, with the construction of Thomas Edison's \"Black Maria\", the first motion-picture studio in West Orange, New Jersey. The cities and towns on the Hudson River and Hudson Palisades offered land at costs considerably less than New York City across the river and benefited greatly as a result of the phenomenal growth of the film industry at the turn of the 20th century.[17] [18] [19][20]  The industry began attracting both capital and innovative workforces. In 1907, when the Kalem Company began using Fort Lee as a location for filming in the area, other filmmakers quickly followed. In 1909, a forerunner of Universal Studios, the Champion Film Company, built the first studio.[21] Others quickly followed and either built new studios or leased facilities in Fort Lee. In the 1910s and 1920s, film companies such as the Independent Moving Pictures Company, Peerless Pictures Studios, Solax Studios, Eclair, Goldwyn Pictures Corporation, Star Film (Georges Méliès), World Film Company, Biograph Studios, Fox Film Corporation, Société Pathé Frères, Metro-Goldwyn-Mayer Studios Inc., Victor Film Company, and Selznick International Pictures were all making pictures in Fort Lee. Many notable actors, such as Mary Pickford, got their start at Biograph Studios.[22][23][24]  In New York, the Kaufman Astoria Studios in Queens, which was built during the silent film era, was used by the Marx Brothers and W.C. Fields. The Edison Studios were located in the Bronx. Chelsea, Manhattan, was also frequently used.  Other Eastern cities, most notably Chicago and Cleveland, also served as early centers for film production.[25][26]  In the West, California was already quickly emerging as a major film production center. In Colorado, Denver was home to the Art-O-Graf Film Company, and Walt Disney's early Laugh-O-Gram Studio was based in Kansas City, Missouri.  From 1908, Jacksonville, Florida's motion picture industry saw more than 30 silent film companies establish studios in town, including Kalem Studios, Metro Pictures (later MGM), Edison Studios, Majestic Films,[27] King-Bee Films Corporation, Vim Comedy Company, Norman Studios, Gaumont Film Company and the Lubin Manufacturing Company.  Picture City, Florida was a planned site for a movie picture production center in the 1920s, but due to the 1928 Okeechobee hurricane, the idea collapsed and Picture City, Florida returned to its original name of Hobe Sound. An attempt to establish a film production center in Detroit also proved unsuccessful.[28]  The film patent wars of the early 20th century helped the spread of film companies to other parts of the US, outside New York. Many filmmakers worked with equipment for which they did not own the rights to use. Therefore, filming in New York could be dangerous, as it was close to Edison's company headquarters and close to the agents the company sent out to seize cameras.[29]  An alternative was Los Angeles, which had mild winters, a large selection of places to film, and, most importantly, it was only 90 miles to the border of Mexico, in case they needed to flee from Edison's enforcement agents. By 1912, most major film companies had set up production facilities in Southern California, near or in Los Angeles, because of the region's favorable year-round weather.[30]  The 1908 Selig Polyscope Company production of The Count of Monte Cristo, directed by Francis Boggs and starring Hobart Bosworth, was claimed as the first to have been filmed in Los Angeles, in 1907. A plaque was unveiled by the city, in 1957, at Dearden's flagship store on the corner of Main Street and 7th Street, to mark the filming on the site when it had been a Chinese laundry.[31] Bosworth's widow suggested the city had got the date and location wrong, and that the film was actually shot in nearby Venice, which at the time was an independent city.[32] In the Sultan's Power, directed by Boggs for Selig Polyscope Company, also starring Bosworth, is considered the first film shot entirely in Los Angeles, with shooting at 7th and Olive Streets, in 1909.[33][32]  In early 1910, director D. W. Griffith was sent by the Biograph Company to the West Coast with his acting troupe, consisting of actors Blanche Sweet, Lillian Gish, Mary Pickford, Lionel Barrymore, and others. They started filming on a vacant lot near Georgia Street in downtown Los Angeles. While there, the company decided to explore new territories, traveling several miles north to Hollywood: a little village that was friendly and enjoyed the movie company filming there. Griffith then filmed the first movie ever shot in Hollywood, In Old California, a Biograph melodrama about California in the 19th century, when the state was under Mexican rule. Griffith stayed there for months and made several films before returning to New York. Also in 1910, Selig Polyscope Company of Chicago established the first film studio in the Los Angeles area in Edendale,[31] and the first studio in Hollywood opened in 1912.[34]: 447  After hearing about Griffith's success in Hollywood, in 1913, many movie-makers headed west to avoid the fees imposed by Thomas Edison, who owned patents on the movie-making process.[35] Nestor Studios of Bayonne, New Jersey, built the first studio in the Hollywood neighborhood in 1911.[dubious – discuss] Nestor Studios, owned by David and William Horsley, later merged with Universal Studios; and William Horsley's other company, Hollywood Film Laboratory, is now the oldest existing company in Hollywood, presently called the Hollywood Digital Laboratory. California's more hospitable and cost-effective climate led to the eventual shift of virtually all filmmaking to the West Coast by the 1930s. At the time, Thomas Edison owned almost all the patents relevant to motion picture production and movie producers on the East Coast acting independently of Edison's Motion Picture Patents Company were often sued or enjoined by Edison and his agents while movie makers working on the West Coast could work independently of Edison's control.[29]  In Los Angeles, the studios and Hollywood grew. Before World War I, films were made in several American cities, but filmmakers tended to gravitate towards southern California as the industry developed. They were attracted by the warm, predictable climate with reliable sunlight, which made it possible to film outdoors year-round.[36] War damage contributed to the decline of the then-dominant European film industry, in favor of the United States, where infrastructure was still intact.[37] The stronger early public health response to the 1918 flu epidemic by Los Angeles[38] compared to other American cities reduced the number of cases there and resulted in a faster recovery, contributing to the increasing dominance of Hollywood over New York City.[37] During the pandemic, public health officials temporarily closed movie theaters in some jurisdictions, large studios suspended production for weeks at a time, and some actors came down with the flu. This caused major financial losses and severe difficulties for small studios, but the industry as a whole more than recovered during the Roaring Twenties.[39]  In the early 20th century, when the medium was new, many Jewish immigrants found employment in the US film industry. They were able to make their mark in a brand-new business: the exhibition of short films in storefront theaters called nickelodeons, after their admission price of a nickel (five cents). Within a few years, men like Samuel Goldwyn, William Fox, Carl Laemmle, Adolph Zukor, Louis B. Mayer, and the Warner Brothers (Harry, Albert, Samuel, and Jack) had switched to the production side of the business. Soon they were the heads of a new kind of enterprise: the movie studio. The US had at least two female directors, producers, and studio heads in these early years: Lois Weber and French-born Alice Guy-Blaché. They also set the stage for the industry's internationalism; the industry is often accused of Amerocentric provincialism.  Other movie makers arrived from Europe after World War I: directors like Ernst Lubitsch, Alfred Hitchcock, Fritz Lang and Jean Renoir; and actors like Rudolph Valentino, Marlene Dietrich, Ronald Colman, and Charles Boyer. They joined a homegrown supply of actors—lured west from the New York City stage after the introduction of sound films—to form one of the 20th century's most remarkable growth industries. At motion pictures' height of popularity in the mid-1940s, the studios were cranking out a total of about 400 movies a year, seen by an audience of 90 million Americans per week.[40]  Sound also became widely used in Hollywood in the late 1920s.[41] After The Jazz Singer, the first film with synchronized voices was successfully released as a Vitaphone talkie in 1927, Hollywood film companies would respond to Warner Bros. and begin to use Vitaphone sound—which Warner Bros. owned until 1928—in future films. By May 1928, Electrical Research Product Incorporated (ERPI), a subsidiary of the Western Electric company, had a monopoly over film sound distribution.[40]  A side effect of these \"talkies\" was that many actors who had made their careers in silent films suddenly found themselves out of work, as they often had bad voices or could not remember their lines. Meanwhile, in 1922, US politician Will H. Hays left politics and formed the movie studio boss organization known as the Motion Picture Producers and Distributors of America (MPPDA).[42] The organization became the Motion Picture Association of America after Hays retired in 1945.  In the early times of talkies, American studios found that their sound productions were rejected in foreign-language markets and even among speakers of other dialects of English. The synchronization technology was still too primitive for dubbing. One of the solutions was creating parallel foreign-language versions of Hollywood films. Around 1930, the American companies[which?] opened a studio in Joinville-le-Pont, France, where the same sets and wardrobe and even mass scenes were used for different time-sharing crews.  Also, foreign unemployed actors, playwrights, and winners of photogenic contests were chosen and brought to Hollywood, where they shot parallel versions of English-language films. These parallel versions had a lower budget, were shot at night, and were directed by second-line American directors who did not speak a foreign language. The Spanish-language crews included people like Luis Buñuel, Enrique Jardiel Poncela, Xavier Cugat, and Edgar Neville. The productions were not very successful in their intended markets, due to the following reasons:  Classical Hollywood cinema, or the Golden Age of Hollywood, is defined as a technical and narrative style characteristic of American cinema from 1913 to 1962, during which thousands of movies were issued from the Hollywood studios. The Classical style began to emerge in 1913, was accelerated in 1917 after the U.S. entered World War I and finally solidified when the film The Jazz Singer was released in 1927, ending the silent film era and increasing box-office profits for the film industry by introducing sound to feature films.  Most Hollywood pictures adhered closely to a formula—Western, slapstick comedy, musical, animated cartoon, biographical—and the same creative teams often worked on films made by the same studio. For example, Cedric Gibbons and Herbert Stothart always worked on MGM films, Alfred Newman worked at 20th Century Fox for twenty years, Cecil B. De Mille's films were almost all made at Paramount, and director Henry King's films were mostly made for 20th Century Fox.  At the same time, one could usually guess which studio made which film, largely because of the actors who appeared in it; MGM, for example, claimed it had contracted \"more stars than there are in heaven.\" Each studio had its own style and characteristic touches which made it possible to know this—a trait that rarely exists today.  For example, To Have and Have Not (1944) is notable not only for the first pairing of actors Humphrey Bogart (1899–1957) and Lauren Bacall (1924–2014), but because it was written by two future winners of the Nobel Prize in Literature: Ernest Hemingway (1899–1961), the author of the novel on which the script was nominally based, and William Faulkner (1897–1962), who worked on the screen adaptation.  After The Jazz Singer was released in 1927, Warner Bros. gained huge success and were able to acquire their own string of movie theaters after purchasing Stanley Theaters and First National Productions in 1928. In contrast, Loews Theaters owned MGM since forming in 1924, while the Fox Film Corporation owned the Fox Theatre. RKO (a 1928 merger between Keith-Orpheum Theaters and the Radio Corporation of America[43]) also responded to the Western Electric\/ERPI monopoly over sound in films, and developed their own method, known as Photophone, to put sound in films.[40]  Paramount, which acquired Balaban and Katz in 1926, would answer to the success of Warner Bros. and RKO by purchasing a number of theaters in the late 1920s, and would hold a monopoly on theaters in Detroit, Michigan.[44] By the 1930s, almost all of the first-run metropolitan theaters in the United States were owned by the Big Five studios—MGM, Paramount Pictures, RKO, Warner Bros., and 20th Century Fox.[45]  Motion picture companies operated under the studio system. The major studios kept thousands of people on salary—actors, producers, directors, writers, stuntmen, craftspersons, and technicians. They owned or leased Movie ranches in rural Southern California for location shooting of westerns and other large-scale genre films, and the major studios owned hundreds of theaters in cities and towns across the nation in 1920 film theaters that showed their films and that were always in need of fresh material.  In 1930, MPPDA President Will Hays created the Hays (Production) Code, which followed censorship guidelines and went into effect after government threats of censorship expanded by 1930.[46] However, the code was never enforced until 1934 after the Catholic watchdog organization The Legion of Decency—appalled by some of the provocative films and lurid advertising of the era later classified Pre-Code Hollywood—threatened a boycott of motion pictures if it did not go into effect.[47] The films that did not obtain a seal of approval from the Production Code Administration had to pay a $25,000 fine (equivalent to $470,568 in 2024) and could not profit in the theaters, as the MPPDA controlled every theater in the country through the Big Five studios.  Throughout the 1930s, as well as most of the golden age, MGM dominated the film screen and had the top stars in Hollywood, and they were also credited for creating the Hollywood star system altogether.[48] Some MGM stars included \"King of Hollywood\" Clark Gable, Lionel Barrymore, Jean Harlow, Norma Shearer, Greta Garbo, Joan Crawford, Jeanette MacDonald, Gene Raymond, Spencer Tracy, Judy Garland, and Gene Kelly.[48]  Another great achievement of American cinema during this era came through Walt Disney's animation company. In 1937, Disney created the most successful film of its time, Snow White and the Seven Dwarfs.[49] This distinction was promptly topped in 1939 when Selznick International created what is still, when adjusted for inflation, the most successful film of all time in Gone with the Wind.[50]  Many film historians have remarked upon the many great works of cinema that emerged from this period of highly regimented filmmaking. One reason this was possible is that, with so many movies being made, not everyone had to be a big hit. A studio could gamble on a medium-budget feature with a good script and relatively unknown actors: Citizen Kane, directed by Orson Welles (1915–1985) and often regarded as the greatest film of all time, fits this description. In other cases, strong-willed directors like Howard Hawks (1896–1977), Alfred Hitchcock (1899–1980), and Frank Capra (1897–1991) battled the studios to achieve their artistic visions.  The apogee of the studio system may have been the year 1939, which saw the release of such classics as The Wizard of Oz, Gone with the Wind, Stagecoach, Mr. Smith Goes to Washington, Wuthering Heights, Only Angels Have Wings, Ninotchka and Midnight. Among the other films from the Golden Age period that are now considered to be classics: Casablanca, It's a Wonderful Life, It Happened One Night, the original King Kong, Mutiny on the Bounty, Top Hat, City Lights, Red River, The Lady from Shanghai, Rear Window, On the Waterfront, Rebel Without a Cause, Some Like It Hot, and The Manchurian Candidate.  The studio system and the Golden Age of Hollywood succumbed to two forces that developed in the late 1940s:  In 1938, Walt Disney's Snow White and the Seven Dwarfs was released during a run of lackluster films from the major studios, and quickly became the highest-grossing film released to that point. Embarrassingly for the studios, it was an independently produced animated film that did not feature any studio-employed stars.[51] This stoked already widespread frustration at the practice of block-booking, in which studios would only sell an entire year's schedule of films at a time to theaters and use the lock-in to cover for releases of mediocre quality.  Assistant Attorney General Thurman Arnold—a noted \"trust buster\" of the Roosevelt administration—took this opportunity to initiate proceedings against the eight largest Hollywood studios in July 1938 for violations of the Sherman Antitrust Act.[52][53] The federal suit resulted in five of the eight studios (the \"Big Five\": Warner Bros., MGM, Fox, RKO and Paramount) reaching a compromise with Arnold in October 1940 and signing a consent decree agreeing to, within three years:  The \"Little Three\" (Universal Studios, United Artists, and Columbia Pictures), who did not own any theaters, refused to participate in the consent decree.[52][53] A number of independent film producers were also unhappy with the compromise and formed a union known as the Society of Independent Motion Picture Producers and sued Paramount for the monopoly they still had over the Detroit Theaters—as Paramount was also gaining dominance through actors like Bob Hope, Paulette Goddard, Veronica Lake, Betty Hutton, crooner Bing Crosby, Alan Ladd, and longtime actor for studio Gary Cooper too- by 1942. The Big Five studios did not meet the requirements of the Consent of Decree during WWII, without major consequence, but after the war ended they joined Paramount as defendants in the Hollywood antitrust case, as did the Little Three studios.[54]  The United States Supreme Court eventually ruled in United States v. Paramount Pictures, Inc. that the major studios ownership of theaters and film distribution was a violation of the Sherman Antitrust Act. As a result, the studios began to release actors and technical staff from their contracts with the studios. This changed the paradigm of filmmaking by the major Hollywood studios, as each could have an entirely different cast and creative team.  The decision resulted in the gradual loss of the characteristics that made Metro-Goldwyn-Mayer, Paramount Pictures, Universal Studios, Columbia Pictures, RKO Pictures, and 20th Century Fox films immediately identifiable. Certain movie people, such as Cecil B. DeMille, either remained contract artists until the end of their careers or used the same creative teams on their films so that a DeMille film still looked like one whether it was made in 1932 or 1956.  Post-classical cinema is the changing methods of storytelling in the New Hollywood. It has been argued that new approaches to drama and characterization played upon audience expectations acquired in the classical period: chronology may be scrambled, storylines may feature \"twist endings\", and lines between the antagonist and protagonist may be blurred. The roots of post-classical storytelling may be seen in film noir, in Rebel Without a Cause (1955), and in Hitchcock's storyline-shattering Psycho.  The New Hollywood is the emergence of a new generation of film school-trained directors who had absorbed the techniques developed in Europe in the 1960s as a result of the French New Wave; the 1967 film Bonnie and Clyde marked the beginning of American cinema rebounding as well, as a new generation of films would afterward gain success at the box offices as well.[55] Filmmakers like Francis Ford Coppola, Steven Spielberg, George Lucas, Brian De Palma, Stanley Kubrick, Martin Scorsese, Roman Polanski, and William Friedkin came to produce fare that paid homage to the history of film and developed upon existing genres and techniques. Inaugurated by the 1969 release of Andy Warhol's Blue Movie, the phenomenon of adult erotic films being publicly discussed by celebrities (like Johnny Carson and Bob Hope),[56] and taken seriously by critics (like Roger Ebert),[57][58] a development referred to, by Ralph Blumenthal of The New York Times, as \"porno chic\", and later known as the Golden Age of Porn, began, for the first time, in modern American culture.[56][59][60] According to award-winning author Toni Bentley, Radley Metzger's 1976 film The Opening of Misty Beethoven, based on the play Pygmalion by George Bernard Shaw (and its derivative, My Fair Lady), and due to attaining a mainstream level in storyline and sets,[61] is considered the \"crown jewel\" of this 'Golden Age'.[62][63]  At the height of his fame in the early 1970s, Charles Bronson was the world's No. 1 box office attraction, commanding $1 million per film.[64] In the 1970s, the films of New Hollywood filmmakers were often both critically acclaimed and commercially successful. While the early New Hollywood films like Bonnie and Clyde and Easy Rider had been relatively low-budget affairs with amoral heroes and increased sexuality and violence, the enormous success enjoyed by Friedkin with The Exorcist, Spielberg with Jaws, Coppola with The Godfather and Apocalypse Now, Scorsese with Taxi Driver, Kubrick with 2001: A Space Odyssey, Polanski with Chinatown, and Lucas with American Graffiti and Star Wars, respectively helped to give rise to the modern \"blockbuster\", and induced studios to focus ever more heavily on trying to produce enormous hits.[65]  In the US, the PG-13 rating was introduced in 1984 to accommodate films that straddled the line between PG and R, which was mainly due to the controversies surrounding the violent content of the PG films Indiana Jones and the Temple of Doom and Gremlins (both 1984).[66]  Film makers in the 1990s had access to technological, political and economic innovations that had not been available in previous decades. Dick Tracy (1990) became the first 35 mm feature film with a digital soundtrack. Batman Returns (1992) was the first film to make use of the Dolby Digital six-channel stereo sound that has since become the industry standard. Computer-generated imagery was greatly facilitated when it became possible to transfer film images into a computer and manipulate them digitally. The possibilities became apparent in director James Cameron's Terminator 2: Judgment Day (1991), in images of the shape-changing character T-1000. Computer graphics or CG advanced to a point where Jurassic Park (1993) was able to use the techniques to create realistic-looking animals. Jackpot (2001) became the first film that was shot entirely in digital.[67] In the film Titanic, Cameron wanted to push the boundary of special effects with his film, and enlisted Digital Domain and Pacific Data Images to continue the developments in digital technology which the director pioneered while working on The Abyss and Terminator 2: Judgment Day. Many previous films about the RMS Titanic shot water in slow motion, which did not look wholly convincing.[68] Cameron encouraged his crew to shoot their 45-foot-long (14 m) miniature of the ship as if \"we're making a commercial for the White Star Line\".  Even The Blair Witch Project (1999), a low-budget indie horror film by Eduardo Sanchez and Daniel Myrick, was a huge financial success. Filmed on a budget of just $35,000, without any big stars or special effects, the film grossed $248  million with the use of modern marketing techniques and online promotion. Though not on the scale of George Lucas's $1  billion prequel to the Star Wars Trilogy, The Blair Witch Project earned the distinction of being the most profitable film of all time, in terms of percentage gross.[67]  The success of Blair Witch as an indie project remains among the few exceptions, however, and control of The Big Five studios over film making continued to increase through the 1990s. The Big Six companies all enjoyed a period of expansion in the 1990s. They each developed different ways to adjust to rising costs in the film industry, especially the rising salaries of movie stars, driven by powerful agents. The biggest stars like Sylvester Stallone, Russell Crowe, Tom Cruise, Nicole Kidman, Sandra Bullock, Arnold Schwarzenegger, Mel Gibson, Kevin Bacon, and Julia Roberts received between $15–$20  million per film and in some cases were even given a share of the film's profits.[67]  Screenwriters on the other hand were generally paid less than the top actors or directors, usually under $1 million per film. However, the single largest factor driving rising costs was special effects. By 1999 the average cost of a blockbuster film was $60 million before marketing and promotion, which cost another $80 million.[67]  Since the beginning of 21st century, the theatrical marketplace has slowly been dominated by the superhero genre. As of 2022[update], they are the best-paying productions for actors, because paychecks in other genres have shrunk for even top actors.[70] In 2023 and 2024, however, Hollywood experts pointed to 'superhero fatigue' as an emerging trend.[71] Actors such as Paul Dano and directors like Matthew Vaughn have made similar arguments.[72][73]  In 2021, despite the COVID-19 pandemic in the United States, blockbuster films such as Black Widow, F9, Death on the Nile and West Side Story were released in theaters after being postponed from their initial 2020 release dates.[74]  Various studios responded to the crisis with controversial decisions to forgo the theatrical window and give their films day-and-date releases. NBCUniversal released Trolls World Tour directly to video-on-demand rental on April 10,[77] while simultaneously receiving limited domestic theatrical screenings via drive-in cinemas;[78] CEO Jeff Shell claims that the film had reached nearly $100 million in revenue within the first three weeks.[79][80] The decision was opposed by AMC Theatres, which then announced that its screenings of Universal Pictures films would cease immediately, though the two companies would eventually agree to a 2-week theatrical window.[81][82][83][84][85] By December 2020, Warner Bros. Pictures announced their decision to simultaneously release its slate of 2021 films in both theaters and its streaming site HBO Max for a period of one month to maximize viewership.[86] However, by 2023, industry strikes by the Writers Guild of America (WGA) and Screen Actors Guild (SAG-AFTRA) highlighted growing disputes over streaming residuals, AI technology in writing and acting, and fair compensation, reflecting the broader challenges faced by Hollywood's evolving economic model. The move was vehemently criticized by various industry figures, many of who were reportedly uninformed of the decision before the announcement and felt deceived by the studio.[87]  2019 onwards has seen the rise of American streaming platforms, such as Netflix, Disney+, Paramount+, and Apple TV+, which came to rival traditional cinema.[88][89] Industry commentators have noted the increasing treatment of films as \"content\" by corporations that correlate with the increased popularity of streaming platforms.[90] This involves the blurring of boundaries between films, television and other forms of media as more people consume them together in a variety of ways, with individual films defined more by their brand identity and commercial potential rather than their medium, stories and artistry.[88][91] Critic Matt Zoller Seitz has described the release of Avengers: Endgame in 2019 as \"represent[ing] the decisive defeat of 'cinema' by 'content'\" due to its grand success as a \"piece of entertainment\" defined by the Marvel brand that culminates a series of blockbuster films that has traits of serial television.[88]  The films Space Jam: A New Legacy and Red Notice have been cited as examples of this treatment, with the former being described by many critics as \"a lengthy infomercial for HBO Max\", featuring scenes and characters recalling various Warner Bros. properties such as Casablanca, The Matrix and Austin Powers,[92][93][94][95] while the latter is a $200 million heist film from Netflix that critics described \"a movie that feels more processed by a machine [...] instead of anything approaching artistic intent or even an honest desire to entertain.\"[96][97][98] Some have expressed that Space Jam demonstrates the industry's increasingly cynical treatment of films as mere intellectual property (IP) to be exploited, an approach which critic Scott Mendelson called \"IP for the sake of IP.\"[93][99][100][94]  Martin Scorsese has warned that cinema as an art form is \"being systematically devalued, sidelined, demeaned, and reduced\" to \"content\" and called blockbusters' overemphasis on box-office returns \"repulsive\".[101][102] Quentin Tarantino has opined that the current era of cinema is one of the worst in Hollywood history.[103][104] During a masterclass at the 2023 Sarajevo Film Festival, Charlie Kaufman criticized mainstream blockbusters, stating that \"[a]t this point, the only thing that makes money is garbage\" and encouraged industry professionals to \"make movies outside of the studio system as much as possible\".[105][106] James Gray noted in an interview with Deadline, \"When you make movies that only make a ton of money and only one kind of movie, you begin to get a large segment of the population out of the habit of going to the movies\", which causes viewership to decrease, though clarified that he has \"no problem with a comic book movie\". As a solution to the lack of \"investment in the broad-based engagement with the product\", he suggests that studios \"be willing to lose money for a couple of years on art film divisions, and in the end they will be happier.\"[107]  In the 1930s and 1940s, the Democrats and the Republicans alike saw political opportunities in Hollywood and President Franklin Roosevelt was an early adopter, capitalizing on Hollywood's stars in a national campaign.[108] Melvyn Douglas and his wife Helen toured Washington, D.C., in 1939 and met the key New Dealers.[109]  Endorsement letters from leading actors were signed, and radio appearances and printed advertising were made. Movie stars were used to draw a large audience into the political view of the party. By the 1960s, John F. Kennedy was a new, young face for Washington, and his strong friendship with Frank Sinatra exemplified this new era of glamour. The last moguls of Hollywood were gone, and younger, newer executives and producers began pushing more liberal ideas.[110][111]  Celebrities and money attracted politicians to the high-class, glittering Hollywood lifestyle. As Ron Brownstein wrote in his book The Power and the Glitter, television in the 1970s and 1980s was an enormously important new media in politics and Hollywood helped in that media with actors making speeches on their political beliefs, like Jane Fonda against the Vietnam War.[112] Despite most celebrities and producers being left-leaning and tending to support the Democratic Party,[113][114] this era produced some Republican actors and producers such as Clint Eastwood and Jerry Bruckheimer. Support groups such as the Friends of Abe were set up to support conservative causes in Hollywood, which is perceived as biased against conservatives.[115] Former actor Ronald Reagan became governor of California and subsequently became the 40th president of the United States. It continued with Arnold Schwarzenegger as California's governor in 2003.[116]  Today, donations from Hollywood help to fund federal politics.[117] On February 20, 2007, for example, Democratic then-presidential candidate Barack Obama had a $2,300-a-plate Hollywood gala, being hosted by DreamWorks founders David Geffen, Jeffrey Katzenberg, and Steven Spielberg at the Beverly Hilton.[117]  Native advertising is information designed to persuade in more subtle ways than classic propaganda. A modern example common in the United States is Copaganda, in which TV shows display unrealistically flattering portrayals of law enforcement, in part to borrow equipment and get their assistance in blocking off streets to more easily film on location.[118] Other reputation laundering accusations have been leveled in the entertainment industry, including burnishing the image of the Mafia.[119]  Product placement also has been a point of criticism, with the tobacco industry promoting smoking on screen.[120] The Centers for Disease Control and Prevention cites that 18% of teen smokers would not start smoking if films with smoking were automatically given an 'R' rating, which would save 1 million lives.[121]    Hollywood producers sometimes seek to comply with the Chinese government's censorship requirements in a bid to access the country's restricted and lucrative cinema market,[122] with the second-largest box office in the world as of 2016. This includes prioritizing sympathetic portrayals of Chinese characters in movies, such as changing the villains in Red Dawn from Chinese to North Koreans.[122] Due to many topics forbidden in China, such as Dalai Lama and Winnie-the-Pooh being involved in the South Park's episode \"Band in China\", South Park was entirely banned in China after the episode's broadcast.[123] The 2018 film Christopher Robin, the new Winnie-the-Pooh movie, was denied a Chinese release.[123]  Although Tibet was previously a cause célèbre in Hollywood, featuring in films including Kundun and Seven Years in Tibet, in the 21st century this is no longer the case.[124] In 2016, Marvel Entertainment attracted criticism for its decision to cast Tilda Swinton as \"The Ancient One\" in the film adaptation Doctor Strange, using a white woman to play a traditionally Tibetan character.[125] Actor and high-profile Tibet supporter Richard Gere stated that he was no longer welcome to participate in mainstream Hollywood films after criticizing the Chinese government and calling for a boycott of the 2008 Summer Olympics in Beijing.[124][126]  Hollywood also self-censored any negative depictions of Nazis for most of the 1930s to maintain access to German audiences.[127] Around that time economic censorship resulted in the self-censoring of content to please the group wielding their economic influence.[127] The Hays Code was an industry-led effort from 1930 to 1967 to strict self-censorship to appease religious objections to certain content and stave off any government censorship that could have resulted.[127]  Political economy of communication researchers have long focused on the international or global presence, power, profitability and popularity of Hollywood films. Books on global Hollywood by Toby Miller and Richard Maxwell,[128] Janet Wasko and Mary Erickson,[129] Kerry Segrave,[130] John Trump Bour[131] and Tanner Mirles[132] examine the international political economy of Hollywood's power.  According to Tanner Mirles, Hollywood relies on four capitalist strategies \"to attract and integrate non-US film producers, exhibitors and audiences into its ambit: ownership, cross-border productions with subordinate service providers, content licensing deals with exhibitors, and blockbusters designed to travel the globe.\"[133]  In 1912, American film companies were largely immersed in the competition for the domestic market. It was difficult to satisfy the huge demand for films created by the nickelodeon boom. Motion Picture Patents Company members such as Edison Studios, also sought to limit competition from French, Italian, and other imported films. Exporting films, then, became lucrative to these companies. Vitagraph Studios was the first American company to open its own distribution offices in Europe, establishing a branch in London in 1906, and a second branch in Paris shortly after.[134]  Other American companies were moving into foreign markets as well, and American distribution abroad continued to expand until the mid-1920s. Originally, a majority of companies sold their films indirectly. However, since they were inexperienced in overseas trading, they simply sold the foreign rights to their films to foreign distribution firms or export agents. Gradually, London became a center for the international circulation of US films.[134]  Many British companies made a profit by acting as the agents for this business, and by doing so, they weakened British production by turning over a large share of the UK market to American films. By 1911, approximately 60 to 70 percent of films imported into Great Britain were American. The United States was also doing well in Germany, Australia, and New Zealand.[134]  More recently, in the last 20th and early 21st century, as globalization intensified and the United States government actively promoted free trade agendas and trade on cultural products, Hollywood became a worldwide cultural source. The success of Hollywood export markets is reflected in the boom of American multinational media corporations across the globe and the ability to make big-budget films that appeal to popular tastes in many different cultures.[135]  Even in a globalized world, global production remained clustered in Hollywood. Contributing factors are that the United States has the largest single home market in dollar terms, entertaining and highly visible Hollywood movies have global appeal, and the role of English as a universal language contributes to compensating for higher fixed costs of production.  Hollywood has moved more deeply into Chinese markets, although influenced by China's censorship. Films made in China are censored, strictly avoiding themes like \"ghosts, violence, murder, horror, and demons.\" Such plot elements risk being cut. Hollywood has had to make \"approved\" films, corresponding to official Chinese standards, but with aesthetic standards sacrificed to box office profits. Even Chinese audiences found it boring to wait for the release of great American movies dubbed in their native language.[136]  Women are statistically underrepresented in creative positions in the center of the US film industry, Hollywood. This underrepresentation has been called the \"celluloid ceiling\", a variant on the employment discrimination term \"glass ceiling\". In 2013, the \"top-paid actors ... made 2+1⁄2 times as much money as the top-paid actresses.\"[137] Older male actors made more than their female counterparts of the same age, with \"female movie stars mak[ing] the most money on average per film at age 34, while male stars earn the most at 51.\"[138]  The 2013 Celluloid Ceiling Report conducted by the Center for the Study of Women in Television and Film at San Diego State University collected a list of statistics gathered from \"2,813 individuals employed by the 250 top domestic grossing films of 2012.\"[139]  Women accounted for:  A New York Times article stated that only 15% of the top films in 2013 had women for a lead acting role.[140] The author of the study noted that \"The percentage of female speaking roles has not increased much since the 1940s when they hovered around 25 percent to 28 percent.\" \"Since 1998, women's representation in behind-the-scenes roles other than directing has gone up just 1 percent.\" Women \"directed the same percent of the 250 top-grossing films in 2012 (9 percent) as they did in 1998.\"[137]    On May 10, 2021, NBC announced that it would not televise the 79th Golden Globe Awards in 2022 in support of a boycott of the Hollywood Foreign Press Association (HFPA) by multiple media companies over inadequate efforts to address lack of diversity within the membership of the association with people of color, but that it would be open to televise the ceremony in 2023 if the HFPA were successful in its efforts to reform.[141] The HFPA would be disbanded two years later as a result of this and other scandals.[142][143]  American cinema has often reflected and propagated negative stereotypes towards foreign nationals and ethnic minorities.[144] For example, Russians and Russian Americans are usually portrayed as brutal mobsters, ruthless agents and villains.[145][146][147] According to Russian American professor Nina L. Khrushcheva, \"You can't even turn the TV on and go to the movies without reference to Russians as horrible.\"[148] Italians and Italian Americans are usually associated with organized crime and the American Mafia.[149][150][151] Hispanic and Latino Americans are largely depicted as sexualized figures such as the Latino macho or the Latina vixen, gang members, (illegal) immigrants, or entertainers.[152] However, representation in Hollywood has improved in recent years, gaining traction in the 1990s, and no longer emphasizes oppression, exploitation, or resistance as primary themes. According to Charles Ramírez Berg, third-wave films \"do not accentuate Chicano oppression or resistance; ethnicity in these films exists as one fact of several that shape characters' lives and stamps their personalities.\"[153] Filmmakers like Edward James Olmos and Robert Rodriguez were able to represent the Hispanic and Latino American experience like none had on screen before, and actors like Hilary Swank, Jordana Brewster, Jessica Alba, Camilla Belle, Al Madrigal, Alexis Bledel, Sofía Vergara, Ana de Armas, and Rachel Zegler have become successful. In the last decade, minority filmmakers like Chris Weitz, Alfonso Gomez-Rejon, and Patricia Riggen have been given applied narratives. Films that portray Hispanic and Latino Americans include La Bamba (1987), Selena (1997), The Mask of Zorro (1998), Goal II (2007), Overboard (2018), Father of the Bride (2022), and Josefina López's Real Women Have Curves, originally a play which premiered in 1990 and was later released as a film in 2002.[153]  African-American representation in Hollywood improved drastically towards the end of the 20th century after the fall of the studio system and that trend endures in the 21st century as minority representation continues to increase.[154][155][156][157] In old Hollywood, it was not uncommon for white actors to wear black face.[158]  According to Korean American actor Daniel Dae Kim, Asian and Asian American men \"have been portrayed as inscrutable villains and asexualized kind of eunuchs.\"[151] The Media Action Network for Asian Americans accused the director and studio of Aloha of whitewashing the cast of the film, and the director, Cameron Crowe, apologized about Emma Stone being miscast as a character who is meant to be of one quarter Chinese and one quarter Hawaiian descent.[159][160][161] Throughout the 20th century, acting roles in film were relatively few, and many available roles were narrow characters. In the 21st century, young Asian American comedians and filmmakers have found an outlet on YouTube allowing them to gain a strong and loyal fanbase among their fellow Asian Americans.[162] Although more recently the film Crazy Rich Asians has been lauded in the United States for featuring a predominantly Asian cast,[163] it was criticized elsewhere for casting biracial and non-Chinese actors as ethnically Chinese characters. The film Always Be My Maybe was lauded for taking familiar rom-com beats and cleverly layering in smart social commentary.[164]  Before the September 11 attacks, Arabs and Arab Americans were often portrayed as terrorists.[151] The decision to hire Naomi Scott, the daughter of an English father and a Gujarati Ugandan-Indian mother, to play the lead of Jasmine in the film Aladdin also drew criticism as well as accusations of racism, as some commentators expected the role to go to an actress of Arab or Middle Eastern origin.[165] In January 2018, it was reported that white extras were being applied brown make-up during filming to \"blend in\", which caused an outcry and condemnation among fans and critics, branding the practice as \"an insult to the whole industry\" while accusing the producers of not recruiting people with Middle Eastern or North African heritage. Disney responded to the controversy by saying, \"Diversity of our cast and background performers was a requirement and only in a handful of instances when it was a matter of specialty skills, safety and control (special effects rigs, stunt performers and handling of animals) were crew made up to blend in.\"[166][167]  Hollywood's workflow is unique in that much of its workforce does not report to the same factory each day, nor follow the same routine from day to day, but films at distant locations around the world, with a schedule dictated by the scenes being filmed rather than what makes the most sense for productivity. For instance, an urban film shot entirely on location at night would require the bulk of its crews to work a graveyard shift, while a situational comedy series that shoots primarily on stage with only one or two days a week on location would follow a more traditional work schedule. Westerns are often shot in desert locations far from the homes of the crew in areas with limited hotels that necessitate long drives before and after a shooting day, which take advantage of as many hours of sunlight available, ultimately requiring workers to put in 16 or 17 hours a day from the time they leave their home to the time they return.[168][169]  Amidst a broad decline in the power of organized labor in America in the 20th and early 21st century, all of the major studios have continued to maintain contracts with unions through the Alliance of Motion Picture and Television Producers (AMPTP), a trade alliance representing the film studios and television networks. Due to the casual nature of employment in Hollywood, it is only through sectoral bargaining that individual workers can express their rights to minimum wage guarantees and access to pension and health plans that carry over from production to production and offer the studios access to a trained workforce able to step onto a set on day one with the knowledge and experience to handle the highly technical equipment they are asked to operate.[170]  The majority of the workers in Hollywood are represented by several unions and guilds. The 150,000 member-strong International Alliance of Theatrical Stage Employees (IATSE) represents most of the crafts, such as the grips, electricians, and camera people, as well as editors, sound engineers, and hair & make-up artists. The Screen Actors Guild (SAG) is the next largest group representing some 130,000 actors and performers, the Directors Guild of America (DGA) represents the directors and production managers, the Writers Guild of America (WGA) representing writers, and the International Brotherhood of Teamsters (IBT) represents the drivers.[171][172]  While the relationship between labor and management has generally been amicable over the years, working together with the state to develop safe protocols to continue working during COVID-19 and lobbying together in favor of tax incentives, contract negotiations have reported to get contentious over changes in the industry and as a response to rising income inequality. In 1945 six-month set-decorator strike, the relationship turned bloody between strikers, scabs, strikebreakers, and studio security.[173][174][175][176]  In recent years, Hollywood has faced challenges such as strikes by writers and actors, which have led to significant production cuts and layoffs across the industry. These strikes have resulted in union contracts that offer more money and protections against artificial intelligence, but they have also caused a slowdown in production and a rise in unemployment among film and TV workers. "},"meta":{},"created_at":"2025-03-22T14:25:42.274677Z","updated_at":"2025-03-22T14:25:42.274677Z","inner_id":21,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":30,"annotations":[{"id":30,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"2337afe1-79ac-4d72-a296-03afee5a629e","import_id":null,"last_action":null,"bulk_created":false,"task":30,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Animation is a filmmaking technique whereby still images are manipulated to create moving images. In traditional animation, images are drawn or painted by hand on transparent celluloid sheets to be photographed and exhibited on film. Animation has been recognized as an artistic medium, specifically within the entertainment industry. Many animations are either traditional animations or computer animations made with computer-generated imagery (CGI). Stop motion animation, in particular claymation, has continued to exist alongside these other forms.  Animation is contrasted with live action, although the two do not exist in isolation. Many moviemakers have produced films that are a hybrid of the two. As CGI increasingly approximates photographic imagery, filmmakers can easily composite 3D animations into their film rather than using practical effects for showy visual effects (VFX).  Computer animation can be very detailed 3D animation, while 2D computer animation (which may have the look of traditional animation) can be used for stylistic reasons, low bandwidth, or faster real-time renderings. Other common animation methods apply a stop motion technique to two- and three-dimensional objects like paper cutouts, puppets, or clay figures.  An animated cartoon, or simply a cartoon, is an animated film, usually short, that features an exaggerated visual style. This style is often inspired by comic strips, gag cartoons, and other non-animated art forms. Cartoons frequently include anthropomorphic animals, superheroes, or the adventures of human protagonists. The action often revolves around exaggerated physical humor, particularly in predator\/prey dynamics (e.g. cats and mices, coyotes and birds), where violent pratfalls such as falls, collisions, and explosions occur, often in ways that would be lethal in the real life.  During in the 1980s, the term 'cartoon' was shortened to toon, referring to characters in animated productions, or more specifically, cartoonishly-drawn characters. This term gained popularity in 1988 with the live-action\/animated hybrid film Who Framed Roger Rabbit, which introduced ToonTown—a world inhabited by animated characters. In 1990, Tiny Toon Adventures embraced the classic cartoon spirit, introducing a new generation of characters. Then, in 1993, Animaniacs followed, featuring the rubber-hose-styled Warner siblings, Yakko, Wakko, and Dot who are trapped in the 1930s, eventually escaped and found themselves in the Warner Bros. water tower in the 1990s.  The illusion of animation—as in motion pictures in general—has traditionally been attributed to the persistence of vision and later to the phi phenomenon and beta movement, but the exact neurological causes are still uncertain. The illusion of motion caused by a rapid succession of images that minimally differ from each other, with unnoticeable interruptions, is a stroboscopic effect. While animators traditionally used to draw each part of the movements and changes of figures on transparent cels that could be moved over a separate background, computer animation is usually based on programming paths between key frames to maneuver digitally created figures throughout a digitally created environment.  Analog mechanical animation media that rely on the rapid display of sequential images include the phenakistiscope, zoetrope, flip book, praxinoscope, and film. Television and video are popular electronic animation media that originally were analog and now operate digitally. For display on computers, technology such as the animated GIF and Flash animation were developed.  In addition to short films, feature films, television series, animated GIFs, and other media dedicated to the display of moving images, animation is also prevalent in video games, motion graphics, user interfaces, and visual effects.[1]  The physical movement of image parts through simple mechanics—for instance, moving images in magic lantern shows—can also be considered animation. The mechanical manipulation of three-dimensional puppets and objects to emulate living beings has a very long history in automata. Electronic automata were popularized by Disney as animatronics.  The word animation comes to the Latin word animātiō, meaning 'bestowing of life'.[2] The earlier meaning of the English word is 'liveliness' and has been in use much longer than the meaning of 'moving image medium'.  Long before modern animation began, audiences around the world were captivated by the magic of moving characters. For centuries, master artists and craftsmen have brought puppets, automatons, shadow puppets, and fantastical lanterns to life, inspiring the imagination through physically manipulated wonders.[3]  In 1833, the stroboscopic disc (better known as the phenakistiscope) introduced the principle of modern animation, which would also be applied in the zoetrope (introduced in 1866), the flip book (1868), the praxinoscope (1877) and film.  When cinematography eventually broke through in the 1890s, the wonder of the realistic details in the new medium was seen as its biggest accomplishment. It took years before animation found its way to the cinemas. The successful short The Haunted Hotel (1907) by J. Stuart Blackton popularized stop motion and reportedly inspired Émile Cohl to create Fantasmagorie (1908), regarded as the oldest known example of a complete traditional (hand-drawn) animation on standard cinematographic film. Other great artistic and very influential short films were created by Ladislas Starevich with his puppet animations since 1910 and by Winsor McCay with detailed hand-drawn animation in films such as Little Nemo (1911) and Gertie the Dinosaur (1914).[4]  During the 1910s, the production of animated \"cartoons\" became an industry in the US.[5] Successful producer John Randolph Bray and animator Earl Hurd, patented the cel animation process that dominated the animation industry for the rest of the century.[6][7] Felix the Cat, who debuted in 1919, became the first fully realized anthropomorphic animal character in the history of American animation.[8]  In 1928, Steamboat Willie, featuring Mickey Mouse and Minnie Mouse, popularized film-with-synchronized-sound and put Walt Disney's studio at the forefront of the animation industry. Although Disney Animation's actual output relative to total global animation output has always been very small, the studio has overwhelmingly dominated the \"aesthetic norms\" of animation ever since.[9]  The enormous success of Mickey Mouse is seen as the start of the golden age of American animation that would last until the 1960s. The United States dominated the world market of animation with a plethora of cel-animated theatrical shorts.[10] Several studios would introduce characters that would become very popular and would have long-lasting careers, including Walt Disney Productions' Goofy (1932) and Donald Duck (1934), Fleischer Studios\/Paramount Cartoon Studios' Out of the Inkwell' Koko the Clown (1918), Bimbo and Betty Boop (1930), Popeye (1933) and Casper the Friendly Ghost (1945), Warner Bros. Cartoon Studios' Looney Tunes' Porky Pig (1935), Daffy Duck (1937), Elmer Fudd (1937–1940), Bugs Bunny (1938–1940), Tweety (1942), Sylvester the Cat (1945), Wile E. Coyote and the Road Runner (1949), MGM cartoon studio's Tom and Jerry (1940) and Droopy, Universal Cartoon Studios' Woody Woodpecker (1940), Terrytoons\/20th Century Fox's Mighty Mouse (1942), and United Artists' Pink Panther (1963).  In 1917, Italian-Argentine director Quirino Cristiani made the first feature-length film El Apóstol (now lost), which became a critical and commercial success. It was followed by Cristiani's Sin dejar rastros in 1918, but one day after its premiere, the film was confiscated by the government.[12]  After working on it for three years, Lotte Reiniger released the German feature-length silhouette animation Die Abenteuer des Prinzen Achmed in 1926, the oldest extant animated feature.[13]  In 1937, Walt Disney Studios premiered their first animated feature Snow White and the Seven Dwarfs, still one of the highest-grossing traditional animation features as of May 2020[update].[14][15] The Fleischer studios followed this example in 1939 with Gulliver's Travels with some success. Partly due to foreign markets being cut off by the Second World War, Disney's next features Pinocchio, Fantasia (both 1940), Fleischer Studios' second animated feature Mr. Bug Goes to Town (1941–1942) and Disney's feature films Cinderella (1950), Alice in Wonderland (1951) and Lady and the Tramp (1955) failed at the box office. For several decades, Disney was the only American studio to regularly produce animated features, until Ralph Bakshi became the first to release more than a handful of features. Sullivan-Bluth Studios began to regularly produce animated features starting with An American Tail in 1986.[16]  Although relatively few titles became as successful as Disney's features, other countries developed their own animation industries that produced both short and feature theatrical animations in a wide variety of styles, relatively often including stop motion and cutout animation techniques. Soviet Soyuzmultfilm animation studio, founded in 1936, produced 20 films (including shorts) per year on average and reached 1,582 titles in 2018. China, Czechoslovakia \/ Czech Republic, Italy, France, and Belgium were other countries that more than occasionally released feature films, while Japan became a true powerhouse of animation production, with its own recognizable and influential anime style of effective limited animation.[citation needed]  Animation became very popular on television since the 1950s, when television sets started to become common in most developed countries. Cartoons were mainly programmed for children, on convenient time slots, and especially US youth spent many hours watching Saturday-morning cartoons. Many classic cartoons found a new life on the small screen and by the end of the 1950s, the production of new animated cartoons started to shift from theatrical releases to TV series. Hanna-Barbera Productions was especially prolific and had huge hit series, such as The Flintstones (1960–1966) (the first prime time animated series), Scooby-Doo (since 1969) and Belgian co-production The Smurfs (1981–1989). The constraints of American television programming and the demand for an enormous quantity resulted in cheaper and quicker limited animation methods and much more formulaic scripts. Quality dwindled until more daring animation surfaced in the late 1980s and in the early 1990s with hit series, the first cartoon of The Simpsons (1987), which later developed into its own show (in 1989) and SpongeBob SquarePants (since 1999) as part of a \"renaissance\" of American animation.[citation needed]  While US animated series also spawned successes internationally, many other countries produced their own child-oriented programming, relatively often preferring stop motion and puppetry over cel animation. Japanese anime TV series became very successful internationally since the 1960s, and European producers looking for affordable cel animators relatively often started co-productions with Japanese studios, resulting in hit series such as Barbapapa (The Netherlands\/Japan\/France 1973–1977), Wickie und die starken Männer\/小さなバイキング ビッケ (Vicky the Viking) (Austria\/Germany\/Japan 1974), Maya the Honey Bee (Japan\/Germany 1975) and The Jungle Book (Italy\/Japan 1989).[citation needed]  Computer animation was gradually developed since the 1940s. 3D wireframe animation started popping up in the mainstream in the 1970s, with an early (short) appearance in the sci-fi thriller Futureworld (1976).[17]  The Rescuers Down Under was the first feature film to be completely created digitally without a camera.[18] It was produced using the Computer Animation Production System (CAPS), developed by Pixar in collaboration with The Walt Disney Company in the late 1980s, in a style similar to traditional cel animation.[19][20][21]  The so-called 3D style, more often associated with computer animation, became the dominant technique following the success of Pixar's Toy Story (1995), the first computer-animated feature in this style.[22]  Most of the cel animation studios switched to producing mostly computer-animated films around the 1990s, as it proved cheaper and more profitable. Not only the very popular 3D animation style was generated with computers, but also most of the films and series with a more traditional hand-crafted appearance, in which the charming characteristics of cel animation could be emulated with software, while new digital tools helped developing new styles and effects.[23][24][25][26][27][28]  In 2010, the animation market was estimated to be worth circa US$80 billion.[29] By 2021, the value had increased to an estimated US$370 billion.[30] Animated feature-length films returned the highest gross margins (around 52%) of all film genres between 2004 and 2013.[31] Animation as an art and industry continues to thrive as of the early 2020s.[32][33][34]  The clarity of animation makes it a powerful tool for instruction, while its total malleability also allows exaggeration that can be employed to convey strong emotions and to thwart reality. It has therefore been widely used for other purposes than mere entertainment.[35]  During World War II, animation was widely exploited for propaganda. Many American studios, including Warner Bros. and Disney, lent their talents and their cartoon characters to convey to the public certain war values. These efforts extended to other countries well into the Cold War era, particularly as it pertained to \"combatting\" communism. For example, the English 1954 adaptation of George Orwell's Animal Farm (the nation's first feature-length animated film) is speculated to have had its production funded by the CIA.[36][37]  Animation has been very popular in television commercials, both due to its graphic appeal, and the humour it can provide. Some animated characters in commercials have survived for decades, such as Snap, Crackle and Pop in advertisements for Kellogg's cereals.[38] Tex Avery was the producer of the first Raid \"Kills Bugs Dead\" commercials in 1966, which were very successful for the company.[39]  Apart from their success in movie theaters and television series, many cartoon characters would also prove lucrative when licensed for all kinds of merchandise and for other media.  Animation has traditionally been very closely related to comic books. While many comic book characters found their way to the screen (which is often the case in Japan, where many manga are adapted into anime), original animated characters also commonly appear in comic books and magazines. Somewhat similarly, characters and plots for video games (an interactive form of animation that became its own medium) have been derived from films and vice versa.[40]  Some of the original content produced for the screen can be used and marketed in other media. Stories and images can easily be adapted into children's books and other printed media. Songs and music have appeared on records and as streaming media.[citation needed]  While very many animation companies commercially exploit their creations outside moving image media, The Walt Disney Company is the best known and most extreme example. Since first being licensed for a children's writing tablet in 1929, their Mickey Mouse mascot has been depicted on an enormous amount of products, as have many other Disney characters. This may have influenced some pejorative use of Mickey's name, but licensed Disney products sell well, and the so-called Disneyana has many avid collectors, and even a dedicated Disneyana Fan Club (since 1984).[41]  Disneyland opened in 1955 and features many attractions that were based on Disney's cartoon characters. Its enormous success spawned several other Disney theme parks and resorts. Disney's earnings from the theme parks have relatively often been higher than those from their movies.  As with any other form of media, animation has instituted awards for excellence in the field. Many are part of general or regional film award programs, like the China's Golden Rooster Award for Best Animation (since 1981). Awards programs dedicated to animation, with many categories, include ASIFA-Hollywood's Annie Awards, the Emile Awards in Europe and the Anima Mundi awards in Brazil.[42][43][44]  Apart from Academy Awards for Best Animated Short Film (since 1932) and Best Animated Feature (since 2002), animated movies have been nominated and rewarded in other categories, relatively often for Best Original Song and Best Original Score.  Beauty and the Beast was the first animated film nominated for Best Picture, in 1991. Up (2009) and Toy Story 3 (2010) also received Best Picture nominations, after the academy expanded the number of nominees from five to ten.[45]  The creation of non-trivial animation works (i.e., longer than a few seconds) has developed as a form of filmmaking, with certain unique aspects.[46] Traits common to both live-action and animated feature films are labor intensity and high production costs.[47]  The most important difference is that once a film is in the production phase, the marginal cost of one more shot is higher for animated films than live-action films.[48] It is relatively easy for a director to ask for one more take during principal photography of a live-action film, but every take on an animated film must be manually rendered by animators (although the task of rendering slightly different takes has been made less tedious by modern computer animation).[49] It is pointless for a studio to pay the salaries of dozens of animators to spend weeks creating a visually dazzling five-minute scene if that scene fails to effectively advance the plot of the film.[50] Thus, animation studios starting with Disney began the practice in the 1930s of maintaining story departments where storyboard artists develop every single scene through storyboards, then handing the film over to the animators only after the production team is satisfied that all the scenes make sense as a whole.[51] While live-action films are now also storyboarded, they enjoy more latitude to depart from storyboards (i.e., real-time improvisation).[citation needed][52]  Another problem unique to animation is the requirement to maintain a film's consistency from start to finish, even as films have grown longer and teams have grown larger. Animators, like all artists, necessarily have individual styles, but must subordinate their individuality in a consistent way to whatever style is employed on a particular film.[53][54] Since the early 1980s, teams of about 500 to 600 people, of whom 50 to 70 are animators, typically have created feature-length animated films. It is relatively easy for two or three artists to match their styles; synchronizing those of dozens of artists is more difficult.[55]  This problem is usually solved by having a separate group of visual development artists develop an overall look and palette for each film before the animation begins.[53] While animators must \"sacrifice their personal drawing styles so that the work of many hands appears to be that of one\", visual development artists are allowed to \"create new worlds, new characters, and new entertainment possibilities in their own individualistic graphic styles\".[53] Character designers on the visual development team draw model sheets to show how each character should look like with different facial expressions, posed in different positions, and viewed from different angles.[56][57] On traditionally animated projects, maquettes were often sculpted to further help the animators see how characters would look from different angles.[56][58]  Unlike live-action films, animated films were traditionally developed beyond the synopsis stage through the storyboard format; the storyboard artists would then receive credit for writing the film.[59][60]  The traditional approach worked for several decades because prior to the 1960s, no one except Disney was attempting to regularly produce feature-length animated films.[60]  All other animation studios, with occasional exceptions, were producing short films only a few minutes in length.[60]  For short films, it was enough for the storyboard artists to work up a few visual gags and then string them together to form a crude plot.[60]  In 1960, Hanna-Barbera pioneered the longer animated sitcom format for television with The Flintstones.[60]  Hanna-Barbera and the other early television animation studios soon discovered that storyboarding was far too inefficient to fill up a half-hour episode on the extremely tight budgets typical of television.[60]  During the 1960s, these studios experimented with a more efficient method for developing story material: a screenwriter is hired to draft a written screenplay which is approved and handed over to the storyboard artists for storyboarding.[60] This method creates significant tension between screenwriters and storyboard artists, in that some artists feel that people who cannot draw should not be writing for animation, while some writers feel that artists do not understand how to write.[60]  Despite that tension, it has become and remains the dominant method by which animation studios develop both feature-length films and television shows.[60]  Traditional animation (also called cel animation or hand-drawn animation) is the process that was used for most animated films of the 20th century.[61] The individual frames of a traditionally animated film are photographs of drawings, first drawn on paper.[62] To create the illusion of movement, each drawing differs slightly from the one before it. The animators' drawings are traced or photocopied onto transparent acetate sheets called cels,[63] which are filled in with paints in assigned colors or tones on the side opposite the line drawings.[64] The completed character cels are photographed one-by-one against a painted background by a rostrum camera onto motion picture film.[65]  The traditional cel animation process became obsolete by the beginning of the 21st century. In modern traditionally animated films, animators' drawings and the backgrounds are either scanned into or drawn directly into a computer system.[1][66] Various software programs are used to color the drawings and simulate camera movement and effects.[67] The final animated piece is output to one of several delivery media, including traditional 35 mm film and newer media with digital video.[1][68] The \"look\" of traditional cel animation is still preserved, and the character animators' work has remained essentially the same over the past 90 years.[58] Some animation producers have used the term \"tradigital\" (a play on the words \"traditional\" and \"digital\") to describe cel animation that uses significant computer technology.  Examples of traditionally animated feature films include Pinocchio (United States, 1940),[69] Animal Farm (United Kingdom, 1954), Lucky and Zorba (Italy, 1998), and The Illusionist (British-French, 2010). Traditionally animated films produced with the aid of computer technology include The Lion King (US, 1994), Anastasia (US, 1997), The Prince of Egypt (US, 1998), Akira (Japan, 1988),[70] Spirited Away (Japan, 2001), The Triplets of Belleville (France, 2003), and The Secret of Kells (Irish-French-Belgian, 2009).  Full animation is the process of producing high-quality traditionally animated films that regularly use detailed drawings and plausible movement,[71] having a smooth animation.[72] Fully animated films can be made in a variety of styles, from more realistically animated works like those produced by the Walt Disney studio (The Little Mermaid, Beauty and the Beast, Aladdin, The Lion King) to the more 'cartoon' styles of the Warner Bros. animation studio. Many of the Disney animated features are examples of full animation, as are non-Disney works, The Secret of NIMH (US, 1982), The Iron Giant (US, 1999), and Nocturna (Spain, 2007). Fully animated films are often animated on \"twos\", sometimes on \"ones\", which means that 12 to 24 drawings are required for a single second of film.[73]  Limited animation involves the use of less detailed or more stylized drawings and methods of movement usually a choppy or \"skippy\" movement animation.[74] Limited animation uses fewer drawings per second, thereby limiting the fluidity of the animation. This is a more economic technique. Pioneered by the artists at the American studio United Productions of America,[75] limited animation can be used as a method of stylized artistic expression, as in Gerald McBoing-Boing (US, 1951), Yellow Submarine (UK, 1968), and certain anime produced in Japan.[76] Its primary use, however, has been in producing cost-effective animated content for media for television (the work of Hanna-Barbera,[77] Filmation,[78] and other TV animation studios[79]) and later the Internet (web cartoons).  Rotoscoping is a technique patented by Max Fleischer in 1917 where animators trace live-action movement, frame by frame.[80] The source film can be directly copied from actors' outlines into animated drawings,[81] as in The Lord of the Rings (US, 1978), or used in a stylized and expressive manner, as in Waking Life (US, 2001) and A Scanner Darkly (US, 2006). Some other examples are Fire and Ice (US, 1983), Heavy Metal (1981), and Aku no Hana (Japan, 2013).[citation needed]  Live-action\/animation is a technique combining hand-drawn characters into live action shots or live-action actors into animated shots.[82] One of the earlier uses was in Koko the Clown when Koko was drawn over live-action footage.[83] Walt Disney and Ub Iwerks created a series of Alice Comedies (1923–1927), in which a live-action girl enters an animated world. Other examples include Allegro Non Troppo (Italy, 1976), Who Framed Roger Rabbit (US, 1988), Volere volare (Italy 1991), Space Jam (US, 1996) and Osmosis Jones (US, 2001).[citation needed]  Stop motion is used to describe animation created by physically manipulating real-world objects and photographing them one frame of film at a time to create the illusion of movement.[84] There are many different types of stop-motion animation, usually named after the materials used to create the animation.[85] Computer software is widely available to create this type of animation; traditional stop-motion animation is usually less expensive but more time-consuming to produce than current computer animation.[85]  Computer animation encompasses a variety of techniques, the unifying factor being that the animation is created digitally on a computer.[67][108] 2D animation techniques tend to focus on image manipulation while 3D techniques usually build virtual worlds in which characters and objects move and interact.[109] 3D animation can create images that seem real to the viewer.[110]  2D animation figures are created or edited on the computer using 2D bitmap graphics and 2D vector graphics.[111] This includes automated computerized versions of traditional animation techniques, interpolated morphing,[112] onion skinning[113] and interpolated rotoscoping. 2D animation has many applications, including After Effects Animation, analog computer animation, Flash animation, and PowerPoint animation. Cinemagraphs are still photographs in the form of an animated GIF file of which part is animated.[114]  Final line advection animation is a technique used in 2D animation,[115] to give artists and animators more influence and control over the final product as everything is done within the same department.[116] Speaking about using this approach in Paperman, John Kahrs said that \"Our animators can change things, actually erase away the CG underlayer if they want, and change the profile of the arm.\"[117]  When working with game animations, skeletal 2D animations are commonly created using tools like Spine, DragonBones, Blender COA Tools, Rive, and the built-in Unity editor. The primary benefit of this approach is the ability to reuse images, which reduces the amount of graphics stored in RAM. This principle of maximizing resource efficiency means that by reusing existing elements, you can enhance the visual appeal of animations without needing to create additional graphics.[118]  3D animation is digitally modeled and manipulated by an animator. The 3D model maker usually starts by creating a 3D polygon mesh for the animator to manipulate.[119] A mesh typically includes many vertices that are connected by edges and faces, which give the visual appearance of form to a 3D object or 3D environment.[119] Sometimes, the mesh is given an internal digital skeletal structure called an armature that can be used to control the mesh by weighting the vertices.[120][121] This process is called rigging and can be used in conjunction with key frames to create movement.[122]  Other techniques can be applied, mathematical functions (e.g., gravity, particle simulations), simulated fur or hair, and effects, fire and water simulations.[123] These techniques fall under the category of 3D dynamics.[124] "},"meta":{},"created_at":"2025-03-22T14:25:42.274677Z","updated_at":"2025-03-22T14:25:42.274677Z","inner_id":22,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":31,"annotations":[{"id":31,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"3a7a526a-553c-4096-abd7-35a065697514","import_id":null,"last_action":null,"bulk_created":false,"task":31,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A corporate tax, also called corporation tax or company tax, is a type of direct tax levied on the income or capital of corporations and other similar legal entities. The tax is usually imposed at the national level, but it may also be imposed at state or local levels in some countries. Corporate taxes may be referred to as income tax or capital tax, depending on the nature of the tax.  The purpose of corporate tax is to generate revenue for the government by taxing the profits earned by corporations. The tax rate varies from country to country and is usually calculated as a percentage of the corporation's net income or capital. Corporate tax rates may also differ for domestic and foreign corporations.  Many countries have tax laws that require corporations to pay taxes on their worldwide income, regardless of where the income is earned. However, some countries have territorial tax systems, which only require corporations to pay taxes on income earned within the country's borders.  A country's corporate tax may apply to:  Company income subject to tax is often determined much like taxable income for individual taxpayers. Generally, the tax is imposed on net profits. In some jurisdictions, rules for taxing companies may differ significantly from rules for taxing individuals. Certain corporate acts or types of entities may be exempt from tax.  The incidence of corporate taxation is a subject of significant debate among economists and policymakers. Evidence suggests that some portion of the corporate tax falls on owners of capital, workers, and shareholders, but the ultimate incidence of the tax is an unresolved question.[1]  Economists disagree as to how much of the burden of the corporate tax falls on owners, workers, consumers, and landowners, and how the corporate tax affects economic growth and economic inequality.[2] More of the burden probably falls on capital in large open economies such as the US.[3] Some studies place the burden more on labor.[4][5][6] According to one study: \"Regression analysis shows that a one-percentage-point increase in the marginal state corporate tax rate reduces wages 0.14 to 0.36 percent.\"[7] There have been other studies.[8][9][10][11][12][13]  According to the Adam Smith Institute, \"Clausing (2012), Gravelle (2010) and Auerbach (2005), the three best reviews we found, basically conclude that most of the tax falls on capital, not labour.\"  A 2022 meta-analysis found that the impact of corporate taxes on economic growth was exaggerated and that it could not be ruled out that the impact of corporate taxation on economic growth was zero.[14]  Regardless of who bears the burden, corporation tax has been used as a tool of economic policy, with the main goal being economic stabilization. In times of economic downturn, lowering the corporate tax rates is meant to encourage investment, while in cases of an overheating economy adjusting the corporate tax is used to slow investment.[15]  Another use of the corporate tax is to encourage investments in some specific industries. One such case could be the current tax benefits afforded to the oil and gas industry. A less recent example was the effort to restore heavy industries in the US[15] by enacting the 1981 Accelerated Cost Recovery System (ACRS), which offered favorable depreciation allowances that would in turn lower taxes and increase cash flow, thus encouraging investment during the recession.  The agriculture industry, for example, could profit from the reassesment of their farming equipment. Under this new system, automobiles and breeding swine obtained a three year depreciation value; storage facilities, most equipment and breeding cattle and sheep became five year property; and land improvements were fifteen year property. The depreciation defined by ACRS was thus sizeably larger than under the previous tax system.[16]  A corporate tax is a tax imposed on the net profit of a corporation that is taxed at the entity level in a particular jurisdiction. Net profit for corporate tax is generally the financial statement net profit with modifications, and may be defined in great detail within each country's tax system. Such taxes may include income or other taxes. The tax systems of most countries impose an income tax at the entity level on the certain type(s) of entities (company or corporation). The rate of tax varies by jurisdiction. The tax may have an alternative base, such as assets, payroll, or income computed in an alternative manner.  Most countries exempt certain types of corporate events or transactions from income tax. For example, events related to the formation or reorganization of the corporation, which are treated as capital costs. In addition, most systems provide specific rules for taxation of the entity and\/or its members upon winding up or dissolution of the entity.  In systems where financing costs are allowed as reductions of the tax base (tax deductions), rules may apply that differentiate between classes of member-provided financing. In such systems, items characterized as interest may be deductible, perhaps subject to limitations, while items characterized as dividends are not. Some systems limit deductions based on simple formulas, such as a debt-to-equity ratio, while other systems have more complex rules.  Some systems provide a mechanism whereby groups of related corporations may obtain benefit from losses, credits, or other items of all members within the group. Mechanisms include combined or consolidated returns as well as group relief (direct benefit from items of another member).  Many systems additionally tax shareholders of those entities on dividends or other distributions by the corporation. A few systems provide for partial integration of entity and member taxation. This may be accomplished by \"imputation systems\" or franking credits. In the past, mechanisms have existed for advance payment of member tax by corporations, with such payment offsetting entity level tax.  Many systems (particularly sub-country level systems) impose a tax on particular corporate attributes.  Such non-income taxes may be based on capital stock issued or authorized (either by number of shares or value), total equity, net capital, or other measures unique to corporations.  Corporations, like other entities, may be subject to withholding tax obligations upon making certain varieties of payments to others.  These obligations are generally not the tax of the corporation, but the system may impose penalties on the corporation or its officers or employees for failing to withhold and pay over such taxes. A company has been defined as a juristic person having an independent and separate existence from its shareholders. Income of the company is computed and assessed separately in the hands of the company. In certain cases, distributions from the company to its shareholders as dividends are taxed as income to the shareholders.  Corporations' property tax, payroll tax, withholding tax, excise tax, customs duties, value added tax, and other common taxes, are generally not referred to as \"corporate tax\".  Characterization as a corporation for tax purposes is based on the form of organization, with the exception of United States Federal[17] and most states income taxes, under which an entity may elect to be treated as a corporation and taxed at the entity level or taxed only at the member level.[18]  See Limited liability company,  Partnership taxation, S corporation, Sole proprietorship.  Most jurisdictions, including the United Kingdom[19] and the United States,[18] tax corporations on their income. The United States taxes most types of corporate income at 21%.[18]  The United States taxes corporations under the same framework of tax law as individuals, with differences related to the inherent natures of corporations and individuals or unincorporated entities. For example, individuals are not formed, amalgamated, or acquired, and corporations do not incur medical expenses except by way of compensating individuals.[20]  Most systems tax both domestic and foreign corporations. Often, domestic corporations are taxed on worldwide income while foreign corporations are taxed only on income from sources within the jurisdiction.  The United States defines taxable income for a corporation as all gross income, i.e. sales plus other income minus cost of goods sold and tax exempt income less allowable tax deductions, without the allowance of the standard deduction applicable to individuals.[21]  The United States' system requires that differences in principles for recognizing income and deductions differing from financial accounting principles like the timing of income or deduction, tax exemption for certain income, and disallowance or limitation of certain tax deductions  be disclosed in considerable detail for non-small corporations on Schedule M-3 to Form 1120.[22]  The United States taxes resident corporations, i.e. those organized within the country, on their worldwide income, and nonresident, foreign corporations only on their income from sources within the country.[23]  Hong Kong taxes resident and nonresident corporations only on income from sources within the country.[24]  Corporate tax rates generally are the same for differing types of income, yet the US graduated its tax rate system where corporations with lower levels of income pay a lower rate of tax, with rates varying from 15% on the first $50,000 of income to 35% on incomes over $10,000,000, with phase-outs.[26]  The corporate income tax rates differ between US states and range from 2.5% to 11.5%.[27]  The Canadian system imposes tax at different rates for different types of corporations, allowing lower rates for some smaller corporations.[28]  Tax rates vary by jurisdiction and some countries have sub-country level jurisdictions like provinces, cantons, prefectures, cities, or other that also impose corporate income tax like Canada, Germany, Japan, Switzerland, and the United States.[29] Some jurisdictions impose tax at a different rate on an alternative tax base.  Examples of corporate tax rates for a few English-speaking countries include:  Corporate tax rates vary widely by country, leading some corporations to shield earnings within offshore subsidiaries or to redomicile within countries with lower tax rates.  In comparing national corporate tax rates one should also take into account the taxes on dividends paid to shareholders. For example, the overall U.S. tax on corporate profits of 35% is less than or similar to that of European countries such as Germany, Ireland, Switzerland and the United Kingdom, which have lower corporate tax rates but higher taxes on dividends paid to shareholders.[36]  Corporate tax rates across the Organisation for Economic Co-operation and Development (OECD) are shown in the table.  The corporate tax rates in other jurisdictions include:  [41]  [41]  In October 2021 some 136 countries agreed to enforce a corporate tax rate of at least 15% from 2023 after the talks on a minimum rate led by OECD for a decade.[42]  Most systems that tax corporations also impose income tax on shareholders of corporations when earnings are distributed.[43]  Such distribution of earnings is generally referred to as a dividend.  The tax may be at reduced rates.  For example, the United States provides for reduced amounts of tax on dividends received by  individuals and by corporations.[44]  The company law of some jurisdictions prevents corporations from distributing amounts to shareholders except as distribution of earnings.  Such earnings may be determined under company law principles or tax principles. In such jurisdictions, exceptions are usually provided with respect to distribution of shares of the company, for winding up, and in limited other situations.  Other jurisdictions treat distributions as distributions of earnings taxable to shareholders if earnings are available to be distributed, but do not prohibit distributions in excess of earnings.  For example, under the United States system each corporation must maintain a calculation of its earnings and profits (a tax concept similar to retained earnings).[45]  A distribution to a shareholder is considered to be from earnings and profits to the extent thereof unless an exception applies.[46]  The United States provides reduced tax on dividend income of both corporations and individuals.  Other jurisdictions provide corporations a means of designating, within limits, whether a distribution is a distribution of earnings taxable to the shareholder or a return of capital.  The following illustrates the dual level of tax concept:  Widget Corp earns 100 of profits before tax in each of years 1 and 2.  It distributes all the earnings in year 3, when it has no profits.  Jim owns all of Widget Corp.  The tax rate in the residence jurisdiction of Jim and Widget Corp is 30%.  Many systems provide that certain corporate events are not taxable to corporations or shareholders.  Significant restrictions and special rules often apply.  The rules related to such transactions are often quite complex.  Most systems treat the formation of a corporation by a controlling corporate shareholder as a nontaxable event.  Many systems, including the United States and Canada, extend this tax free treatment to the formation of a corporation by any group of shareholders in control of the corporation.[47]  Generally, in tax free formations the tax attributes of assets and liabilities are transferred to the new corporation along with such assets and liabilities.  Example:  John and Mary are United States residents who operate a business.  They decide to incorporate for business reasons.  They transfer the assets of the business to Newco, a newly formed Delaware corporation of which they are the sole shareholders, subject to accrued liabilities of the business in exchange solely for common shares of Newco.  Under United States principles, this transfer does not cause tax to John, Mary, or Newco.  If on the other hand Newco also assumes a bank loan in excess of the basis of the assets transferred less the accrued liabilities, John and Mary will recognize taxable gain for such excess.[48]  Corporations may merge or acquire other corporations in a manner a particular tax system treats as nontaxable to either of the corporations and\/or to their shareholders.  Generally, significant restrictions apply if tax free treatment is to be obtained.[49]  For example, Bigco acquires all of the shares of Smallco from Smallco shareholders in exchange solely for Bigco shares.  This acquisition is not taxable to Smallco or its shareholders under U.S. or Canadian tax law if certain requirements are met, even if Smallco is then liquidated into or merged or amalgamated with Bigco.  In addition, corporations may change key aspects of their legal identity, capitalization, or structure in a tax free manner under most systems.  Examples of reorganizations that may be tax free include mergers, amalgamations, liquidations of subsidiaries, share for share exchanges, exchanges of shares for assets, changes in form or place of organization, and recapitalizations.[50]  Most jurisdictions allow a tax deduction for interest expense incurred by a corporation in carrying out its trading activities.  Where such interest is paid to related parties, such deduction may be limited.  Without such limitation, owners could structure financing of the corporation in a manner that would provide for a tax deduction for much of the profits, potentially without changing the tax on shareholders.  For example, assume a corporation earns profits of 100 before interest expense and would normally distribute 50 to shareholders.  If the corporation is structured so that deductible interest of 50 is payable to the shareholders, it will cut its tax to half the amount due if it merely paid a dividend.  A common form of limitation is to limit the deduction for interest paid to related parties to interest charged at arm's length rates on debt not exceeding a certain portion of the equity of the paying corporation.  For example, interest paid on related party debt in excess of three times equity may not be deductible in computing taxable income.  The United States, United Kingdom, and French tax systems apply a more complex set of tests to limit deductions.  Under the U.S. system, related party interest expense in excess of 50% of cash flow is generally not currently deductible, with the excess potentially deductible in future years.[51]  The classification of instruments as debt on which interest is deductible or as equity with respect to which distributions are not deductible can be complex in some systems.[52] The Internal Revenue Service had proposed complex regulations under this section (see TD 7747, 1981-1 CB 141) which were soon withdrawn (TD 7920, 1983-2 CB 69).[citation needed]  Most jurisdictions tax foreign corporations differently from domestic corporations.[53]  No international laws limit the ability of a country to tax its nationals and residents (individuals and entities).  However, treaties and practicality impose limits on taxation of those outside its borders, even on income from sources within the country.  Most jurisdictions tax foreign corporations on business income within the jurisdiction when earned through a branch or permanent establishment in the jurisdiction.  This tax may be imposed at the same rate as the tax on business income of a resident corporation or at a different rate.[54]  Upon payment of dividends, corporations are generally subject to withholding tax only by their country of incorporation.  Many countries impose a branch profits tax on foreign corporations to prevent the advantage the absence of dividend withholding tax would otherwise provide to foreign corporations.  This tax may be imposed at the time profits are earned by the branch or at the time they are remitted or deemed remitted outside the country.[55]  Branches of foreign corporations may not be entitled to all of the same deductions as domestic corporations.  Some jurisdictions do not recognize inter-branch payments as actual payments, and income or deductions arising from such inter-branch payments are disregarded.[56]  Some jurisdictions impose express limits on tax deductions of branches.  Commonly limited deductions include management fees and interest.  Nathan M. Jenson argues that low corporate tax rates are a minor determinate of a multinational company when setting up their headquarters in a country.[57]  Most jurisdictions allow interperiod allocation or deduction of losses in some manner for corporations, even where such deduction is not allowed for individuals.  A few jurisdictions allow losses (usually defined as negative taxable income) to be deducted by revising or amending prior year taxable income.[58]  Most jurisdictions allow such deductions only in subsequent periods.  Some jurisdictions impose time limitations as to when loss deductions may be utilized.  Several jurisdictions provide a mechanism whereby losses or tax credits of one corporation may be used by another corporation where both corporations are commonly controlled (together, a group).  In the United States and Netherlands, among others, this is accomplished by filing a single tax return including the income and loss of each group member.  This is referred to as a consolidated return in the United States and as a fiscal unity in the Netherlands.  In the United Kingdom, this is accomplished directly on a pairwise basis called group relief.  Losses of one group member company may be \"surrendered\" to another group member company, and the latter company may deduct the loss against profits.  The United States has extensive regulations dealing with consolidated returns.[59]  One such rule requires matching of income and deductions on intercompany transactions within the group by use of \"deferred intercompany transaction\" rules.  In addition, a few systems provide a tax exemption for dividend income received by corporations.  The Netherlands system provides a \"participation exception\" to taxation for corporations owning more than 25% of the dividend paying corporation.  A key issue in corporate tax is the setting of prices charged by related parties for goods, services or the use of property. Many jurisdictions have guidelines on transfer pricing which allow tax authorities to adjust transfer prices used. Such adjustments may apply in both an international and a domestic context.  Most income tax systems levy tax on the corporation and, upon distribution of earnings (dividends), on the shareholder.  This results in a dual level of tax.  Most systems require that income tax be withheld on distribution of dividends to foreign shareholders, and some also require withholding of tax on distributions to domestic shareholders.  The rate of such withholding tax may be reduced for a shareholder under a tax treaty.  Some systems tax some or all dividend income at lower rates than other income.  The United States has historically provided a dividends received deduction to corporations with respect to dividends from other corporations in which the recipient owns more than 10% of the shares.  For tax years 2004–2010, the United States also has imposed a reduced rate of taxation on dividends received by individuals.[60]  Some systems currently attempt or in the past have attempted to integrate taxation of the corporation with taxation of shareholders to mitigate the dual level of taxation.  As a current example, Australia provides for a \"franking credit\" as a benefit to shareholders.  When an Australian company pays a dividend to a domestic shareholder, it reports the dividend as well as a notional tax credit amount.  The shareholder utilizes this notional credit to offset shareholder level income tax.[citation needed]  A previous system was utilised in the United Kingdom, called the advance corporation tax (ACT).  When a company paid a dividend, it was required to pay an amount of ACT, which it then used to offset its own taxes.  The ACT was included in income by the shareholder resident in the United Kingdom or certain treaty countries, and treated as a payment of tax by the shareholder.  To the extent that deemed tax payment exceeded taxes otherwise due, it was refundable to the shareholder.  Many jurisdictions incorporate some sort of alternative tax computation.  These computations may be based on assets, capital, wages, or some alternative measure of taxable income.  Often the alternative tax functions as a minimum tax.  United States federal income tax incorporates an alternative minimum tax.  This tax is computed at a lower tax rate (20% for corporations), and imposed based on a modified version of taxable income.  Modifications include longer depreciation lives assets under MACRS, adjustments related to costs of developing natural resources, and an addback of certain tax exempt interest.  The U.S. state of Michigan previously taxed businesses on an alternative base that did not allow compensation of employees as a tax deduction and allowed full deduction of the cost of production assets upon acquisition.  Some jurisdictions, such as Swiss cantons and certain states within the United States, impose taxes based on capital.  These may be based on total equity per audited financial statements,[61] a computed amount of assets less liabilities[62] or quantity of shares outstanding.[63]  In some jurisdictions, capital based taxes are imposed in addition to the income tax.[62]  In other jurisdictions, the capital taxes function as alternative taxes.  Mexico imposes an alternative tax on corporations, the IETU.[citation needed]  The tax rate is lower than the regular rate, and there are adjustments for salaries and wages, interest and royalties, and depreciable assets.  Most systems require that corporations file an annual income tax return.[64]  Some systems (such as the Canadian, United Kingdom and United States systems) require that taxpayers self assess tax on the tax return.[65]  Other systems provide that the government must make an assessment for tax to be due.[citation needed]  Some systems require certification of tax returns in some manner by accountants licensed to practice in the jurisdiction, often the company's auditors.[66]  Tax returns can be fairly simple or quite complex.  The systems requiring simple returns often base taxable income on financial statement profits with few adjustments, and may require that audited financial statements be attached to the return.[67]  Returns for such systems generally require that the relevant financial statements be attached to a simple adjustment schedule.  By contrast, United States corporate tax returns require both computation of taxable income from components thereof and reconciliation of taxable income to financial statement income.  Many systems require forms or schedules supporting particular items on the main form.  Some of these schedules may be incorporated into the main form.  For example, the Canadian corporate return, Form T-2, an eight-page form, incorporates some detail schedules but has nearly 50 additional schedules that may be required.  Some systems have different returns for different types of corporations or corporations engaged in specialized businesses.  The United States has 13 variations on the basic Form 1120[68] for S corporations, insurance companies, Domestic international sales corporations, foreign corporations, and other entities.  The structure of the forms and imbedded schedules vary by type of form.  Preparation of non-simple corporate tax returns can be time consuming.  For example, the U.S. Internal Revenue Service states in the instructions for Form 1120 that the average time needed to complete form is over 56 hours, not including record keeping time and required attachments.  Tax return due dates vary by jurisdiction, fiscal or tax year, and type of entity.[69]  In self-assessment systems, payment of taxes is generally due no later than the normal due date, though advance tax payments may be required.[70]  Canadian corporations must pay estimated taxes monthly.[71]  In each case, final payment is due with the corporation tax return.  U.S.  United Kingdom  Canada  United Kingdom  United States "},"meta":{},"created_at":"2025-03-22T14:25:42.274677Z","updated_at":"2025-03-22T14:25:42.274677Z","inner_id":23,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":32,"annotations":[{"id":32,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"e200adaf-e1dc-4a0d-b35f-37ae0ff7aa6b","import_id":null,"last_action":null,"bulk_created":false,"task":32,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Screenwriting or scriptwriting is the art and craft of writing scripts for mass media such as feature films, television productions or video games. It is often a freelance profession.  Screenwriters are responsible for researching the story, developing the narrative, writing the script, screenplay, dialogues and delivering it, in the required format, to development executives. Screenwriters therefore have great influence over the creative direction and emotional impact of the screenplay and, arguably, of the finished film.  Screenwriters either pitch original ideas to producers, in the hope that they will be optioned or sold; or are commissioned by a producer to create a screenplay from a concept, true story, existing screen work or literary work, such as a novel, poem, play, comic book, or short story.  The act of screenwriting takes many forms across the entertainment industry. Often, multiple writers work on the same script at different stages of development with different tasks. Over the course of a successful career, a screenwriter might be hired to write in a wide variety of roles.  Some of the most common forms of screenwriting jobs include:  Spec scripts are feature film or television show scripts written without the commission of, but is on speculation of sale to a film studio, production company, or TV network. The content is usually invented solely by the screenwriter, however spec screenplays can also be based on established works or real people and events. The spec script is a Hollywood sales tool with the vast majority of scripts written each year are spec scripts, but only a small percentage make it to the screen.[1] Though a spec script is usually a wholly original work, it can also be an adaptation.  In television writing, a spec script is a sample teleplay written to demonstrate the writer's knowledge of a show and ability to imitate its style and conventions. It is submitted to the show's producers in hopes of being hired to write future episodes of the show. Budding screenwriters attempting to break into the business generally begin by writing one or more spec scripts.  Although writing spec scripts is part of any writer's career, the Writers Guild of America forbids members to write \"on speculation\". The distinction is that a spec script is written as a sample by the writer on his or her own; what is forbidden is writing a script for a specific producer without a contract. In addition to writing a script on speculation, it is generally not advised to write camera angles or other directional terminology, as these are likely to be ignored. A director may write up a shooting script themselves, a script that guides the team in what to do in order to carry out the director's vision of how the script should look. The director may ask the original writer to co-write it with them or to rewrite a script that satisfies both the director and producer of the film\/TV show.  Spec writing is also unique in that the writer must pitch the idea to producers. In order to sell the script, it must have an excellent title, good writing, and a great logline, laying out what the movie is about. A well-written logline will convey the tone of the film, introduce the main character, and touch on the primary conflict. Usually the logline and title work in tandem to draw people in, and it is highly suggested to incorporate irony into them when possible. These things, along with nice, clean writing will hugely impact whether or not a producer picks up a spec script.  A commissioned screenplay is written by a hired writer. The concept is usually developed long before the screenwriter is brought on, and often has multiple writers work on it before the script is given a green light. The plot development is usually based on highly successful novels, plays, TV shows, and even video games, and the rights to which have been legally acquired.  Scripts written on assignment are screenplays created under contract with a studio, production company, or producer. These are the most common assignments sought after in screenwriting. A screenwriter can get an assignment either exclusively or from \"open\" assignments. A screenwriter can also be approached and offered an assignment. Assignment scripts are generally adaptations of an existing idea or property owned by the hiring company,[2] but can also be original works based on a concept created by the writer or producer.  Most produced films are rewritten to some extent during the development process. Frequently, they are not rewritten by the original writer of the script.[3] Many established screenwriters, as well as new writers whose work shows promise but lacks marketability, make their living rewriting scripts.  When a script's central premise or characters are good but the script is otherwise unusable, a different writer or team of writers is contracted to do an entirely new draft, often referred to as a \"page one rewrite\". When only small problems remain, such as bad dialogue or poor humor, a writer is hired to do a \"polish\" or \"punch-up\".  Depending on the size of the new writer's contributions, screen credit may or may not be given. For instance, in the American film industry, credit to rewriters is given only if 50% or more of the script is substantially changed.[4] These standards can make it difficult to establish the identity and number of screenwriters who contributed to a film's creation.  When established writers are called in to rewrite portions of a script late in the development process, they are commonly referred to as script doctors. Prominent script doctors include Christopher Keane, Steve Zaillian, William Goldman, Robert Towne, Mort Nathan, Quentin Tarantino, Carrie Fisher, and Peter Russell.[5][6] Many up-and-coming screenwriters work as ghostwriters.[citation needed]  A freelance television writer typically uses spec scripts or previous credits and reputation to obtain a contract to write one or more episodes for an existing television show. After an episode is submitted, rewriting or polishing may be required.  A staff writer for a TV show generally works in-house, writing and rewriting episodes. Staff writers—often given other titles, such as story editor or producer—work both as a group and individually on episode scripts to maintain the show's tone, style, characters, and plots.[7]  Serialized television series will typically have a basic premise and setting that creates a story engine that can drive individual episodes, subplots, and developments.[8]  Television show creators write the television pilot and bible of new television series. They are responsible for creating and managing all aspects of a show's characters, style, and plots. Frequently, a creator remains responsible for the show's day-to-day creative decisions throughout the series run as showrunner, head writer, or story editor.  The process of writing for soap operas and telenovelas is different from that used by prime time shows, due in part to the need to produce new episodes five days a week for several months. In one example cited by Jane Espenson, screenwriting is a \"sort of three-tiered system\":[9]  Espenson notes that a recent trend has been to eliminate the role of the mid-level writer, relying on the senior writers to do rough outlines and giving the other writers a bit more freedom. Regardless, when the finished scripts are sent to the top writers, the latter do a final round of rewrites. Espenson also notes that a show that airs daily, with characters who have decades of history behind their voices, necessitates a writing staff without the distinctive voice that can sometimes be present in prime-time series.[9]  Game shows feature live contestants, but still use a team of writers as part of a specific format.[10] This may involve the slate of questions and even specific phrasing or dialogue on the part of the host. Writers may not script the dialogue used by the contestants, but they work with the producers to create the actions, scenarios, and sequence of events that support the game show's concept.  With the continued development and increased complexity of video games, many opportunities are available to employ screenwriters in the field of video game design. Video game writers work closely with the other game designers to create characters, scenarios, and dialogue.[11]  Several main screenwriting theories help writers approach the screenplay by systematizing the structure, goals and techniques of writing a script. The most common kinds of theories are structural. Screenwriter William Goldman is widely quoted as saying \"Screenplays are structure\".  According to this approach, the three acts are: the setup (of the setting, characters, and mood), the confrontation (with obstacles), and the resolution (culminating in a climax and a dénouement). In a two-hour film, the first and third acts each last about thirty minutes, with the middle act lasting about an hour, but nowadays many films begin at the confrontation point and segue immediately to the setup or begin at the resolution and return to the setup.  In Writing Drama, French writer and director Yves Lavandier shows a slightly different approach.[12] As do most theorists, he maintains that every human action, whether fictitious or real, contains three logical parts: before the action, during the action, and after the action. But since the climax is part of the action, Lavandier maintains that the second act must include the climax, which makes for a much shorter third act than is found in most screenwriting theories.  Besides the three-act structure, it is also common to use a four- or five-act structure in a screenplay, and some screenplays may include as many as twenty separate acts.  The hero's journey, also referred to as the monomyth, is an idea formulated by noted mythologist Joseph Campbell. The central concept of the monomyth is that a pattern can be seen in stories and myths across history. Campbell defined and explained that pattern in his book The Hero with a Thousand Faces (1949).[13]  Campbell's insight was that important myths from around the world, which have survived for thousands of years, all share a fundamental structure. This fundamental structure contains a number of stages, which include:  Later, screenwriter Christopher Vogler refined and expanded the hero's journey for the screenplay form in his book, The Writer's Journey: Mythic Structure for Writers (1993).[14]  Syd Field introduced a new theory he called \"the paradigm\".[15] He introduced the idea of a plot point into screenwriting theory[16] and defined a plot point as \"any incident, episode, or event that hooks into the action and spins it around in another direction\".[17] These are the anchoring pins of the story line, which hold everything in place.[18] There are many plot points in a screenplay, but the main ones that anchor the story line in place and are the foundation of the dramatic structure, he called plot points I and II.[19][20] Plot point I occurs at the end of Act 1; plot point II at the end of Act 2.[16] Plot point I is also called the key incident because it is the true beginning of the story[21] and, in part, what the story is about.[22]  In a 120-page screenplay, Act 2 is about sixty pages in length, twice the length of Acts 1 and 3.[23] Field noticed that in successful movies, an important dramatic event usually occurs at the middle of the picture, around page sixty. The action builds up to that event, and everything afterward is the result of that event. He called this event the centerpiece or midpoint.[24] This suggested to him that the middle act is actually two acts in one. So, the three-act structure is notated 1, 2a, 2b, 3, resulting in Aristotle’s three acts being divided into four pieces of approximately thirty pages each.[25]  Field defined two plot points near the middle of Acts 2a and 2b, called pinch I and pinch II, occurring around pages 45 and 75 of the screenplay, respectively, whose functions are to keep the action on track, moving it forward, either toward the midpoint or plot point II.[26] Sometimes there is a relationship between pinch I and pinch II: some kind of story connection.[27]  According to Field, the inciting incident occurs near the middle of Act 1,[28] so-called because it sets the story into motion and is the first visual representation of the key incident.[29] The inciting incident is also called the dramatic hook, because it leads directly to plot point I.[30]  Field referred to a tag, an epilogue after the action in Act 3.[31]  Here is a chronological list of the major plot points that are congruent with Field's Paradigm:  The sequence approach to screenwriting, sometimes known as \"eight-sequence structure\", is a system developed by Frank Daniel, while he was the head of the Graduate Screenwriting Program at USC. It is based in part on the fact that, in the early days of cinema, technical matters forced screenwriters to divide their stories into sequences, each the length of a reel (about ten minutes).[32]  The sequence approach mimics that early style. The story is broken up into eight 10–15 minute sequences. The sequences serve as \"mini-movies\", each with their own compressed three-act structure. The first two sequences combine to form the film's first act. The next four create the film's second act. The final two sequences complete the resolution and dénouement of the story. Each sequence's resolution creates the situation which sets up the next sequence.  Michael Hauge divides primary characters into four categories. A screenplay may have more than one character in any category.  Secondary characters are all the other people in the screenplay and should serve as many of the functions above as possible.[34]  Motivation is whatever the character hopes to accomplish by the end of the movie. Motivation exists on outer and inner levels.  Motivation alone is not sufficient to make the screenplay work. There must be something preventing the hero from getting what he or she wants. That something is conflict.  Fundamentally, the screenplay is a unique literary form. It is like a musical score, in that it is intended to be interpreted on the basis of other artists' performance, rather than serving as a finished product for the enjoyment of its audience. For this reason, a screenplay is written using technical jargon and tight, spare prose when describing stage directions. Unlike a novel or short story, a screenplay focuses on describing the literal, visual aspects of the story, rather than on the internal thoughts of its characters. In screenwriting, the aim is to evoke those thoughts and emotions through subtext, action, and symbolism.[36]  Most modern screenplays, at least in Hollywood and related screen cultures, are written in a style known as the master-scene format[37][38] or master-scene script.[39] The format is characterized by six elements, presented in the order in which they are most likely to be used in a script:  Scripts written in master-scene format are divided into scenes: \"a unit of story that takes place at a specific location and time\".[40] Scene headings (or slugs) indicate the location the following scene is to take place in, whether it is interior or exterior, and the time-of-day it appears to be. Conventionally, they are capitalized, and may be underlined or bolded. In production drafts, scene headings are numbered.  Next are action lines, which describe stage direction and are generally written in the present tense with a focus only on what can be seen or heard by the audience.  Character names are in all caps, centered in the middle of the page, and indicate that a character is speaking the following dialogue. Characters who are speaking off-screen or in voice-over are indicated by the suffix (O.S.) and (V.O) respectively.  Parentheticals provide stage direction for the dialogue that follows. Most often this is to indicate how dialogue should be performed (for example, angry) but can also include small stage directions (for example, picking up vase). Overuse of parentheticals is discouraged.[41]  Dialogue blocks are offset from the page's margin by 3.7\" and are left-justified. Dialogue spoken by two characters at the same time is written side by side and is conventionally known as dual-dialogue.[42]  The final element is the scene transition and is used to indicate how the current scene should transition into the next. It is generally assumed that the transition will be a cut, and using \"CUT TO:\" will be redundant.[43][44] Thus the element should be used sparingly to indicate a different kind of transition such as \"DISSOLVE TO:\".  Screenwriting applications such as Final Draft (software), Celtx, Fade In (software), Slugline, Scrivener (software), and Highland, allow writers to easily format their script to adhere to the requirements of the master screen format.  Imagery can be used in many metaphoric ways. In The Talented Mr. Ripley, the title character talked of wanting to close the door on himself sometime, and then, in the end, he did. Pathetic fallacy is also frequently used; rain to express a character feeling depressed, sunny days promote a feeling of happiness and calm. Imagery can be used to sway the emotions of the audience and to clue them in to what is happening.  Imagery is well defined in City of God. The opening image sequence sets the tone for the entire film. The film opens with the shimmer of a knife's blade on a sharpening stone. A drink is being prepared, The knife's blade shows again, juxtaposed is a shot of a chicken letting loose of its harness on its feet. All symbolising 'The One that got away'. The film is about life in the favelas in Rio - sprinkled with violence and games and ambition.  Since the advent of sound film, or \"talkies\", dialogue has taken a central place in much of mainstream cinema. In the cinematic arts, the audience understands the story only through what they see and hear: action, music, sound effects, and dialogue. For many screenwriters, the only way their audiences can hear the writer's words is through the characters' dialogue. This has led writers such as Diablo Cody, Joss Whedon, and Quentin Tarantino to become well known for their dialogue—not just their stories.  Bollywood and other Indian film industries use separate dialogue writers in addition to the screenplay writers.[45]  Plot, according to Aristotle's Poetics, refers to the sequence events connected by cause and effect in a story. A story is a series of events conveyed in chronological order. A plot is the same series of events deliberately arranged to maximize the story's dramatic, thematic, and emotional significance. E.M.Forster famously gives the example \"The king died and then the queen died\" is a story.\" But \"The king died and then the queen died of grief\" is a plot.[46] For Trey Parker and Matt Stone this is best summarized as a series of events connected by either the word \"therefore\" or the word \"however\".[47]  A number of American universities offer specialized Master of Fine Arts and undergraduate programs in screenwriting, including USC, DePaul University, American Film Institute, Loyola Marymount University, Chapman University, NYU, UCLA,  Boston University and the University of the Arts. In Europe, the United Kingdom has an extensive range of MA and BA Screenwriting Courses including London College of Communication, Bournemouth University, Edinburgh University, and Goldsmiths College (University of London).  Some schools offer non-degree screenwriting programs, such as the TheFilmSchool, The International Film and Television School Fast Track, and the UCLA Professional \/ Extension Programs in Screenwriting.  New York Film Academy offers both degree and non-degree educational systems with campuses all around the world.  A variety of other educational resources for aspiring screenwriters also exist, including books, seminars, websites and podcasts, such as the Scriptnotes podcast.  The first true screenplay is thought to be from George Melies' 1902 film A Trip to the Moon. The movie is silent, but the screenplay still contains specific descriptions and action lines that resemble a modern-day script. As time went on and films became longer and more complex, the need for a screenplay became more prominent in the industry. The introduction of movie theaters also impacted the development of screenplays, as audiences became more widespread and sophisticated, so the stories had to be as well. Once the first non-silent movie was released in 1927, screenwriting became a hugely important position within Hollywood. The \"studio system\" of the 1930s only heightened this importance, as studio heads wanted productivity. Thus, having the \"blueprint\" (continuity screenplay) of the film beforehand became extremely optimal. Around 1970, the \"spec script\" was first created, and changed the industry for writers forever. Now, screenwriting for television (teleplays) is considered as difficult and competitive as writing is for feature films.[48]  Screenwriting has been the focus of a number of films:  In the United States, completed works may be copyrighted, but ideas and plots may not be. Any document written after 1978 in the U.S. is automatically copyrighted even without legal registration or notice. However, the Library of Congress will formally register a screenplay. U.S. Courts will not accept a lawsuit alleging that a defendant is infringing on the plaintiff's copyright in a work until the plaintiff registers the plaintiff's claim to those copyrights with the Copyright Office.[52] This means that a plaintiff's attempts to remedy an infringement will be delayed during the registration process.[53] Additionally, in many infringement cases, the plaintiff will not be able recoup attorney fees or collect statutory damages for copyright infringement, unless the plaintiff registered before the infringement began.[54] For the purpose of establishing evidence that a screenwriter is the author of a particular screenplay (but not related to the legal copyrighting status of a work), the Writers Guild of America registers screenplays. However, since this service is one of record keeping and is not regulated by law, a variety of commercial and non-profit organizations exist for registering screenplays. Protection for teleplays, formats, as well as screenplays may be registered for instant proof-of-authorship by third-party assurance vendors.[citation needed]  There is a line of precedent in several states (including California and New York) that allows for \"idea submission\" claims, based on the notion that submission of a screenplay—or even a mere pitch for one—to a studio under very particular sets of factual circumstances could potentially give rise to an implied contract to pay for the ideas embedded in that screenplay, even if an alleged derivative work does not actually infringe the screenplay author's copyright.[55] The unfortunate side effect of such precedents (which were supposed to protect screenwriters) is that it is now that much harder to break into screenwriting. Naturally, motion picture and television production firms responded by categorically declining to read all unsolicited screenplays from unknown writers;[56] accepting screenplays only through official channels like talent agents, managers, and attorneys; and forcing screenwriters to sign broad legal releases before their screenplays will be actually accepted, read, or considered.[55] In turn, agents, managers, and attorneys have become extremely powerful gatekeepers on behalf of the major film studios and media networks.[56] One symptom of how hard it is to break into screenwriting as a result of such case law is that in 2008, Universal resisted construction of a bike path along the Los Angeles River next to its studio lot because it would worsen their existing problem with desperate amateur screenwriters throwing copies of their work over the studio wall.[57] "},"meta":{},"created_at":"2025-03-22T14:25:42.275677Z","updated_at":"2025-03-22T14:25:42.275677Z","inner_id":24,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":33,"annotations":[{"id":33,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"c4e02f1a-772a-4854-8708-2e197d89d7ca","import_id":null,"last_action":null,"bulk_created":false,"task":33,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"In general usage, a financial plan is a comprehensive evaluation of an individual's current pay and future financial state by using current known variables to predict future income, asset values and withdrawal plans.[1]  This often includes a budget which organizes an individual's finances and sometimes includes a series of steps or specific goals for spending and saving in the future. This plan allocates future income to various types of expenses, such as rent or utilities, and also reserves some income for short-term and long-term savings. A financial plan is sometimes referred to as an investment plan, but in personal finance, a financial plan can focus on other specific areas such as risk management, estates, college, or retirement.  In business, \"financial forecast\"  or \"financial plan\" can also refer to a projection across a time horizon, typically an annual one, of income and expenses for a company, division, or department;[2]  see Budget § Corporate budget. More specifically, a financial plan can also refer to the three primary financial statements (balance sheet, income statement, and cash flow statement) created within a business plan. A financial plan can also be an estimation of cash needs and a decision on how to raise the cash, such as through borrowing or issuing additional shares in a company.[3]  Note that the financial plan  may then contain prospective financial statements, which are similar, but different, to those of a budget.  Financial plans are the entire financial accounting overview of a company. Complete financial plans contain all periods and transaction types. It's a combination of the financial statements which independently only reflect a past, present, or future state of the company. Financial plans are the collection of the historical, present, and future financial statements; for example, a (historical & present) costly expense from an operational issue is normally presented prior to the issuance of the prospective financial statements which propose a solution to said operational issue.  The confusion surrounding the term \"financial plans\" might stem from the fact that there are many types of financial statement reports. Individually, financial statements show either the past, present, or future financial results. More specifically, financial statements also only reflect the specific categories which are relevant. For instance, investing activities are not adequately displayed in a balance sheet. A financial plan is a combination of the individual financial statements and reflect all categories of transactions (operations & expenses & investing) over time.[4]  Some period-specific financial statement examples include pro forma statements (historical period) and prospective statements (current and future period). Compilations are a type of service which involves \"presenting, in the form of financial statements, information that is the representation of management\".[5]  There are two types of \"prospective financial statements\": financial  forecasts & financial projections and both relate to the current\/future time period.   Prospective financial statements are a time period-type of financial statement which may reflect the current\/future financial status of a company using three main reports\/financial statements: cash flow statement, income statement, and balance sheet. \"Prospective financial statements are of two types- forecasts and projections. Forecasts are based on management's expected financial position, results of operations, and cash flows.\"[6] Pro Forma statements take previously recorded results, the historical financial data, and present a \"what-if\": \"what-if\" a transaction had happened sooner.[7]  While the common usage of the term \"financial plan\" often refers to a formal and defined series of steps or goals, there is some technical confusion about what the term \"financial plan\" actually means in the industry. For example, one of the industry's leading professional organizations, the Certified Financial Planner Board of Standards, lacks any definition for the term \"financial plan\" in its Standards of Professional Conduct publication. This publication outlines the professional financial planner's job, and explains the process of financial planning, but the term \"financial plan\" never appears in the publication's text.[8]  The accounting and finance industries have distinct responsibilities and roles. When the products of their work are combined, it produces a complete picture, a financial plan. A financial analyst studies the data and facts (regulations\/standards), which are processed, recorded, and presented by accountants. Normally, finance personnel study the data results - meaning what has happened or what might happen - and propose a solution to an inefficiency. Investors and financial institutions must see both the issue and the solution to make an informed decision. Accountants and financial planners are both involved with presenting issues and resolving inefficiencies, so together, the results and explanation are provided in a financial plan.  Textbooks used in universities offering financial planning-related courses also generally do not define the term 'financial plan'. For example, Sid Mittra, Anandi P. Sahu, and Robert A Crane, authors of Practicing Financial Planning for Professionals[9] do not define what a financial plan is, but merely defer to the Certified Financial Planner Board of Standards' definition of 'financial planning'.  When drafting a financial plan, the company should establish the planning horizon,[10] which is the time period of the plan, whether it be on a short-term (usually 12 months) or long-term (two to five years) basis. Also, the individual projects and investment proposals of each operational unit within the company should be totaled and treated as one large project. This process is called aggregation.[11] "},"meta":{},"created_at":"2025-03-22T14:25:42.275677Z","updated_at":"2025-03-22T14:25:42.275677Z","inner_id":25,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":34,"annotations":[{"id":34,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"5d53e339-386a-499e-8acd-f772ed96b5f7","import_id":null,"last_action":null,"bulk_created":false,"task":34,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  A mortgage loan or simply mortgage (\/ˈmɔːrɡɪdʒ\/), in civil law jurisdictions known also as a hypothec loan, is a loan used either by purchasers of real property to raise funds to buy real estate, or by existing property owners to raise funds for any purpose while putting a lien on the property being mortgaged. The loan is \"secured\" on the borrower's property through a process known as mortgage origination. This means that a legal mechanism is put into place which allows the lender to take possession and sell the secured property (\"foreclosure\" or \"repossession\") to pay off the loan in the event the borrower defaults on the loan or otherwise fails to abide by its terms. The word mortgage is derived from a Law French term used in Britain in the Middle Ages meaning \"death pledge\" and refers to the pledge ending (dying) when either the obligation is fulfilled or the property is taken through foreclosure.[2] A mortgage can also be described as \"a borrower giving consideration in the form of a collateral for a benefit (loan)\".  Mortgage borrowers can be individuals mortgaging their home or they can be businesses mortgaging commercial property (for example, their own business premises, residential property let to tenants, or an investment portfolio). The lender will typically be a financial institution, such as a bank, credit union or building society, depending on the country concerned, and the loan arrangements can be made either directly or indirectly through intermediaries. Features of mortgage loans such as the size of the loan, maturity of the loan, interest rate, method of paying off the loan, and other characteristics can vary considerably. The lender's rights over the secured property take priority over the borrower's other creditors, which means that if the borrower becomes bankrupt or insolvent, the other creditors will only be repaid the debts owed to them from a sale of the secured property if the mortgage lender is repaid in full first.  In many jurisdictions, it is normal for home purchases to be funded by a mortgage loan. Few individuals have enough savings or liquid funds to enable them to purchase property outright. In countries where the demand for home ownership is highest, strong domestic markets for mortgages have developed. Mortgages can either be funded through the banking sector (that is, through short-term deposits) or through the capital markets through a process called \"securitization\", which converts pools of mortgages into fungible bonds that can be sold to investors in small denominations.  According to Anglo-American property law, a mortgage occurs when an owner (usually of a fee simple \/ freehold interest in real property, but frequently leasehold in England and Wales) pledges his or her interest (right to the property) as security or collateral for a loan. Therefore, a mortgage is an encumbrance (limitation) on the right to the property just as an easement would be, but because most mortgages occur as a condition for new loan money, the word mortgage has become the generic term for a loan secured by such real property. As with other types of loans, mortgages have an interest rate and are scheduled to amortize over a set period of time, typically 30 years in the United States. All types of real property can be, and usually are, secured with a mortgage and bear an interest rate that is supposed to reflect the lender's risk.  Mortgage lending is the primary mechanism used in many countries to finance private ownership of residential and commercial property (see commercial mortgages). Although the terminology and precise forms will differ from country to country, the basic components tend to be similar:  Many other specific characteristics are common to many markets, but the above are the essential features. Governments usually regulate many aspects of mortgage lending, either directly (through legal requirements, for example) or indirectly (through regulation of the participants or the financial markets, such as the banking industry), and often through state intervention (direct lending by the government, direct lending by state-owned banks, or sponsorship of various entities). Other aspects that define a specific mortgage market may be regional, historical, or driven by specific characteristics of the legal or financial system.  Mortgage loans are generally structured as long-term loans, the periodic payments for which are similar to an annuity and calculated according to the time value of money formulae. The most basic arrangement would require a fixed monthly payment over a period of ten to thirty years, depending on local conditions. Over this period the principal component of the loan (the original loan) would be slowly paid down through amortization. In practice, many variants are possible and common worldwide and within each country.  Lenders provide funds against property to earn interest income, and generally borrow these funds themselves (for example, by taking deposits or issuing bonds). The price at which the lenders borrow money, therefore, affects the cost of borrowing. Lenders may also, in many countries, sell the mortgage loan to other parties who are interested in receiving the stream of cash payments from the borrower, often in the form of a security (by means of a securitization).  Mortgage lending will also take into account the (perceived) riskiness of the mortgage loan, that is, the likelihood that the funds will be repaid (usually considered a function of the creditworthiness of the borrower); that if they are not repaid, the lender will be able to foreclose on the real estate assets; and the financial, interest rate risk and time delays that may be involved in certain circumstances.  During the mortgage loan approval process, a mortgage loan underwriter verifies the financial information that the applicant has provided as to income, employment, credit history and the value of the home being purchased via an appraisal.[4] An appraisal may be ordered. The underwriting process may take a few days to a few weeks. Sometimes the underwriting process takes so long that the provided financial statements need to be resubmitted so they are current.[5] It is advisable to maintain the same employment and not to use or open new credit during the underwriting process. Any changes made in the applicant's credit, employment, or financial information could result in the loan being denied.  There are many types of mortgages used worldwide, but several factors broadly define the characteristics of the mortgage. All of these may be subject to local regulation and legal requirements.  The two basic types of amortized loans are the fixed rate mortgage (FRM) and adjustable-rate mortgage (ARM) (also known as a floating rate or variable rate mortgage). In some countries, such as the United States, fixed rate mortgages are the norm, but floating rate mortgages are relatively common. Combinations of fixed and floating rate mortgages are also common, whereby a mortgage loan will have a fixed rate for some period, for example the first five years, and vary after the end of that period.  The charge to the borrower depends upon the credit risk in addition to the interest rate risk. The mortgage origination and underwriting process involves checking credit scores, debt-to-income, downpayments (deposits), assets, and assessing property value. Jumbo mortgages and subprime lending are not supported by government guarantees and face higher interest rates. Other innovations described below can affect the rates as well.  Upon making a mortgage loan for the purchase of a property, lenders usually require that the borrower make a down payment (called a deposit in English law); that is, contribute a portion of the cost of the property. This down payment may be expressed as a portion of the value of the property (see below for a definition of this term). The loan to value ratio (or LTV) is the size of the loan against the value of the property. Therefore, a mortgage loan in which the purchaser has made a down payment of 20% has a loan to value ratio of 80%. For loans made against properties that the borrower already owns, the loan to value ratio will be imputed against the estimated value of the property.  The loan to value ratio is considered an important indicator of the riskiness of a mortgage loan: the higher the LTV, the higher the risk that the value of the property (in case of foreclosure) will be insufficient to cover the remaining principal of the loan.  Since the value of the property is an important factor in understanding the risk of the loan, determining the value is a key factor in mortgage lending. The value may be determined in various ways, but the most common are:  In most countries, a number of more or less standard measures of creditworthiness may be used. Common measures include payment to income (mortgage payments as a percentage of gross or net income); debt to income (all debt payments, including mortgage payments, as a percentage of income); and various net worth measures. In many countries, credit scores are used in lieu of or to supplement these measures. There will also be requirements for documentation of the creditworthiness, such as income tax returns, pay stubs, etc. the specifics will vary from location to location. Income tax incentives usually can be applied in forms of tax refunds or tax deduction schemes. The first implies that income tax paid by individual taxpayers will be refunded to the extent of interest on mortgage loans taken to acquire residential property. Income tax deduction implies lowering tax liability to the extent of interest rate paid for the mortgage loan.  Some lenders may also require a potential borrower have one or more months of \"reserve assets\" available. In other words, the borrower may be required to show the availability of enough assets to pay for the housing costs (including mortgage, taxes, etc.) for a period of time in the event of the job loss or other loss of income.  Many countries have lower requirements for certain borrowers, or \"no-doc\" \/ \"low-doc\" lending standards that may be acceptable under certain circumstances.  Many countries have a notion of standard or conforming mortgages that define a perceived acceptable level of risk, which may be formal or informal, and may be reinforced by laws, government intervention, or market practice. For example, a standard mortgage may be considered to be one with no more than 70–80% LTV and no more than one-third of gross income going to mortgage debt.  A standard or conforming mortgage is a key concept as it often defines whether or not the mortgage can be easily sold or securitized, or, if non-standard, may affect the price at which it may be sold. In the United States, a conforming mortgage is one which meets the established rules and procedures of the two major government-sponsored entities in the housing finance market (including some legal requirements). In contrast, lenders who decide to make nonconforming loans are exercising a higher risk tolerance and do so knowing that they face more challenge in reselling the loan. Many countries have similar concepts or agencies that define what are \"standard\" mortgages. Regulated lenders (such as banks) may be subject to limits or higher-risk weightings for non-standard mortgages. For example, banks and mortgage brokerages in Canada face restrictions on lending more than 80% of the property value; beyond this level, mortgage insurance is generally required.[6]  In some countries with currencies that tend to depreciate, foreign currency mortgages are common, enabling lenders to lend in a stable foreign currency, whilst the borrower takes on the currency risk that the currency will depreciate and they will therefore need to convert higher amounts of the domestic currency to repay the loan.  In addition to the two standard means of setting the cost of a mortgage loan (fixed at a set interest rate for the term, or variable relative to market interest rates), there are variations in how that cost is paid, and how the loan itself is repaid. Repayment depends on locality, tax laws and prevailing culture. There are also various mortgage repayment structures to suit different types of borrower.  The most common way to repay a secured mortgage loan is to make regular payments toward the principal and interest over a set term, commonly referred to as (self) amortization in the U.S. and as a repayment mortgage in the UK. A mortgage is a form of annuity (from the perspective of the lender), and the calculation of the periodic payments is based on the time value of money formulas. Certain details may be specific to different locations: interest may be calculated on the basis of a 360-day year, for example; interest may be compounded daily, yearly, or semi-annually; prepayment penalties may apply; and other factors. There may be legal restrictions on certain matters, and consumer protection laws may specify or prohibit certain practices.  Depending on the size of the loan and the prevailing practice in the country the term may be short (10 years) or long (50 years plus). In the UK and U.S., 25 to 30 years is the usual maximum term (although shorter periods, such as 15-year mortgage loans, are common). Mortgage payments, which are typically made monthly, contain a repayment of the principal and an interest element. The amount going toward the principal in each payment varies throughout the term of the mortgage. In the early years the repayments are mostly interest. Towards the end of the mortgage, payments are mostly for principal. In this way, the payment amount determined at outset is calculated to ensure the loan is repaid at a specified date in the future. This gives borrowers assurance that by maintaining repayment the loan will be cleared at a specified date if the interest rate does not change. Some lenders and 3rd parties offer a bi-weekly mortgage payment program designed to accelerate the payoff of the loan. Similarly, a mortgage can be ended before its scheduled end by paying some or all of the remainder prematurely, called curtailment.[7]  An amortization schedule is typically worked out taking the principal left at the end of each month, multiplying by the monthly rate and then subtracting the monthly payment. This is typically generated by an amortization calculator using the following formula:      A = P ⋅    r ( 1 + r  )  n     ( 1 + r  )  n   − 1      {\\displaystyle A=P\\cdot {\\frac {r(1+r)^{n}}{(1+r)^{n}-1}}}    where:  The main alternative to a principal and interest mortgage is an interest-only mortgage, where the principal is not repaid throughout the term. This type of mortgage is common in the UK, especially when associated with a regular investment plan. With this arrangement regular contributions are made to a separate investment plan designed to build up a lump sum to repay the mortgage at maturity. This type of arrangement is called an investment-backed mortgage or is often related to the type of plan used: endowment mortgage if an endowment policy is used, similarly a personal equity plan (PEP) mortgage, Individual Savings Account (ISA) mortgage or pension mortgage. Historically, investment-backed mortgages offered various tax advantages over repayment mortgages, although this is no longer the case in the UK. Investment-backed mortgages are seen as higher risk as they are dependent on the investment making sufficient return to clear the debt.  Until recently[when?] it was not uncommon for interest only mortgages to be arranged without a repayment vehicle, with the borrower gambling that the property market will rise sufficiently for the loan to be repaid by trading down at retirement (or when rent on the property and inflation combine to surpass the interest rate).[citation needed]  Recent Financial Services Authority guidelines to UK lenders regarding interest-only mortgages have tightened the criteria on new lending on an interest-only basis. The problem for many people has been the fact that no repayment vehicle had been implemented, or the vehicle itself (e.g. endowment\/ISA policy) performed poorly and therefore insufficient funds were available to repay balance at the end of the term.  Moving forward, the FSA under the Mortgage Market Review (MMR) have stated there must be strict criteria on the repayment vehicle being used. As such the likes of Nationwide and other lenders have pulled out of the interest-only market.  A resurgence in the equity release market has been the introduction of interest-only lifetime mortgages. Where an interest-only mortgage has a fixed term, an interest-only lifetime mortgage will continue for the rest of the mortgagors life. These schemes have proved of interest to people who do like the roll-up effect (compounding) of interest on traditional equity release schemes. They have also proved beneficial to people who had an interest-only mortgage with no repayment vehicle and now need to settle the loan. These people can now effectively remortgage onto an interest-only lifetime mortgage to maintain continuity.  Interest-only lifetime mortgage schemes are currently offered by two lenders – Stonehaven and more2life. They work by having the options of paying the interest on a monthly basis. By paying off the interest means the balance will remain level for the rest of their life. This market is set to increase[timeframe?] as more retirees require finance in retirement.  For older borrowers (typically in retirement), it may be possible to arrange a mortgage where neither the principal nor interest is repaid. The interest is rolled up with the principal, increasing the debt each year.  These arrangements are variously called reverse mortgages, lifetime mortgages or equity release mortgages (referring to home equity), depending on the country. The loans are typically not repaid until the borrowers are deceased, hence the age restriction.  Through the Federal Housing Administration, the U.S. government insures reverse mortgages via a program called the HECM (Home Equity Conversion Mortgage). Unlike standard mortgages (where the entire loan amount is typically disbursed at the time of loan closing) the HECM program allows the homeowner to receive funds in a variety of ways: as a one time lump sum payment; as a monthly tenure payment which continues until the borrower dies or moves out of the house permanently; as a monthly payment over a defined period of time; or as a credit line.[8][better source needed]  In the U.S. a partial amortization or balloon loan is one where the amount of monthly payments due are calculated (amortized) over a certain term, but the outstanding balance on the principal is due at some point short of that term. In the UK, a partial repayment mortgage is quite common, especially where the original mortgage was investment-backed.  Graduated payment mortgage loans have increasing costs over time and are geared to young borrowers who expect wage increases over time. Balloon payment mortgages have only partial amortization, meaning that amount of monthly payments due are calculated (amortized) over a certain term, but the outstanding principal balance is due at some point short of that term, and at the end of the term a balloon payment is due. When interest rates are high relative to the rate on an existing seller's loan, the buyer can consider assuming the seller's mortgage.[9] A wraparound mortgage is a form of seller financing that can make it easier for a seller to sell a property. A biweekly mortgage has payments made every two weeks instead of monthly.  Budget loans include taxes and insurance in the mortgage payment;[10] package loans add the costs of furnishings and other personal property to the mortgage. Buydown mortgages allow the seller or lender to pay something similar to points to reduce interest rate and encourage buyers.[11] Homeowners can also take out equity loans in which they receive cash for a mortgage debt on their house. Shared appreciation mortgages are a form of equity release. In the US, foreign nationals due to their unique situation face Foreign National mortgage conditions.  Flexible mortgages allow for more freedom by the borrower to skip payments or prepay. Offset mortgages allow deposits to be counted against the mortgage loan. In the UK there is also the endowment mortgage where the borrowers pay interest while the principal is paid with a life insurance policy.  Commercial mortgages typically have different interest rates, risks, and contracts than personal loans. Participation mortgages allow multiple investors to share in a loan. Builders may take out blanket loans which cover several properties at once. Bridge loans may be used as temporary financing pending a longer-term loan. Hard money loans provide financing in exchange for the mortgaging of real estate collateral.  In most jurisdictions, a lender may foreclose the mortgaged property if certain conditions occur – principally, non-payment of the mortgage loan. Subject to local legal requirements, the property may then be sold. Any amounts received from the sale (net of costs) are applied to the original debt. In some jurisdictions, mortgage loans are non-recourse loans: if the funds recouped from sale of the mortgaged property are insufficient to cover the outstanding debt, the lender may not have recourse to the borrower after foreclosure. In other jurisdictions, the borrower remains responsible for any remaining debt.  In virtually all jurisdictions, specific procedures for foreclosure and sale of the mortgaged property apply, and may be tightly regulated by the relevant government. There are strict or judicial foreclosures and non-judicial foreclosures, also known as power of sale foreclosures. In some jurisdictions, foreclosure and sale can occur quite rapidly, while in others, foreclosure may take many months or even years. In many countries, the ability of lenders to foreclose is extremely limited, and mortgage market development has been notably slower.  A study issued by the UN Economic Commission for Europe compared German, US, and Danish mortgage systems. The German Bausparkassen (savings and loans associations) reported nominal interest rates of approximately 6 per cent per annum in the last 40 years (as of 2004). Bausparkassen are not identical with banks that give mortgages. In addition, they charge administration and service fees (about 1.5 per cent of the loan amount). However, in the United States, the average interest rates for fixed-rate mortgages in the housing market started in the tens and twenties in the 1980s and have (as of 2004) reached about 6 per cent per annum. However, gross borrowing costs are substantially higher than the nominal interest rate and amounted for the last 30 years to 10.46 per cent. In Denmark, similar to the United States mortgage market, interest rates have fallen to 6 per cent per annum. A risk and administration fee amounts to 0.5 per cent of the outstanding debt. In addition, an acquisition fee is charged which amounts to one per cent of the principal.[12]  The mortgage industry of the United States is a major financial sector. The federal government created several programs, or government sponsored entities, to foster mortgage lending, construction and encourage home ownership. These programs include the Government National Mortgage Association (known as Ginnie Mae), the Federal National Mortgage Association (known as Fannie Mae) and the Federal Home Loan Mortgage Corporation (known as Freddie Mac).  The US mortgage sector has been the center of major financial crises over the last century. Unsound lending practices resulted in the National Mortgage Crisis of the 1930s, the savings and loan crisis of the 1980s and 1990s and the subprime mortgage crisis of 2007 which led to the 2010 foreclosure crisis.  In the United States, the mortgage loan involves two separate documents: the mortgage note (a promissory note) and the security interest evidenced by the \"mortgage\" document; generally, the two are assigned together, but if they are split traditionally the holder of the note and not the mortgage has the right to foreclose.[13] For example, Fannie Mae promulgates a standard form contract Multistate Fixed-Rate Note 3200[14] and also separate security instrument mortgage forms which vary by state.[15]  In Canada, the Canada Mortgage and Housing Corporation (CMHC) is the country's national housing agency, providing mortgage loan insurance, mortgage-backed securities, housing policy and programs, and housing research to Canadians.[16] It was created by the federal government in 1946 to address the country's post-war housing shortage, and to help Canadians achieve their homeownership goals.  The most common mortgage in Canada is the five-year fixed-rate closed mortgage, as opposed to the U.S. where the most common type is the 30-year fixed-rate open mortgage.[17] Throughout the financial crisis and the ensuing recession, Canada's mortgage market continued to function well, partly due to the residential mortgage market's policy framework, which includes an effective regulatory and supervisory regime that applies to most lenders. Since the crisis, however, the low interest rate environment that has arisen has contributed to a significant increase in mortgage debt in the country.[18]  In April 2014, the Office of the Superintendent of Financial Institutions (OSFI) released guidelines for mortgage insurance providers aimed at tightening standards around underwriting and risk management. In a statement, the OSFI has stated that the guideline will \"provide clarity about best practices in respect of residential mortgage insurance underwriting, which contribute to a stable financial system.\" This comes after several years of federal government scrutiny over the CMHC, with former Finance Minister Jim Flaherty musing publicly as far back as 2012 about privatizing the Crown corporation.[19]  In an attempt to cool down the real estate prices in Canada, Ottawa introduced a mortgage stress test effective 17 October 2016.[20] Under the stress test, every home buyer who wants to get a mortgage from any federally regulated lender should undergo a test in which the borrower's affordability is judged based on a rate that is not lower than a stress rate set by the Bank of Canada. For high-ratio mortgage (loan to value of more than 80%), which is insured by Canada Mortgage and Housing Corporation, the rate is the maximum of the stress test rate and the current target rate. However, for uninsured mortgage, the rate is the maximum of the stress test rate and the target interest rate plus 2%.[21] This stress test has lowered the maximum mortgage approved amount for all borrowers in Canada.  The stress-test rate consistently increased until its peak of 5.34% in May 2018 and it was not changed until July 2019 in which for the first time in three years it decreased to 5.19%.[22] This decision may reflect the push-back from the real-estate industry[23] and the introduction of the first-time home buyer incentive program (FTHBI) by the Canadian government in the 2019 Canadian federal budget. Because of all the criticisms from real estate industry, Canada finance minister Bill Morneau ordered to review and consider changes to the mortgage stress test in December 2019.[24]  The mortgage industry of the United Kingdom has traditionally been dominated by building societies, but from the 1970s the share of the new mortgage loans market held by building societies has declined substantially. Between 1977 and 1987, the share fell from 96% to 66% while that of banks and other institutions rose from 3% to 36%. (The figures have since changed further in favour of banks in part due to demutualisation.) There are currently over 200 significant separate financial organizations supplying mortgage loans to house buyers in Britain. The major lenders include building societies, banks, specialized mortgage corporations, insurance companies, and pension funds.  In the UK variable-rate mortgages are more common than in the United States.[25][26] This is in part because mortgage loan financing relies less on fixed income securitized assets (such as mortgage-backed securities) than in the United States, Denmark, and Germany, and more on retail savings deposits like Australia and Spain.[25][26] Thus, lenders prefer variable-rate mortgages to fixed rate ones and whole-of-term fixed rate mortgages are generally not available. Nevertheless, in recent years fixing the rate of the mortgage for short periods has become popular and the initial two, three, five and, occasionally, ten years of a mortgage can be fixed.[27] From 2007 to the beginning of 2013 between 50% and 83% of new mortgages had initial periods fixed in this way.[28]  Home ownership rates are comparable to the United States, but overall default rates are lower.[25] Prepayment penalties during a fixed rate period are common, whilst the United States has discouraged their use.[25] Like other European countries and the rest of the world, but unlike most of the United States, mortgages loans are usually not nonrecourse debt, meaning debtors are liable for any loan deficiencies after foreclosure.[25][29]  The customer-facing aspects of the residential mortgage sector are regulated by the Financial Conduct Authority (FCA), and lenders' financial probity is overseen by a separate regulator, the Prudential Regulation Authority (PRA) which is part of the Bank of England. The FCA and PRA were established in 2013 with the aim of responding to criticism of regulatory failings highlighted by the financial crisis of 2007–2008 and its aftermath.[30][31][32]  Western European countries present a diversified landscape, with some countries (France, Belgium, Germany, the Netherlands, Denmark) where fixed-rate mortgages are the norm and some countries (Austria, Greece, Italy, Portugal, Spain, Sweden) favouring adjustable-rate mortgages.[25][26][33] Much of Europe has home ownership rates comparable to the United States, but overall default rates are lower in Europe than in the United States.[25] Mortgage loan financing relies less on securitizing mortgages and more on formal government guarantees backed by covered bonds (such as the Pfandbriefe) and deposits, except Denmark and Germany where asset-backed securities are also common.[25][26] Prepayment penalties are still common, whilst the United States has discouraged their use.[25] Unlike much of the United States, mortgage loans are usually not nonrecourse debt.[25]  Within the European Union, covered bonds market volume (covered bonds outstanding) amounted to about €2 trillion at year-end 2007 with Germany, Denmark, Spain, and France each having outstandings above €200 billion.[34] Pfandbrief-like securities have been introduced in more than 25 European countries—and in recent years also in the U.S. and other countries outside Europe—each with their own unique law and regulations.[35]  On July 28, 2008, US Treasury Secretary Henry Paulson announced that, along with four large U.S. banks, the Treasury would attempt to kick start a market for these securities in the United States, primarily to provide an alternative form of mortgage-backed securities.[36] Similarly, in the UK \"the Government is inviting views on options for a UK framework to deliver more affordable long-term fixed-rate mortgages, including the lessons to be learned from international markets and institutions\".[37]  George Soros's October 10, 2008 The Wall Street Journal editorial promoted the Danish mortgage market model.[38]  Mortgages in Malaysia can be categorised into two different groups: conventional home loan and Islamic home loan. Under the conventional home loan, banks normally charge a fixed interest rate, a variable interest rate, or both. These interest rates are tied to a base rate (individual bank's benchmark rate).  For Islamic home financing, it follows the Sharia Law and comes in 2 common types: Bai’ Bithaman Ajil (BBA) or Musharakah Mutanaqisah (MM). Bai' Bithaman Ajil is when the bank buys the property at current market price and sells it back to you at a much higher price. Musharakah Mutanaqisah is when the bank buys the property together with you. You will then slowly buy the bank's portion of the property through rental (whereby a portion of the rental goes to paying for the purchase of a part of the bank's share in the property until the property comes to your complete ownership).  Islamic Sharia law prohibits the payment or receipt of interest, meaning that Muslims cannot use conventional mortgages. The Islamic mortgage loan cancels any form of interest because of doctrines, so in the mortgage loan process, the lender and the borrower are more like a capital-shared partnership than a debt relationship.[39] However, real estate is far too expensive for most people to buy outright using cash: Islamic mortgages solve this problem by having the property change hands twice. In one variation, the bank will buy the house outright and then act as a landlord. The homebuyer, in addition to paying rent, will pay a contribution towards the purchase of the property. When the last payment is made, the property changes hands.[clarification needed]  Typically, this may lead to a higher final price for the buyers. This is because in some countries (such as the United Kingdom and India) there is a stamp duty which is a tax charged by the government on a change of ownership. Because ownership changes twice in an Islamic mortgage, a stamp tax may be charged twice. Many other jurisdictions have similar transaction taxes on change of ownership which may be levied. In the United Kingdom, the dual application of stamp duty in such transactions was removed in the Finance Act 2003 in order to facilitate Islamic mortgages.[40]  An alternative scheme involves the bank reselling the property according to an installment plan, at a price higher than the original price.  Both of these methods compensate the lender as if they were charging interest, but the loans are structured in a way that in name they are not, and the lender shares the financial risks involved in the transaction with the homebuyer.[citation needed]  Mortgage insurance is an insurance policy designed to protect the mortgagee (lender) from any default by the mortgagor (borrower). It is used commonly in loans with a loan-to-value ratio over 80%, and employed in the event of foreclosure and repossession.  This policy is typically paid for by the borrower as a component to final nominal (note) rate, or in one lump sum up front, or as a separate and itemized component of monthly mortgage payment. In the last case, mortgage insurance can be dropped when the lender informs the borrower, or its subsequent assigns, that the property has appreciated, the loan has been paid down, or any combination of both to relegate the loan-to-value under 80%.  In the event of repossession, banks, investors, etc. must resort to selling the property to recoup their original investment (the money lent) and are able to dispose of hard assets (such as real estate) more quickly by reductions in price. Therefore, the mortgage insurance acts as a hedge should the repossessing authority recover less than full and fair market value for any hard asset. "},"meta":{},"created_at":"2025-03-22T14:25:42.275677Z","updated_at":"2025-03-22T14:25:42.275677Z","inner_id":26,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":35,"annotations":[{"id":35,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.310403Z","updated_at":"2025-03-22T14:25:42.310403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"16e26737-2100-46cf-9df2-5c765712361b","import_id":null,"last_action":null,"bulk_created":false,"task":35,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"International finance (also referred to as international monetary economics or international macroeconomics) is the branch of monetary and macroeconomic interrelations between two or more countries.[1][2] International finance examines the dynamics of the global financial system, international monetary systems, balance of payments, exchange rates, foreign direct investment, and how these topics relate to international trade.[1][2][3]  Sometimes referred to as multinational finance, international finance is additionally concerned with matters of international financial management. Investors and multinational corporations must assess and manage international risks such as political risk and foreign exchange risk, including transaction exposure, economic exposure, and translation exposure.[4][5]  Some examples of key concepts within international finance are the Mundell–Fleming model, the optimum currency area theory, purchasing power parity, interest rate parity, and the international Fisher effect. Whereas the study of international trade makes use of mostly microeconomic concepts, international finance research investigates predominantly macroeconomic concepts.  The foreign exchange and political risk dimensions of international finance largely stem from sovereign nations having the right and power to issue currencies, formulate their own economic policies, impose taxes, and regulate movement of people, goods, and capital across their borders.[6]  The idea of fiat currency was established just over a thousand years ago in China during the Yuan, Tang, Song and Ming dynasties. In the Tang dynasty (618–907) there was a high demand for metallic currency that exceeded the supply of precious metals. The people were already familiar with the use of credit notes, and they rapidly began accepting pieces of paper or paper drafts.[7]  A shortage of coins forced these people to change from coins to notes. During the Song dynasty (960–1276), there was a booming business in the Sichuan region that led to a shortage of copper money. This led to traders issuing private notes covered by a monetary reserve. This was considered to be the first ever legal tender. Paper money became the only legal tender in the Yuan dynasty (1276–1367) and issuing of notes was conferred to the Ministry of Finance during the Ming dynasty (1368–1644).[7] Fiat money can serve as a good currency if it can handle the role that a nation's economy needs of its monetary unit: storing value, providing a numerical account, and facilitating exchange. It also has excellent seigniorage, meaning it is more cost-efficient than a currency directly tied to produce than a currency directly tied to a commodity.[8]  On the International stage fiat currencies were not truly relevant until the US removed its currency from the gold standard in 1971.[8] At this point other nations followed suit creating an environment where an infinite amount of money could be created. Before this, a nation's currency—which was unaccredited by precious metals—would not be accepted in exchange for goods and services outside of the host country where it was produced.[9]  The Establishment of the International Monetary Fund (IMF) and the World Bank are one of the most significant turning points in the History of international finance. Through Decades of negotiation between international powers and the persistence of economic superpowers no single event inspired unity of determining the fair rules of trade and monetary policy than the Second World War. In Bretton Woods, New Hampshire, delegates from 44 nations gathered to determine what would be the rules for international trade after the war.[10]  After the Bretton Woods Conference was completed the framework for the IMF and World Bank were laid out and begun to be developed. As a result, international trade skyrocketed since exchange between countries and between continents finally had a measurable way to determine exchange rates and fair value of currency.[11]  Individual countries' banks were no longer the determining factor in determining a fair exchange rate, removing inconsistencies between individual countries' monetary systems.[12]  The Bretton Woods system did not last very long, as after WW2 the United States was the physical owner of most of the world's gold supply. This meant countries' currencies were supposed to be pegged to a resource over which the US had a near monopoly. This state of affairs only lasted around 20 years as most notably in 1971 the French who were skeptical of the US dollar being the world's reserve currency reclaimed most of their gold that they exported to the US for protection.[13] This action was inherently a destabilizing force to the US dollar since at any time before this individuals or businesses were able to exchange their US dollars for gold. Many other nations followed suit in a metaphorical \"Gold Rush\" to get gold from the US by exchanging dollars. The result of this action was the world's reserve currency, the US dollar, no longer being pegged to gold from 1971, with Richard Nixon removing the convertibility factor of the US dollar. This fundamentally changed international finance as no longer was the world's currency based on anything physical, it transitioned into a fiat currency (money without intrinsic value that is used as money because of government decree).[14] "},"meta":{},"created_at":"2025-03-22T14:25:42.275677Z","updated_at":"2025-03-22T14:25:42.275677Z","inner_id":27,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":36,"annotations":[{"id":36,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.311403Z","updated_at":"2025-03-22T14:25:42.311403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"4784b242-332c-4be3-86e9-024430d5d711","import_id":null,"last_action":null,"bulk_created":false,"task":36,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"    Empirical methods  Prescriptive and policy  A stock market, equity market, or share market is the aggregation of buyers and sellers of stocks (also called shares), which represent ownership claims on businesses; these may include securities listed on a public stock exchange as well as stock that is only traded privately, such as shares of private companies that are sold to investors through equity crowdfunding platforms. Investments are usually made with an investment strategy in mind.  The total market capitalization of all publicly traded stocks worldwide rose from US$2.5 trillion in 1980 to US$111 trillion by the end of 2023.[1]  As of 2016[update], there are 60 stock exchanges in the world. Of these, there are 16 exchanges with a market capitalization of $1 trillion or more, and they account for 87% of global market capitalization. Apart from the Australian Securities Exchange, these 16 exchanges are all in North America, Europe, or Asia.[2]  By country, the largest stock markets as of January 2022 are in the United States of America (about 59.9%), followed by Japan (about 6.2%) and United Kingdom (about 3.9%).[3]  A stock exchange is an exchange (or bourse) where stockbrokers and traders can buy and sell shares (equity stock), bonds, and other securities. Many large companies have their stocks listed on a stock exchange. This makes the stock more liquid and thus more attractive to many investors. The exchange may also act as a guarantor of settlement. These and other stocks may also be traded \"over the counter\" (OTC), that is, through a dealer. Some large companies will have their stock listed on more than one exchange in different countries, so as to attract international investors.[4]  Stock exchanges may also cover other types of securities, such as fixed-interest securities (bonds) or (less frequently) derivatives, which are more likely to be traded OTC.  Trade in stock markets means the transfer (in exchange for money) of a stock or security from a seller to a buyer. This requires these two parties to agree on a price. Equities (stocks or shares) confer an ownership interest in a particular company.  Participants in the stock market range from small individual stock investors to larger investors, who can be based anywhere in the world, and may include banks, insurance companies, pension funds and hedge funds. Their buy or sell orders may be executed on their behalf by a stock exchange trader.  Some exchanges are physical locations where transactions are carried out on a trading floor, by a method known as open outcry. This method is used in some stock exchanges and commodities exchanges, and involves traders shouting bid and offer prices. The other type of stock exchange has a network of computers where trades are made electronically. An example of such an exchange is the NASDAQ.  A potential buyer bids a specific price for a stock, and a potential seller asks a specific price for the same stock. Buying or selling at the Market means you will accept any ask price or bid price for the stock. When the bid and ask prices match, a sale takes place, on a first-come, first-served basis if there are multiple bidders at a given price.  The purpose of a stock exchange is to facilitate the exchange of securities between buyers and sellers, thus providing a marketplace. The exchanges provide real-time trading information on the listed securities, facilitating price discovery.  The New York Stock Exchange (NYSE) is a physical exchange, with a hybrid market for placing orders electronically from any location as well as on the trading floor. Orders executed on the trading floor enter by way of exchange members and flow down to a floor broker, who submits the order electronically to the floor trading post for the Designated market maker (\"DMM\") for that stock to trade the order. The DMM's job is to maintain a two-sided market, making orders to buy and sell the security when there are no other buyers or sellers. If a bid–ask spread exists, no trade immediately takes place – in this case, the DMM may use their own resources (money or stock) to close the difference. Once a trade has been made, the details are reported on the \"tape\" and sent back to the brokerage firm, which then notifies the investor who placed the order. Computers play an important role, especially for program trading.  The NASDAQ is an electronic exchange, where all of the trading is done over a computer network. The process is similar to the New York Stock Exchange. One or more NASDAQ market makers will always provide a bid and ask the price at which they will always purchase or sell 'their' stock.  The Paris Bourse, now part of Euronext, is an order-driven, electronic stock exchange. It was automated in the late 1980s. Prior to the 1980s, it consisted of an open outcry exchange. Stockbrokers met on the trading floor of the Palais Brongniart. In 1986, the CATS trading system was introduced, and the order matching system was fully automated.  People trading stock will prefer to trade on the most popular exchange since this gives the largest number of potential counter parties (buyers for a seller, sellers for a buyer) and probably the best price. However, there have always been alternatives such as brokers trying to bring parties together to trade outside the exchange. Some third markets that were popular are Instinet, and later Island and Archipelago (the latter two have since been acquired by Nasdaq and NYSE, respectively). One advantage is that this avoids the commissions of the exchange. However, it also has problems such as adverse selection.[5] Financial regulators have probed dark pools.[6][7]  Market participants include individual retail investors, institutional investors (e.g., pension funds, insurance companies, mutual funds, index funds, exchange-traded funds, hedge funds, investor groups, banks and various other financial institutions), and also publicly traded corporations trading in their own shares. Robo-advisors, which automate investment for individuals are also major participants.  In 2021, the value of world stock markets experienced an increase of 26.5%, amounting to US$22.3 trillion. Developing economies contributed US$9.9 trillion and developed economies US$12.4 trillion. Asia and Oceania accounted for 45%, Europe had 37%, and America had 16%, while Africa had 2% of the global market.[8]  Factors such as high trading prices, market ratings, information about stock exchange dynamics, and financial institutions can influence individual and corporate participation in stock markets. Additionally, the appeal of stock ownership, driven by the potential for higher returns compared to other financial instruments, plays a crucial role in attracting individuals to invest in the stock market.  Regional and country-specific factors can also impact stock market participation rates. For example, in the United States, stock market participation rates vary widely across states, with regional factors potentially influencing these disparities. It is noted that individual participation costs alone cannot explain such large differences in participation rates from state to state, indicating the presence of other regional factors at play.[9]  Behavioral factors are recognized as significant influences on stock market participation, as evidenced by the low participation rates observed in the Ghanaian stock market.[10]  Factors such as factor endowments, geography, political stability, liberal trade policies, foreign direct investment inflows, and domestic industrial capacity are also identified as important in determining participation.[11]  Indirect investment involves owning shares indirectly, such as via a mutual fund or an exchange traded fund. Direct investment involves direct ownership of shares.[12]  Direct ownership of stock by individuals rose slightly from 17.8% in 1992 to 17.9% in 2007, with the median value of these holdings rising from $14,778 to $17,000.[13][14] Indirect participation in the form of retirement accounts rose from 39.3% in 1992 to 52.6% in 2007, with the median value of these accounts more than doubling from $22,000 to $45,000 in that time.[13][14] Rydqvist, Spizman, and Strebulaev attribute the differential growth in direct and indirect holdings to differences in the way each are taxed in the United States. Investments in pension funds and 401ks, the two most common vehicles of indirect participation, are taxed only when funds are withdrawn from the accounts. Conversely, the money used to directly purchase stock is subject to taxation as are any dividends or capital gains they generate for the holder. In this way, the current tax code incentivizes individuals to invest indirectly.[15]  Rates of participation and the value of holdings differ significantly across strata of income. In the bottom quintile of income, 5.5% of households directly own stock and 10.7% hold stocks indirectly in the form of retirement accounts.[14] The top decile of income has a direct participation rate of 47.5% and an indirect participation rate in the form of retirement accounts of 89.6%.[14] The median value of directly owned stock in the bottom quintile of income is $4,000 and is $78,600 in the top decile of income as of 2007.[16] The median value of indirectly held stock in the form of retirement accounts for the same two groups in the same year is $6,300 and $214,800 respectively.[16] Since the Great Recession of 2008 households in the bottom half of the income distribution have lessened their participation rate both directly and indirectly from 53.2% in 2007 to 48.8% in 2013, while over the same period households in the top decile of the income distribution slightly increased participation 91.7% to 92.1%.[17] The mean value of direct and indirect holdings at the bottom half of the income distribution moved slightly downward from $53,800 in 2007 to $53,600 in 2013.[17] In the top decile, mean value of all holdings fell from $982,000 to $969,300 in the same time.[17] The mean value of all stock holdings across the entire income distribution is valued at $269,900 as of 2013.[17]  The racial composition of stock market ownership shows households headed by whites are nearly four and six times as likely to directly own stocks than households headed by blacks and Hispanics respectively. As of 2011 the national rate of direct participation was 19.6%, for white households the participation rate was 24.5%, for black households it was 6.4% and for Hispanic households it was 4.3%. Indirect participation in the form of 401k ownership shows a similar pattern with a national participation rate of 42.1%, a rate of 46.4% for white households, 31.7% for black households, and 25.8% for Hispanic households. Households headed by married couples participated at rates above the national averages with 25.6% participating directly and 53.4% participating indirectly through a retirement account. 14.7% of households headed by men participated in the market directly and 33.4% owned stock through a retirement account. 12.6% of female-headed households directly owned stock and 28.7% owned stock indirectly.[14]  In a 2003 paper by Vissing-Jørgensen attempts to explain disproportionate rates of participation along wealth and income groups as a function of fixed costs associated with investing. Her research concludes that a fixed cost of $200 per year is sufficient to explain why nearly half of all U.S. households do not participate in the market.[18] Participation rates have been shown to strongly correlate with education levels, promoting the hypothesis that information and transaction costs of market participation are better absorbed by more educated households. Behavioral economists Harrison Hong, Jeffrey Kubik and Jeremy Stein suggest that sociability and participation rates of communities have a statistically significant impact on an individual's decision to participate in the market. Their research indicates that social individuals living in states with higher than average participation rates are 5% more likely to participate than individuals that do not share those characteristics.[19] This phenomenon also explained in cost terms. Knowledge of market functioning diffuses through communities and consequently lowers transaction costs associated with investing.  In 12th-century France, the courtiers de change were concerned with managing and regulating the debts of agricultural communities on behalf of the banks. Because these men also traded with debts, they could be called the first brokers. The Italian historian Lodovico Guicciardini described how, in late 13th-century Bruges, commodity traders gathered outdoors at a market square containing an inn owned by a family called Van der Beurze, and in 1409 they became the \"Brugse Beurse\", institutionalizing what had been, until then, an informal meeting.[20] The idea quickly spread around Flanders and neighboring countries and \"Beurzen\" soon opened in Ghent and Rotterdam. International traders, and specially the Italian bankers, present in Bruges since the early 13th-century, took back the word in their countries to define the place for stock market exchange: first the Italians (Borsa), but soon also the French (Bourse), the Germans (börse), Russians (birža), Czechs (burza), Swedes (börs), Danes and Norwegians (børs). In most languages, the word coincides with that for money bag, dating back to the Latin bursa, from which obviously also derives the name of the Van der Beurse family.  In the middle of the 13th century, Venetian bankers began to trade in government securities. In 1351 the Venetian government outlawed spreading rumors intended to lower the price of government funds. Bankers in Pisa, Verona, Genoa and Florence also began trading in government securities during the 14th century. This was only possible because these were independent city-states not ruled by a duke but a council of influential citizens. Italian companies were also the first to issue shares. Companies in England and the Low Countries followed in the 16th century. Around this time, a joint stock company—one whose stock is owned jointly by the shareholders—emerged and became important for the colonization of what Europeans called the \"New World\".[21]  There are now stock markets in virtually every developed and most developing economies, with the world's largest markets being in the United States, United Kingdom, Japan, India, China, Canada, Germany, France, South Korea and the Netherlands.[22]  Even in the days before perestroika, socialism was never a monolith. Within the Communist countries, the spectrum of socialism ranged from the quasi-market, quasi-syndicalist system of Yugoslavia to the centralized totalitarianism of neighboring Albania. One time I asked Professor von Mises, the great expert on the economics of socialism, at what point on this spectrum of statism would he designate a country as \"socialist\" or not. At that time, I wasn't sure that any definite criterion existed to make that sort of clear-cut judgment. And so I was pleasantly surprised at the clarity and decisiveness of Mises's answer. \"A stock market,\" he answered promptly. \"A stock market is crucial to the existence of capitalism and private property. For it means that there is a functioning market in the exchange of private titles to the means of production. There can be no genuine private ownership of capital without a stock market: there can be no true socialism if such a market is allowed to exist.\" The stock market is one of the most important ways for companies to raise money, along with debt markets which are generally more imposing but do not trade publicly.[24] This allows businesses to be publicly traded, and raise additional financial capital for expansion by selling shares of ownership of the company in a public market. The liquidity that an exchange affords the investors enables their holders to quickly and easily sell securities. This is an attractive feature of investing in stocks, compared to other less liquid investments such as property and other immoveable assets.  History has shown that the price of stocks and other assets is an important part of the dynamics of economic activity, and can influence or be an indicator of social mood. An economy where the stock market is on the rise is considered to be an up-and-coming economy. The stock market is often considered the primary indicator of a country's economic strength and development.[25]  Rising share prices, for instance, tend to be associated with increased business investment and vice versa. Share prices also affect the wealth of households and their consumption. Therefore, central banks tend to keep an eye on the control and behavior of the stock market and, in general, on the smooth operation of financial system functions. Financial stability is the raison d'être of central banks.[26]  Exchanges also act as the clearinghouse for each transaction, meaning that they collect and deliver the shares, and guarantee payment to the seller of a security. This eliminates the risk to an individual buyer or seller that the counterparty could default on the transaction.[27]  The smooth functioning of all these activities facilitates economic growth in that lower costs and enterprise risks promote the production of goods and services as well as possibly employment. In this way the financial system is assumed to contribute to increased prosperity, although some controversy exists as to whether the optimal financial system is bank-based or market-based.[28]  Events such as the 2007–2008 financial crisis have prompted a heightened degree of scrutiny of the impact of the structure of stock markets[29][30] (called market microstructure), in particular to the stability of the financial system and the transmission of systemic risk.[31]  A transformation is the move to electronic trading to replace human trading of listed securities.[30]  Changes in stock prices are mostly caused by external factors such as socioeconomic conditions, inflation, exchange rates. Intellectual capital does not affect a company stock's current earnings. Intellectual capital contributes to a stock's return growth.[32]  The efficient-market hypothesis (EMH) is a hypothesis in financial economics that states that asset prices reflect all available information at the current time.  The 'hard' efficient-market hypothesis does not explain the cause of events such as the crash in 1987, when the Dow Jones Industrial Average plummeted 22.6 percent—the largest-ever one-day fall in the United States.[33]  This event demonstrated that share prices can fall dramatically even though no generally agreed upon definite cause has been found: a thorough search failed to detect any 'reasonable' development that might have accounted for the crash. (Such events are predicted to occur strictly by randomness, although very rarely.) It seems also to be true more generally that many price movements (beyond those which are predicted to occur 'randomly') are not occasioned by new information; a study of the fifty largest one-day share price movements in the United States in the post-war period seems to confirm this.[33]  A 'soft' EMH has emerged which does not require that prices remain at or near equilibrium, but only that market participants cannot systematically profit from any momentary 'market anomaly'. Moreover, while EMH predicts that all price movement (in the absence of change in fundamental information) is random (i.e. non-trending)[dubious – discuss],[34] many studies have shown a marked tendency for the stock market to trend over time periods of weeks or longer. Various explanations for such large and apparently non-random price movements have been promulgated. For instance, some research has shown that changes in estimated risk, and the use of certain strategies, such as stop-loss limits and value at risk limits, theoretically could cause financial markets to overreact. But the best explanation seems to be that the distribution of stock market prices is non-Gaussian[35] (in which case EMH, in any of its current forms, would not be strictly applicable).[36][37]  Other research has shown that psychological factors may result in exaggerated (statistically anomalous) stock price movements (contrary to EMH which assumes such behaviors 'cancel out'). Psychological research has demonstrated that people are predisposed to 'seeing' patterns, and often will perceive a pattern in what is, in fact, just noise, e.g. seeing familiar shapes in clouds or ink blots. In the present context, this means that a succession of good news items about a company may lead investors to overreact positively, driving the price up. A period of good returns also boosts the investors' self-confidence, reducing their (psychological) risk threshold.[38]  Another phenomenon—also from psychology—that works against an objective assessment is group thinking. As social animals, it is not easy to stick to an opinion that differs markedly from that of a majority of the group. An example with which one may be familiar is the reluctance to enter a restaurant that is empty; people generally prefer to have their opinion validated by those of others in the group.  In one paper the authors draw an analogy with gambling.[39] In normal times the market behaves like a game of roulette; the probabilities are known and largely independent of the investment decisions of the different players. In times of market stress, however, the game becomes more like poker (herding behavior takes over). The players now must give heavy weight to the psychology of other investors and how they are likely to react psychologically.[40]  Stock markets play an essential role in growing industries that ultimately affect the economy through transferring available funds from units that have excess funds (savings) to those who are suffering from funds deficit (borrowings) (Padhi and Naik, 2012). In other words, capital markets facilitate funds movement between the above-mentioned units. This process leads to the enhancement of available financial resources which in turn affects the economic growth positively.  Economic and financial theories argue that stock prices are affected by macroeconomic trends. Macroeconomic trends include such as changes in GDP, unemployment rates, national income, price indices, output, consumption, unemployment, inflation, saving, investment, energy, international trade, immigration, productivity, aging populations, innovations, international finance.[41] increasing corporate profit, increasing profit margins, higher concentration of business, lower company income, less vigorous activity, less progress, lower investment rates, lower productivity growth, less employee share of corporate revenues,[42] decreasing Worker to Beneficiary ratio (year 1960 5:1, year 2009 3:1, year 2030 2.2:1),[43] increasing female to male ratio college graduates.[44]  Sometimes, the market seems to react irrationally to economic or financial news, even if that news is likely to have no real effect on the fundamental value of securities itself.[45] However, this market behaviour may be more apparent than real, since often such news was anticipated, and a counter reaction may occur if the news is better (or worse) than expected. Therefore, the stock market may be swayed in either direction by press releases, rumors, euphoria and mass panic.  Over the short-term, stocks and other securities can be battered or bought by any number of fast market-changing events, making the stock market behavior difficult to predict. Emotions can drive prices up and down, people are generally not as rational as they think, and the reasons for buying and selling are generally accepted.  Behaviorists argue that investors often behave irrationally when making investment decisions thereby incorrectly pricing securities, which causes market inefficiencies, which, in turn, are opportunities to make money.[46] However, the whole notion of EMH is that these non-rational reactions to information cancel out, leaving the prices of stocks rationally determined.  A stock market crash is often defined as a sharp dip in share prices of stocks listed on the stock exchanges. In parallel with various economic factors, a reason for stock market crashes is also due to panic and investing public's loss of confidence. Often, stock market crashes end speculative economic bubbles.  There have been famous stock market crashes that have ended in the loss of billions of dollars and wealth destruction on a massive scale. An increasing number of people are involved in the stock market, especially since the social security and retirement plans are being increasingly privatized and linked to stocks and bonds and other elements of the market. There have been a number of famous stock market crashes like the Wall Street Crash of 1929, the stock market crash of 1973–4, the Black Monday of 1987, the Dot-com bubble of 2000, and the Stock Market Crash of 2008.  One of the most famous stock market crashes started October 24, 1929, on Black Thursday. The Dow Jones Industrial Average lost 50% during this stock market crash. It was the beginning of the Great Depression.  Another famous crash took place on October 19, 1987 – Black Monday. The crash began in Hong Kong and quickly spread around the world.  By the end of October, stock markets in Hong Kong had fallen 45.5%, Australia 41.8%, Spain 31%, the United Kingdom 26.4%, the United States 22.68%, and Canada 22.5%. Black Monday itself was the largest one-day percentage decline in stock market history – the Dow Jones fell by 22.6% in a day. The names \"Black Monday\" and \"Black Tuesday\" are also used for October 28–29, 1929, which followed Terrible Thursday—the starting day of the stock market crash in 1929.  The crash in 1987 raised some puzzles – main news and events did not predict the catastrophe and visible reasons for the collapse were not identified. This event raised questions about many important assumptions of modern economics, namely, the theory of rational human conduct, the theory of market equilibrium and the efficient-market hypothesis. For some time after the crash, trading in stock exchanges worldwide was halted, since the exchange computers did not perform well owing to enormous quantity of trades being received at one time. This halt in trading allowed the Federal Reserve System and central banks of other countries to take measures to control the spreading of worldwide financial crisis. In the United States the SEC introduced several new measures of control into the stock market in an attempt to prevent a re-occurrence of the events of Black Monday.  This marked the beginning of the Great Recession. Starting in 2007 and lasting through 2009, financial markets experienced one of the sharpest declines in decades. It was more widespread than just the stock market as well. The housing market, lending market, and even global trade experienced unimaginable decline. Sub-prime lending led to the housing bubble bursting and was made famous by movies like The Big Short where those holding large mortgages were unwittingly falling prey to lenders. This saw banks and major financial institutions completely fail in many cases and took major government intervention to remedy during the period. From October 2007 to March 2009, the S&P 500 fell 57% and wouldn't recover to its 2007 levels until April 2013.  The 2020 stock market crash was a major and sudden global stock market crash that began on 20 February 2020 and ended on 7 April. This market crash was due to the sudden outbreak of the global pandemic, COVID-19. The crash ended with a new deal that had a positive impact on the market.[48]  Since the early 1990s, many of the largest exchanges have adopted electronic 'matching engines' to bring together buyers and sellers, replacing the open outcry system. Electronic trading now accounts for the majority of trading in many developed countries. Computer systems were upgraded in the stock exchanges to handle larger trading volumes in a more accurate and controlled manner. The SEC modified the margin requirements in an attempt to lower the volatility of common stocks, stock options and the futures market. The New York Stock Exchange and the Chicago Mercantile Exchange introduced the concept of a circuit breaker. The circuit breaker halts trading if the Dow declines a prescribed number of points for a prescribed amount of time. In February 2012, the Investment Industry Regulatory Organization of Canada (IIROC) introduced single-stock circuit breakers.[49]  The movements of the prices in global, regional or local markets are captured in price indices called stock market indices, of which there are many, e.g. the S&P, the FTSE, the Euronext indices and the NIFTY & SENSEX of India. Such indices are usually market capitalization weighted, with the weights reflecting the contribution of the stock to the index. The constituents of the index are reviewed frequently to include\/exclude stocks in order to reflect the changing business environment.  Financial innovation has brought many new financial instruments whose pay-offs or values depend on the prices of stocks. Some examples are exchange-traded funds (ETFs), stock index and stock options, equity swaps, single-stock futures, and stock index futures. These last two may be traded on futures exchanges (which are distinct from stock exchanges—their history traces back to commodity futures exchanges), or traded over-the-counter. As all of these products are only derived from stocks, they are sometimes considered to be traded in a (hypothetical) derivatives market, rather than the (hypothetical) stock market.  Stock that a trader does not actually own may be traded using short selling; margin buying may be used to purchase stock with borrowed funds; or, derivatives may be used to control large blocks of stocks for a much smaller amount of money than would be required by outright purchase or sales.  In short selling, the trader borrows stock (usually from his brokerage which holds its clients shares or its own shares on account to lend to short sellers) then sells it on the market, betting that the price will fall. The trader eventually buys back the stock, making money if the price fell in the meantime and losing money if it rose. Exiting a short position by buying back the stock is called \"covering\". This strategy may also be used by unscrupulous traders in illiquid or thinly traded markets to artificially lower the price of a stock. Hence most markets either prevent short selling or place restrictions on when and how a short sale can occur. The practice of naked shorting is illegal in most (but not all) stock markets.  In margin buying, the trader borrows money (at interest) to buy a stock and hopes for it to rise. Most industrialized countries have regulations that require that if the borrowing is based on collateral from other stocks the trader owns outright, it can be a maximum of a certain percentage of those other stocks' value. In the United States, the margin requirements have been 50% for many years (that is, if you want to make a $1000 investment, you need to put up $500, and there is often a maintenance margin below the $500).  A margin call is made if the total value of the investor's account cannot support the loss of the trade. (Upon a decline in the value of the margined securities additional funds may be required to maintain the account's equity, and with or without notice the margined security or any others within the account may be sold by the brokerage to protect its loan position. The investor is responsible for any shortfall following such forced sales.)  Regulation of margin requirements (by the Federal Reserve) was implemented after the Crash of 1929. Before that, speculators typically only needed to put up as little as 10 percent (or even less) of the total investment represented by the stocks purchased. Other rules may include the prohibition of free-riding: putting in an order to buy stocks without paying initially (there is normally a three-day grace period for delivery of the stock), but then selling them (before the three-days are up) and using part of the proceeds to make the original payment (assuming that the value of the stocks has not declined in the interim).  Financial markets can be divided into different subtypes:  While the stock market is the marketplace for buying and selling company stocks, the foreign exchange market, also known as forex or FX, is the global marketplace for the purchase and sale of national currencies. It serves several functions, including facilitating currency conversions, managing foreign exchange risk through futures and forwards, and providing a platform for speculative investors to earn a profit on FX trading. The market includes various types of products, such as the spot market, futures market, forward market, swap market, and options market. For example, the spot market involves the immediate buying and selling of currencies, while the forward market allows for the buying and selling of currencies at an agreed exchange rate, with the actual exchange taking place at a future delivery date. The foreign exchange market is needed for facilitating global trade, including investments, the exchange of goods and services, and financial transactions, and it is considered one of the largest markets in the global economy.[52][53]  The electronic trading market refers to the digital marketplace where financial instruments such as stocks, bonds, currencies, commodities, and derivatives are bought and sold through online platforms. This market operates via electronic trading platforms, also known as online trading platforms, which are software applications that enable the trading of financial products over a network, typically through a financial intermediary. Platforms, such as eToro, Plus500, Robinhood, and AvaTrade serve as a digital medium for trading financial instruments and make financial markets more accessible, allowing individual investors to participate in trading without the need for traditional brokers or substantial capital. They also provide features such as real-time market data, stock price analysis, research reports, and news updates, which support decision-making in trading activities.[54]  These platforms often incorporate systems, such as the Martingale Trading System, used in forex trading. Additionally, online trading has evolved to include mobile trading apps, enabling transactions to be conducted remotely via smartphones.[55]  Many strategies can be classified as either fundamental analysis or technical analysis. Fundamental analysis refers to analyzing companies by their financial statements found in SEC filings, business trends, and general economic conditions. Technical analysis studies price actions in markets through the use of charts and quantitative techniques to attempt to forecast price trends based on historical performance, regardless of the company's financial prospects. One example of a technical strategy is the Trend following method, used by John W. Henry and Ed Seykota, which uses price patterns and is also rooted in risk management and diversification.  Additionally, many choose to invest via passive index funds. In this method, one holds a portfolio of the entire stock market or some segment of the stock market (such as the S&P 500 Index or Wilshire 5000). The principal aim of this strategy is to maximize diversification, minimize taxes from realizing gains, and ride the general trend of the stock market to rise.  Responsible investment emphasizes and requires a long-term horizon on the basis of fundamental analysis only, avoiding hazards in the expected return of the investment. Socially responsible investing is another investment preference.  The average annual growth rate of the stock market, as measured by the S&P 500 index, has historically been around 10%.[56] This figure represents the long-term average return and is often cited as a benchmark for assessing the performance of the stock market as a whole.  The market's results from one year to the next may vary substantially from the long-term average. For instance, in 2012–2021, the S&P 500 index had an average annual return of 14.8%.[57] However, individual annual returns can fluctuate widely, with some years experiencing negative growth and others seeing substantial gains.  While the average stock market return is around 10% per year, there is also the impact of inflation, resulting in investors' losing purchasing power of 2% to 3% every year due to it, which reduces the real rate of return on investments.[58]  Taxation is a consideration of all investment strategies; profit from owning stocks, including dividends received, is subject to different tax rates depending on the type of security and the holding period. Most profit from stock investing is taxed via a capital gains tax. In many countries, the corporations pay taxes to the government and the shareholders once again pay taxes when they profit from owning the stock, known as \"double taxation\".  The Indian stock exchanges, Bombay Stock Exchange and National Stock Exchange of India, have been rocked by several high-profile corruption scandals.[59][60] At times, the Securities and Exchange Board of India (SEBI) has barred various individuals and entities from trading on the exchanges for stock manipulation, especially in illiquid small-cap and penny stocks.[61][62][63] "},"meta":{},"created_at":"2025-03-22T14:25:42.275677Z","updated_at":"2025-03-22T14:25:42.275677Z","inner_id":28,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":37,"annotations":[{"id":37,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.311403Z","updated_at":"2025-03-22T14:25:42.311403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"8ed6ab46-54ea-4d95-b844-93a07d281778","import_id":null,"last_action":null,"bulk_created":false,"task":37,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Insider trading is the trading of a public company's stock or other securities (such as bonds or stock options) based on material, nonpublic information about the company.[1] In various countries, some kinds of trading based on insider information is illegal. The rationale for this prohibition of insider trading differs between countries\/regions. Some view it as unfair to other investors in the market who do not have access to the information, as the investor with inside information could potentially make larger profits than an investor (without such information) could make.[2] However, insider trading is also prohibited to prevent the director of a company (the insider) from abusing a company's confidential information for the director's personal gain.[3]  The rules governing insider trading are complex and vary significantly from country to country. The extent of enforcement also varies from one country to another. The definition of insider in one jurisdiction can be broad and may cover not only insiders themselves but also any persons related to them, such as brokers, associates, and even family members. A person who becomes aware of non-public information and trades on that basis may be guilty of a crime.  Trading by specific insiders, such as employees, is commonly permitted as long as it does not rely on material information not available to the general public. Many jurisdictions require that such trading be reported so that the transactions can be monitored. In the United States and several other jurisdictions, trading conducted by corporate officers, key employees, directors, or significant shareholders must be reported to the regulator or publicly disclosed, usually within a few business days of the trade. In these cases, insiders in the United States are required to file Form 4 with the U.S. Securities and Exchange Commission (SEC) when buying or selling shares of their own companies. The authors of one study claim that illegal insider trading raises the cost of capital for securities issuers, thus decreasing overall economic growth.[4] Some economists, such as Henry Manne, argued that insider trading should be allowed and could, in fact, benefit markets.[5]  There has long been \"considerable academic debate\" among business and legal scholars over whether or not insider trading should be illegal.[6] Several arguments against outlawing insider trading have been identified: for example, although insider trading is illegal, most insider trading is never detected by law enforcement, and thus the illegality of insider trading might give the public the potentially misleading impression that \"stock market trading is an unrigged game that anyone can play.\"[6] Some legal analysis has questioned whether insider trading actually harms anyone in the legal sense, since some have questioned whether insider trading causes anyone to suffer an actual \"loss\" and whether anyone who suffers a loss is owed an actual legal duty by the insiders in question.[6] Opponents of political insider trading point to conflict of interests and social distrust.[7]  Rules prohibiting or criminalizing insider trading on material non-public information exist in most jurisdictions around the world (Bhattacharya and Daouk, 2002), but the details and the efforts to enforce them vary considerably. In the United States, Sections 16(b) and 10(b) of the Securities Exchange Act of 1934 directly and indirectly address insider trading. The U.S. Congress enacted this law after the stock market crash of 1929.[8] While the United States is generally viewed as making the most serious efforts to enforce its insider trading laws,[9] the broader scope of the European model legislation provides a stricter framework against illegal insider trading.[10][11] In the European Union and the United Kingdom, all trading on non-public information is, under the rubric of market abuse, subject at a minimum to civil penalties and possible criminal penalties as well.[11] UK's Financial Conduct Authority has the responsibility to investigate and prosecute insider dealing, defined by the Criminal Justice Act 1993.  Financial Action Task Force on Money Laundering (FATF) can apply to domestic politically exposed persons.[12]  In the United States, Canada, Australia, Germany and Romania for mandatory reporting purposes, corporate insiders are defined as a company's officers, directors and any beneficial owners of more than 10% of a class of the company's equity securities. Trades made by these types of insiders in the company's own stock, based on material non-public information, are considered fraudulent since the insiders are violating the fiduciary duty that they owe to the shareholders. The corporate insider, simply by accepting employment, has undertaken a legal obligation to the shareholders to put the shareholders' interests before their own, in matters related to the corporation. When insiders buy or sell based on company-owned information, they are said to be violating their obligation to the shareholders or investors.  For example, illegal insider trading would occur if the chief executive officer of Company A learned (prior to a public announcement) that Company A would be taken over and then bought shares in Company A while knowing that the share price would likely rise. In the United States and many other jurisdictions, \"insiders\" are not just limited to corporate officials and major shareholders where illegal insider trading is concerned but can include any individual who trades shares based on material non-public information in violation of some duty of trust. This duty may be imputed; for example, in many jurisdictions, in cases where a corporate insider \"tips\" a friend about non-public information likely to have an effect on the company's share price, the duty the corporate insider owes the company is now imputed to the friend and the friend violates a duty to the company if he trades on the basis of this information.  Liability for inside trading violations generally cannot be avoided by passing on the information in an \"I scratch your back; you scratch mine\" or quid pro quo arrangement if the person receiving the information knew or should have known that the information was material non-public information. In the United States, at least one court has indicated that the insider who releases the non-public information must have done so for an improper purpose. In the case of a person who receives the insider information (called the \"tippee\"), the tippee must also have been aware that the insider released the information for an improper purpose.[13]  One commentator has argued that if Company A's CEO did not trade on undisclosed takeover news, but instead passed the information on to his brother-in-law who traded on it, illegal insider trading would still have occurred (albeit by proxy, by passing it on to a \"non-insider\" so Company A's CEO would not get his hands dirty).[14]: 589   The misappropriation theory of insider trading is now accepted in U.S. law. It states that anyone who misappropriates material non-public information and trades on that information in any stock may be guilty of insider trading. This can include elucidating material non-public information from an insider with the intention of trading on it or passing it on to someone who will.  This theory constitutes the background for the securities regulation that enforces the insider trading.[15] Disgorgement represents ill-gotten gains (or losses avoided) resulting from individuals violating the securities laws.  In general in the countries where the insider trading is forbidden, the competent Authority seeks disgorgement to ensure that securities law violators do not profit from their illegal activity.  When appropriate, the disgorged funds are returned to the injured investors.  Disgorgements can be ordered in either administrative proceedings or civil actions, and the cases can be settled or litigated. Payment of disgorgement can be either completely or partially waived based on the defendant demonstrating an inability to pay. In settled administrative proceedings, Enforcement may recommend, if appropriate, that the disgorgement be waived. There are several approaches in order to quantify the disgorgement; an innovative procedure based on probability theory was defined by Marcello Minenna by directly analyzing the time periods of the involved transactions in the insider trading.[16]  Proving that someone has been responsible for a trade can be difficult because traders may try to hide behind nominees, offshore companies, and other proxies. The Securities and Exchange Commission (SEC) prosecutes over 50 cases each year, with many being settled administratively out of court. The SEC and several stock exchanges actively monitor trading, looking for suspicious activity.[17][18][19] The SEC does not have criminal enforcement authority but can refer serious matters to the U.S. Attorney's Office for further investigation and prosecution.  In the United States and most non-European jurisdictions, not all trading on non-public information is illegal insider trading.[11] For example, a person in a restaurant who hears the CEO of Company A at the next table tell the CFO that the company's profits will be higher than expected and then buys the stock is not guilty of insider trading—unless he or she had some closer connection to the company or company officers.[20] However, even where the tippee is not himself an insider, where the tippee knows that the information is non-public and the information is paid for, or the tipper receives a benefit for giving it, then in the broader-scope jurisdictions the subsequent trading is illegal.[20][21]  Notwithstanding, information about a tender offer (usually regarding a merger or acquisition) is held to a higher standard. If this type of information is obtained (directly or indirectly) and there is reason to believe it is nonpublic, there is a duty to disclose it or abstain from trading.[22]  In the United States in addition to civil penalties, the trader may also be subject to criminal prosecution for fraud or where SEC regulations have been broken, the U.S. Department of Justice (DOJ) may be called to conduct an independent parallel investigation. If the DOJ finds criminal wrongdoing, the department may file criminal charges.[23]  The advent of the Internet has provided a forum for the commercialisation of trading on insider information. In 2016 a number of dark web sites were identified as marketplaces where such non-public information was bought and sold. At least one such site used bitcoin to avoid currency restrictions and to impede tracking. Such sites also provide a place for soliciting for corporate informants, where non-public information may be used for purposes[24] other than stock trading.[25]  A study of political insider trading found existing regulation including STOCK Act results in conflict of interest and contributes to social distrust.[7] Information asymmetry enjoyed by politicians was found to be high, which does not confirm the predictions of social contract theory.[7] Political insider trading by persons which are not required to report according to STOCK Act was found.[7] Higher insider trading was found when legislature is in session and in periods with higher geopolitical risk.[26]  Legal trades by insiders are common,[8] as employees of publicly traded corporations often have stock or stock options. These trades are made public in the United States through Securities and Exchange Commission filings that are also being made available by academic researchers as structured datasets.[27][28]  U.S. SEC Rule 10b5-1 clarified that the prohibition against insider trading does not require proof that an insider actually used material nonpublic information when conducting a trade; possession of such information alone is sufficient to violate the provision, and the SEC would infer that an insider in possession of material nonpublic information used this information when conducting a trade. However, SEC Rule 10b5-1 also created for insiders an affirmative defense if the insider can demonstrate that the trades conducted on behalf of the insider were conducted as part of a pre-existing contract or written binding plan for trading in the future.[29]  For example, if an insider expects to retire after a specific period of time and, as part of retirement planning, the insider has adopted a written binding plan to sell a specific amount of the company's stock every month for two years, and the insider later comes into possession of material nonpublic information about the company, trades based on the original plan might not constitute prohibited insider trading.  There are very limited laws against \"insider trading\" in the commodities markets if, for no other reason than that the concept of an \"insider\" is not immediately analogous to commodities themselves (corn, wheat, steel, etc.). However, analogous activities such as front running are illegal under US commodity and futures trading laws. For example, a commodity broker can be charged with fraud for receiving a large purchase order from a client (one likely to affect the price of that commodity) and then purchasing that commodity before executing the client's order to benefit from the anticipated price increase.[citation needed]  Some economists and legal scholars (such as Henry Manne, Milton Friedman, Thomas Sowell, Daniel Fischel, and Frank H. Easterbrook) have argued that laws against insider trading should be repealed. They claim that insider trading based on material nonpublic information benefits investors, in general, by more quickly introducing new information into the market.[citation needed]  Friedman, laureate of the Nobel Memorial Prize in Economics, said: \"You want more insider trading, not less. You want to give the people most likely to have knowledge about deficiencies of the company an incentive to make the public aware of that.\" Friedman did not believe that the trader should be required to make his trade known to the public, because the buying or selling pressure itself is information for the market.[14]: 591–7   Others argue that insider trading is a victimless act: a willing buyer and a willing seller agree to trade property that the seller rightfully owns, with no prior contract (according to this view) having been made between the parties to refrain from trading if there is asymmetric information. The Atlantic has described the process as \"arguably the closest thing that modern finance has to a victimless crime\".[30]  Legalization advocates also question why \"trading\" where one party has more information than the other is legal in other markets, such as real estate, but not in the stock market. For example, if a geologist knows there is a high likelihood of the discovery of petroleum under Farmer Smith's land, he may be entitled to make Smith an offer for the land, and buy it, without first telling Farmer Smith of the geological data.[31]  Advocates of legalization make free speech arguments. Punishment for communicating about a development pertinent to the next day's stock price might seem an act of censorship.[32]  Some authors have used these arguments to propose legalizing insider trading on negative information (but not on positive information). Since negative information is often withheld from the market, trading on such information has a higher value for the market than trading on positive information.[33][34]  The US and the UK vary in the way the law is interpreted and applied with regard to insider trading. In the UK, the relevant laws are the Criminal Justice Act 1993, Part V, Schedule 1; the Financial Services and Markets Act 2000, which defines an offence of \"market abuse\";[35] and the European Union Regulation No 596\/2014.[36][37] The principle is that it is illegal to trade on the basis of market-sensitive information that is not generally known.  This is a much broader scope than under U.S. law. The key differences from U.S. law are that no relationship to either the issuer of the security or the tipster is required; all that is required is that the guilty party traded (or caused trading) whilst having inside information, and there is no scienter requirement under UK law.[10][38][39]  Japan enacted its first law against insider trading in 1988. Roderick Seeman said, \"Even today many Japanese do not understand why this is illegal. Indeed, previously it was regarded as common sense to make a profit from your knowledge.\"[40]  In Malta the law follows the European broader scope model.  The relevant statute is the Prevention of Financial Markets Abuse Act of 2005, as amended.[41][42] Earlier acts included the Financial Markets Abuse Act in 2002, and the Insider Dealing and Market Abuse Act of 1994.[43]  The International Organization of Securities Commissions (IOSCO) paper on the \"Objectives and Principles of Securities Regulation\" (updated to 2003)[44] states that the three objectives of good securities market regulation are investor protection, ensuring that markets are fair, efficient and transparent, and reducing systemic risk.  The discussion of these \"Core Principles\" state that \"investor protection\" in this context means \"Investors should be protected from misleading, manipulative or fraudulent practices, including insider trading, front running or trading ahead of customers and the misuse of client assets.\" More than 85 percent of the world's securities and commodities market regulators are members of IOSCO and have signed on to these Core Principles.  The World Bank and International Monetary Fund now use the IOSCO Core Principles in reviewing the financial health of different country's regulatory systems as part of these organization's financial sector assessment program, so laws against insider trading based on non-public information are now expected by the international community. Enforcement of insider trading laws varies widely from country to country, but the vast majority of jurisdictions now outlaw the practice, at least in principle.  Larry Harris claims that differences in the effectiveness with which countries restrict insider trading help to explain the differences in executive compensation among those countries. The US, for example, has much higher CEO salaries than have Japan or Germany, where insider trading is less effectively restrained.[14]: 593   The current Australian legislation arose out of the report of a 1989 parliamentary committee report which recommended removal of the requirement that the trader be 'connected' with the body corporate.[45]  This may have weakened the importance of the fiduciary duty rationale and possibly brought new potential offenders within its ambit.  In Australia if a person possesses inside information and knows, or ought reasonably to know, that the information is not generally available and is materially price sensitive then the insider must not trade.  Nor must she or he procure another to trade and must not tip another.  Information will be considered generally available if it consists of readily observable matter or it has been made known to common investors and a reasonable period for it to be disseminated among such investors has elapsed.  The practice of insider trading is an illegal act under Brazilian law, since it constitutes unfair behavior that threatens the security and equality of legal conditions in the market. Since 2001, the practice is also considered a crime. Law 6,385\/1976,[46] as amended by Law 10,303\/2001,[47] provided for Article 27-D, which typifies the conduct of \"Using relevant information not yet disclosed to the market, of which he is aware and from which he must maintain secrecy, capable of providing, for himself or for others, undue advantage, through trading, on his own behalf or on behalf of a third party, with securities: Penalty - imprisonment, from 1 (one) to 5 (five) years, and a fine of up to 3 (three) times the amount of the illicit advantage obtained as a result of the crime.\"[citation needed]  The first conviction handed down in Brazil for the practice of the offense of \"misuse of privileged information\" occurred in 2011, by federal judge Marcelo Costenaro Cavali, of the Sixth Criminal Court of São Paulo.[48] It is the case of the Sadia-Perdigão merger. The former Director of Finance and Investor Relations, Luiz Gonzaga Murat Júnior, was sentenced to one year and nine months in prison in an open regime, replaceable by community service, and the inability to exercise the position of administrator or fiscal councilor of a publicly traded company for the time he serves his sentence, in addition to a fine of R$349,711.53. The then member of the board of directors Romano Ancelmo Fontana Filho was sentenced to prison for one year and five months in an open regime, also replaceable by community service, in addition to not being able to exercise the position of administrator or fiscal councilor of a publicly-held company. He was also fined R$374,940.52.[citation needed]  In 2008, police uncovered an insider trading conspiracy involving Bay Street and Wall Street lawyer Gil Cornblum who had worked at Sullivan & Cromwell and was working at Dorsey & Whitney, and a former lawyer, Stan Grmovsek, who were found to have gained over $10 million in illegal profits over a 14-year span.[49] Cornblum committed suicide by jumping from a bridge as he was under investigation and shortly before he was to be arrested but before criminal charges were laid against him, one day before his alleged co-conspirator Grmovsek pled guilty.[50][51][52] Grmovsek pleaded guilty to insider trading and was sentenced to 39 months in prison.[53] This was the longest term ever imposed for insider trading in Canada. These crimes were explored in Mark Coakley's 2011 non-fiction book, Tip and Trade.  The majority of shares in China before 2005 were non-tradeable shares that were not sold on the stock exchange publicly but privately. To make shares more accessible, the China Securities Regulation Commission (CSRC) required the companies to convert the non-tradeable shares into tradeable shares. There was a deadline for companies to convert their shares and the deadline was short, and due to this there was a large amount of exchanges, and many of these were conducted based on critical inside information. At the time, insider trading did not lead to prison time. Generally, punishment may include monetary fees or temporary relieving from a position in the company, but prison time is rare. However, in 2015, the Chinese fund manager Xu Xiang was arrested for insider trading, and in 2017, he was sentenced to five and a half years in prison and fined 11 billion yuan.[54]  In 2014, the European Union (EU) adopted legislation (Criminal Sanctions for Market Abuse Directive) that harmonised criminal sanctions for insider dealing. All EU Member States agreed to introduce maximum prison sentences of at least four years for serious cases of market manipulation and insider dealing, and at least two years for improper disclosure of insider information.[55]  Insider trading in India is an offense according to Sections 12A and 15G of the Securities and Exchange Board of India Act, 1992, and the Securities and Exchange Board of India (Prohibition of Insider Trading) Regulations, 2015. Insider trading is when one with access to non-public, price-sensitive information about the securities of the company subscribes, buys, sells, or deals, or agrees to do so or counsels another to do so as principal or agent. Price-sensitive information is information that materially affects the value of the securities. The penalty for insider trading is imprisonment, which may extend to five years, and a minimum of five lakh rupees (500,000) to 25 crore rupees (250 million) or three times the profit made, whichever is higher.[56]  The Wall Street Journal, in a 2014 article entitled \"Why It's Hard to Catch India's Insider Trading\", said that despite a widespread belief that insider trading takes place on a regular basis in India, there were few examples of insider traders being prosecuted in India.[57] One former top regulator said that in India insider trading is deeply rooted and especially rampant because regulators do not have the tools to address it.[57] In the few cases where prosecution has taken place, cases have sometimes taken more than a decade to reach trial, and punishments have been light; and despite SEBI by law having the ability to demand penalties of up to $4 million, the few fines that were levied for insider trading have usually been under $200,000.[57]  The U.S. SEC alleged that in 2009 Kuwaiti trader Hazem Al-Braikan engaged in insider trading after misleading the public about possible takeover bids for two companies.[58][59] Three days after Al-Braikan was sued by the SEC, he was found dead of a gunshot wound to the head in his home in Kuwait City on July 26, 2009, in what Kuwaiti police called a suicide.[58][59][60] The SEC later reached a $6.5 million settlement of civil insider trading charges, with his estate and others.[59]  In 2009, a journalist in Nettavisen (Thomas Gulbrandsen) was sentenced to four months in prison for insider trading.[61]  The longest prison sentence in a Norwegian trial where the main charge was insider trading, was for eight years (two suspended) when Alain Angelil was convicted in a district court on December 9, 2011.[62][63]  Under Republic Act 8799 or the Securities Regulation Code, insider trading in the Philippines is illegal.[64]  Although insider trading in the UK has been illegal since 1980, it proved difficult to successfully prosecute individuals accused of insider trading. There were a number of notorious cases where individuals were able to escape prosecution. Instead the UK regulators relied on a series of fines to punish market abuses.  These fines were widely perceived as an ineffective deterrent,[65] and there was a statement of intent by the UK regulator (the Financial Services Authority) to use its powers to enforce the legislation (specifically the Financial Services and Markets Act 2000). Between 2009 and 2012 the FSA secured 14 convictions in relation to insider dealing.  Until the 21st century and the European Union's market abuse laws, the United States was the leading country in prohibiting insider trading made on the basis of material non-public information.[11] Thomas Newkirk and Melissa Robertson of the SEC summarize the development of US insider trading laws.[8] Insider trading has a base offense level of 8, which puts it in Zone A under the U.S. Sentencing Guidelines. This means that first-time offenders are eligible to receive probation rather than incarceration.[66]  U.S. insider trading prohibitions are based on English and American common law prohibitions against fraud. In 1909, well before the Securities Exchange Act was passed, the United States Supreme Court ruled that a corporate director who bought that company's stock when he knew the stock's price was about to increase committed fraud by buying but not disclosing his inside information.  Section 15 of the Securities Act of 1933[67] contained prohibitions of fraud in the sale of securities, later greatly strengthened by the Securities Exchange Act of 1934.[68]  Section 16(b) of the Securities Exchange Act of 1934 prohibits short-swing profits (from any purchases and sales within any six-month period) made by corporate directors, officers, or stockholders owning more than 10% of a firm's shares. Under Section 10(b) of the 1934 Act, SEC Rule 10b-5, prohibits fraud related to securities trading.  The Insider Trading Sanctions Act of 1984 and the Insider Trading and Securities Fraud Enforcement Act of 1988 place penalties for illegal insider trading as high as three times the amount of profit gained or loss avoided from illegal trading.[69]  SEC regulation FD (\"Fair Disclosure\") requires that if a company intentionally discloses material non-public information to one person, it must simultaneously disclose that information to the public at large. In the case of unintentional disclosure of material non-public information to one person, the company must make a public disclosure \"promptly\".[14]: 586   Insider trading, or similar practices, are also regulated by the SEC under its rules on takeovers and tender offers under the Williams Act.  Much of the development of insider trading law has resulted from court decisions.  In 1909, the Supreme Court of the United States ruled in Strong v. Repide[70] that a director who expects to act in a way that affects the value of shares cannot use that knowledge to acquire shares from those who do not know of the expected action. Even though, in general, ordinary relations between directors and shareholders in a business corporation are not of such a fiduciary nature as to make it the duty of a director to disclose to a shareholder general knowledge regarding the value of the shares of the company before he purchases any from a shareholder, some cases involve special facts that impose such duty.  In 1968, the Second Circuit Court of Appeals advanced a \"level playing field\" theory of insider trading in SEC v. Texas Gulf Sulphur Co.[71] The court stated that anyone in possession of inside information must either disclose the information or refrain from trading. Officers of the Texas Gulf Sulphur Company had used inside information about the discovery of the Kidd Mine to make profits by buying shares and call options on company stock.[72]  In 1984, the Supreme Court of the United States ruled in the case of Dirks v. Securities and Exchange Commission[73] that tippees (receivers of second-hand information) are liable if they had reason to believe that the tipper had breached a fiduciary duty in disclosing confidential information. One such example would be if the tipper received any personal benefit from the disclosure, thereby breaching his or her duty of loyalty to the company. In Dirks, the \"tippee\" received confidential information from an insider, a former employee of a company. The reason the insider disclosed the information to the tippee, and the reason the tippee disclosed the information to third parties, was to blow the whistle on massive fraud at the company. As a result of the tippee's efforts the fraud was uncovered, and the company went into bankruptcy. But, while the tippee had given the \"inside\" information to clients who made profits from the information, the U.S. Supreme Court ruled that the tippee could not be held liable under the federal securities laws—for the simple reason that the insider from whom he received the information was not releasing the information for an improper purpose (a personal benefit), but rather for the purpose of exposing the fraud. The Supreme Court ruled that the tippee could not have been aiding and abetting a securities law violation committed by the insider—for the simple reason that no securities law violation had been committed by the insider.  (In 2019, in the case of United States v. Blaszczak, the U.S. Court of Appeals for the Second Circuit ruled that the “personal-benefit” test announced in Dirks does not apply to Title 18 fraud statutes, such as 18 USC 1348.[74][75])  In Dirks, the Supreme Court also defined the concept of \"constructive insiders\", who are lawyers, investment bankers, and others who receive confidential information from a corporation while providing services to the corporation. Constructive insiders are also liable for insider trading violations if the corporation expects the information to remain confidential, since they acquire the fiduciary duties of the true insider.  The next expansion of insider trading liability came in SEC vs. Materia[76] 745 F.2d 197 (2d Cir. 1984), the case that first introduced the misappropriation theory of liability for insider trading. Materia, a financial printing firm proofreader, and clearly not an insider by any definition, was found to have determined the identity of takeover targets based on proofreading tender offer documents in the course of his employment. After a two-week trial, the district court found him liable for insider trading, and the Second Circuit Court of Appeals affirmed holding that the theft of information from an employer, and the use of that information to purchase or sell securities in another entity, constituted a fraud in connection with the purchase or sale of a securities. The misappropriation theory of insider trading was born, and liability further expanded to encompass a larger group of outsiders.  In United States v. Carpenter[77] (1986) the U.S. Supreme Court cited an earlier ruling while unanimously upholding mail and wire fraud convictions for a defendant who received his information from a journalist rather than from the company itself. The journalist R. Foster Winans was also convicted, on the grounds that he had misappropriated information belonging to his employer, The Wall Street Journal. In that widely publicized case, Winans traded in advance of \"Heard on the Street\" columns appearing in the Journal.[78]  The Court stated in Carpenter: \"It is well established, as a general proposition, that a person who acquires special knowledge or information by virtue of a confidential or fiduciary relationship with another is not free to exploit that knowledge or information for his own personal benefit but must account to his principal for any profits derived therefrom.\"  However, in upholding the securities fraud (insider trading) convictions, the justices were evenly split.  In 1997, the U.S. Supreme Court adopted the misappropriation theory of insider trading in United States v. O'Hagan,[79] 521 U.S. 642, 655 (1997). O'Hagan was a partner in a law firm representing Grand Metropolitan, while it was considering a tender offer for Pillsbury Company. O'Hagan used this inside information by buying call options on Pillsbury stock, resulting in profits of over $4.3 million. O'Hagan claimed that neither he nor his firm owed a fiduciary duty to Pillsbury, so he did not commit fraud by purchasing Pillsbury options.[80]  The Court rejected O'Hagan's arguments and upheld his conviction.  The \"misappropriation theory\" holds that a person commits fraud \"in connection with\" a securities transaction and thereby violates 10(b) and Rule 10b-5, when he misappropriates confidential information for securities trading purposes, in breach of a duty owed to the source of the information. Under this theory, a fiduciary's undisclosed, self-serving use of a principal's information to purchase or sell securities, in breach of a duty of loyalty and confidentiality, defrauds the principal of the exclusive use of the information. In lieu of premising liability on a fiduciary relationship between company insider and purchaser or seller of the company's stock, the misappropriation theory premises liability on a fiduciary-turned-trader's deception of those who entrusted him with access to confidential information.  The Court specifically recognized that a corporation's information is its property: \"A company's confidential information ... qualifies as property to which the company has a right of exclusive use. The undisclosed misappropriation of such information in violation of a fiduciary duty ... constitutes fraud akin to embezzlement – the fraudulent appropriation to one's own use of the money or goods entrusted to one's care by another.\"  In 2000, the SEC enacted SEC Rule 10b5-1, which defined trading \"on the basis of\" inside information as any time a person trades while aware of material nonpublic information. It is no longer a defense for one to say that one would have made the trade anyway. The rule also created an affirmative defense for pre-planned trades.  In Morgan Stanley v. Skowron, 989 F. Supp. 2d 356 (S.D.N.Y. 2013), applying New York's faithless servant doctrine, the court held that a hedge fund's portfolio manager engaging in insider trading in violation of his company's code of conduct, which also required him to report his misconduct, must repay his employer the full $31 million his employer paid him as compensation during his period of faithlessness.[81][82][83][84] The court called the insider trading the \"ultimate abuse of a portfolio manager's position\".[82] The judge also wrote: \"In addition to exposing Morgan Stanley to government investigations and direct financial losses, Skowron's behavior damaged the firm's reputation, a valuable corporate asset.\"[82]  In 2014, in the case of United States v. Newman, the United States Court of Appeals for the Second Circuit cited the Supreme Court's decision in Dirks, and ruled that for a \"tippee\" (a person who used information they received from an insider) to be guilty of insider trading, the tippee must have been aware not only that the information was insider information, but must also have been aware that the insider released the information for an improper purpose (such as a personal benefit). The Court concluded that the insider's breach of a fiduciary duty not to release confidential information—in the absence of an improper purpose on the part of the insider—is not enough to impose criminal liability on either the insider or the tippee.[13]  In 2016, in the case of Salman v. United States, the U.S. Supreme Court held that the benefit a tipper must receive as predicate for an insider-trader prosecution of a tippee need not be pecuniary, and that giving a 'gift' of a tip to a family member is presumptively an act for the personal though intangible benefit of the tipper.[21]  Members of the US Congress are not exempt from the laws that ban insider trading.[85] Because they generally do not have a confidential relationship with the source of the information they receive, however, they do not meet the usual definition of an \"insider\".[86] House of Representatives rules[87] may however consider congressional insider trading unethical. A 2004 study found that stock sales and purchases by senators outperformed the market by 12.3% per year.[88] Peter Schweizer points out several examples of insider trading by members of Congress, including action taken by Spencer Bachus following a private, behind-the-doors meeting on the evening of September 18, 2008 when Hank Paulson and Ben Bernanke informed members of Congress about the issues due to the financial crisis of 2007–2008, Bachus then shorted stocks the next morning and cashed in his profits within a week.[89] Also attending the same meeting were Senator Dick Durbin and House Speaker John Boehner; the same day (trade effective the next day), Durbin sold mutual-fund shares worth $42,696, and reinvested it all with Warren Buffett. Also the same day (trade effective the next day), Boehner cashed out of an equity mutual fund.[90][91]  In May 2007, a bill entitled the Stop Trading on Congressional Knowledge Act, or STOCK Act was introduced that would hold congressional and federal employees liable for stock trades they made using information they gained through their jobs and also regulate analysts or political intelligence firms that research government activities.[92] The STOCK Act was enacted on April 4, 2012. As of 2021, in the approximately nine month period up to September 2021, Senate and House members disclosed 4,000 trades worth at least $315 million of stocks and bonds.[93]  Anil Kumar, a senior partner at management consulting firm McKinsey & Company, pleaded guilty in 2010 to insider trading in a \"descent from the pinnacle of the business world\".[94]  Chip Skowron, a hedge fund co-portfolio manager of FrontPoint Partners LLC's health care funds, was convicted of insider trading in 2011, for which he served five years in prison. He had been tipped off by a consultant to a company that the company was about to make a negative announcement regarding its clinical trial for a drug.[95][96][97][98] At first Skowron denied the charges against him, and his defense attorney said he would plead not guilty, saying \"We look forward to responding to the allegations more fully in court at the appropriate time\".[99][50][100] However, after the consultant charged with tipping him off pleaded guilty, he changed his position, and admitted his guilt.[99]  Rajat Gupta, who had been managing partner of McKinsey & Co. and a director at Goldman Sachs Group Inc. and Procter & Gamble Co., was convicted by a federal jury in 2012 and sentence to two years in prison for leaking inside information to hedge fund manager Raj Rajaratnam who was sentenced to 11 years in prison. The case was prosecuted by the office of United States Attorney for the Southern District of New York Preet Bharara.[101]  Mathew Martoma, former hedge fund trader and portfolio manager at S.A.C. Capital Advisors, was accused of generating possibly the largest single insider trading transaction profit in history at a value of $276 million.[102] He was convicted in February 2014, and is serving a nine-year prison sentence.[102][103]  With the guilty plea by Perkins Hixon in 2014 for insider trading from 2010 to 2013 while at Evercore Partners, Bharara said in a press release that 250 defendants whom his office had charged since August 2009 had now been convicted.[104]  On December 10, 2014, a federal appeals court overturned the insider trading convictions of two former hedge fund traders, Todd Newman and Anthony Chiasson, based on the \"erroneous\" instructions given to jurors by the trial judge.[105] The decision was expected to affect the appeal of the separate insider-trading conviction of former SAC Capital portfolio manager Michael Steinberg[106] and the U.S. Attorney[107] and the SEC[108] in 2015 did drop their cases against Steinberg and others.  In 2016, Sean Stewart, a former managing director at Perella Weinberg Partners LP and vice president at JPMorgan Chase, was convicted on allegations he tipped his father on pending health-care deals. The father, Robert Stewart, previously had pleaded guilty but did not testify during his son's trial. It was argued that by way of compensation for the tip, the father had paid more than $10,000 for Sean's wedding photographer.[109]  In 2017, Billy Walters, Las Vegas sports bettor, was convicted of making $40 million on private information of Dallas-based dairy processing company Dean Foods, and sentenced to five years in prison. Walters's source, company director Thomas C. Davis employing a prepaid cell phone and sometimes the code words \"Dallas Cowboys\" for Dean Foods, helped him from 2008 to 2014 realize profits and avoid losses in the stock, the federal jury found. Golfer Phil Mickelson \"was also mentioned during the trial as someone who had traded in Dean Foods shares and once owed nearly $2 million in gambling debts to\" Walters. Mickelson \"made roughly $1 million trading Dean Foods shares; he agreed to forfeit those profits in a related civil case brought by the Securities and Exchange Commission\". Walters appealed the verdict, but in December 2018 his conviction was upheld by the 2nd U.S. Circuit Court of Appeals in Manhattan.[110][111]  In 2018, David Blaszczak, the \"king of political intelligence\",[112] Theodore Huber and Robert Olan, two partners at hedge fund Deerfield Management, and Christopher Worrall, an employee at the Centers for Medicare and Medicaid Services (CMS), were convicted for insider trading by the U.S. Attorney's Office in the Southern District of New York.[113]  Worrall leaked confidential government information that he stole from CMS to Blaszczak, and Blaszczak passed that information to Huber and Olan, who made $7 million trading securities.[113][114]  The convictions were upheld in 2019 by the Second Circuit, U.S. Court of Appeals in Manhattan; that opinion was vacated by the Supreme Court in 2021, and the Second Circuit is now reconsidering its decision.[115]  In 2023, Terren Peizer was charged with insider trading by the SEC, which alleged that he sold $20 million of Ontrak Inc. stock while he was in possession of material nonpublic negative information.[116][117] Peizer was the CEO and chairman of Ontrak.[118][119] In addition, the U.S. Department of Justice announced criminal charges of securities fraud against Peizer, charging that thereby he had avoided $12 million in losses; he was arrested.[120][116][119][121]  The case was tried in the U.S. District Court for the Central District of California before U.S. District Judge Dale S. Fischer.[120]  He was convicted of all three charges in June 2024, and faces up to 65 years in prison.[122][123]  General information  Articles and opinions  Data on insider trading "},"meta":{},"created_at":"2025-03-22T14:25:42.275677Z","updated_at":"2025-03-22T14:25:42.275677Z","inner_id":29,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":38,"annotations":[{"id":38,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.311403Z","updated_at":"2025-03-22T14:25:42.311403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"ec80175c-d3a2-4d7b-b82c-17055a152781","import_id":null,"last_action":null,"bulk_created":false,"task":38,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  A tax is a mandatory financial charge or levy imposed on an individual or legal entity by a governmental organization to support government spending and public expenditures collectively or to regulate and reduce negative externalities.[1] Tax compliance refers to policy actions and individual behavior aimed at ensuring that taxpayers are paying the right amount of tax at the right time and securing the correct tax allowances and tax relief.[2] The first known taxation occurred in Ancient Egypt around 3000–2800 BC.[3] Taxes consist of direct or indirect taxes and may be paid in money or as labor equivalent.  All countries have a tax system in place to pay for public, common societal, or agreed national needs and for the functions of government.[citation needed] Some countries levy a flat percentage rate of taxation on personal annual income, but most scale taxes are progressive based on brackets of yearly income amounts. Most countries charge a tax on an individual's income and corporate income. Countries or sub-units often also impose wealth taxes, inheritance taxes, gift taxes, property taxes, sales taxes, use taxes, environmental taxes, payroll taxes, duties, or tariffs. It is also possible to levy a tax on tax, as with a gross receipts tax.  In economic terms (circular flow of income), taxation transfers wealth from households or businesses to the government. This affects economic growth and welfare, which can be increased (known as fiscal multiplier) or decreased (known as excess burden of taxation). Consequently, taxation is a highly debated topic by some, as although taxation is deemed necessary by consensus for society to function and grow in an orderly and equitable manner through the government provision of public goods and public services,[4][5][6][7] others such as libertarians are anti-taxation and denounce taxation broadly or in its entirety, classifying taxation as theft or extortion through coercion along with the use of force.[8] Within market economies, taxation is considered the most viable option to operate the government (instead of widespread state ownership of the means of production), as taxation enables the government to generate revenue without heavily interfering with the market and private businesses; taxation preserves the efficiency and productivity of the private sector by allowing individuals and companies to make their own economic decisions, engage in flexible production, competition, and innovation as a result of market forces.  Certain countries (usually small in size or population, which results in a smaller infrastructure and social expenditure) function as tax havens by imposing minimal taxes on the personal income of individuals and corporate income. These tax havens attract capital from abroad (particularly from larger economies) while resulting in loss of tax revenues within other non-haven countries (through base erosion and profit shifting).  Legal and economic definitions of taxes differ, such that many transfers to governments are not considered taxes by economists. For example, some transfers to the public sector are comparable to prices. Examples include tuition at public universities and fees for utilities provided by local governments. Governments also obtain resources by \"creating\" money and coins (for example, by printing bills and by minting coins), through voluntary gifts (for example, contributions to public universities and museums), by imposing penalties (such as traffic fines), by borrowing and confiscating criminal proceeds. From the view of economists, a tax is a non-penal, yet compulsory transfer of resources from the private to the public sector, levied on a basis of predetermined criteria and without reference to specific benefits received.  In modern taxation systems, governments levy taxes in money, but in-kind and corvée taxation are characteristic of traditional or pre-capitalist states and their functional equivalents. The method of taxation and the government expenditure of taxes raised is often highly debated in politics and economics. Tax collection is performed by a government agency such as the Internal Revenue Service (IRS) in the United States, His Majesty's Revenue and Customs (HMRC) in the United Kingdom, the Canada Revenue Agency or the Australian Taxation Office. When taxes are not fully paid, the state may impose civil penalties (such as fines or forfeiture) or criminal penalties (such as incarceration) on the non-paying entity or individual.[10]  The levying of taxes aims to raise revenue to fund governing, to alter prices in order to affect demand, or to regulate some form of cost or benefit. States and their functional equivalents throughout history have used the money provided by taxation to carry out many functions. Some of these include expenditures on economic infrastructure (roads, public transportation, sanitation, legal systems, public security, public education, public health systems), military, scientific research & development, culture and the arts, public works, distribution, data collection and dissemination, public insurance, and the operation of government itself. A government's ability to raise taxes is called its fiscal capacity.  When expenditures exceed tax revenue, a government accumulates government debt. A portion of taxes may be used to service past debts. Governments also use taxes to fund welfare and public services. These services can include education systems, pensions for the elderly, unemployment benefits, transfer payments, subsidies and public transportation. Energy, water and waste management systems are also common public utilities.  According to the proponents of the chartalist theory of money creation, taxes are not needed for government revenue, as long as the government in question is able to issue fiat money. According to this view, the purpose of taxation is to maintain the stability of the currency, express public policy regarding the distribution of wealth, subsidizing certain industries or population groups or isolating the costs of certain benefits, such as highways or social security.[11]  The Organisation for Economic Co-operation and Development (OECD) publishes an analysis of the tax systems of member countries. As part of such analysis, OECD has developed a definition and system of classification of internal taxes,[12] generally followed below. In addition, many countries impose taxes (tariffs) on the import of goods.  Many jurisdictions tax the income of individuals and of business entities, including corporations. Generally, the authorities impose a tax on net profits from a business, on net gains, and on other income. Computation of income subject to tax may be determined under accounting principles used in the jurisdiction, which tax-law principles in the jurisdiction may modify or replace. The incidence of taxation varies by system, and some systems may be viewed as progressive or regressive. Rates of tax may vary or be constant (flat) by income level. Many systems allow individuals certain personal allowances and other non-business reductions to taxable income, although business deductions tend to be favored over personal deductions.  Tax-collection agencies often collect personal income tax on a pay-as-you-earn basis, with corrections made after the end of the tax year. These corrections take one of two forms:  Income-tax systems often make deductions available that reduce the total tax liability by reducing total taxable income. They may allow losses from one type of income to count against another – for example, a loss on the stock market may be deducted against taxes paid on wages. Other tax systems may isolate the loss, such that business losses can only be deducted against business income tax by carrying forward the loss to later tax years.  In economics, a negative income tax (abbreviated NIT) is a progressive income tax system where people earning below a certain amount receive supplemental payment from the government instead of paying taxes to the government.  Most jurisdictions imposing an income tax treat capital gains as part of income subject to tax. Capital gain is generally a gain on sale of capital assets—that is, those assets not held for sale in the ordinary course of business. Capital assets include personal assets in many jurisdictions. Some jurisdictions provide preferential rates of tax or only partial taxation for capital gains. Some jurisdictions impose different rates or levels of capital-gains taxation based on the length of time the asset was held. Because tax rates are often much lower for capital gains than for ordinary income, there is widespread controversy and dispute about the proper definition of capital.  Corporate tax refers to income tax, capital tax, net-worth tax, or other taxes imposed on corporations. Rates of tax and the taxable base for corporations may differ from those for individuals or for other taxable persons.  Many countries provide publicly funded retirement or healthcare systems.[13] In connection with these systems, the country typically requires employers or employees to make compulsory payments.[14] These payments are often computed by reference to wages or earnings from self-employment. Tax rates are generally fixed, but a different rate may be imposed on employers than on employees.[15] Some systems provide an upper limit on earnings subject to the tax. A few systems provide that the tax is payable only on wages above a particular amount. Such upper or lower limits may apply for retirement but not for health-care components of the tax. Some have argued that such taxes on wages are a form of \"forced savings\" and not really a tax, while others point to redistribution through such systems between generations (from newer cohorts to older cohorts) and across income levels (from higher income levels to lower income-levels) which suggests that such programs are really taxed and spending programs.  Unemployment and similar taxes are often imposed on employers based on the total payroll. These taxes may be imposed in both the country and sub-country levels.[16]  A wealth tax is levied on the total value of personal assets, including: bank deposits, real estate, assets in insurance and pension plans, ownership of unincorporated businesses, financial securities, and personal trusts.[17] Liabilities (primarily mortgages and other loans) are typically deducted, hence it is sometimes called a net wealth tax.  Recurrent property taxes may be imposed on immovable property (real property) and on some classes of movable property. In addition, recurrent taxes may be imposed on the net wealth of individuals or corporations.[18] Many jurisdictions impose inheritance tax on property at time of inheritance or gift tax at the time of gift transfer. Some jurisdictions impose taxes on financial or capital transactions.  A property tax (or millage tax) is an  ad valorem tax levy on the value of a property that the owner of the property is required to pay to a government in which the property is situated. Multiple jurisdictions may tax the same property. There are three general varieties of property: land, improvements to land (immovable human-made things, e.g. buildings), and personal property (movable things). Real estate or realty is the combination of land and improvements to the land.  Property taxes are usually charged on a recurrent basis (e.g., yearly). A common type of property tax is an annual charge on the ownership of real estate, where the tax base is the estimated value of the property. For a period of over 150 years from 1695, the government of England levied a window tax, with the result that one can still see listed buildings with windows bricked up in order to save their owner's money. A similar tax on hearths existed in France and elsewhere, with similar results. The two most common types of event-driven property taxes are stamp duty, charged upon change of ownership, and inheritance tax, which many countries impose on the estates of the deceased.  In contrast with a tax on real estate (land and buildings), a land-value tax (or LVT) is levied only on the unimproved value of the land (\"land\" in this instance may mean either the economic term, i.e., all-natural resources, or the natural resources associated with specific areas of the Earth's surface: \"lots\" or \"land parcels\"). Proponents of the land-value tax argue that it is economically justified, as it will not deter production, distort market mechanisms or otherwise create deadweight losses the way other taxes do.[19]  When real estate is held by a higher government unit or some other entity not subject to taxation by the local government, the taxing authority may receive a payment in lieu of taxes to compensate it for some or all of the foregone tax revenues.  In many jurisdictions (including many American states), there is a general tax levied periodically on residents who own personal property (personalty) within the jurisdiction. Vehicle and boat registration fees are subsets of this kind of tax. The tax is often designed with blanket coverage and large exceptions for things like food and clothing. Household goods are often exempt when kept or used within the household.[20] Any otherwise non-exempt object can lose its exemption if regularly kept outside the household.[20] Thus, tax collectors often monitor newspaper articles for stories about wealthy people who have lent art to museums for public display, because the artworks have then become subject to personal property tax.[20] If an artwork had to be sent to another state for some touch-ups, it may have become subject to personal property tax in that state as well.[20]  Inheritance tax, also called estate tax, are taxes that arise for inheritance or inherited income.[21] In United States tax law, there is a distinction between an estate tax and an inheritance tax: the former taxes the personal representatives of the deceased, while the latter taxes the beneficiaries of the estate. However, this distinction does not apply in other jurisdictions; for example, if using this terminology UK inheritance tax would be an estate tax.   An expatriation tax is a tax on individuals who renounce their citizenship or residence. The tax is often imposed based on a deemed disposition of all the individual's property. One example is the United States under the American Jobs Creation Act, where any individual who has a net worth of $2 million or an average income-tax liability of $127,000 who renounces his or her citizenship and leaves the country is automatically assumed to have done so for tax avoidance reasons and is subject to a higher tax rate.[22]  Historically, in many countries, a contract needs to have a stamp affixed to make it valid. The charge for the stamp is either a fixed amount or a percentage of the value of the transaction. In most countries, the stamp has been abolished but stamp duty remains. Stamp duty is levied in the UK on the purchase of shares and securities, the issue of bearer instruments, and certain partnership transactions. Its modern derivatives, stamp duty reserve tax and stamp duty land tax, are respectively charged on transactions involving securities and land. Stamp duty has the effect of discouraging speculative purchases of assets by decreasing liquidity. In the United States, transfer tax is often charged by the state or local government and (in the case of real property transfers) can be tied to the recording of the deed or other transfer documents.  Some countries' governments will require a declaration of the taxpayers' balance sheet (assets and liabilities), and from that exact a tax on net worth (assets minus liabilities), as a percentage of the net worth, or a percentage of the net worth exceeding a certain level. The tax may be levied on \"natural\" or \"legal persons.\"  A value-added tax (VAT), also known as Goods and Services Tax (GST), Single Business Tax, or Turnover Tax in some countries, applies the equivalent of a sales tax to every operation that creates value. To give an example, sheet steel is imported by a machine manufacturer. That manufacturer will pay the VAT on the purchase price, remitting that amount to the government. The manufacturer will then transform the steel into a machine, selling the machine for a higher price to a wholesale distributor. The manufacturer will collect the VAT on the higher price but will remit to the government only the excess related to the \"value-added\" (the price over the cost of the sheet steel). The wholesale distributor will then continue the process, charging the retail distributor the VAT on the entire price to the retailer, but remitting only the amount related to the distribution mark-up to the government. The last VAT amount is paid by the eventual retail customer who cannot recover any of the previously paid VAT. For a VAT and sales tax of identical rates, the total tax paid is the same, but it is paid at differing points in the process.  VAT is usually administrated by requiring the company to complete a VAT return, giving details of VAT it has been charged (referred to as input tax) and VAT it has charged to others (referred to as output tax). The difference between output tax and input tax is payable to the Local Tax Authority.  Many tax authorities have introduced automated VAT which has increased accountability and auditability, by utilizing computer systems, thereby also enabling anti-cybercrime offices as well.[23]  Sales taxes are levied when a commodity is sold to its final consumer. Retail organizations contend that such taxes discourage retail sales. The question of whether they are generally progressive or regressive is a subject of much current debate. People with higher incomes spend a lower proportion of them, so a flat-rate sales tax will tend to be regressive. It is therefore common to exempt food, utilities, and other necessities from sales taxes, since poor people spend a higher proportion of their incomes on these commodities, so such exemptions make the tax more progressive. This is the classic \"You pay for what you spend\" tax, as only those who spend money on non-exempt (i.e. luxury) items pay the tax.[citation needed]  A small number of U.S. states rely entirely on sales taxes for state revenue, as those states do not levy a state income tax. Such states tend to have a moderate to a large amount of tourism or inter-state travel that occurs within their borders, allowing the state to benefit from taxes from people the state would otherwise not tax. In this way, the state is able to reduce the tax burden on its citizens. The U.S. states that do not levy a state income tax are Alaska, Tennessee, Florida, Nevada, South Dakota, Texas,[24] Washington state, and Wyoming. Additionally, New Hampshire and Tennessee levy state income taxes only on dividends and interest income. Of the above states, only Alaska and New Hampshire do not levy a state sales tax.  In the United States, there is a growing movement[25] for the replacement of all federal payroll and income taxes (both corporate and personal) with a national retail sales tax and monthly tax rebate to households of citizens and legal resident aliens. The tax proposal is named FairTax. In Canada, the federal sales tax is called the Goods and Services Tax (GST) and now stands at 5%. The provinces of British Columbia, Saskatchewan, Manitoba, and Prince Edward Island also have a provincial sales tax [PST]. The provinces of Nova Scotia, New Brunswick, Newfoundland & Labrador, and Ontario have harmonized their provincial sales taxes with the GST—Harmonized Sales Tax [HST], and thus is a full VAT. The province of Quebec collects the Quebec Sales Tax [QST] which is based on the GST with certain differences. Most businesses can claim back the GST, HST, and QST they pay, and so effectively it is the final consumer who pays the tax.  An excise duty is an indirect tax imposed upon goods during the process of their manufacture, production or distribution, and is usually proportionate to their quantity or value. Excise duties were first introduced into England in the year 1643, as part of a scheme of revenue and taxation devised by parliamentarian John Pym and approved by the Long Parliament. These duties consisted of charges on beer, ale, cider, cherry wine, and tobacco, to which list were afterward added paper, soap, candles, malt, hops, and sweets. The basic principle of excise duties was that they were taxes on the production, manufacture, or distribution of articles which could not be taxed through the customs house, and revenue derived from that source is called excise revenue proper. The fundamental conception of the term is that of a tax on articles produced or manufactured in a country. In the taxation of such articles of luxury as spirits, beer, tobacco, and cigars, it has been the practice to place a certain duty on the importation of these articles (a customs duty).[26]  Excises (or exemptions from them) are also used to modify consumption patterns of a certain area (social engineering). For example, a high excise is used to discourage alcohol consumption, relative to other goods. This may be combined with hypothecation if the proceeds are then used to pay for the costs of treating illness caused by alcohol use disorder. Similar taxes may exist on tobacco, pornography, marijuana etc., and they may be collectively referred to as \"sin taxes\". A carbon tax is a tax on the consumption of carbon-based non-renewable fuels, such as petrol, diesel-fuel, jet fuels, and natural gas. The object is to reduce the release of carbon into the atmosphere. In the United Kingdom, vehicle excise duty is an annual tax on vehicle ownership.  An import or export tariff (also called customs duty or impost) is a charge for the movement of goods through a political border. Tariffs discourage trade, and they may be used by governments to protect domestic industries. A proportion of tariff revenues is often hypothecated to pay the government to maintain a navy or border police. The classic ways of cheating a tariff are smuggling or declaring a false value of goods. Tax, tariff and trade rules in modern times are usually set together because of their common impact on industrial policy, investment policy, and agricultural policy. A trade bloc is a group of allied countries agreeing to minimize or eliminate tariffs against trade with each other, and possibly to impose protective tariffs on imports from outside the bloc. A customs union has a common external tariff, and the participating countries share the revenues from tariffs on goods entering the customs union.  In some societies, tariffs also could be imposed by local authorities on the movement of goods between regions (or via specific internal gateways). A notable example is the likin, which became an important revenue source for local governments in the late Qing China.  Occupational taxes or license fees may be imposed on businesses or individuals engaged in certain businesses. Many jurisdictions impose a tax on vehicles.  A poll tax, also called a per capita tax, or capitation tax, is a tax that levies a set amount per individual. It is an example of the concept of fixed tax. One of the earliest taxes mentioned in the Bible of a half-shekel per annum from each adult Jew (Ex. 30:11–16) was a form of the poll tax. Poll taxes are administratively cheap because they are easy to compute and collect and difficult to cheat. Economists have considered poll taxes economically efficient because people are presumed to be in fixed supply and poll taxes, therefore, do not lead to economic distortions. However, poll taxes are very unpopular because poorer people pay a higher proportion of their income than richer people. In addition, the supply of people is in fact not fixed over time: on average, couples will choose to have fewer children if a poll tax is imposed.[27][failed verification] The introduction of a poll tax in medieval England was the primary cause of the 1381 Peasants' Revolt. Scotland was the first to be used to test the new poll tax in 1989 with England and Wales in 1990. The change from progressive local taxation based on property values to a single-rate form of taxation regardless of ability to pay (the Community Charge, but more popularly referred to as the Poll Tax), led to widespread refusal to pay and to incidents of civil unrest, known colloquially as the 'Poll Tax Riots'.  Some types of taxes have been proposed but not actually adopted in any major jurisdiction. These include:  An ad valorem tax is one where the tax base is the value of a good, service, or property. Sales taxes, tariffs, property taxes, inheritance taxes, and value-added taxes are different types of ad valorem tax. An ad valorem tax is typically imposed at the time of a transaction (sales tax or value-added tax (VAT)) but it may be imposed on an annual basis (property tax) or in connection with another significant event (inheritance tax or tariffs).  In contrast to ad valorem taxation is a per unit tax, where the tax base is the quantity of something, regardless of its price. An excise tax is an example.  Consumption tax refers to any tax on non-investment spending and can be implemented by means of a sales tax, consumer value-added tax, or by modifying an income tax to allow for unlimited deductions for investment or savings.  This includes natural resources consumption tax, greenhouse gas tax (i.e. carbon tax), \"sulfuric tax\", and others. The stated purpose is to reduce the environmental impact by repricing. Economists describe environmental impacts as negative externalities. As early as 1920, Arthur Pigou suggested a tax to deal with externalities (see also the section on Increased economic welfare below). The proper implementation of environmental taxes has been the subject of a long-lasting debate.  An important feature of tax systems is the percentage of the tax burden as it relates to income or consumption. The terms progressive, regressive, and proportional are used to describe the way the rate progresses from low to high, from high to low, or proportionally. The terms describe a distribution effect, which can be applied to any type of tax system (income or consumption) that meets the definition.  The terms can also be used to apply meaning to the taxation of select consumption, such as a tax on luxury goods and the exemption of basic necessities may be described as having progressive effects as it increases a tax burden on high end consumption and decreases a tax burden on low end consumption.[28][29][30]  Taxes are sometimes referred to as \"direct taxes\" or \"indirect taxes\". The meaning of these terms can vary in different contexts, which can sometimes lead to confusion. An economic definition, by Atkinson, states that \"...direct taxes may be adjusted to the individual characteristics of the taxpayer, whereas indirect taxes are levied on transactions irrespective of the circumstances of buyer or seller.\"[31] According to this definition, for example, income tax is \"direct\", and sales tax is \"indirect\".   In law, the terms may have different meanings. In U.S. constitutional law, for instance, direct taxes refer to poll taxes and property taxes, which are based on simple existence or ownership. Indirect taxes are imposed on events, rights, privileges, and activities.[32] Thus, a tax on the sale of the property would be considered an indirect tax, whereas the tax on simply owning the property itself would be a direct tax.  Governments may charge user fees, tolls, or other types of assessments in exchange of particular goods, services, or use of property. These are generally not considered taxes, as long as they are levied as payment for a direct benefit to the individual paying.[33] Such fees include:  Some scholars refer to certain economic effects as taxes, though they are not levies imposed by governments. These include:  The first known system of taxation was in Ancient Egypt around 3000–2800 BC, in the First Dynasty of the Old Kingdom of Egypt.[3] The earliest and most widespread forms of taxation were the corvée and the tithe. The corvée was forced labor provided to the state by peasants too poor to pay other forms of taxation (labor in ancient Egyptian is a synonym for taxes).[36] Records from the time document that the Pharaoh would conduct a biennial tour of the kingdom, collecting tithes from the people. Other records are granary receipts on limestone flakes and papyrus.[37] Early taxation is also described in the Bible. In Genesis (chapter 47, verse 24 – the New International Version), it states \"But when the crop comes in, give a fifth of it to Pharaoh. The other four-fifths you may keep as seed for the fields and as food for yourselves and your households and your children\". Samgharitr is the name mentioned for the Tax collector in the Vedic texts.[38] In Hattusa, the capital of the Hittite Empire, grains were collected as a tax from the surrounding lands, and stored in silos as a display of the king's wealth.[39]  In the Persian Empire, a regulated and sustainable tax system was introduced by Darius I the Great in 500 BC;[40] the Persian system of taxation was tailored to each Satrapy (the area ruled by a Satrap or provincial governor). At differing times, there were between 20 and 30 Satrapies in the Empire and each was assessed according to its supposed productivity. It was the responsibility of the Satrap to collect the due amount and to send it to the treasury, after deducting his expenses (the expenses and the power of deciding precisely how and from whom to raise the money in the province, offer maximum opportunity for rich pickings). The quantities demanded from the various provinces gave a vivid picture of their economic potential. For instance, Babylon was assessed for the highest amount and for a startling mixture of commodities; 1,000 silver talents and four months supply of food for the army. India, a province fabled for its gold, was to supply gold dust equal in value to the very large amount of 4,680 silver talents. Egypt was known for the wealth of its crops; it was to be the granary of the Persian Empire (and, later, of the Roman Empire) and was required to provide 120,000 measures of grain in addition to 700 talents of silver.[41] This tax was exclusively levied on Satrapies based on their lands, productive capacity and tribute levels.[42]  The Rosetta Stone, a tax concession issued by Ptolemy V in 196 BC and written in three languages \"led to the most famous decipherment in history—the cracking of hieroglyphics\".[43]  In the Roman Republic, taxes were collected from individuals at the rate of between 1% and 3% of the assessed value of their total property. However, since it was extremely difficult to facilitate the collection of the tax, the government auctioned it every year. The winning tax farmers (called publicani) paid the tax revenue to the government in advance and then kept the taxes collected from individuals. The publicani paid the tax revenue in coins, but collected the taxes using other exchange media, thus relieving the government of the work to carry out the currency conversion themselves. The revenue payment essentially worked as a loan to the government, which paid interest on it. Although this scheme was a profitable enterprise for the government as well as the publicani, it was later replaced by a direct tax system by the emperor Augustus; after which, each province was obliged to pay 1% tax on wealth and a flat rate on each adult. This brought about regular census and shifted the tax system more towards taxing an individual's income rather than wealth.[44]  Islamic rulers imposed Zakat (a tax on Muslims) and Jizya (a poll tax on conquered non-Muslims). In India this practice began in the 11th century.  Numerous records of government tax collection in Europe since at least the 17th century are still available today. But taxation levels are hard to compare to the size and flow of the economy since production numbers are not as readily available. Government expenditures and revenue in France during the 17th century went from about 24.30 million livres in 1600–10 to about 126.86 million livres in 1650–59 to about 117.99 million livres in 1700–10 when government debt had reached 1.6 billion livres. In 1780–89, it reached 421.50 million livres.[45] Taxation as a percentage of production of final goods may have reached 15–20% during the 17th century in places such as France, the Netherlands, and Scandinavia. During the war-filled years of the eighteenth and early nineteenth century, tax rates in Europe increased dramatically as war became more expensive and governments became more centralized and adept at gathering taxes. This increase was greatest in England, Peter Mathias and Patrick O'Brien found that the tax burden increased by 85% over this period. Another study confirmed this number, finding that per capita tax revenues had grown almost sixfold over the eighteenth century, but that steady economic growth had made the real burden on each individual only double over this period before the industrial revolution. Effective tax rates were higher in Britain than France in the years before the French Revolution, twice in per capita income comparison, but they were mostly placed on international trade. In France, taxes were lower but the burden was mainly on landowners, individuals, and internal trade and thus created far more resentment.[46]  Taxation as a percentage of GDP 2016 was 45.9% in Denmark, 45.3% in France, 33.2% in the United Kingdom, 26% in the United States, and among all OECD members an average of 34.3%.[47][48]  In monetary economies prior to fiat banking, a critical form of taxation was seigniorage, the tax on the creation of money.  Other obsolete forms of taxation include:  Some principalities taxed windows, doors, or cabinets to reduce consumption of imported glass and hardware. Armoires, hutches, and wardrobes were employed to evade taxes on doors and cabinets. In some circumstances, taxes are also used to enforce public policy like congestion charge (to cut road traffic and encourage public transport) in London. In Tsarist Russia, taxes were clamped on beards. Today, one of the most-complicated taxation systems worldwide is in Germany. Three-quarters of the world's taxation literature refers to the German system.[citation needed] Under the German system, there are 118 laws, 185 forms, and 96,000 regulations, spending €3.7 billion to collect the income tax.[citation needed] In the United States, the IRS has about 1,177 forms and instructions,[49] 28.4111 megabytes of Internal Revenue Code[50] which contained 3.8 million words as of 1 February 2010,[51] numerous tax regulations in the Code of Federal Regulations,[52] and supplementary material in the Internal Revenue Bulletin.[53] Today, governments in more advanced economies (i.e. Europe and North America) tend to rely more on direct taxes, while developing economies (i.e. several African countries) rely more on indirect taxes.  In economic terms, taxation transfers wealth from households or businesses to the government of a nation. Adam Smith writes in The Wealth of Nations that  The side-effects of taxation (such as economic distortions) and theories about how best to tax are an important subject in microeconomics. Taxation is almost never a simple transfer of wealth. Economic theories of taxation approach the question of how to maximize economic welfare through taxation.  A 2019 study looking at the impact of tax cuts for different income groups, it was tax cuts for low-income groups that had the greatest positive impact on employment growth.[55] Tax cuts for the wealthiest top 10% had a small impact.[55]  Law establishes from whom a tax is collected. In many countries, taxes are imposed on businesses (such as corporate taxes or portions of payroll taxes). However, who ultimately pays the tax (the tax \"burden\") is determined by the marketplace as taxes become embedded into production costs. Economic theory suggests that the economic effect of tax does not necessarily fall at the point where it is legally levied. For instance, a tax on employment paid by employers will impact the employee, at least in the long run. The greatest share of the tax burden tends to fall on the most inelastic factor involved—the part of the transaction which is affected least by a change in price. So, for instance, a tax on wages in a town will (at least in the long run) affect property-owners in that area.  Depending on how quantities supplied and demanded to vary with price (the \"elasticities\" of supply and demand), a tax can be absorbed by the seller (in the form of lower pre-tax prices), or by the buyer (in the form of higher post-tax prices). If the elasticity of supply is low, more of the tax will be paid by the supplier. If the elasticity of demand is low, more will be paid by the customer; and, contrariwise for the cases where those elasticities are high. If the seller is a competitive firm, the tax burden is distributed over the factors of production depending on the elasticities thereof; this includes workers (in the form of lower wages), capital investors (in the form of loss to shareholders), landowners (in the form of lower rents), entrepreneurs (in the form of lower wages of superintendence) and customers (in the form of higher prices).  To show this relationship, suppose that the market price of a product is $1.00 and that a $0.50 tax is imposed on the product that, by law, is to be collected from the seller. If the product has an elastic demand, a greater portion of the tax will be absorbed by the seller. This is because goods with elastic demand cause a large decline in quantity demanded a small increase in price. Therefore, in order to stabilize sales, the seller absorbs more of the additional tax burden. For example, the seller might drop the price of the product to $0.70 so that, after adding in the tax, the buyer pays a total of $1.20, or $0.20 more than he did before the $0.50 tax was imposed. In this example, the buyer has paid $0.20 of the $0.50 tax (in the form of a post-tax price) and the seller has paid the remaining $0.30 (in the form of a lower pre-tax price).[56]  The purpose of taxation is to provide for government spending without inflation. The provision of public goods such as roads and other infrastructure, schools, a social safety net, public health systems, national defense, law enforcement, and a courts system increases the economic welfare of society if the benefit outweighs the costs involved.  The existence of a tax can increase economic efficiency in some cases. If there is a negative externality associated with a good (meaning that it has negative effects not felt by the consumer) then a free market will trade too much of that good. By taxing the good, the government can raise revenue to address specific problems while increasing overall welfare.  The goal is to tax people when they are creating societal costs in addition to their personal costs. By taxing goods with negative externalities, the government attempts to increase economic efficiency while raising revenues.  This type of tax is called a Pigovian tax, after economist Arthur Pigou who wrote about it in his 1920 book \"The Economics of Welfare\".[57]  Pigovian taxes might target the undesirable production of greenhouse gases which cause climate change (namely a carbon tax), polluting fuels (such as petrol), water or air pollution (namely an ecotax), goods which incur public healthcare costs (such as alcohol or tobacco), and excess demand of certain public goods (such as traffic congestion pricing). The idea is to aim taxes at people that cause an above-average amount of societal harm so the free market incorporates all costs as opposed to only personal costs, with the benefit of lowering the overall tax burden for people who cause less societal harm.  Progressive taxation generally reduces economic inequality, even when the tax revenue is not redistributed from higher-income individuals to lower-income individuals.[58][59] However, in a highly specific condition, progressive taxation increases economic inequality when lower-income individuals consume goods and services produced by higher-income individuals, who in turn consume only from other higher-income individuals (trickle-up effect).[60]  Most taxes (see below) have side effects that reduce economic welfare, either by mandating unproductive labor (compliance costs) or by creating distortions to economic incentives (deadweight loss and perverse incentives).[citation needed]  Although governments must spend money on tax collection activities, some of the costs, particularly for keeping records and filling out forms, are borne by businesses and by private individuals. These are collectively called costs of compliance. More complex tax systems tend to have higher compliance costs. This fact can be used as the basis for practical or moral arguments in favor of tax simplification (such as the FairTax or OneTax, and some flat tax proposals).  In the absence of negative externalities, the introduction of taxes into a market reduces economic efficiency by causing deadweight loss. In a competitive market, the price of a particular economic good adjusts to ensure that all trades which benefit both the buyer and the seller of a good occur. The introduction of a tax causes the price received by the seller to be less than the cost to the buyer by the amount of the tax. This causes fewer transactions to occur, which reduces economic welfare; the individuals or businesses involved are less well off than before the tax. The tax burden and the amount of deadweight cost is dependent on the elasticity of supply and demand for the good taxed.  Most taxes—including income tax and sales tax—can have significant deadweight costs. The only way to avoid deadweight costs in an economy that is generally competitive is to refrain from taxes that change economic incentives. Such taxes include the land value tax,[61] where the tax is on a good in completely inelastic supply. By taxing the value of unimproved land as opposed to what's built on it, a land value tax does not increase taxes on landowners for improving their land. This is opposed to traditional property taxes which reward land abandonment and disincentivize construction, maintenance, and repair. Another example of a tax with few deadweight costs is a lump sum tax such as a poll tax (head tax) which is paid by all adults regardless of their choices. Arguably a windfall profits tax which is entirely unanticipated can also fall into this category.  Deadweight loss does not account for the effect taxes have in leveling the business playing field. Businesses that have more money are better suited to fend off competition. It is common that an industry with a small amount of very large corporations has a very high barrier of entry for new entrants coming into the marketplace. This is due to the fact that the larger the corporation, the better its position to negotiate with suppliers. Also, larger companies may be able to operate at low or even negative profits for extended periods of time, thus pushing out competition. More progressive taxation of profits, however, would reduce such barriers for new entrants, thereby increasing competition and ultimately benefiting consumers.[62]  Complexity of the tax code in developed economies offers perverse tax incentives. The more details of tax policy there are, the more opportunities for legal tax avoidance and illegal tax evasion. These not only result in lost revenue but involve additional costs: for instance, payments made for tax advice are essentially deadweight costs because they add no wealth to the economy. Perverse incentives also occur because of non-taxable 'hidden' transactions; for instance, a sale from one company to another might be liable for sales tax, but if the same goods were shipped from one branch of a corporation to another, no tax would be payable.  To address these issues, economists often suggest simple and transparent tax structures that avoid providing loopholes. Sales tax, for instance, can be replaced with a value added tax which disregards intermediate transactions.  Following Nicolas Kaldor's research, public finance in developing countries is strongly tied to state capacity and financial development. As state capacity develops, states not only increase the level of taxation but also the pattern of taxation. With larger tax bases and the diminishing importance of trading tax, income tax gains more importance.[63] According to Tilly's argument, state capacity evolves as a response to the emergence of war. War is an incentive for states to raise taxes and strengthen states' capacity. Historically, many taxation breakthroughs took place during wartime. The introduction of income tax in Britain was due to the Napoleonic War in 1798. The US first introduced income tax during the Civil War.[64] Taxation is constrained by the fiscal and legal capacities of a country.[64] Fiscal and legal capacities also complement each other. A well-designed tax system can minimize efficiency loss and boost economic growth. With better compliance and better support to financial institutions and individual property, the government will be able to collect more tax. Although wealthier countries have higher tax revenue, economic growth does not always translate to higher tax revenue. For example, in India, increases in exemptions lead to the stagnation of income tax revenue at around 0.5% of GDP since 1986.[65]  Researchers for EPS PEAKS[66] stated that the core purpose of taxation is revenue mobilization, providing resources for National Budgets, and forming an important part of macroeconomic management. They said economic theory has focused on the need to \"optimize\" the system through balancing efficiency and equity, understanding the impacts on production, and consumption as well as distribution, redistribution, and welfare.  They state that taxes and tax relief have also been used as a tool for behavioral change, to influence investment decisions, labor supply, consumption patterns, and positive and negative economic spillovers (externalities), and ultimately, the promotion of economic growth and development. The tax system and its administration also play an important role in state-building and governance, as a principal form of \"social contract\" between the state and citizens who can, as taxpayers, exert accountability on the state as a consequence.  The researchers wrote that domestic revenue forms an important part of a developing country's public financing as it is more stable and predictable than Overseas Development Assistance and necessary for a country to be self-sufficient. They found that domestic revenue flows are, on average, already much larger than ODA, with aid worth less than 10% of collected taxes in Africa as a whole.  However, in a quarter of African countries Overseas Development Assistance does exceed tax collection,[67] with these more likely to be non-resource-rich countries. This suggests countries making the most progress replacing aid with tax revenue tend to be those benefiting disproportionately from rising prices of energy and commodities.  The author[66] found tax revenue as a percentage of GDP varying greatly around a global average of 19%.[68] This data also indicates countries with higher GDP tend to have higher tax to GDP ratios, demonstrating that higher income is associated with more than proportionately higher tax revenue. On average, high-income countries have tax revenue as a percentage of GDP of around 22%, compared to 18% in middle-income countries and 14% in low-income countries.  In high-income countries, the highest tax-to-GDP ratio is in Denmark at 47% and the lowest is in Kuwait at 0.8%, reflecting low taxes from strong oil revenues. The long-term average performance of tax revenue as a share of GDP in low-income countries has been largely stagnant, although most have shown some improvement in more recent years. On average, resource-rich countries have made the most progress, rising from 10% in the mid-1990s to around 17% in 2008. Non-resource-rich countries made some progress, with average tax revenues increasing from 10% to 15% over the same period.[69]  Many low-income countries have a tax-to-GDP ratio of less than 15% which could be due to low tax potentials, such as a limited taxable economic activity, or low tax effort due to policy choice, non-compliance, or administrative constraints.  Some low-income countries have relatively high tax-to-GDP ratios due to resource tax revenues (e.g. Angola) or relatively efficient tax administration (e.g. Kenya, Brazil) whereas some middle-income countries have lower tax-to-GDP ratios (e.g. Malaysia) which reflect a more tax-friendly policy choice.  While overall tax revenues have remained broadly constant, the global trend shows trade taxes have been declining as a proportion of total revenues (IMF, 2011), with the share of revenue shifting away from border trade taxes towards domestically levied sales taxes on goods and services. Low-income countries tend to have a higher dependence on trade taxes, and a smaller proportion of income and consumption taxes when compared to high-income countries.[70]  One indicator of the taxpaying experience was captured in the \"Doing Business\" survey,[71] which compares the total tax rate, time spent complying with tax procedures, and the number of payments required through the year, across 176 countries. The \"easiest\" countries in which to pay taxes are located in the Middle East with the UAE ranking first, followed by Qatar and Saudi Arabia, most likely reflecting low tax regimes in those countries. Countries in Sub-Saharan Africa are among the \"hardest\" to pay with the Central African Republic, Republic of Congo, Guinea and Chad in the bottom 5, reflecting higher total tax rates and a greater administrative burden to comply.  The below facts were compiled by EPS PEAKS researchers:[66]  Aid interventions in revenue can support revenue mobilization for growth, improve tax system design and administrative effectiveness, and strengthen governance and compliance.[66] The author of the Economics Topic Guide found that the best aid modalities for revenue depend on country circumstances, but should aim to align with government interests and facilitate effective planning and implementation of activities under evidence-based tax reform. Lastly, she found that identifying areas for further reform requires country-specific diagnostic assessment: broad areas for developing countries identified internationally (e.g. IMF) include, for example, property taxation for local revenues, strengthening expenditure management, and effective taxation of extractive industries and multinationals.[66]  Every tax, however, is, to the person who pays it, a badge, not of slavery, but of liberty. – Adam Smith (1776), Wealth of Nations[84]  According to most political philosophies, taxes are justified as they fund activities that are necessary and beneficial to society. Additionally, progressive taxation can be used to reduce economic inequality in a society. According to this view, taxation in modern nation-states benefit the majority of the population and social development.[85] A common presentation of this view, paraphrasing various statements by Oliver Wendell Holmes Jr. is \"Taxes are the price of civilization\".[86]  It can also be argued that in a democracy, because the government is the party performing the act of imposing taxes, society as a whole decides how the tax system should be organized.[87] The American Revolution's \"No taxation without representation\" slogan implied this view. For traditional conservatives, the payment of taxation is justified as part of the general obligations of citizens to obey the law and support established institutions. The conservative position is encapsulated in perhaps the most famous adage of public finance, \"An old tax is a good tax\".[88] Conservatives advocate the \"fundamental conservative premise that no one should be excused from paying for government, lest they come to believe that government is costless to them with the certain consequence that they will demand more government 'services'.\"[89] Social democrats generally favor higher levels of taxation to fund public provision of a wide range of services such as universal health care and education, as well as the provision of a range of welfare benefits.[90] As argued by Anthony Crosland and others, the capacity to tax income from capital is a central element of the social democratic case for a mixed economy as against Marxist arguments for comprehensive public ownership of capital.[91] American libertarians recommend a minimal level of taxation in order to maximize the protection of liberty.[92]  Compulsory taxation of individuals, such as income tax, is often justified on grounds including territorial sovereignty, and the social contract. Defenders of business taxation argue that it is an efficient method of taxing income that ultimately flows to individuals, or that separate taxation of business is justified on the grounds that commercial activity necessarily involves the use of publicly established and maintained economic infrastructure, and that businesses are in effect charged for this use.[93] Georgist economists argue that all of the economic rent collected from natural resources (land, mineral extraction, fishing quotas, etc.) is unearned income, and belongs to the community rather than any individual. They advocate a high tax (the \"Single Tax\") on land and other natural resources to return this unearned income to the state, but no other taxes.  Because payment of tax is compulsory and enforced by the legal system, rather than voluntary like crowdfunding, some political philosophies view taxation as theft, extortion, slavery, as a violation of property rights, or tyranny, accusing the government of levying taxes via force and coercive means.[94] Objectivists, anarcho-capitalists, and right-wing libertarians see taxation as government aggression through the lens of the non-aggression principle. The view that democracy legitimizes taxation is rejected by those who argue that the right to property is inalienable, and consequently cannot be abridged by the government.[95] According to Ludwig von Mises, \"society as a whole\" should not make such decisions, due to methodological individualism.[96] Some libertarian opponents of taxation claim that governmental protection, such as police and defense forces, might be replaced by market alternatives such as private defense agencies, arbitration agencies or voluntary contributions.[97]  Murray Rothbard argued in The Ethics of Liberty in 1982 that taxation is theft and that tax resistance is therefore legitimate: \"Just as no one is morally required to answer a robber truthfully when he asks if there are any valuables in one's house, so no one can be morally required to answer truthfully similar questions asked by the state, e.g., when filling out income tax returns.\"[98][99]  Many view government spending as an inefficient use of capital, and that the same projects that the government seeks to develop can be developed by private companies at much lower costs. This line of argument holds that government workers are not as personally invested in the efficiency of the projects, so the overspending happens at every step of the way. In the same regard, many public officials are not elected for their project management skills, so the projects can be mishandled. In the United States, President George W. Bush proposed in his 2009 budget \"to terminate or reduce 151 discretionary programs\" which were inefficient or ineffective.[100]  Additionally, critics of taxation note that the process of taxation, not only unjustly takes money of citizens, it also unjustly takes considerable time away from citizens. For example, it is estimated by the American Action Forum that Americans spend 6.5 billion hours annually preparing their taxes.[101][102] This is equivalent of roughly 741,501 years of life lost every year to complete tax forms and other related paperwork.  Karl Marx assumed that taxation would be unnecessary after the advent of communism and looked forward to the \"withering away of the state\". In socialist economies such as that of China, taxation played a minor role, since most government income was derived from the ownership of enterprises, and it was argued by some that monetary taxation was not necessary.[103] While the morality of taxation is sometimes questioned, most arguments about taxation revolve around the degree and method of taxation and associated government spending, not taxation itself.  Tax choice is the theory that taxpayers should have more control with how their individual taxes are allocated. If taxpayers could choose which government organizations received their taxes, opportunity cost decisions would integrate their partial knowledge.[104] For example, a taxpayer who allocated more of his taxes on public education would have less to allocate on public healthcare. Supporters argue that allowing taxpayers to demonstrate their preferences would help ensure that the government succeeds at efficiently producing the public goods that taxpayers truly value.[105] This would end real estate speculation, business cycles, unemployment and distribute wealth much more evenly.[citation needed] Joseph Stiglitz's Henry George Theorem predicts its sufficiency because—as George also noted—public spending raises land value.  Geoists (Georgists and geolibertarians) state that taxation should primarily collect economic rent, in particular the value of land, for both reasons of economic efficiency as well as morality. The efficiency of using economic rent for taxation is (as economists agree[106][107][108]) due to the fact that such taxation cannot be passed on and does not create any dead-weight loss, and that it removes the incentive to speculate on land.[109] Its morality is based on the Geoist premise that private property is justified for products of labor but not for land and natural resources.[110]  Economist and social reformer Henry George opposed sales taxes and protective tariffs for their negative impact on trade.[111] He also believed in the right of each person to the fruits of their own labor and productive investment. Therefore, income from paid labor and proper capital should remain untaxed. For this reason many Geoists—in particular those that call themselves geolibertarian—share the view with libertarians that these types of taxation (but not all) are immoral and even theft. George stated there should be one single tax: the Land Value Tax, which is considered both efficient and moral.[110] Demand for specific land is dependent on nature, but even more so on the presence of communities, trade, and government infrastructure, particularly in urban environments. Therefore, the economic rent of land is not the product of one particular individual and it may be claimed for public expenses. According to George, this would end real estate bubbles, business cycles, unemployment and distribute wealth much more evenly.[110] Joseph Stiglitz's Henry George Theorem predicts its sufficiency for financing public goods because those raise land value.[112]  John Locke stated that whenever labor is mixed with natural resources, such as is the case with improved land, private property is justified under the proviso that there must be enough other natural resources of the same quality available to others.[113] Geoists state that the Lockean proviso is violated wherever land value is greater than zero. Therefore, under the assumed principle of equal rights of all people to natural resources, the occupier of any such land must compensate the rest of society to the amount of that value. For this reason, geoists generally believe that such payment cannot be regarded as a true 'tax', but rather a compensation or fee.[114] This means that while Geoists also regard taxation as an instrument of social justice, contrary to social democrats and social liberals they do not regard it as an instrument of redistribution but rather a 'predistribution' or simply a correct distribution of the commons.[115]  Modern geoists note that land in the classical economic meaning of the word referred to all natural resources, and thus also includes resources such as mineral deposits, water bodies and the electromagnetic spectrum, to which privileged access also generates economic rent that must be compensated. Under the same reasoning most of them also consider pigouvian taxes as compensation for environmental damage or privilege as acceptable and even necessary.[116][117]  In economics, the Laffer curve is a theoretical representation of the relationship between government revenue raised by taxation and all possible rates of taxation. It is used to illustrate the concept of taxable income elasticity (that taxable income will change in response to changes in the rate of taxation). The curve is constructed by thought experiment. First, the amount of tax revenue raised at the extreme tax rates of 0% and 100% is considered. It is clear that a 0% tax rate raises no revenue, but the Laffer curve hypothesis is that a 100% tax rate will also generate no revenue because at such a rate there is no longer any incentive for a rational taxpayer to earn any income, thus the revenue raised will be 100% of nothing. If both a 0% rate and 100% rate of taxation generate no revenue, it follows from the extreme value theorem that there must exist at least one rate in between where tax revenue would be a maximum. The Laffer curve is typically represented as a graph that starts at 0% tax, zero revenue, rises to a maximum rate of revenue raised at an intermediate rate of taxation, and then falls again to zero revenue at a 100% tax rate.  One potential result of the Laffer curve is that increasing tax rates beyond a certain point will become counterproductive for raising further tax revenue. A hypothetical Laffer curve for any given economy can only be estimated and such estimates are sometimes controversial. The New Palgrave Dictionary of Economics reports that estimates of revenue-maximizing tax rates have varied widely, with a mid-range of around 70%.[118]  Most governments take revenue that exceeds that which can be provided by non-distortionary taxes or through taxes that give a double dividend. Optimal taxation theory is the branch of economics that considers how taxes can be structured to give the least deadweight costs, or to give the best outcomes in terms of social welfare. The Ramsey problem deals with minimizing deadweight costs. Because deadweight costs are related to the elasticity of supply and demand for a good, it follows that putting the highest tax rates on the goods for which there are most inelastic supply and demand will result in the least overall deadweight costs. Some economists sought to integrate optimal tax theory with the social welfare function, which is the economic expression of the idea that equality is valuable to a greater or lesser extent. If individuals experience diminishing returns from income, then the optimum distribution of income for society involves a progressive income tax. Mirrlees optimal income tax is a detailed theoretical model of the optimum progressive income tax along these lines. Over the last years the validity of the theory of optimal taxation was discussed by many political economists.[119]  Taxes are most often levied as a percentage, called the tax rate. An important distinction when talking about tax rates is to distinguish between the marginal rate and the effective tax rate. The effective rate is the total tax paid divided by the total amount the tax is paid on, while the marginal rate is the rate paid on the next dollar of income earned. For example, if income is taxed on a formula of 5% from $0 up to $50,000, 10% from $50,000 to $100,000, and 15% over $100,000, a taxpayer with income of $175,000 would pay a total of $18,750 in taxes. "},"meta":{},"created_at":"2025-03-22T14:25:42.275677Z","updated_at":"2025-03-22T14:25:42.275677Z","inner_id":30,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":39,"annotations":[{"id":39,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.311403Z","updated_at":"2025-03-22T14:25:42.311403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"b66d432a-25a0-4348-8332-36c0589bb5bb","import_id":null,"last_action":null,"bulk_created":false,"task":39,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Financial analysis (also known as financial statement analysis, accounting analysis, or analysis of finance) refers to an assessment of the viability, stability, and profitability of a business, sub-business, project or investment.   It is performed by professionals who prepare reports using ratios and other techniques, that make use of information taken from financial statements and other reports. These reports are usually presented to top management as one of their bases in making business decisions.   Financial analysis may determine if a business will:  Financial analysts often assess the following elements of a firm:  Both 2 and 3 are based on the company's balance sheet, which indicates the financial condition of a business as of a given point in time.  Financial analysts often compare financial ratios (of solvency, profitability, growth, etc.):  Comparing financial ratios is merely one way of conducting financial analysis.  Financial analysts can also use percentage analysis which involves reducing a series of figures as a percentage of some base amount.[1]  For example, a group of items can be expressed as a percentage of net income. When proportionate changes in the same figure over a given time period expressed as a percentage is known as horizontal analysis.[2]  Vertical or common-size analysis reduces all items on a statement to a \"common size\" as a percentage of some base value which assists in comparability with other companies of different sizes.[3] As a result, all Income Statement items are divided by Sales, and all Balance Sheet items are divided by Total Assets.[4]  Another method is comparative analysis. This provides a better way to determine trends. Comparative analysis presents the same information for two or more time periods and is presented side-by-side to allow for easy analysis.[5]  Financial ratios face several theoretical challenges: "},"meta":{},"created_at":"2025-03-22T14:25:42.275677Z","updated_at":"2025-03-22T14:25:42.275677Z","inner_id":31,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":40,"annotations":[{"id":40,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.311403Z","updated_at":"2025-03-22T14:25:42.311403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"c189a926-d78a-4e92-8fff-0a8163b3cbcf","import_id":null,"last_action":null,"bulk_created":false,"task":40,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"In finance, a portfolio is a collection of investments.  The term \"portfolio\" refers to any combination of financial assets such as stocks, bonds and cash. Portfolios may be held by individual investors or managed by financial professionals, hedge funds, banks and other financial institutions. It is a generally accepted principle that a portfolio is designed according to the investor's risk tolerance, time frame and investment objectives. The monetary value of each asset may influence the risk\/reward ratio of the portfolio.  When determining asset allocation, the aim is to maximise the expected return and minimise the risk. This is an example of a multi-objective optimization problem: many efficient solutions are available and the preferred solution must be selected by considering a tradeoff between risk and return. In particular, a portfolio A is dominated by another portfolio A' if A' has a greater expected gain and a lesser risk than A. If no portfolio dominates A, A is a Pareto-optimal portfolio. The set of Pareto-optimal returns and risks is called the Pareto efficient frontier for the Markowitz portfolio selection problem.[1] Recently, an alternative approach to portfolio diversification has been suggested in the literatures that combines risk and return in the optimization problem.[2]  There are many types of portfolios including the market portfolio and the zero-investment portfolio.[3] A portfolio's asset allocation may be managed utilizing any of the following investment approaches and principles: dividend weighting, equal weighting, capitalization-weighting, price-weighting, risk parity, the capital asset pricing model, arbitrage pricing theory, the Jensen Index, the Treynor ratio, the Sharpe diagonal (or index) model, the value at risk model, modern portfolio theory and others.  There are several methods for calculating portfolio returns and performance. One traditional method is using quarterly or monthly money-weighted returns; however, the true time-weighted method is a method preferred by many investors in financial markets.[4] There are also several models for measuring the performance attribution of a portfolio's returns when compared to an index or benchmark, partly viewed as investment strategy.   "},"meta":{},"created_at":"2025-03-22T14:25:42.275677Z","updated_at":"2025-03-22T14:25:42.275677Z","inner_id":32,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":41,"annotations":[{"id":41,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.311403Z","updated_at":"2025-03-22T14:25:42.311403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"acaf91a7-307a-4f14-bff9-b0b227707b39","import_id":null,"last_action":null,"bulk_created":false,"task":41,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Real estate investing involves purchasing, owning, managing, renting, or selling real estate to generate profit or long-term wealth. A real estate investor or entrepreneur may participate actively or passively in real estate transactions. In contrast, real estate development focuses on building, improving, or renovating properties. The primary goal of real estate investing is to increase value or generate a profit through strategic decision-making and market analysis.   Investors analyze real estate projects by identifying property types, as each type requires a unique investment strategy. Valuation is a critical factor in assessing real estate investments, as it determines a property’s true worth, guiding investors in purchases, sales, financing, and risk management. Accurate valuation helps investors avoid overpaying for assets, maximize returns, and minimize financial risk. Additionally, proper valuation plays a crucial role in securing financing, as lenders use valuations to determine loan amounts and interest rates.  Financing is fundamental to real estate investing, as investors rely on a combination of debt and equity to fund transactions. The capital stack represents the hierarchy of financing sources in a real estate investment, with debt issuers taking on lower risk in exchange for fixed interest income, while equity investors assume greater risk to participate in the upside potential of a property. Investors seek to improve net operating income (NOI) by increasing revenues or reducing operating expenses to enhance profitability.  The success of a real estate investment depends on factors such as market conditions, property management, financial structuring, and risk assessment. Understanding the deal cycle, valuation techniques, and capital stack enables investors to make informed decisions and optimize their investment returns across different property types.[1]  During the 1980s, real estate investment funds became increasingly involved in international real estate development. This shift led to real estate becoming a global asset class. Investing in real estate in foreign countries often requires specialized knowledge of the real estate market in that country. As international real estate investment became increasingly common in the early 21st century, the availability and quality of information regarding international real estate markets increased.[2] Real estate is one of the primary areas of investment in China, where an estimated 70% of household wealth is invested in real estate.[3]  On September 14, 1960, President Dwight D. Eisenhower signed legislation establishing a new framework for income-producing real estate investment, merging the advantages of real estate ownership with those of stock-based investments.  This legislation introduced Real Estate Investment Trusts (REITs), allowing everyday Americans to access the benefits of commercial real estate investment, which had previously been limited to large financial institutions and wealthy individuals. The Modern REIT Era was further shaped by the Tax Reform Act of 1986, which granted REITs the ability to actively operate and manage real estate assets, rather than solely owning or financing them.   Since then, the U.S. REIT approach has become successful and advanced framework which serves as the model for around 40 countries around the world.[4]  Real estate investing follows a structured deal cycle, which outlines the key stages involved in identifying, acquiring, managing, and exiting a real estate investment. The cycle typically consists of several phases, including sourcing opportunities, underwriting, due diligence, financing, closing, asset management, and disposition. Investors, whether institutional or private, evaluate potential deals based on factors such as market conditions, property performance, risk assessment, and return objectives.[citation needed]  The process begins with deal sourcing, where investors identify opportunities through broker relationships, off-market transactions, or direct acquisitions. Once a deal is found, the underwriting and due diligence phase involves analyzing financials, market trends, legal risks, and site inspections. Securing financing through either through debt, equity, or a combination—follows, allowing investors to structure the capital stack efficiently. After closing, asset management strategies, such as leasing, renovations, or repositioning, are implemented to optimize value. The final stage, disposition, involves selling or refinancing the asset to maximize returns.[citation needed]  Property types dictate how many investors think about their investments. Real estate property types can be split between residential real estate and commercial real estate. Some property types in residential real estate include single family residential, condominiums, townhouses, duplexes, triplexes, mobile homes, and ADUs (Accessory Dwelling Units). Property types for commercial real estate includes office, industrial, retail, hospitality, multifamily, and specialty property types.[5] Some specialized property types within commercial real estate includes data centers, healthcare, facilities, student housing, senior housing, agriculture. Strategies for investing in real estate includes core, core plus, value-add, and opportunistic.[6] Each of these strategies dictate the risk and risk adjusted returns from each of these strategies. The property types would dictate what strategies to use for maximizing returns.  Real estate markets in most countries are not as organized or efficient as markets for other, more liquid investment instruments. Individual properties are unique to themselves and not directly interchangeable, which makes evaluating investments less certain. Unlike other investments, real estate is fixed in a specific location and derives much of its value from that location. With residential real estate, the perceived safety of a neighborhood and the number of services or amenities nearby can increase the value of a property. For this reason, the economic and social situation in an area is often a major factor in determining the value of its real estate.[7]  Property valuation is often the preliminary step taken during a real estate investment. Information asymmetry is commonplace in real estate markets, where one party may have more accurate information regarding the actual value of the property. Real estate investors typically use a variety of real estate appraisal techniques to determine the value of properties before purchase. This typically includes gathering documents and information about the property, inspecting the physical property, and comparing it to the market value of similar properties.[8] A common method of valuing real estate is by dividing its net operating income by its capitalization rate, or CAP rate.[9]  In commercial real estate, underwriting valuation is conducted using three primary methods: the income approach, the cost approach, and the comparison approach, each providing a method to accessing a property's value. The income approach is estimating the potential income a property can generate based on assessing the market rents and then subtracting the typical operating expenses to get a net operating income. Determining the market cap rate by accessing commercial properties in the area helps determine value through the CAP rate formula: net operating income\/Property Value = CAP rate.[10] A reversion of this formula can determine the value of the property using the formula: Property Value = net operating income\/CAP rate. The cost approach determines the cost of rebuilding or replacing the asset itself using the formula: Property Value=(Cost of Building New−Depreciation)+Land Value. This approach is based on the principle that a buyer would not pay more for a property than the cost to construct a similar one. The comparison approach (also known as the sales comparison approach) is a method that determines a property’s value by analyzing the recent sales prices of similar properties usually referred to as comparables or \"comps\" in the same market. This assumes that a buyer will not pay more for a property than they would for a comparable substitute.  Numerous national and international real estate appraisal associations exist to standardize property valuation. Some of the larger of these include the Appraisal Institute, the Royal Institution of Chartered Surveyors and the International Valuation Standards Council.[8]  Investment properties are often purchased from a variety of sources, including market listings, real estate agents or brokers, banks, government entities such as Fannie Mae, public auctions, sales by owners, and real estate investment trusts.  Real estate assets are typically expensive, and investors will generally not pay the entire amount of the purchase price of a property in cash. Usually, a large portion of the purchase price will be financed using some sort of financial instrument or debt, such as a mortgage loan collateralized by the property itself. The amount of the purchase price financed by debt is referred to as leverage. The amount financed by the investor's own capital, through cash or other asset transfers, is referred to as equity. The ratio of leverage to total appraised value (often referred to as \"LTV\", or loan to value for a conventional mortgage) is one mathematical measure of the risk an investor is taking by using leverage to finance the purchase of a property. Investors usually seek to decrease their equity requirements and increase their leverage, so that their return on investment is maximized. Lenders and other financial institutions usually have minimum equity requirements for real estate investments they are being asked to finance, typically on the order of 20% of appraised value. Investors seeking low equity requirements may explore alternate financing arrangements as part of the purchase of a property (for instance, seller financing, seller subordination, private equity sources, etc.)  If the property requires substantial repair, traditional lenders like banks will often not lend on a property and the investor may be required to borrow from a private lender using a short-term bridge loan like a hard money loan. Hard money loans are usually short-term loans where the lender charges a much higher interest rate because of the higher-risk nature of the loan. Hard money loans are typically at a much lower loan-to-value ratio than conventional mortgages.  Some real estate investment organizations, such as real estate investment trusts (REITs) and some pension funds and hedge funds, have large enough capital reserves and investment strategies to allow 100% equity in the properties that they purchase. This minimizes the risk which comes from leverage but also limits potential return on investment.  By leveraging the purchase of an investment property, the required periodic payments to service the debt create an ongoing (and sometimes large) negative cash flow beginning from the time of purchase. This is sometimes referred to as the carry cost or \"carry\" of the investment.  To be successful, real estate investors must manage their cash flows to create enough positive income from the property to at least offset the carry costs.[citation needed]  In the United States, with the signing of the JOBS Act in April 2012 by President Obama, there was an easing on investment solicitations. A newer method of raising equity in smaller amounts is through real estate crowdfunding which can pool accredited and non-accredited investors together in a special purpose vehicle for all or part of the equity capital needed for the acquisition. Fundrise was the first company to crowdfund a real estate investment in the United States.[11][12]  The capital stack in real estate investment represents the hierarchy of financing sources used to fund a project, with each layer carrying different levels of risk and return. At the base of the stack is senior debt. Typically, senior debt is provided by banks, life insurance companies, or agency lenders looking to deploy capital and receive predictable cash flow through repayment of loans.  It has the lowest risk since it is collateralized by the property and has first priority in repayment or a first lien. However, it also offers the lowest returns, generally ranging from 3-8%.[13] Above senior loans is mezzanine debt, which bridges the gap between senior debt and common equity.[14] This layer is riskier since it is repaid only after senior debt, but it offers higher interest rates, typically 8-15%. Mezzanine debt may be structured as subordinated debt or preferred equity and sometimes includes conversion rights in case of borrower default. Further up the capital stack, preferred equity sits between mezzanine debt and common equity.[15] Generally, projects will have either preferred equity or mezzanine debt instead of both. Preferred equity investors receive fixed or variable preferred returns (10-20%) before any distributions to common equity holders. While they have more security than common equity investors, they lack the foreclosure rights that debt holders possess. At the top of the stack is common equity, which carries the highest risk but also the greatest potential returns, typically yielding 15% or more in opportunistic deals. Common equity holders are paid last, after all debt and preferred equity obligations are met, meaning their returns depend entirely on the success of the project. However, they also have decision-making power and control over the asset.   Investors in real estate generally play exclusively in the different tranches of the capital stack. This specialization often requires real estate investors to require real estate capital markets experts to help broker transactions to help capitalize projects. Capital markets brokers in real estate serve as intermediaries between property owners, investors, and lenders, facilitating debt and equity financing. In return, capital markets specialists require a commission generally 0.5-2% of the deal as compensation.[5]  Real estate properties may generate revenue through a number of means, including net operating income, tax shelter offsets, equity build-up, and capital appreciation.  Net operating income is the sum of all profits from rents and other sources of ordinary income generated by a property, minus the sum of ongoing expenses, such as maintenance, utilities, fees, taxes, and other expenses. Rent is one of the main sources of revenue in commercial real estate investment. Tenants pay an agreed upon sum to landlords in exchange for the use of real property, and may also pay a portion of upkeep or operating expenses on the property.[16]  Tax shelter offsets occur in one of three ways: depreciation (which may sometimes be accelerated), tax credits, and carryover losses which reduce tax liability charged against income from other sources for a period of 27.5 years. Some tax shelter benefits can be transferable, depending on the laws governing tax liability in the jurisdiction where the property is located.  These can be sold to others for a cash return or other benefits.  Equity build-up is the increase in the investor's equity ratio as the portion of debt service payments devoted to principal accrue over time. Equity build-up counts as positive cash flow from the asset where the debt service payment is made out of income from the property, rather than from independent income sources.  Capital appreciation is the increase in the market value of the asset over time, realized as a cash flow when the property is sold. Capital appreciation can be very unpredictable unless it is part of a development and improvement strategy. The purchase of a property for which the majority of the projected cash flows are expected from capital appreciation (prices going up) rather than other sources is considered speculation rather than investment. Research results that found that real estate firms are more likely to take a smaller stake in larger assets when investing abroad (Mauck & Price, 2017).  Some individuals and companies focus their investment strategy on purchasing properties that are in some stage of foreclosure. A property is considered in pre-foreclosure when the homeowner has defaulted on their mortgage loan. Formal foreclosure processes vary by state and may be judicial or non-judicial, which affects the length of time the property is in the pre-foreclosure phase. Once the formal foreclosure processes are underway, these properties can be purchased at a public sale, usually called a foreclosure auction or sheriff's sale. If the property does not sell at the public auction, then ownership of the property is returned to the lender.[17] Properties at this phase are called Real Estate Owned, or REOs.  Once a property is sold at the foreclosure auction or as an REO, the lender may keep the proceeds to satisfy their mortgage and any legal costs that they incurred minus the costs of the sale and any outstanding tax obligations.  The foreclosing bank or lending institution has the right to continue to honor tenant leases (if there are tenants in the property) during the REO phase but usually, the bank wants the property vacant to sell it more easily.[18]  Buy, rehab, rent, refinance (BRRR)[19] is a real estate investment strategy, used by real estate investors who have experience renovating or rehabbing properties to \"flip\" houses.[20] BRRR is different from \"flipping\" houses. Flipping houses implies buying a property and quickly selling it for a profit, with or without repairs. BRRR is a long-term investment strategy that involves renting out a property and letting it appreciate in value before selling it. Renting out a BRRR property provides a stable passive income source that is used to cover mortgage payments while home price appreciation increases future capital gains.[21]  The phrase was slightly updated in a 2022 Bloomberg News article noting that BiggerPockets added \"Repeat\" to the end, making it \"BRRRR\" to describe a real estate investing strategy of Buy, Rehab, Rent, Refinance, Repeat.[22]  According to Lima et al. (2022), in Ireland, the financialization of rental housing, which includes the entry of institutional investors into urban rental housing markets, contributed to structural factors that create homelessness directly by worsening affordability and security in the private rental market, and indirectly by influencing state policy.[23][24] It was found that the history, politics, and geography of the REITs cause the collapse of Irelands market (Waldron, 2018). "},"meta":{},"created_at":"2025-03-22T14:25:42.275677Z","updated_at":"2025-03-22T14:25:42.275677Z","inner_id":33,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":42,"annotations":[{"id":42,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.311403Z","updated_at":"2025-03-22T14:25:42.311403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"b19dcd83-26f2-4db5-8197-ec442e15b118","import_id":null,"last_action":null,"bulk_created":false,"task":42,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"This is an accepted version of this page   Accounting, also known as accountancy, is the process of recording and processing information about economic entities, such as businesses and corporations.[1][2] Accounting measures the results of an organization's economic activities and conveys this information to a variety of stakeholders, including investors, creditors, management, and regulators.[3] Practitioners of accounting are known as accountants. The terms \"accounting\" and \"financial reporting\" are often used interchangeably.[4]  Accounting can be divided into several fields including financial accounting, management accounting, tax accounting and cost accounting.[5] Financial accounting focuses on the reporting of an organization's financial information, including the preparation of financial statements, to the external users of the information, such as investors, regulators and suppliers.[6] Management accounting focuses on the measurement, analysis and reporting of information for internal use by management to enhance business operations.[1][6] The recording of financial transactions, so that summaries of the financials may be presented in financial reports, is known as bookkeeping, of which double-entry bookkeeping is the most common system.[7] Accounting information systems are designed to support accounting functions and related activities.  Accounting has existed in various forms and levels of sophistication throughout human history. The double-entry accounting system in use today was developed in medieval Europe, particularly in Venice, and is usually attributed to the Italian mathematician and Franciscan friar Luca Pacioli.[8]  Today, accounting is facilitated by accounting organizations such as standard-setters, accounting firms and professional bodies. Financial statements are usually audited by accounting firms,[9] and are prepared in accordance with generally accepted accounting principles (GAAP).[6] GAAP is set by various standard-setting organizations such as the Financial Accounting Standards Board (FASB) in the United States[1] and the Financial Reporting Council in the United Kingdom. As of 2012, \"all major economies\" have plans to converge towards or adopt the International Financial Reporting Standards (IFRS).[10][11]  Accounting is thousands of years old and can be traced to ancient civilizations.[12][13][14] One early development of accounting dates back to ancient Mesopotamia and is closely related to developments in writing, counting and money;[12] there is also evidence of early forms of bookkeeping in ancient Iran,[15][16] and early auditing systems by the ancient Egyptians and Babylonians.[13] By the time of Emperor Augustus, the Roman government had access to detailed financial information.[17]  Many concepts related to today's accounting seem to be initiated in medieval's Middle East. For example, Jewish communities used double-entry bookkeeping in the early-medieval period[18][19] and Muslim societies, at least since the 10th century also used many modern accounting concepts.[20]  The spread of the use of Arabic numerals, instead of the Roman numbers historically used in Europe, increased efficiency of accounting procedures among Mediterranean merchants,[21] who further refined accounting in medieval Europe.[22] With the development of joint-stock companies, accounting split into financial accounting and management accounting.  The first published work on a double-entry bookkeeping system was the Summa de arithmetica, published in Italy in 1494 by Luca Pacioli (the \"Father of Accounting\").[23][24] Accounting began to transition into an organized profession in the nineteenth century,[25][26] with local professional bodies in England merging to form the Institute of Chartered Accountants in England and Wales in 1880.[27]  Both the words \"accounting\" and \"accountancy\" were in use in Great Britain by the mid-1800s and are derived from the words accompting and accountantship used in the 18th century.[28] In Middle English (used roughly between the 12th and the late 15th century), the verb \"to account\" had the form accounten, which was derived from the Old French word aconter,[29] which is in turn related to the Vulgar Latin word computare, meaning \"to reckon\". The base of computare is putare, which \"variously meant to prune, to purify, to correct an account, hence, to count or calculate, as well as to think\".[29]  The word \"accountant\" is derived from the French word compter, which is also derived from the Italian and Latin word computare. The word was formerly written in English as \"accomptant\", but in process of time the word, which was always pronounced by dropping the \"p\", became gradually changed both in pronunciation and in orthography to its present form.[30]  Accounting has variously been defined as the keeping or preparation of the financial records of transactions of the firm, the analysis, verification and reporting of such records and \"the principles and procedures of accounting\"; it also refers to the job of being an accountant.[31][32][33]  Accountancy refers to the occupation or profession of an accountant,[34][35][36] particularly in British English.[31][32]  Accounting has several subfields or subject areas, including financial accounting, management accounting, auditing, taxation and accounting information systems.[5]  Financial accounting focuses on the reporting of an organization's financial information to external users of the information, such as investors, potential investors and creditors. It calculates and records business transactions and prepares financial statements for the external users in accordance with generally accepted accounting principles (GAAP).[6] GAAP, in turn, arises from the wide agreement between accounting theory and practice, and changes over time to meet the needs of decision-makers.[1]  Financial accounting produces past-oriented reports—for example financial statements are often published six to ten months after the end of the accounting period—on an annual or quarterly basis, generally about the organization as a whole.[6]  Management accounting focuses on the measurement, analysis and reporting of information that can help managers in making decisions to fulfill the goals of an organization. In management accounting, internal measures and reports are based on cost–benefit analysis, and are not required to follow the generally accepted accounting principle (GAAP).[6] In 2014 CIMA created the Global Management Accounting Principles (GMAPs). The result of research from across 20 countries in five continents, the principles aim to guide best practice in the discipline.[37]  Management accounting produces past-oriented reports with time spans that vary widely, but it also encompasses future-oriented reports such as budgets. Management accounting reports often include financial and non financial information, and may, for example, focus on specific products and departments.[6]  Intercompany accounting focuses on the measurement, analysis and reporting of information between separate entities that are related, such as a parent company and its subsidiary companies.  Intercompany accounting concerns record keeping of transactions between companies that have common ownership such as a parent company and a partially or wholly owned subsidiary. Intercompany transactions are also recorded in accounting when business is transacted between companies with a common parent company (subsidiaries).[38][39]  Auditing is the verification of assertions made by others regarding a payoff,[40] and in the context of accounting it is the \"unbiased examination and evaluation of the financial statements of an organization\".[41] Audit is a professional service that is systematic and conventional.[42]  An audit of financial statements aims to express or disclaim an independent opinion on the financial statements. The auditor expresses an independent opinion on the fairness with which the financial statements presents the financial position, results of operations, and cash flows of an entity, in accordance with the generally accepted accounting principles  (GAAP) and \"in all material respects\". An auditor is also required to identify circumstances in which the generally accepted accounting principles (GAAP) have not been consistently observed.[43]  An accounting information system is a part of an organization's information system used for processing accounting data.[44] Many corporations use artificial intelligence-based information systems. The banking and finance industry uses AI in fraud detection. The retail industry uses AI for customer services. AI is also used in the cybersecurity industry. It involves computer hardware and software systems using statistics and modeling.[45]  Many accounting practices have been simplified with the help of accounting computer-based software. An enterprise resource planning (ERP) system is commonly used for a large organisation and it provides a comprehensive, centralized, integrated source of information that companies can use to manage all major business processes, from purchasing to manufacturing to human resources. These systems can be cloud based and available on demand via application or browser, or available as software installed on specific computers or local servers, often referred to as on-premise.  Tax accounting in the United States concentrates on the preparation, analysis and presentation of tax payments and tax returns. The U.S. tax  system requires the use of specialised accounting principles for tax purposes which can differ from the generally accepted accounting principles (GAAP) for financial reporting.[46] U.S. tax law covers four basic forms of business ownership: sole proprietorship, partnership, corporation, and limited liability company. Corporate and personal income are taxed at different rates, both varying according to income levels and including varying marginal rates (taxed on each additional dollar of income) and average rates (set as a percentage of overall income).[46]  Forensic accounting is a specialty practice area of accounting that describes engagements that result from actual or anticipated disputes or litigation.[47] \"Forensic\" means \"suitable for use in a court of law\", and it is to that standard and potential outcome that forensic accountants generally have to work.  Political campaign accounting deals with the development and implementation of financial systems and the accounting of financial transactions in compliance with laws governing political campaign operations. This branch of accounting was first formally introduced in the March 1976 issue of The Journal of Accountancy.[48]  Professional accounting bodies include the American Institute of Certified Public Accountants (AICPA) and the other 179 members of the International Federation of Accountants (IFAC),[49] including Institute of Chartered Accountants of Scotland (ICAS), Institute of Chartered Accountants of Pakistan (ICAP), CPA Australia, Institute of Chartered Accountants of India, Association of Chartered Certified Accountants (ACCA) and Institute of Chartered Accountants in England and Wales (ICAEW). Some countries have a single professional accounting body and, in some other countries, professional bodies for subfields of the accounting professions also exist, for example the Chartered Institute of Management Accountants (CIMA) in the UK and Institute of management accountants in the United States.[50] Many of these professional bodies offer education and training including qualification and administration for various accounting designations, such as certified public accountant (AICPA) and chartered accountant.[51][52]  Depending on its size, a company may be legally required to have their financial statements audited by a qualified auditor, and audits are usually carried out by accounting firms.[9]  Accounting firms grew in the United States and Europe in the late nineteenth and early twentieth century, and through several mergers there were large international accounting firms by the mid-twentieth century. Further large mergers in the late twentieth century led to the dominance of the auditing market by the \"Big Five\" accounting firms: Arthur Andersen, Deloitte, Ernst & Young, KPMG and PricewaterhouseCoopers.[53] The demise of Arthur Andersen following the Enron scandal reduced the Big Five to the Big Four.[54]  Generally accepted accounting principles (GAAP) are accounting standards issued by national regulatory bodies. In addition, the International Accounting Standards Board (IASB) issues the International Financial Reporting Standards (IFRS) implemented by 147 countries.[1] Standards for international audit and assurance, ethics, education, and public sector accounting are all set by independent standard settings boards supported by IFAC. The International Auditing and Assurance Standards Board sets international standards for auditing, assurance, and quality control; the International Ethics Standards Board for Accountants (IESBA)[55] sets the internationally appropriate principles-based Code of Ethics for Professional Accountants; the International Accounting Education Standards Board (IAESB) sets professional accounting education standards;[56] and International Public Sector Accounting Standards Board (IPSASB) sets accrual-based international public sector accounting standards.[57][4]  Organizations in individual countries may issue accounting standards unique to the countries. For example, in Australia, the Australian Accounting Standards Board manages the issuance of the accounting standards in line with IFRS. In the United States the Financial Accounting Standards Board (FASB) issues the Statements of Financial Accounting Standards, which form the basis of US GAAP,[1] and in the United Kingdom the Financial Reporting Council (FRC) sets accounting standards.[58] However, as of 2012 \"all major economies\" have plans to converge towards or adopt the IFRS.[10]  At least a bachelor's degree in accounting or a related field is required for most accountant and auditor job positions, and some employers prefer applicants with a master's degree.[59] A degree in accounting may also be required for, or may be used to fulfill the requirements for, membership to professional accounting bodies. For example, the education during an accounting degree can be used to fulfill the American Institute of CPA's (AICPA) 150 semester hour requirement,[60] and associate membership with the Certified Public Accountants Association of the UK is available after gaining a degree in finance or accounting.[61]  A doctorate is required in order to pursue a career in accounting academia, for example, to work as a university professor in accounting.[62][63] The Doctor of Philosophy (PhD) and the Doctor of Business Administration (DBA) are the most popular degrees. The PhD is the most common degree for those wishing to pursue a career in academia, while DBA programs generally focus on equipping business executives for business or public careers requiring research skills and qualifications.[62]  Professional accounting qualifications include the chartered accountant designations and other qualifications including certificates and diplomas.[64]  In Scotland, chartered accountants of ICAS undergo Continuous Professional Development and abide by the ICAS code of ethics.[65] In England and Wales, chartered accountants of the ICAEW undergo annual training, and are bound by the ICAEW's code of ethics and subject to its disciplinary procedures.[66]  In the United States, the requirements for joining the AICPA as a Certified Public Accountant are set by the Board of Accountancy of each state, and members agree to abide by the AICPA's Code of Professional Conduct and Bylaws.  The ACCA is the largest global accountancy body with over 320,000 members, and the organisation provides an 'IFRS stream' and a 'UK stream'. Students must pass a total of 14 exams, which are arranged across three levels.[67]  Accounting research is research in the effects of economic events on the process of accounting, the effects of reported information on economic events, and the roles of accounting in organizations and society.[68][69] It encompasses a broad range of research areas including financial accounting, management accounting, auditing and taxation.[70]  Accounting research is carried out both by academic researchers and practicing accountants. Methodologies in academic accounting research include archival research, which examines \"objective data collected from repositories\"; experimental research, which examines data \"the researcher gathered by administering treatments to subjects\"; analytical research, which is \"based on the act of formally modeling theories or substantiating ideas in mathematical terms\"; interpretive research, which emphasizes the role of language, interpretation and understanding in accounting practice, \"highlighting the symbolic structures and taken-for-granted themes which pattern the world in distinct ways\"; critical research, which emphasizes the role of power and conflict in accounting practice; case studies; computer simulation; and field research.[71][72]  Empirical studies document that leading accounting journals publish in total fewer research articles than comparable journals in economics and other business disciplines,[73] and consequently, accounting scholars[74] are relatively less successful in academic publishing than their business school peers.[75] Due to different publication rates between accounting and other business disciplines, a recent study based on academic author rankings concludes that the competitive value of a single publication in a top-ranked journal is highest in accounting and lowest in marketing.[76]  The year 2001 witnessed a series of financial information frauds involving Enron, auditing firm Arthur Andersen, the telecommunications company WorldCom, Qwest and Sunbeam, among other well-known corporations. These problems highlighted the need to review the effectiveness of accounting standards, auditing regulations and corporate governance principles. In some cases, management manipulated the figures shown in financial reports to indicate a better economic performance. In others, tax and regulatory incentives encouraged over-leveraging of companies and decisions to bear extraordinary and unjustified risk.[77]  The Enron scandal deeply influenced the development of new regulations to improve the reliability of financial reporting, and increased public awareness about the importance of having accounting standards that show the financial reality of companies and the objectivity and independence of auditing firms.[77]  In addition to being the largest bankruptcy reorganization in American history, the Enron scandal undoubtedly is the biggest audit failure[78] causing the dissolution of Arthur Andersen, which at the time was one of the five largest accounting firms in the world. After a series of revelations involving irregular accounting procedures conducted throughout the 1990s, Enron filed for Chapter 11 bankruptcy protection in December 2001.[79]  One consequence of these events was the passage of the Sarbanes–Oxley Act in the United States in 2002, as a result of the first admissions of fraudulent behavior made by Enron. The act significantly raises criminal penalties for securities fraud, for destroying, altering or fabricating records in federal investigations or any scheme or attempt to defraud shareholders.[80]  Accounting fraud is an intentional misstatement or omission in the accounting records by management or employees which involves the use of deception. It is a criminal act and a breach of civil tort. It may involve collusion with third parties.[81]  An accounting error is an unintentional misstatement or omission in the accounting records, for example misinterpretation of facts, mistakes in processing data, or oversights leading to incorrect estimates.[81] Acts leading to accounting errors are not criminal but may breach civil law, for example, the tort of negligence.  The primary responsibility for the prevention and detection of fraud and errors rests with the entity's management.[81] "},"meta":{},"created_at":"2025-03-22T14:25:42.275677Z","updated_at":"2025-03-22T14:25:42.275677Z","inner_id":34,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":43,"annotations":[{"id":43,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.311403Z","updated_at":"2025-03-22T14:25:42.311403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"6292aaae-c1e8-41eb-b2e5-9e7399fa6ff0","import_id":null,"last_action":null,"bulk_created":false,"task":43,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":" Crowdfunding is the practice of funding a project or venture by raising money from a large number of people, typically via the internet.[1][2] Crowdfunding is a form of crowdsourcing and alternative finance. In 2015, over US$34 billion was raised worldwide by crowdfunding.[3]  Although similar concepts can also be executed through mail-order subscriptions, benefit events, and other methods, the term crowdfunding refers to internet-mediated registries.[4] This modern crowdfunding model is generally based on three types of actors – the project initiator who proposes the idea or project to be funded, individuals or groups who support the idea, and a moderating organization (the \"platform\") that brings the parties together to launch the idea.[5]  The term crowdfunding was coined in 2006 by entrepreneur and technologist, Michael Sullivan, to differentiate traditional fundraising with the trends of native Internet projects, companies and community efforts to support various kinds of creators. Crowdfunding has been used to fund a wide range of for-profit entrepreneurial ventures such as artistic and creative projects,[6] medical expenses, travel, and community-oriented social entrepreneurship projects.[7] Although crowdfunding has been suggested to be highly linked to sustainability, empirical validation has shown that sustainability plays only a fractional role in crowdfunding.[8] Its use has also been criticized for funding quackery, especially costly and fraudulent cancer treatments.[9][10][11][12]  Funding by collecting small donations from many people has a long history with many roots. Books have been funded in this way in the past; authors and publishers would advertise book projects in praenumeration or subscription schemes. The book would be written and published if enough subscribers signaled their readiness to buy the book once it was out. The subscription business model is not exactly crowdfunding, since the actual flow of money only begins with the arrival of the product. However, the list of subscribers has the power to create the necessary confidence among investors that is needed to risk the publication.[14]  War bonds are theoretically a form of crowdfunding military conflicts. London's mercantile community saved the Bank of England in the 1730s when customers demanded their pounds to be converted into gold – they supported the currency until confidence in the pound was restored, thus crowdfunding their own money. A clearer case of modern crowdfunding is Auguste Comte's scheme to issue notes for the public support of his further work as a philosopher. The \"Première Circulaire Annuelle adressée par l'auteur du Système de Philosophie Positive\" was published on March 14, 1850, and several of these notes, blank and with sums, have survived.[13] The cooperative movement of the 19th and 20th centuries is a broader precursor. It generated collective groups, such as community or interest-based groups, pooling subscribed funds to develop new concepts, products, and means of distribution and production, particularly in rural areas of Western Europe and North America. In 1885, when government sources failed to provide funding to build a monumental base for the Statue of Liberty, a newspaper-led campaign attracted small donations from 160,000 donors.[14]  Crowdfunding on the internet first gained popular and mainstream use in the arts and music communities.[15] One of the earlier instances of online crowdfunding in the music industry was in 1997, when fans of the British rock band Marillion raised US$60,000 in donations through an Internet campaign to underwrite an entire U.S. tour however this was not crowdfunding in its true sense as it wasn't asked for by the band and only reluctantly taken. The band subsequently used this method to fund their studio albums.[16][17][18] This built on the success of crowdfunding via magazines, such as the 1992 campaign by the Vegan Society that crowdfunded the production of the Truth or Dairy video documentary.[19] In the film industry, writer\/director Mark Tapio Kines designed a website in 1997 for his then-unfinished first feature film, the independent drama Foreign Correspondents. By early 1999, he had raised more than US$125,000 through the site from various fans and investors, providing him with the funds to complete his film.[20] In 2002, the \"Free Blender\" campaign was an early software crowdfunding precursor.[21][22] The campaign aimed for open-sourcing the Blender 3D computer graphics software by collecting €100,000 from the community, while offering additional benefits for donating members.[23][24]  The first company to engage in this business model was the U.S. website ArtistShare (2001).[25][26] As the model matured, more crowdfunding sites started to appear on the web such as Kiva (2005), The Point (2008, precursor to Groupon), Indiegogo (2008), Kickstarter (2009), GoFundMe (2010), Microventures (2010), YouCaring (2011).,[27][28] and Redshine Publication (2012) for book publication.[29]  The phenomenon of crowdfunding is older than the term \"crowdfunding\". The earliest recorded use of the word was in August 2006.[30] Crowdfunding is a part of crowdsourcing, which is a much wider phenomenon itself.  The Crowdfunding Centre's May 2014 report identified two primary types of crowdfunding:  Reward-based crowdfunding has been used for a wide range of purposes, including album recording and motion-picture promotion,[32] free software development, inventions development, scientific research,[33] and civic projects.[34]  Many characteristics of rewards-based crowdfunding, also called non-equity crowdfunding, have been identified by research studies. In rewards-based crowdfunding, funding does not rely on location. The distance between creators and investors on Sellaband was about 3,000 miles when the platform introduced royalty sharing. The funding for these projects is distributed unevenly, with a few projects accounting for the majority of overall funding. Additionally, funding increases as a project nears its goal, encouraging what is called \"herding behavior\". Research also shows that friends and family account for a large, or even majority, portion of early fundraising. This capital may encourage subsequent funders to invest in the project. While funding does not depend on location, observation shows that funding is largely tied to the locations of traditional financing options. In reward-based crowdfunding, funders are often too hopeful about project returns and must revise expectations when returns are not met.[15]  Equity crowdfunding is the collective effort of individuals to support efforts initiated by other people or organizations through the provision of finance in the form of equity.[35] In the United States, legislation that is mentioned in the 2012 JOBS Act will allow for a wider pool of small investors with fewer restrictions following the implementation of the act.[36] Unlike non-equity crowdfunding, equity crowdfunding contains heightened \"information asymmetries.\" The creator must not only produce the product for which they are raising capital, but also create equity through the construction of a company.[15] Equity crowdfunding, unlike donation and rewards-based crowdfunding, involves the offer of securities which include the potential for a return on investment. Syndicates, which involve many investors following the strategy of a single lead investor, can be effective in reducing information asymmetry and in avoiding the outcome of market failure associated with equity crowdfunding.[37]  Another kind of crowdfunding is to raise funds for a project where a digital security is offered as a reward to funders which is known as Initial coin offering (abbreviated to ICO).[38] Some value tokens are endogenously created by particular open decentralized networks that are used to incentivize client computers of the network to expend scarce computer resources on maintaining the protocol network. These value tokens may or may not exist at the time of the crowdsale, and may require substantial development effort and eventual software release before the token is live and establishes a market value. Although funds may be raised simply for the value token itself, funds raised on blockchain-based crowdfunding can also represent equity, bonds, or even \"market-maker seats of governance\" for the entity being funded.[39] Examples of such crowd sales are Augur decentralized, distributed prediction market software which raised US$4 million from more than 3500 participants;[39] Ethereum blockchain; and \"the Decentralized Autonomous Organization\".[40][41][42][43]  Debt-based crowdfunding, (also known as \"peer-to-peer\", \"P2P\", \"marketplace lending\", or \"crowdlending\") arose with the founding of Zopa in the UK in 2005[44] and in the US in 2006, with the launches of Lending Club and Prosper.com.[45] Borrowers apply online, generally for free, and their application is reviewed and verified by an automated system, which also determines the borrower's credit risk and interest rate. Investors buy securities in a fund that makes the loans to individual borrowers or bundles of borrowers. Investors make money from interest on the unsecured loans; the system operators make money by taking a percentage of the loan and a loan servicing fee.[45] In 2009, institutional investors entered the P2P lending arena; for example in 2013, Google invested $125 million in Lending Club.[45] In 2014, in the US, P2P lending totaled about $5 billion.[46] In 2014, in the UK, P2P platforms lent businesses £749 million, a growth of 250% from 2012 to 2014, and lent retail customers £547 million, a growth of 108% from 2012 to 2014.[47]: 23  In both countries in 2014, about 75% of all the money transferred through crowdfunding went through P2P platforms.[46] Lending Club went public in December 2014 at a valuation around $9 billion.[45]  Litigation crowdfunding allows plaintiffs or defendants to reach out to hundreds of their peers simultaneously in a semi-private and confidential manner to obtain funding, either seeking donations or providing a reward in return for funding. It also allows investors to purchase a stake in a claim they have funded, which may allow them to get back more than their investment if the case succeeds (the reward is based on the compensation received by the litigant at the end of his or her case, known as a contingent fee in the United States, a success fee in the United Kingdom, or a pactum de quota litis in many civil law systems).[48] LexShares is a platform that allows accredited investors to invest in lawsuits.[49]  Donation-based crowdfunding is the collective effort of individuals to help charitable causes.[50] In donation-based crowdfunding, funds are raised for religious, social environmental, or other purposes.[51] Donors come together to create an online community around a common cause to help fund services and programs to combat a variety of issues including healthcare[52] and community development.[53] The major aspect of donor-based crowdfunding is that there is no reward for donating; rather, it is based on the donor's altruistic reasoning.[54] Ethical concerns have been raised to the increasing popularity of donation-based crowdfunding, which can be affected by fraudulent campaigns and privacy issues.[55]  The inputs of the individuals in the crowd trigger the crowdfunding process and influence the ultimate value of the offerings or outcomes of the process. Individuals act as agents of the offering, selecting, and promoting of the projects in which they believe. They sometimes play a donor role oriented towards providing help on social projects. In some cases, they become shareholders and contribute to the development and growth of the offering. Individuals disseminate information about projects they support in their online communities, generating further support (promoters).  The motivation for consumer participation stems from the feeling of being at least partly responsible for the success of others people's initiatives (desire for patronage), striving to be a part of a communal social initiative (desire for social participation), and seeking a payoff from monetary contributions (desire for investment).[5] Additionally, individuals participate in crowdfunding to see new products before the public. Early access often allows funders to participate more directly in the development of the product. Crowdfunding is also particularly attractive to funders who are family and friends of a creator. It helps to mediate the terms of their financial agreement and manage each group's expectations for the project.[15]  An individual who takes part in crowdfunding initiatives tends to have several distinct traits – innovative orientation, which stimulates the desire to try new modes of interacting with firms and other consumers; social identification with the content, cause, or project selected for funding, which sparks the desire to be a part of the initiative; and (monetary) exploitation, which motivates the individual to participate by expecting a payoff.[5] Crowdfunding platforms are motivated to generate income by drawing worthwhile projects and generous funders. These sites also seek widespread public attention for their projects and platform.[15]  Crowdfunding websites helped companies and individuals worldwide raise US$89 million from members of the public in 2010, $1.47 billion in 2011, and $2.66 billion in 2012 — $1.6 billion of the 2012 amount was raised in North America.[56]  Crowdfunding is expected to reach US$1 trillion in 2025.[57] A May 2014 report, released by the United Kingdom-based The Crowdfunding Centre and titled \"The State of the Crowdfunding Nation\", presented data showing that during March 2014, more than US$60,000 were raised on an hourly basis via global crowdfunding initiatives. Also during this period, 442 crowdfunding campaigns were launched globally on a daily basis.[31]  The future growth potential of crowdfunding platforms also depends on their financing volume with venture capital. Between January 2017 and April 2020 globally 99 venture capital financing rounds for crowdfunding platforms took place with more than half a billion USD of total money raised. The median amount per venture capital financing rounds for crowdfunding was $5 million in the U.S. and $1.5 million in Europe between January 2017 and April 2020.[58]  In 2015, it was predicted that over 2,000 crowdfunding sites would be available to choose from in 2016.[59] As of 2021, there are 1,478 crowdfunding organizations in the US (Crunchbase, 2021).[60] As of January 2021, Kickstarter has raised more than $5.6 billion spread over 197,425 projects.[61]  Crowdfunding platforms have differences in the services they provide and the type of projects they support.[5]  Curated crowdfunding platforms serve as \"network orchestrators\" by curating the offerings that are allowed on the platform. They create the necessary organizational systems and conditions for resource integration among other players to take place.[5] Relational mediators act as an intermediary between supply and demand. They replace traditional intermediaries (such as traditional record companies, venture capitalists). These platforms link new artists, designers, project initiators with committed supporters who believe in the persons behind the projects strongly enough to provide monetary support.[15]  In response to arbitrary crowdfunding curation on existing platforms, an open source alternative called Selfstarter[62] emerged in late 2012 from the project Lockitron after it was rejected from Kickstarter.[63] While Selfstarter required the creators of the project to set up hosting and payment processing, it proved that projects could successfully crowdfund without middlemen taking a significant percentage of the money raised.  In the summer of 1885, crowdfunding averted a crisis that threatened the completion of the Statue of Liberty. Construction of the statue's pedestal stalled due to a lack of financing. Fundraising efforts for the project fell short of the necessary amount by more than a third. New York Governor Grover Cleveland refused to appropriate city funds for the project, and Congress could not agree on a funding package.  Recognizing the social and symbolic significance of the statue, publisher Joseph Pulitzer came to the rescue by launching a five-month fundraising campaign in his newspaper The World. The paper solicited contributions by publishing articles that appealed to the emotions of New Yorkers. Donations of all sizes poured in, ranging from $0.15 to $250. More than 160,000 people across America gave, including businessmen, waiters, children, and politicians. The paper chronicled each donation, published letters from contributors on the front page, and kept a running tally of funds raised.  The campaign raised over $100,000 (roughly $2 million today) allowing the city to complete construction of the pedestal. Pulitzer and The World simultaneously saved the Statue of Liberty and gave birth to crowdfunding in American politics.  Crowdfunding for Cairo University  The Egyptian national leader, Mustafa Kamel, launched an initiative for public subscription in favor of establishing the first Egyptian university, and published an advertisement in Al-Ahram newspaper in October 1906 calling on Egyptians to fulfill the nation's debt and not procrastinate with it. Indeed, many people including school children rushed to donate, and the patriots encouraged this subscription until donations exceeded 4,400 Egyptian pounds.  The National University was opened on December 21, 1908, in a large ceremony in the hall of the Shura Council of Laws, in the presence of Khedive Abbas II and senior statesmen and notables. Its director was the politician and writer Ahmed Lutfi al-Sayyid while the chairman of its board of directors was King Fouad the first. In 1953 the National University changed its name to Cairo University.  Marillion started crowdfunding in 1997. Fans of the British rock band raised $60,000 (£39,000) via the internet to help finance a North American tour.[64][17] The Professional Contractors Group, a trade body representing freelancers in the UK, raised £100,000 over a two-week period in 1999[65] from some 2000 freelancers threatened by a government measure known as IR35. In 2003, jazz composer Maria Schneider (musician) launched the first crowdfunding campaign on ArtistShare for a new recording.[66] The recording was funded by her fans and became the first recording in history to win a Grammy Award without being available in retail stores.  Oliver Twisted (Erik Estrada, Karen Black) was an early crowdfunded film.[67] Subscribers of The Blue Sheet formed The Florida Film Investment Co (FFI) in January 1995, and started selling shares of stock at $10 a share to fund the $80,000 – $100,000 film. The Movie was filmed in Oct 1996. The film was distributed by RGH\/Lion's Shares Pictures.[68]  In 2004, Electric Eel Shock, a Japanese rock band, raised £10,000 from 100 fans (the Samurai 100) by offering them a lifetime membership on the band's guestlist.[69] Two years later, they became the fastest band to raise a US$50,000 budget on SellaBand.[70] Franny Armstrong later created a donation system for her feature film The Age of Stupid.[71] Over five years, from June 2004 to June 2009 (release date), she raised £1,500,000.[72]  As of late 2022, the highest reported funding by a crowdfunded project to date is Star Citizen, an online space trading and combat video game being developed by Chris Roberts and Cloud Imperium Games; it has raised over $500M to date, and while it has a devoted fan base, criticism has arisen for being a potential scam.[73]  On April 17, 2014, The Guardian media outlet published a list of \"20 of the most significant projects\" launched on the Kickstarter platform prior to the date of publication,[74] including: Musician Amanda Palmer raised US$1.2 million from 24,883 backers in June 2012 to make a new album and art book.[75]  Other campaigns include:  Kickstarter has been used to successfully revive or launch television and film projects that could not get funding elsewhere.[79] These are the current record holders for projects in the \"film\" category:  A number of private companies thrive off of crowdfunding and offer services related to a number of platforms. Examples include large companies like BackerKit that principally offer data analysis of campaigns, or Y Combinator, which acts as a startup accelerator and receives a significant number of its applicants from platforms such as Kickstarter and Indiegogo.[83] The Italian-American company Atellani USA was originally founded with the intent to market, accelerate, and invest in startups wanting to publicize their ideas via crowdfunding platforms like Kickstarter, often designing the startup's campaign and online material.  Crowdfunding is being explored as a potential funding mechanism for creative work such as blogging and journalism,[84] music, independent film (see crowdfunded film),[85][86] and for funding startup companies.[87][88][89][90]  Community music labels are usually for-profit organizations where \"fans assume the traditional financier role of a record label for artists they believe in by funding the recording process\".[91] Since pioneering crowdfunding in the film industry, Spanner Films has published a \"how-to\" guide.[92] A Financial article published in mid-September 2013 stated that \"the niche for crowdfunding exists in financing films with budgets in the [US]$1 to $10 million range\" and crowdfunding campaigns are \"much more likely to be successful if they tap into a significant pre-existing fan base and fulfill an existing gap in the market.\"[93] Innovative new platforms, such as RocketHub, have emerged that combine traditional funding for creative work with branded crowdsourcing—helping artists and entrepreneurs unite with brands \"without the need for a middle man.\"[94]  A variety of crowdfunding platforms have emerged to allow ordinary web users to support specific philanthropic projects without the need for large amounts of money.[34] GlobalGiving allows individuals to browse through a selection of small projects proposed by nonprofit organizations worldwide, donating funds to projects of their choice. Microcredit crowdfunding platforms such as Kiva (organization) facilitate crowdfunding of loans managed by microcredit organizations in developing countries. The US-based nonprofit Zidisha applies a direct person-to-person lending model to microcredit lending for low-income small business owners in developing countries.[95] In 2017, Facebook initiated \"Fundraisers\", an internal plug-in function that allows its users to raise money for nonprofits.[96]  DonorsChoose.org, founded in 2000, allows public school teachers in the United States to request materials for their classrooms. Individuals can lend money to teacher-proposed projects, and the organization fulfills and delivers supplies to schools. There are also a number of own-branded university crowdfunding websites, which enable students and staff to create projects and receive funding from alumni of the university or the general public. Several dedicated civic crowdfunding platforms have emerged in the US and the UK, some of which have led to the first direct involvement of governments in crowdfunding. In the UK, Spacehive is used by the Mayor of London and Manchester City Council to co-fund civic projects created by citizens.[97] Similarly, dedicated Humanitarian Crowdfunding initiatives are emerging, involving humanitarian organizations, volunteers, and supporters in solving and modeling how to build innovative crowdfunding solutions for the humanitarian community. Likewise, international organizations like the Office for the Coordination of Humanitarian Affairs (OCHA) have been researching and publishing about the topic.[98]  One crowdfunding project, iCancer, was used to support a Phase 1 trial of AdVince, an anti-cancer drug in 2016.[99][100]  Research into the suitability of crowdfunding for civic investment in the UK highlights that the public sector has not fully realized the benefits of a crowdfunding approach.[101]  Real estate crowdfunding is the online pooling of capital from investors to fund mortgages secured by real estate, such as \"fix and flip\" redevelopment of distressed or abandoned properties, equity for commercial and residential projects, acquisition of pools of distressed mortgages, home buyer down payments, and similar real estate related outlets. Investment, via specialized online platforms in the US, is generally completed under Title II of the JOBS Act and is limited to accredited investors. The platforms offer low minimum investments, often $100 – $10,000.[102][103] There are over 75 real estate crowdfunding platforms in the United States.[104] The growth of real estate crowdfunding is a global trend. During 2014 and 2015, more than 150 platforms have been created throughout the world, such as in China, the Middle East, or France. In Europe, some compare this growing industry to that of e-commerce ten years earlier.[105] Examples of real estate crowdfunding platforms are EquityMultiple, Fundrise, Yieldstreet, CrowdStreet, RealtyMogul, and SmartCrowd, the first digital real estate crowdfunding platform of its kind in the Middle East.[106][107][108]  In Europe, the requirements towards investors are not as high as in the United States, lowering the entry barrier into the real estate investments in general.[109] Real estate crowdfunding can include various project types from commercial to residential developments, planning gain opportunities, build to hold (such as social housing), and many more. The report from Cambridge Centre for Alternative Finance addresses both real estate crowdfunding and peer 2 peer lending (property) in the UK.[110]  One of the challenges of posting new ideas on crowdfunding sites is there may be little or no intellectual property (IP) protection provided by the sites themselves. Once an idea is posted, it can be copied. As Slava Rubin, founder of IndieGoGo, said: \"We get asked that all the time, 'How do you protect me from someone stealing my idea?' We're not liable for any of that stuff.\"[111] Inventor advocates, such as Simon Brown, founder of the UK-based United Innovation Association, counsel that ideas can be protected on crowdfunding sites through early filing of patent applications, use of copyright and trademark protection as well as a new form of idea protection supported by the World Intellectual Property Organization called Creative Barcode.[112]  A number of platforms have also emerged that specialize in the crowdfunding of scientific projects, such as experiment.com, and The Open Source Science Project.[113][114] In the scientific community, these new options for research funding are seen ambivalently. Advocates of crowdfunding for science emphasize that it allows early-career scientists to apply for their own projects early on, that it forces scientists to communicate clearly and comprehensively to a broader public, that it may alleviate problems of the established funding systems which are seen to fund conventional, mainstream projects, and that it gives the public a say in science funding.[115] In turn, critics are worried about quality control on crowdfunding platforms. If non-scientists were allowed to make funding decisions, it would be more likely that \"panda bear science\" is funded, i.e. research with broad appeal but lacking scientific substance.[116]  Initial studies found that crowdfunding is used within science, mostly by young researchers to fund small parts of their projects, and with high success rates. At the same time, funding success seems to be strongly influenced by non-scientific factors like humor, visualizations, or the ease and security of payment.[117]  In order to fund online and print publications, journalists are enlisting the help of crowdfunding. Crowdfunding allows for small start-ups and individual journalists to fund their work without the institutional help of major public broadcasters. Stories are publicly pitched using crowdfunding platforms such as Kickstarter, Indiegogo, or Spot.us. The funds collected from crowdsourcing may be put toward travel expenses or purchasing equipment. Crowdfunding in journalism may also be viewed as a way to allow audiences to participate in news production and in creating a participatory culture.[118] Though deciding which stories are published is a role that traditionally belongs to editors at more established publications, crowdfunding can give the public an opportunity to provide input in deciding which stories are reported. This is done by funding certain reporters and their pitches. Donating can be seen as an act that \"bonds\" reporters and their readers. This is because readers are expressing interest for their work, which can be \"personally motivating\" or \"gratifying\" for reporters.[119]  Spot.us, which was closed in February 2015, was a crowdfunding platform that was specifically meant for journalism.[118][120] The website allowed for readers, individual donors, registered Spot.us reporters, or news organizations to fund or donate talent toward a pitch of their choosing. While funders are not normally involved in editorial control, Spot.us allowed for donors or \"community members\" to become involved with the co-creation of a story. This gave them the ability to edit articles, submit photographs, or share leads and information.[119] According to an analysis by Public Insight Network, Spot.us was not sustainable for various reasons. Many contributors were not returning donors and often, projects were funded by family and friends. The overall market for crowdfunding journalism may also be a factor; donations for journalism projects accounted for .13 percent of the $2.8 billion that was raised in 2013.[120]  Traditionally, journalists are not involved in advertising and marketing. Crowdfunding means that journalists are attracting funders while trying to remain independent, which may pose a conflict. Therefore, being directly involved with financial aspects can call journalistic integrity and journalistic objectivity into question. This is also due to the fact that journalists may feel some pressure or \"a sense of responsibility\" toward funders who support a particular project.[118] Crowdfunding can also allow for a blurred line between professional and non-professional journalism because if enough interest is generated, anyone may have their work published.[121] Crowdfunding enables freelance journalists to travel to the sites to find the new sources.[118]  There is some hope that crowdfunding has potential as a tool open for use by groups of people traditionally more marginalized.  The World Bank published a report titled \"Crowdfunding's Potential for the Developing World\" which states that \"While crowdfunding is still largely a developed world phenomenon, with the support of governments and development organizations it could become a useful tool in the developing world as well. Substantial reservoirs of entrepreneurial talent, activity, and capital lay dormant in many emerging economies ... Crowdfunding and crowdfund investing have several important roles to play in the developing world's entrepreneurial and venture finance ecosystem.\"[122]  As the popularity of crowdfunding expanded, the SEC, state governments, and Congress responded by enacting and refining many capital-raising exemptions to allow easier access to alternative funding sources. Initially, the Securities Act of 1933 banned companies from soliciting capital from the general public for private offerings. However, \"President Obama signed the Jumpstart Our Small Businesses Act ('JOBS Act') into law on April 5, 2012, which removed the ban on general solicitation activities for issuers qualifying under a new exemption called 'Rule 506(c).'\" A company can now broadly solicit and generally advertise an offering and still be compliant with the exemption's requirements if:  Another change was the amendment of SEC Rule 147. Section 3(a)(11) of the Securities Act allows for unlimited capital raising from investors in a single state through an intrastate exemption. However, the SEC created Rule 147 with a number of requirements to ensure compliance. For example, the intrastate solicitation was allowed, but a single out-of-state offer could destroy the exemption. Additionally, the issuer was required to be incorporated and do business in the same state of the intrastate offering. With the expansion of interstate business activities because of the internet, it became difficult for businesses to comply with the exemption. Therefore, on October 26, 2016, the SEC adopted Rule 147(a) which removed many of the restrictions to modernize the Rules. For example, companies would have to do business and have its principal place of business in the state where the offering is sold, and not necessarily where offered per the prior rule.[124]  As of 2024 33 crowdfunding permits were issued for financial institutions.[125]  Crowdfunding campaigns provide producers with several benefits, beyond the strict financial gains.[126] The following are the non-financial benefits of crowdfunding.  There are also financial benefits to the creator. For one, crowdfunding allows creators to attain low-cost capital. Traditionally, a creator would need to look at \"personal savings, home equity loans, personal credit cards, friends and family members, angel investors, and venture capitalists.\" With crowdfunding, creators can find funders from around the world, sell both their product and equity, and benefit from increased information flow. Additionally, crowdfunding that supports pre-buying allows creators to obtain early feedback on the product.[15] Another potential positive effect is the propensity of groups to \"produce an accurate aggregate prediction\" about market outcomes as identified by the author James Surowiecki in his book The Wisdom of Crowds, thereby placing financial backing behind ventures likely to succeed.  Proponents also identify a potential outcome of crowdfunding as an exponential increase in available venture capital. One report claims that if every American family gave one percent of their investable assets to crowdfunding, $300 billion (a 10X increase) would come into venture capital.[129] Proponents also cite that a benefit for companies receiving crowdfunding support is that they retain control of their operations, as voting rights are not conveyed along with ownership when crowdfunding. As part of his response to the Amanda Palmer Kickstarter controversy, Steve Albini expressed his supportive views of crowdfunding for musicians, explaining: \"I've said many times that I think they're part of the new way bands and their audience interact and they can be a fantastic resource, enabling bands to do things essentially in cooperation with their audience.\" Albini described the concept of crowdfunding as \"pretty amazing\".[130]  Crowdfunding, while gaining popularity, also comes with a number of potential risks or barriers.[4] For the creator, as well as the investor, studies show that crowdfunding contains \"high levels of risk, uncertainty, and information asymmetry.\"[15]  For crowdfunding of equity stock purchases, there is some research in social psychology that indicates that, like in all investments, people don't always do their due diligence to determine if it is a sound investment before investing, which leads to making investment decisions based on emotion rather than financial logic.[132] By using crowdfunding, creators also forgo potential support and value that a single angel investor or venture capitalist might offer. Likewise, crowdfunding requires that creators manage their investors. This can be time-consuming and financially burdensome as the number of investors in the crowd rises.[15] Crowdfunding draws a crowd: investors and other interested observers who follow the progress, or lack of progress, of a project. Sometimes it proves easier to raise the money for a project than to make the project a success. Managing communications with many possibly disappointed investors and supporters can be a substantial, and potentially diverting, task.[133]  Some of the most popular fundraising drives are for commercial companies that use the process to reach customers and at the same time market their products and services. This favors companies like microbreweries and specialist restaurants – in effect creating a \"club\" of people who are customers as well as investors. In the US in 2015, new rules from the SEC to regulate equity crowdfunding will mean that larger businesses with more than 500 investors and more than $25 million in assets will have to file reports like a public company. The Wall Street Journal commented: \"It is all the pain of an IPO without the benefits of the IPO.\"[134] These two trends may mean crowdfunding is most suited to small consumer-facing companies rather than tech start-ups.  There are several ways in which a well-regulated crowdfunding platform may provide the possibility of attractive returns for investors:  On crowdfunding platforms, the problem of information asymmetry is exacerbated due to the reduced ability of the investor to conduct due diligence.[37] Early-stage investing is typically localized, as the costs of conducting due diligence before making investment decisions and the costs of monitoring after investing both rise with distance. However, this trend is not observed on crowdfunding platforms – these platforms are not geographically constrained and bring in investors from near and far.[36][136] On non-equity or reward-based platforms, investors try to mitigate this risk by using the amount of capital raised as a signal of performance or quality. On equity-based platforms, crowdfunding syndicates reduce information asymmetry through dual channels – through portfolio diversification and better due diligence as in the case of offline early-stage investing, but also by allowing lead investors with more information and better networks to lead crowds of backers to make investment decisions.[37][137][138]  Crowdfunding platforms also carry the risk of money laundering.[139][140]  The rise of crowdfunding for medical expenses is considered, in large part, a symptom of an inadequate and failing healthcare system in countries such as the United States.[141][142] Healthcare through crowdfunding relies on perceived deservingness and worth, which reproduces unequal outcomes in access.[142]  Rob Solomon, the CEO of GoFundMe, has commented on this: \"The system is terrible. It needs to be rethought and retooled. Politicians are failing us. Health care companies are failing us. Those are realities. I don't want to mince words here. We are facing a huge potential tragedy. We provide relief for a lot of people. But there are people who are not getting relief from us or from the institutions that are supposed to be there. We shouldn't be the solution to a complex set of systemic problems.\"[143]  There are ethical issues in medical crowdfunding. Firstly, there is a loss of patient privacy.[144] Crowdfunding campaigns are generally more financially successful if extensive personal information is disclosed to the public.[144] Secondly, the oversight regarding the veracity of claims is generally limited.[144][145] For instance, physicians are obliged to uphold the ethics of the medical profession, such as patient confidentiality, but this runs in conflict with dishonest crowdfunding efforts.[145] Thirdly, medical crowdfunding perpetuates inequalities—associated with variables such as gender, class, and race—in access to healthcare.[142][144] For instance, there's a socioeconomic gradient with medical fundraising, in which a higher socioeconomic status coincides with higher donation amounts, higher proportions of fundraising targets reached, higher numbers of donations received, and more shares on social media.[146] Finally, the use of medical crowdfunding might reduce the impetus to reform failing infrastructures to healthcare.[144] "},"meta":{},"created_at":"2025-03-22T14:25:42.276677Z","updated_at":"2025-03-22T14:25:42.276677Z","inner_id":35,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":44,"annotations":[{"id":44,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.311403Z","updated_at":"2025-03-22T14:25:42.311403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"7af3e547-37ea-4540-8957-23bb7a001546","import_id":null,"last_action":null,"bulk_created":false,"task":44,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Hollywood usually refers to:  Hollywood may also refer to: "},"meta":{},"created_at":"2025-03-22T14:25:42.276677Z","updated_at":"2025-03-22T14:25:42.276677Z","inner_id":36,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":45,"annotations":[{"id":45,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.311403Z","updated_at":"2025-03-22T14:25:42.311403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"4a77a1b6-93ed-4702-a4fb-241cc97242d2","import_id":null,"last_action":null,"bulk_created":false,"task":45,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Finance refers to monetary resources and to the study and discipline of money, currency, assets and liabilities.[a] As a subject of study, it is related to but distinct from economics, which is the study of the production, distribution, and consumption of goods and services.[b] Based on the scope of financial activities in financial systems, the discipline can be divided into personal, corporate, and public finance.  In these financial systems, assets are bought, sold, or traded as financial instruments, such as currencies, loans, bonds, shares, stocks, options, futures, etc. Assets can also be banked, invested, and insured to maximize value and minimize loss. In practice, risks are always present in any financial action and entities.  Due to its wide scope, a broad range of subfields exists within finance. Asset-, money-, risk- and investment management aim to maximize value and minimize volatility. Financial analysis assesses the viability, stability, and profitability of an action or entity. Some fields are multidisciplinary, such as mathematical finance, financial law, financial economics, financial engineering and financial technology. These fields are the foundation of business and accounting. In some cases, theories in finance can be tested using the scientific method, covered by experimental finance.  The early history of finance parallels the early history of money, which is prehistoric. Ancient and medieval civilizations incorporated basic functions of finance, such as banking, trading and accounting, into their economies. In the late 19th century, the global financial system was formed.  In the middle of the 20th century, finance emerged as a distinct academic discipline,[c] separate from economics.[1] The earliest doctoral programs in finance were established in the 1960s and 1970s.[2] Today, finance is also widely studied through career-focused undergraduate and master's level programs.[3][4]  As outlined, the financial system consists of the flows of capital that take place between individuals and households (personal finance), governments (public finance), and businesses (corporate finance). \"Finance\" thus studies the process of channeling money from savers and investors to entities that need it.[d]Savers and investors have money available which could earn interest or dividends if put to productive use. Individuals, companies and governments must obtain money from some external source, such as loans or credit, when they lack sufficient funds to run their operations.  In general, an entity whose income exceeds its expenditure can lend or invest the excess, intending to earn a fair return. Correspondingly, an entity where income is less than expenditure can raise capital usually in one of two ways: (i) by borrowing in the form of a loan (private individuals), or by selling government or corporate bonds; (ii) by a corporation selling equity, also called stock or shares (which may take various forms: preferred stock or common stock). The owners of both bonds and stock may be institutional investors—financial institutions such as investment banks and pension funds—or private individuals, called private investors or retail investors. (See Financial market participants.)  The lending is often indirect, through a financial intermediary such as a bank, or via the purchase of notes or bonds (corporate bonds, government bonds, or mutual bonds) in the bond market. The lender receives interest, the borrower pays a higher interest than the lender receives, and the financial intermediary earns the difference for arranging the loan.[6][7][8] A bank aggregates the activities of many borrowers and lenders. A bank accepts deposits from lenders, on which it pays interest. The bank then lends these deposits to borrowers. Banks allow borrowers and lenders, of different sizes, to coordinate their activity.  Investing typically entails the purchase of stock, either individual securities or via a mutual fund, for example. Stocks are usually sold by corporations to investors so as to raise required capital in the form of \"equity financing\", as distinct from the debt financing described above. The financial intermediaries here are the investment banks. The investment banks find the initial investors and facilitate the listing of the securities, typically shares and bonds. Additionally, they facilitate the securities exchanges, which allow their trade thereafter, as well as the various service providers which manage the performance or risk of these investments. These latter include mutual funds, pension funds, wealth managers, and stock brokers, typically servicing retail investors (private individuals).  Inter-institutional trade and investment, and fund-management at this scale, is referred to as \"wholesale finance\".  Institutions here extend the products offered, with related trading, to include bespoke options, swaps, and structured products, as well as specialized financing; this \"financial engineering\" is inherently mathematical, and these institutions are then the major employers of \"quants\" (see below). In these institutions, risk management, regulatory capital, and compliance play major roles.  As outlined, finance comprises, broadly, the three areas of personal finance, corporate finance, and public finance.These, in turn, overlap and employ various activities and sub-disciplines—chiefly investments, risk management, and quantitative finance.  Personal finance refers to the practice of budgeting to ensure enough funds are available to meet basic needs, while ensuring there is only a reasonable level of risk to lose said capital. Personal finance may involve paying for education, financing durable goods such as real estate and cars, buying insurance, investing, and saving for retirement.[9] Personal finance may also involve paying for a loan or other debt obligations. The main areas of personal finance are considered to be income, spending, saving, investing, and protection. The following steps, as outlined by the Financial Planning Standards Board,[10] suggest that an individual will understand a potentially secure personal finance plan after:  Corporate finance deals with the actions that managers take to increase the value of the firm to the shareholders, the sources of funding and the capital structure of corporations, and the tools and analysis used to allocate financial resources. While corporate finance is in principle different from managerial finance, which studies the financial management of all firms rather than corporations alone, the concepts are applicable to the financial problems of all firms,[12] and this area is then often referred to as \"business finance\".  Typically, \"corporate finance\" relates to the long term objective of maximizing the value of the entity's assets, its stock, and its return to shareholders, while also balancing risk and profitability. This entails[13] three primary areas:   The latter creates the link with investment banking and securities trading, as above, in that the capital raised will generically comprise debt, i.e. corporate bonds, and equity, often listed shares. Re risk management within corporates, see below.  Financial managers—i.e. as distinct from corporate financiers—focus more on the short term elements of profitability, cash flow, and \"working capital management\" (inventory, credit and debtors), ensuring that the firm can safely and profitably carry out its financial and operational objectives; i.e. that it: (1) can service both maturing short-term debt repayments, and scheduled long-term debt payments, and (2) has sufficient cash flow for ongoing and upcoming operational expenses. (See Financial management and FP&A.)  Public finance describes finance as related to sovereign states, sub-national entities, and related public entities or agencies. It generally encompasses a long-term strategic perspective regarding investment decisions that affect public entities.[15] These long-term strategic periods typically encompass five or more years.[16] Public finance is primarily concerned with:[17]  Central banks, such as the Federal Reserve System banks in the United States and the Bank of England in the United Kingdom, are strong players in public finance. They act as lenders of last resort as well as strong influences on monetary and credit conditions in the economy.[18]  Development finance, which is related, concerns investment in economic development projects provided by a (quasi) governmental institution on a non-commercial basis; these projects would otherwise not be able to get financing. A public–private partnership is primarily used for infrastructure projects: a private sector corporate provides the financing up-front, and then draws profits from taxpayers or users. Climate finance, and the related Environmental finance, address the financial strategies, resources and instruments used in climate change mitigation.  Investment management[12] is the professional asset management of various securities—typically shares and bonds, but also other assets, such as real estate, commodities and alternative investments—in order to meet specified investment goals for the benefit of investors.  As above, investors may be institutions, such as insurance companies, pension funds, corporations, charities, educational establishments, or private investors, either directly via investment contracts or, more commonly, via collective investment schemes like mutual funds, exchange-traded funds, or real estate investment trusts.  At the heart of investment management[12] is asset allocation—diversifying the exposure among these asset classes, and among individual securities within each asset class—as appropriate to the client's investment policy, in turn, a function of risk profile, investment goals, and investment horizon (see Investor profile). Here:  Overlaid is the portfolio manager's investment style—broadly, active vs passive, value vs growth, and small cap vs. large cap—and investment strategy.  In a well-diversified portfolio, achieved investment performance will, in general, largely be a function of the asset mix selected, while the individual securities are less impactful. The specific approach or philosophy will also be significant, depending on the extent to which it is complementary with the market cycle. Risk management here is discussed immediately below.  A quantitative fund is managed using computer-based mathematical techniques (increasingly, machine learning) instead of human judgment. The actual trading is typically automated via sophisticated algorithms.  Risk management, in general, is the study of how to control risks and balance the possibility of gains; it is the process of measuring risk and then developing and implementing strategies to manage that risk.  Financial risk management[20][21] is the practice of protecting corporate value against financial risks, often by \"hedging\" exposure to these using financial instruments. The focus is particularly on credit and market risk, and in banks, through regulatory capital, includes operational risk.  Financial risk management is related to corporate finance[12] in two ways. Firstly, firm exposure to market risk is a direct result of previous capital investments and funding decisions; while credit risk arises from the business's credit policy and is often addressed through credit insurance and provisioning.Secondly, both disciplines share the goal of enhancing or at least preserving, the firm's economic value, and in this context[22] overlaps also enterprise risk management, typically the domain of strategic management.  Here, businesses devote much time and effort to forecasting, analytics and performance monitoring. (See ALM and treasury management.)  For banks and other wholesale institutions,[23] risk management focuses on managing, and as necessary hedging, the various positions held by the institution—both trading positions and long term exposures—and on calculating and monitoring the resultant economic capital, and regulatory capital under Basel III. The calculations here are mathematically sophisticated, and within the domain of quantitative finance as below. Credit risk is inherent in the business of banking, but additionally, these institutions are exposed to counterparty credit risk. Banks typically employ Middle office \"Risk Groups\", whereas front office risk teams provide risk \"services\" (or \"solutions\") to customers.  Additional to diversification, the fundamental risk mitigant here, investment managers will apply various hedging techniques as appropriate,[12] these may relate to the portfolio as a whole or to individual stocks. Bond portfolios are often (instead) managed via cash flow matching or immunization, while for derivative portfolios and positions, traders use \"the Greeks\" to measure and then offset sensitivities. In parallel, managers – active and passive – will monitor tracking error, thereby minimizing and preempting any underperformance vs their \"benchmark\".  Quantitative finance—also referred to as \"mathematical finance\"—includes those finance activities where a sophisticated mathematical model is required,[24] and thus overlaps several of the above.  As a specialized practice area, quantitative finance comprises primarily three sub-disciplines; the underlying theory and techniques are discussed in the next section:  DCF valuation formula widely applied in business and finance, since articulated in 1938. Here, to get the value of the firm, its forecasted free cash flows are discounted to the present using the weighted average cost of capital for the discount factor. For share valuation investors use the related dividend discount model.  Financial theory is studied and developed within the disciplines of management, (financial) economics, accountancy and applied mathematics. In the abstract,[12][25] finance is concerned with the investment and deployment of assets and liabilities over \"space and time\"; i.e., it is about performing valuation and asset allocation today, based on the risk and uncertainty of future outcomes while appropriately incorporating the time value of money.  Determining the present value of these future values, \"discounting\", must be at the risk-appropriate discount rate, in turn, a major focus of finance-theory.[26]As financial theory has roots in many disciplines, including mathematics, statistics, economics, physics, and psychology, it can be considered a mix of an art and science,[27] and there are ongoing related efforts to organize a list of unsolved problems in finance.  Managerial finance  [29]  is the branch of finance that deals with the financial aspects of the management of a company, and the financial dimension of managerial decision-making more broadly. It provides the theoretical underpin for the practice described above, concerning itself with the managerial application of the various finance techniques. Academics working in this area are typically based in business school finance departments, in accounting, or in management science.  The tools addressed and developed relate in the main to managerial accounting and corporate finance:  the former allow management to better understand, and hence act on, financial information relating to profitability and performance; the latter, as above, are about optimizing the overall financial structure, including its impact on working capital. Key aspects of managerial finance thus include:  The discussion, however, extends to business strategy more broadly, emphasizing alignment with the company's overall strategic objectives; and similarly incorporates the managerial perspectives of planning, directing, and controlling.  Financial economics[31] is the branch of economics that studies the interrelation of financial variables, such as prices, interest rates and shares, as opposed to real economic variables, i.e. goods and services. It thus centers on pricing, decision making, and risk management in the financial markets,[31][25] and produces many of the commonly employed financial models. (Financial econometrics is the branch of financial economics that uses econometric techniques to parameterize the relationships suggested.)  The discipline has two main areas of focus:[25] asset pricing and corporate finance; the first being the perspective of providers of capital, i.e. investors, and the second of users of capital; respectively:  Financial mathematics[33] is the field of applied mathematics concerned with financial markets;  Louis Bachelier's doctoral thesis, defended in 1900, is considered to be the first scholarly work in this area. The field is largely focused on the modeling of derivatives—with much emphasis on interest rate- and credit risk modeling—while other important areas include insurance mathematics and quantitative portfolio management. Relatedly, the techniques developed are applied to pricing and hedging a wide range of asset-backed, government, and corporate-securities.  As above, in terms of practice, the field is referred to as quantitative finance and \/ or mathematical finance, and comprises primarily the three areas discussed. The main mathematical tools and techniques are, correspondingly:  Mathematically, these separate into two analytic branches: derivatives pricing uses risk-neutral probability (or arbitrage-pricing probability), denoted by \"Q\"; while risk and portfolio management generally use physical (or actual or actuarial) probability, denoted by \"P\". These are interrelated through the above \"Fundamental theorem of asset pricing\".  The subject has a close relationship with financial economics, which, as outlined, is concerned with much of the underlying theory that is involved in financial mathematics:  generally, financial mathematics will derive and extend the mathematical models suggested. Computational finance is the branch of (applied) computer science that deals with problems of practical interest in finance, and especially[33] emphasizes the numerical methods applied here.  Experimental finance[36] aims to establish different market settings and environments to experimentally observe and provide a lens through which science can analyze agents' behavior and the resulting characteristics of trading flows, information diffusion, and aggregation, price setting mechanisms, and returns processes. Researchers in experimental finance can study to what extent existing financial economics theory makes valid predictions and therefore prove them, as well as attempt to discover new principles on which such theory can be extended and be applied to future financial decisions. Research may proceed by conducting trading simulations or by establishing and studying the behavior of people in artificial, competitive, market-like settings.  Behavioral finance studies how the psychology of investors or managers affects financial decisions and markets[37]  and is relevant when making a decision that can impact either negatively or positively on one of their areas. With more in-depth research into behavioral finance, it is possible to bridge what actually happens in financial markets with analysis based on financial theory.[38] Behavioral finance has grown over the last few decades to become an integral aspect of finance.[39]  Behavioral finance includes such topics as:  A strand of behavioral finance has been dubbed quantitative behavioral finance, which uses mathematical and statistical methodology to understand behavioral biases in conjunction with valuation.   Quantum finance involves applying quantum mechanical approaches to financial theory, providing novel methods and perspectives in the field.[40] Quantum finance is an interdisciplinary field, in which theories and methods developed by quantum physicists and economists are applied to solve financial problems. It represents a branch known as econophysics. Although quantum computational methods have been around for quite some time and use the basic principles of physics to better understand the ways to implement and manage cash flows, it is mathematics that is actually important in this new scenario[41] Finance theory is heavily based on financial instrument pricing such as stock option pricing. Many of the problems facing the finance community have no known analytical solution. As a result, numerical methods and computer simulations for solving these problems have proliferated. This research area is known as computational finance. Many computational finance problems have a high degree of computational complexity and are slow to converge to a solution on classical computers. In particular, when it comes to option pricing, there is additional complexity resulting from the need to respond to quickly changing markets. For example, in order to take advantage of inaccurately priced stock options, the computation must complete before the next change in the almost continuously changing stock market. As a result, the finance community is always looking for ways to overcome the resulting performance issues that arise when pricing options. This has led to research that applies alternative computing techniques to finance. Most commonly used quantum financial models are quantum continuous model, quantum binomial model, multi-step quantum binomial model etc.  The origin of finance can be traced to the beginning of state formation and trade during the Bronze Age. The earliest historical evidence of finance is dated to around 3000 BCE. Banking originated in West Asia, where temples and palaces were used as safe places for the storage of valuables. Initially, the only valuable that could be deposited was grain, but cattle and precious materials were eventually included. During the same period, the Sumerian city of Uruk in Mesopotamia supported trade by lending as well as the use of interest. In Sumerian, \"interest\" was mas, which translates to \"calf\". In Greece and Egypt, the words used for interest, tokos and ms respectively, meant \"to give birth\". In these cultures, interest indicated a valuable increase, and seemed to consider it from the lender's point of view.[42] The Code of Hammurabi (1792–1750 BCE) included laws governing banking operations. The Babylonians were accustomed to charging interest at the rate of 20 percent per year. By 1200 BCE, cowrie shells were used as a form of money in China.  The use of coins as a means of representing money began in the years between 700 and 500 BCE.[43] Herodotus mentions the use of crude coins in Lydia around 687 BCE and, by 640 BCE, the Lydians had started to use coin money more widely and opened permanent retail shops.[44] Shortly after, cities in Classical Greece, such as Aegina, Athens, and Corinth, started minting their own coins between 595 and 570 BCE. During the Roman Republic, interest was outlawed by the Lex Genucia reforms in 342 BCE, though the provision went largely unenforced. Under Julius Caesar, a ceiling on interest rates of 12% was set, and much later under Justinian it was lowered even further to between 4% and 8%.[45]  The first stock exchange was opened in Antwerp in 1531.[46] Since then, popular exchanges such as the London Stock Exchange (founded in 1773) and the New York Stock Exchange (founded in 1793) were created.[47][48]   "},"meta":{},"created_at":"2025-03-22T14:25:42.276677Z","updated_at":"2025-03-22T14:25:42.276677Z","inner_id":37,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":46,"annotations":[{"id":46,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.311403Z","updated_at":"2025-03-22T14:25:42.311403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"6f1b78cf-4e8b-431d-ba87-775b49f3281d","import_id":null,"last_action":null,"bulk_created":false,"task":46,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Film editing is both a creative and a technical part of the post-production process of filmmaking. The term is derived from the traditional process of working with film which increasingly involves the use of digital technology. When putting together some sort of video composition, typically, one would need a collection of shots and footages that vary from one another. The act of adjusting the shots someone has already taken, and turning them into something new is known as film editing.  The film editor works with raw footage, selecting shots and combining them into sequences which create a finished motion picture. Film editing is described as an art or skill, the only art that is unique to cinema, separating filmmaking from other art forms that preceded it, although there are close parallels to the editing process in other art forms such as poetry and novel writing. Film editing is an extremely important tool when attempting to intrigue a viewer. When done properly, a film's editing can captivate a viewer and fly completely under the radar. Because of this, film editing has been given the name “the invisible art.”  On its most fundamental level, film editing is the art, technique and practice of assembling shots into a coherent sequence. The job of an editor is not simply to mechanically put pieces of a film together, cut off film slates or edit dialogue scenes. A film editor must creatively work with the layers of images, story, dialogue, music, pacing, as well as the actors' performances to effectively \"re-imagine\" and even rewrite the film to craft a cohesive whole. Editors usually play a dynamic role in the making of a film. An editor must select only the most quality shots, removing all unnecessary frames to ensure the shot is clean. Sometimes, auteurist film directors edit their own films, for example, Akira Kurosawa, Bahram Beyzai, Steven Soderbergh, and the Coen brothers.  According to “Film Art, An Introduction”, by Bordwell and Thompson, there are four basic areas of film editing that the editor has full control over. The first dimension is the graphic relations between a shot A and shot B. The shots are analyzed in terms of their graphic configurations, including light and dark, lines and shapes, volumes and depths, movement and stasis. The director makes deliberate choices regarding the composition, lighting, color, and movement within each shot, as well as the transitions between them. There are several techniques used by editors to establish graphic relations between shots. These include maintaining overall brightness consistency, keeping important elements in the center of the frame, playing with color differences, and creating visual matches or continuities between shots.  The second dimension is the rhythmic relationship between shot A and shot B. The duration of each shot, determined by the number of frames or length of film, contributes to the overall rhythm of the film. The filmmaker has control over the editing rhythm by adjusting the length of shots in relation to each other. Shot duration can be used to create specific effects and emphasize moments in the film. For example, a brief flash of white frames can convey a sudden impact or a violent moment. On the other hand, lengthening or adding seconds to a shot can allow for audience reaction or to accentuate an action. The length of shots can also be used to establish a rhythmic pattern, such as creating a steady beat or gradually slowing down or accelerating the tempo.  The third dimension is the spatial relationship between shot A and shot B. Editing allows the filmmaker to construct film space and imply a relationship between different points in space. The filmmaker can juxtapose shots to establish spatial holes or construct a whole space out of component parts. For example, the filmmaker can start with a shot that establishes a spatial hole and then follow it with a shot of a part of that space, creating an analytical breakdown.  The final dimension that an editor has control over is the temporal relation between shot A and shot B. Editing plays a crucial role in manipulating the time of action in a film. It allows filmmakers to control the order, duration, and frequency of events, thus shaping the narrative and influencing the audience's perception of time. Through editing, shots can be rearranged, flashbacks and flash-forwards can be employed, and the duration of actions can be compressed or expanded. The main point is that editing gives filmmakers the power to control and manipulate the temporal aspects of storytelling in film.  Between graphic, rhythmic, spatial, and temporal relationships between two shots, an editor has various ways to add a creative element to the film, and enhance the overall viewing experience.  With the advent of digital editing in non-linear editing systems, film editors and their assistants have become responsible for many areas of filmmaking that used to be the responsibility of others. For instance, in past years, picture editors dealt only with just that—picture. Sound, music, and (more recently) visual effects editors dealt with the practicalities of other aspects of the editing process, usually under the direction of the picture editor and director. However, digital systems have increasingly put these responsibilities on the picture editor. It is common, especially on lower budget films, for the editor to sometimes cut in temporary music, mock up visual effects and add temporary sound effects or other sound replacements. These temporary elements are usually replaced with more refined final elements produced by the sound, music and visual effects teams hired to complete the picture.[citation needed] The importance of an editor has become increasingly pivotal to the quality and success of a film due to the multiple roles that have been added to their job.  Early films were short films that were one long, static, and locked-down shot. Motion in the shot was all that was necessary to amuse an audience, so the first films simply showed activity such as traffic moving along a city street. There was no story and no editing. Each film ran as long as there was film in the camera.  The use of film editing to establish continuity, involving action moving from one sequence into another, is attributed to British film pioneer Robert W. Paul's Come Along, Do!, made in 1898 and one of the first films to feature more than one shot.[1] In the first shot, an elderly couple is outside an art exhibition having lunch and then follow other people inside through the door. The second shot shows what they do inside. Paul's 'Cinematograph Camera No. 1' of 1896 was the first camera to feature reverse-cranking, which allowed the same film footage to be exposed several times and thereby to create super-positions and multiple exposures. One of the first films to use this technique, Georges Méliès's The Four Troublesome Heads from 1898, was produced with Paul's camera.  The further development of action continuity in multi-shot films continued in 1899–1900 at the Brighton School in England, where it was definitively established by George Albert Smith and James Williamson. In that year, Smith made As Seen Through a Telescope, in which the main shot shows street scene with a young man tying the shoelace and then caressing the foot of his girlfriend, while an old man observes this through a telescope. There is then a cut to close shot of the hands on the girl's foot shown inside a black circular mask, and then a cut back to the continuation of the original scene.  Even more remarkable was James Williamson's Attack on a China Mission Station, made around the same time in 1900. The first shot shows the gate to the mission station from the outside being attacked and broken open by Chinese Boxer rebels, then there is a cut to the garden of the mission station where a pitched battle ensues. An armed party of British sailors arrived to defeat the Boxers and rescue the missionary's family. The film used the first \"reverse angle\" cut in film history.  James Williamson concentrated on making films taking action from one place shown in one shot to the next shown in another shot in films like Stop Thief! and Fire!, made in 1901, and many others. He also experimented with the close-up, and made perhaps the most extreme one of all in The Big Swallow, when his character approaches the camera and appears to swallow it. These two filmmakers of the Brighton School also pioneered the editing of the film; they tinted their work with color and used trick photography to enhance the narrative. By 1900, their films were extended scenes of up to five minutes long.[2]  Other filmmakers then took up all these ideas including the American Edwin S. Porter, who started making films for the Edison Company in 1901. Porter worked on a number of minor films before making Life of an American Fireman in 1903. The film was the first American film with a plot, featuring action, and even a closeup of a hand pulling a fire alarm. The film comprised a continuous narrative over seven scenes, rendered in a total of nine shots.[3] He put a dissolve between every shot, just as Georges Méliès was already doing, and he frequently had the same action repeated across the dissolves. His film, The Great Train Robbery (1903), had a running time of twelve minutes, with twenty separate shots and ten different indoor and outdoor locations. He used cross-cutting editing method to show simultaneous action in different places.  These early film directors discovered important aspects of motion picture language: that the screen image does not need to show a complete person from head to toe and that splicing together two shots creates in the viewer's mind a contextual relationship. These were the key discoveries that made all non-live or non live-on-videotape narrative motion pictures and television possible—that shots (in this case, whole scenes since each shot is a complete scene) can be photographed at widely different locations over a period of time (hours, days or even months) and combined into a narrative whole.[4] That is, The Great Train Robbery contains scenes shot on sets of a telegraph station, a railroad car interior, and a dance hall, with outdoor scenes at a railroad water tower, on the train itself, at a point along the track, and in the woods. But when the robbers leave the telegraph station interior (set) and emerge at the water tower, the audience believes they went immediately from one to the other. Or that when they climb on the train in one shot and enter the baggage car (a set) in the next, the audience believes they are on the same train.  Sometime around 1918, Russian director Lev Kuleshov did an experiment that proves this point. (See Kuleshov Experiment) He took an old film clip of a headshot of a noted Russian actor and intercut the shot with a shot of a bowl of soup, then with a child playing with a teddy bear, then with a shot an elderly woman in a casket. When he showed the film to people they praised the actor's acting—the hunger in his face when he saw the soup, the delight in the child, and the grief when looking at the dead woman.[5] Of course, the shot of the actor was years before the other shots and he never \"saw\" any of the items. The simple act of juxtaposing the shots in a sequence made the relationship.  Before the widespread use of digital non-linear editing systems, the initial editing of all films was done with a positive copy of the film negative called a film workprint (cutting copy in UK) by physically cutting and splicing together pieces of film.[6] Strips of footage would be hand cut and attached together with tape and then later in time, glue. Editors were very precise; if they made a wrong cut or needed a fresh positive print, it cost the production money and time for the lab to reprint the footage. Additionally, each reprint put the negative at risk of damage. With the invention of a splicer and threading the machine with a viewer such as a Moviola, or \"flatbed\" machine such as a K.-E.-M. or Steenbeck, the editing process sped up a little bit and cuts came out cleaner and more precise. The Moviola editing practice is non-linear, allowing the editor to make choices faster, a great advantage to editing episodic films for television which have very short timelines to complete the work. All film studios and production companies who produced films for television provided this tool for their editors. Flatbed editing machines were used for playback and refinement of cuts, particularly in feature films and films made for television because they were less noisy and cleaner to work with.   They were used extensively for documentary and drama production within the BBC's Film Department. Operated by a team of two, an editor and assistant editor, this tactile process required significant skill but allowed for editors to work extremely efficiently.[7]  Modern film editing has evolved significantly since it was first introduced to the film and entertainment industry. Some other new aspects of editing have been introduced such as color grading and digital workflows. As mentioned earlier, over the course of time, new technology has exponentially enhanced the quality of pictures in films. One of the most important steps in this process was transitioning from analog to digital filmmaking. By doing this, it gives the ability editors to immediately playback scenes, duplication and much more. Additionally digital has simplified and reduced the cost of filmmaking. Digital film is not only cheaper, but lasts longer, is safer, and is overall more efficient. Color grading is a post production process, where the editor manipulates or enhances the color of images, or environments in order to create a color tone. Doing this can alter the setting, tone, and mood of the entirety of scenes, and can enhance reactions that would otherwise have the possibility of being dull or out of place. Color grading is vital to the film editing process, and is technology that allows editors to enhance a story.  Today, most films are edited digitally (on systems such as Media Composer, Final Cut Pro X or Premiere Pro) and bypass the film positive workprint altogether. In the past, the use of a film positive (not the original negative) allowed the editor to do as much experimenting as he or she wished, without the risk of damaging the original. With digital editing, editors can experiment just as much as before except with the footage completely transferred to a computer hard drive.  When the film workprint had been cut to a satisfactory state, it was then used to make an edit decision list (EDL). The negative cutter referred to this list while processing the negative, splitting the shots into rolls, which were then contact printed to produce the final film print or answer print. Today, production companies have the option of bypassing negative cutting altogether. With the advent of digital intermediate (\"DI\"), the physical negative does not necessarily need to be physically cut and hot spliced together; rather the negative is optically scanned into the computer(s) and a cut list is confirmed by a DI editor.  In the early years of film, editing was considered a technical job; editors were expected to \"cut out the bad bits\" and string the film together. Indeed, when the Motion Picture Editors Guild was formed, they chose to be \"below the line\", that is, not a creative guild, but a technical one. Women were not usually able to break into the \"creative\" positions; directors, cinematographers, producers, and executives were almost always men. Editing afforded creative women a place to assert their mark on the filmmaking process. The history of film has included many women editors such as Dede Allen, Anne Bauchens, Margaret Booth, Barbara McLean, Anne V. Coates, Adrienne Fazan, Verna Fields, Blanche Sewell and Eda Warren.[8]  Post-production editing may be summarized by three distinct phases commonly referred to as the editor's cut, the director's cut, and the final cut.  There are several editing stages and the editor's cut is the first. An editor's cut (sometimes referred to as the \"Assembly edit\" or \"Rough cut\") is normally the first pass of what the final film will be when it reaches picture lock. The film editor usually starts working while principal photography starts. Sometimes, prior to cutting, the editor and director will have seen and discussed \"dailies\" (raw footage shot each day) as shooting progresses. As production schedules have shortened over the years, this co-viewing happens less often. Screening dailies give the editor a general idea of the director's intentions. Because it is the first pass, the editor's cut might be longer than the final film. The editor continues to refine the cut while shooting continues, and often the entire editing process goes on for many months and sometimes more than a year, depending on the film. The editor's cut is an opportunity for the editor to shape the story and present their vision of how the film should unfold. It provides a solid foundation for further collaboration with the director, allowing them to assess the initial assembly and provide feedback or guidance on the creative direction.  When shooting is finished, the director can then turn his or her full attention to collaborating with the editor and further refining the cut of the film. This is the time that is set aside where the film editor's first cut is molded to fit the director's vision. In the United States, under the rules of the Directors Guild of America, directors receive a minimum of ten weeks after completion of principal photography to prepare their first cut. While collaborating on what is referred to as the \"director's cut\", the director and the editor go over the entire movie in great detail; scenes and shots are re-ordered, removed, shortened and otherwise tweaked. Often it is discovered that there are plot holes, missing shots or even missing segments which might require that new scenes be filmed. Because of this time working closely and collaborating – a period that is normally far longer and more intricately detailed than the entire preceding film production – many directors and editors form a unique artistic bond. The goal is to align the film with the director's artistic vision and narrative objectives. The director's cut typically involves multiple iterations and discussions until both the director and editor are satisfied with the overall direction of the film.  Often after the director has had their chance to oversee a cut, the subsequent cuts are supervised by one or more producers, who represent the production company or film studio. There have been several conflicts in the past between the director and the studio, sometimes leading to the use of the \"Alan Smithee\" credit signifying when a director no longer wants to be associated with the final release. The final cut is the last stage of post-production editing and represents the definitive version of the film. It is the result of the collaborative efforts between the director, editor, and other key stakeholders. The final cut reflects the agreed-upon creative decisions and serves as the basis for distribution and exhibition.  Mise en scene is the term used to describe all of the lighting, music, placement, costume design, and other elements of a shot. Film editing and Mise en scene go hand in hand with one another. Film editing contributes to the mise en scene of a given shot. When shooting a film, one typically get shots from multiple angles. The angles at which one shoots from are all part of the film's mise en scene.  In motion picture terminology, a montage (from the French for \"putting together\" or \"assembly\") is a film editing technique.  There are at least three senses of the term:  Although film director D. W. Griffith was not part of the montage school, he was one of the early proponents of the power of editing — mastering cross-cutting to show parallel action in different locations, and codifying film grammar in other ways as well. Griffith's work in the teens was highly regarded by Lev Kuleshov and other Soviet filmmakers and greatly influenced their understanding of editing.  Kuleshov was among the first to theorize about the relatively young medium of the cinema in the 1920s. For him, the unique essence of the cinema — that which could be duplicated in no other medium — is editing. He argues that editing a film is like constructing a building. Brick-by-brick (shot-by-shot) the building (film) is erected. His often-cited Kuleshov Experiment established that montage can lead the viewer to reach certain conclusions about the action in a film. Montage works because viewers infer meaning based on context. Sergei Eisenstein was briefly a student of Kuleshov's, but the two parted ways because they had different ideas of montage. Eisenstein regarded montage as a dialectical means of creating meaning. By contrasting unrelated shots he tried to provoke associations in the viewer, which were induced by shocks. But Eisenstein did not always do his own editing, and some of his most important films were edited by Esfir Tobak.[9]  A montage sequence consists of a series of short shots that are edited into a sequence to condense narrative. It is usually used to advance the story as a whole (often to suggest the passage of time), rather than to create symbolic meaning. In many cases, a song plays in the background to enhance the mood or reinforce the message being conveyed. One famous example of montage was seen in the 1968 film 2001: A Space Odyssey, depicting the start of man's first development from apes to humans. Another example that is employed in many films is the sports montage. The sports montage shows the star athlete training over a period of time, each shot having more improvement than the last. Classic examples include Rocky and the Karate Kid.  The word's association with Sergei Eisenstein is often condensed—too simply—into the idea of \"juxtaposition\" or into two words: \"collision montage,\" whereby two adjacent shots that oppose each other on formal parameters or on the content of their images are cut against each other to create a new meaning not contained in the respective shots: Shot a + Shot b = New Meaning c.  The association of collision montage with Eisenstein is not surprising. He consistently maintained that the mind functions dialectically, in the Hegelian sense, that the contradiction between opposing ideas (thesis versus antithesis) is resolved by a higher truth, synthesis. He argued that conflict was the basis of all art, and never failed to see montage in other cultures. For example, he saw montage as a guiding principle in the construction of \"Japanese hieroglyphics in which two independent ideographic characters ('shots') are juxtaposed and explode into a concept. Thus:  He also found montage in Japanese haiku, where short sense perceptions are juxtaposed and synthesized into a new meaning, as in this example:  (枯朶に烏のとまりけり秋の暮)  — Matsuo Basho  As Dudley Andrew notes, \"The collision of attractions from line to line produces the unified psychological effect which is the hallmark of haiku and montage.\"[11]  Continuity editing, developed in the early 1900s, aimed to create a coherent and smooth storytelling experience in films. It relied on consistent graphic qualities, balanced composition, and controlled editing rhythms to ensure narrative continuity and engage the audience. For example, whether an actor's costume remains the same from one scene to the next, or whether a glass of milk held by a character is full or empty throughout the scene.  Because films are typically shot out of sequence, the script supervisor will keep a record of continuity and provide that to the film editor for reference.  The editor may try to maintain continuity of elements, or may intentionally create a discontinuous sequence for stylistic or narrative effect.  The technique of continuity editing, part of the classical Hollywood style, was developed by early European and American directors, in particular, D.W. Griffith in his films such as The Birth of a Nation and Intolerance. The classical style embraces temporal and spatial continuity as a way of advancing the narrative, using such techniques as the 180 degree rule, Establishing shot, and Shot reverse shot. The 180-degree system in film editing ensures consistency in shot composition by keeping relative positions of characters or objects in the frame consistent. It also maintains consistent eye-lines and screen direction to avoid disorientation and confusion for the audience, allowing for clear spatial delineation and a smooth narrative experience. Often, continuity editing means finding a balance between literal continuity and perceived continuity. For instance, editors may condense action across cuts in a non-distracting way. A character walking from one place to another may \"skip\" a section of floor from one side of a cut to the other, but the cut is constructed to appear continuous so as not to distract the viewer.  Early Russian filmmakers such as Lev Kuleshov (already mentioned) further explored and theorized about editing and its ideological nature. Sergei Eisenstein developed a system of editing that was unconcerned with the rules of the continuity system of classical Hollywood that he called Intellectual montage.  Alternatives to traditional editing were also explored by early surrealist and Dada filmmakers such as Luis Buñuel (director of the 1929 Un Chien Andalou) and René Clair (director of 1924's Entr'acte which starred famous Dada artists Marcel Duchamp and Man Ray).  Filmmakers have explored alternatives to continuity editing, focusing on graphic and rhythmic possibilities in their films. Experimental filmmakers like Stan Brakhage and Bruce Conner have used purely graphic elements to join shots, emphasizing light, texture, and shape rather than narrative coherence. Non-narrative films have prioritized rhythmic relations among shots, even employing single-frame shots for extreme rhythmic effects. Narrative filmmakers, such as Busby Berkeley and Yasujiro Ow, have occasionally subordinated narrative concerns to graphic or rhythmic patterns, while films influenced by music videos often feature pulsating rhythmic editing that de-emphasizes spatial and temporal dimensions.  The French New Wave filmmakers such as Jean-Luc Godard and François Truffaut and their American counterparts such as Andy Warhol and John Cassavetes also pushed the limits of editing technique during the late 1950s and throughout the 1960s. French New Wave films and the non-narrative films of the 1960s used a carefree editing style and did not conform to the traditional editing etiquette of Hollywood films. Like its Dada and surrealist predecessors, French New Wave editing often drew attention to itself by its lack of continuity, its demystifying self-reflexive nature (reminding the audience that they were watching a film), and by the overt use of jump cuts or the insertion of material not often related to any narrative. Three of the most influential editors of French New Wave films were the women who (in combination) edited 15 of Godard's films: Francoise Collin, Agnes Guillemot, and Cecile Decugis, and another notable editor is Marie-Josèphe Yoyotte, the first black woman editor in French cinema and editor of The 400 Blows.[9]  Since the late 20th century Post-classical editing has seen faster editing styles with nonlinear, discontinuous action.  Vsevolod Pudovkin noted that the editing process is the one phase of production that is truly unique to motion pictures. Every other aspect of filmmaking originated in a different medium than film (photography, art direction, writing, sound recording), but editing is the one process that is unique to film.[12] Filmmaker Stanley Kubrick was quoted as saying: \"I love editing. I think I like it more than any other phase of filmmaking. If I wanted to be frivolous, I might say that everything that precedes editing is merely a way of producing a film to edit.\"[13] Film editing is significant because it shapes the narrative structure, visual and aesthetic impact, rhythm and pacing, emotional resonance, and overall storytelling of a film. Editors possess a unique creative power to manipulate and arrange shots, allowing them to craft a cinematic experience that engages, entertains, and emotionally connects with the audience. Film editing is a distinct art form within the filmmaking process, enabling filmmakers to realize their vision and bring stories to life on the screen.   According to writer-director Preston Sturges:  [T]here is a law of natural cutting and that this replicates what an audience in a legitimate theater does for itself.  The more nearly the film cutter approaches this law of natural interest, the more invisible will be his cutting. If the camera moves from one person to another at the exact moment that one in the legitimate theatre would have turned his head, one will not be conscious of a cut. If the camera misses by a quarter of a second, one will get a jolt. There is one other requirement: the two shots must be approximate of the same tone value. If one cuts from black to white, it is jarring. At any given moment, the camera must point at the exact spot the audience wishes to look at. To find that spot is absurdly easy: one has only to remember where one was looking at the time the scene was made.[14] Assistant editors aid the editor and director in collecting and organizing all the elements needed to edit the film. The Motion Picture Editors Guild defines an assistant editor as \"a person who is assigned to assist an Editor. His or her duties shall be such as are assigned and performed under the immediate direction, supervision, and responsibility of the editor.\"[15] When editing is finished, they oversee the various lists and instructions necessary to put the film into its final form. Editors of large budget features will usually have a team of assistants working for them. The first assistant editor is in charge of this team and may do a small bit of picture editing as well, if necessary. Assistant editors are responsible for collecting, organizing, and managing all the elements needed for the editing process. This includes footage, sound files, music tracks, visual effects assets, and other media assets. They ensure that everything is properly labeled, logged, and stored in an organized manner, making it easier for the editor to access and work with the materials efficiently. Assistant editors serve as a bridge between the editing team and other departments, facilitating communication and collaboration. They often work closely with the director, editor, visual effects artists, sound designers, and other post-production professionals, relaying information, managing deliverables, and coordinating schedules. Often assistant editors will perform temporary sound, music, and visual effects work. The other assistants will have set tasks, usually helping each other when necessary to complete the many time-sensitive tasks at hand. In addition, an apprentice editor may be on hand to help the assistants. An apprentice is usually someone who is learning the ropes of assisting.[16]  Television shows typically have one assistant per editor. This assistant is responsible for every task required to bring the show to the final form. Lower budget features and documentaries will also commonly have only one assistant. Higher budget films and shows tend to have more than one assistant editor, and in some cases, there can be a full team of assistants.  The organizational aspects job could best be compared to database management. When a film is shot, every piece of picture or sound is coded with numbers and timecode. It is the assistant's job to keep track of these numbers in a database, which, in non-linear editing, is linked to the computer program.[citation needed] The editor and director cut the film using digital copies of the original film and sound, commonly referred to as an \"offline\" edit. When the cut is finished, it is the assistant's job to bring the film or television show \"online\". They create lists and instructions that tell the picture and sound finishers how to put the edit back together with the high-quality original elements. Assistant editing can be seen as a career path to eventually becoming an editor. Many assistants, however, do not choose to pursue advancement to the editor, and are very happy at the assistant level, working long and rewarding careers on many films and television shows.[17]  Notes  Bibliography  Further reading  Wikibooks  Wikiversity   "},"meta":{},"created_at":"2025-03-22T14:25:42.276677Z","updated_at":"2025-03-22T14:25:42.276677Z","inner_id":38,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":47,"annotations":[{"id":47,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.311403Z","updated_at":"2025-03-22T14:25:42.311403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"13d7100c-093a-4b4f-adde-c56fa0e473d8","import_id":null,"last_action":null,"bulk_created":false,"task":47,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  The cinema of India, consisting of motion pictures made by the Indian film industry, has had a large effect on world cinema since the second half of the 20th century.[8][9] Indian cinema is made up of various film industries, each focused on producing films in a specific language, such as Hindi, Telugu, Tamil, Malayalam, Kannada, Bengali, Marathi,Punjabi, Bhojpuri, Assamese, Odia and others.  Major centres of film production across the country include Mumbai, Hyderabad, Chennai, Kolkata, Kochi, Bengaluru, Bhubaneswar-Cuttack, and Guwahati.[details 1] For a number of years, the Indian film industry has ranked first in the world in terms of annual film output.[29] In 2022, Indian cinema earned ₹15,000 crore ($1.9 billion) at the box-office.[5] Ramoji Film City located in Hyderabad is certified by the Guinness World Records as the largest film studio complex in the world measuring over 1,666 acres (674 ha).[30]  Indian cinema is composed of multilingual and multi-ethnic film art. The term 'Bollywood', often mistakenly used to refer to Indian cinema as a whole, specifically denotes the Hindi-language film industry. Indian cinema, however, is an umbrella term encompassing multiple film industries, each producing films in its respective language and showcasing unique cultural and stylistic elements.  In 2021, Telugu cinema emerged as the largest film industry in India in terms of box office, although the main revenue comes from Hindi-dubbed Telugu films.[31][32] In 2022, Hindi cinema represented 33% of box office revenue, followed by Telugu representing 20%, Tamil representing 16%, Kannada representing 8%, and Malayalam representing 6%, with Marathi, Punjabi, Bengali and Gujarati being the other prominent film industries based on revenue.[33][34] As of 2022, the combined revenue of South Indian film industries has surpassed that of the Mumbai-based Hindi-language film industry (Bollywood).[35][36] As of 2022, Telugu cinema leads Indian cinema with 23.3 crore (233 million) tickets sold, followed by Tamil cinema with 20.5 crore (205 million) and Hindi cinema with 18.9 crore (189 million).[37][33]  Indian cinema is a global enterprise,[38] and its films have attracted international attention and acclaim throughout South Asia.[39] Since talkies began in 1931, Hindi cinema has led in terms of box office performance, but in recent years it has faced stiff competition from Telugu cinema.[40][32] Overseas Indians account for 12% of the industry's revenue.[41]  The history of cinema in India extends to the beginning of the film era. Following the screening of the Lumière and Robert Paul moving pictures in London in 1896, commercial cinematography became a worldwide sensation and these films were shown in Bombay (now Mumbai) that same year.[42]  In 1897, a film presentation by filmmaker Professor Stevenson featured a stage show at Calcutta's Star Theatre. With Stevenson's camera and encouragement, Indian photographer Hiralal Sen filmed scenes from that show, exhibited as The Flower of Persia (1898).[43] The Wrestlers (1899), by H. S. Bhatavdekar, showing a wrestling match at the Hanging Gardens in Bombay, was the first film to be shot by an Indian and the first Indian documentary film.[citation needed] From 1913 to 1931, all the movies made in India were silent films, which had no sound and had intertitles.[44]  In 1913, Dadasaheb Phalke released Raja Harishchandra (1913) in Bombay, the first film made in India. It was a silent film incorporating English, Marathi, and Hindi intertitles.[49] It was premiered in Coronation cinema in Girgaon.[50]  Although some claim Shree Pundalik (1912) of Dadasaheb Torne as the first film ever made in India,[51][52][50] some film scholars have argued that Pundalik was not a true Indian film because it was simply a recording of a stage play, filmed by a British cameraman and it was processed in London.[53][54][49] Raja Harishchandra of Phalke had a story based on Hindu Sanskrit legend of Harishchandra, a truthful King and its success led many to consider him a pioneer of Indian cinema.[50] Phalke used an all Indian crew including actors Anna Salunke and D. D. Dabke. He directed, edited, processed the film himself.[49] Phalke saw The Life of Christ (1906) by the French director Alice Guy-Blaché, While watching Jesus on the screen, Phalke envisioned Hindu deities Rama and Krishna instead and decided to start in the business of \"moving pictures\".[55]  In South India, film pioneer Raghupathi Venkaiah Naidu, credited as the father of Telugu cinema, built the first cinemas in Madras (now Chennai), and a film studio was established in the city by Nataraja Mudaliar.[56][57][58] In 1921, Naidu produced the silent film, Bhishma Pratigna, generally considered to be the first Telugu feature film.[59]  The first Tamil and Malayalam films, also silent films, were Keechaka Vadham (1917–1918, R. Nataraja Mudaliar)[60] and Vigathakumaran (1928, J. C. Daniel Nadar). The latter was the first Indian social drama film and featured the first Dalit-caste film actress.[citation needed]  The first chain of Indian cinemas, Madan Theatre, was owned by Parsi entrepreneur Jamshedji Framji Madan, who oversaw the production and distribution of films for the chain.[50] These included film adaptations from Bengal's popular literature and Satyawadi Raja Harishchandra (1917), a remake of Phalke's influential film.[citation needed]  Films steadily gained popularity across India as affordable entertainment for the masses (admission as low as an anna [one-sixteenth of a rupee] in Bombay).[42] Young producers began to incorporate elements of Indian social life and culture into cinema, others brought new ideas from across the world. Global audiences and markets soon became aware of India's film industry.[61]  In 1927, the British government, to promote the market in India for British films over American ones, formed the Indian Cinematograph Enquiry Committee. The ICC consisted of three British and three Indians, led by T. Rangachari, a Madras lawyer.[62] This committee failed to bolster the desired recommendations of supporting British Film, instead recommending support for the fledgling Indian film industry, and their suggestions were set aside.  The first Indian sound film was Alam Ara (1931) made by Ardeshir Irani.[50] Ayodhyecha Raja (1932) was the first sound film of Marathi cinema.[44] Irani also produced South India's first sound film, the Tamil–Telugu bilingual talking picture Kalidas (1931, H. M. Reddy).[63]  The first Telugu film with audible dialogue, Bhakta Prahlada (1932), was directed by H. M. Reddy, who directed the first bilingual (Telugu and Tamil) talkie Kalidas (1931).[64] East India Film Company produced its first Telugu film, Savitri (1933, C. Pullayya), adapted from a stage play by Mylavaram Bala Bharathi Samajam.[65] The film received an honorary diploma at the 2nd Venice International Film Festival.[66] Chittoor Nagayya was one of the first multilingual filmmakers in India.[67][68]  Jumai Shasthi was the first Bengali short film as a talkie.[69]  Jyoti Prasad Agarwala made his first film Joymoti (1935) in Assamese, and later made Indramalati.[citation needed] The first film studio in South India, Durga Cinetone, was built in 1936 by Nidamarthi Surayya in Rajahmundry, Andhra Pradesh.[70][contradictory] The advent of sound to Indian cinema launched musicals such as Indra Sabha and Devi Devyani, marking the beginning of song-and-dance in Indian films.[50] By 1935, studios emerged in major cities such as Madras, Calcutta and Bombay as filmmaking became an established industry, exemplified by the success of Devdas (1935).[71] The first colour film made in India was Kisan Kanya (1937, Moti B).[72] Viswa Mohini (1940) was the first Indian film to depict the Indian movie-making world.[73]  Swamikannu Vincent, who had built the first cinema of South India in Coimbatore, introduced the concept of \"tent cinema\" in which a tent was erected on a stretch of open land to screen films. The first of its kind was in Madras and called Edison's Grand Cinema Megaphone. This was due to the fact that electric carbons were used for motion picture projectors.[74][further explanation needed] Bombay Talkies opened in 1934 and Prabhat Studios in Pune began production of Marathi films.[71] Sant Tukaram (1936) was the first Indian film to be screened at an international film festival,[contradictory] at the 1937 edition of the Venice Film Festival. The film was judged one of the three best films of the year.[75] However, while Indian filmmakers sought to tell important stories, the British Raj banned Wrath (1930) and Raithu Bidda (1938) for broaching the subject of the Indian independence movement.[50][76][77]  The Indian Masala film—a term used for mixed-genre films that combined song, dance, romance, etc.—arose following the Second World War.[71] During the 1940s, cinema in South India accounted for nearly half of India's cinema halls, and cinema came to be viewed as an instrument of cultural revival.[71] The Indian People's Theatre Association (IPTA), an art movement with a communist inclination, began to take shape through the 1940s and the 1950s.[78] IPTA plays, such as Nabanna (1944), prepared the ground for realism in Indian cinema, exemplified by Khwaja Ahmad Abbas's Dharti Ke Lal (Children of the Earth, 1946).[78] The IPTA movement continued to emphasise realism in films Mother India (1957) and Pyaasa (1957), among India's most recognisable cinematic productions.[79]  Following independence, the 1947 partition of India divided the nation's assets and a number of studios moved to Pakistan.[71] Partition became an enduring film subject thereafter.[71] The Indian government had established a Films Division by 1948, which eventually became one of the world's largest documentary film producers with an annual production of over 200 short documentaries, each released in 18 languages with 9,000 prints for permanent film theatres across the country.[80]  The period from the late 1940s to the early 1960s is regarded by film historians as the Golden Age of Indian cinema.[84][85][86] This period saw the emergence of the parallel cinema movement, which emphasised social realism. Mainly led by Bengalis,[87] early examples include Dharti Ke Lal (1946, Khwaja Ahmad Abbas),[88] Neecha Nagar (1946, Chetan Anand),[89] Nagarik (1952, Ritwik Ghatak)[90][91] and Do Bigha Zamin (1953, Bimal Roy), laying the foundations for Indian neorealism[92]  The Apu Trilogy (1955–1959, Satyajit Ray) won prizes at several major international film festivals and firmly established the parallel cinema movement.[93] It was influential on world cinema and led to a rush of coming-of-age films in art house theatres.[94] Cinematographer Subrata Mitra developed the technique of bounce lighting, to recreate the effect of daylight on sets, during the second film of the trilogy[95] and later pioneered other effects such as the photo-negative flashbacks and X-ray digressions.[96]  During the 1950s, Indian cinema reportedly became the world's second largest film industry, earning a gross annual income of ₹250 million (equivalent to ₹26 billion or US$300 million in 2023) in 1953.[97] The government created the Film Finance Corporation (FFC) in 1960 to provide financial support to filmmakers.[98] While serving as Information and Broadcasting Minister of India in the 1960s, Indira Gandhi supported the production of off-beat cinema through the FFC.[98]  Baburao Patel of Filmindia called B. N. Reddy's Malliswari (1951) an \"inspiring motion picture\" which would \"save us the blush when compared with the best of motion pictures of the world\".[99] Film historian Randor Guy called Malliswari scripted by Devulapalli Krishnasastri a \"poem in celluloid, told with rare artistic finesse, which lingers long in the memory\".[100]  Commercial Hindi cinema began thriving, including acclaimed films Pyaasa (1957) and Kaagaz Ke Phool (1959, Guru Dutt) Awaara (1951) and Shree 420 (1955, Raj Kapoor). These films expressed social themes mainly dealing with working-class urban life in India; Awaara presented Bombay as both a nightmare and a dream, while Pyaasa critiqued the unreality of city life.[87]  Epic film Mother India (1957, Mehboob Khan) was the first Indian film to be nominated for the US-based Academy of Motion Picture Arts and Sciences' Academy Award for Best Foreign Language Film[citation needed] and defined the conventions of Hindi cinema for decades.[101][102][103] It spawned a new genre of dacoit films.[104] Gunga Jumna (1961, Dilip Kumar) was a dacoit crime drama about two brothers on opposite sides of the law, a theme that became common in Indian films in the 1970s.[105] Madhumati (1958, Bimal Roy) popularised the theme of reincarnation in Western popular culture.[106]  Actor Dilip Kumar rose to fame in the 1950s, and was the biggest Indian movie star of the time.[107][108] He was a pioneer of method acting, predating Hollywood method actors such as Marlon Brando. Much like Brando's influence on New Hollywood actors, Kumar inspired Hindi actors, including Amitabh Bachchan, Naseeruddin Shah, Shah Rukh Khan and Nawazuddin Siddiqui.[109]  Neecha Nagar (1946) won the Palme d'Or at Cannes[89] and Indian films competed for the award most years in the 1950s and early 1960s.[citation needed] Ray is regarded as one of the greatest auteurs of 20th century cinema,[110] along with his contemporaries Dutt[111] and Ghatak.[112] In 1992, the Sight & Sound Critics' Poll ranked Ray at No. 7 in its list of Top 10 Directors of all time.[113] Multiple films from this era are included among the greatest films of all time in various critics' and directors' polls, including The Apu Trilogy,[114] Jalsaghar, Charulata[115] Aranyer Din Ratri,[116] Pyaasa, Kaagaz Ke Phool, Meghe Dhaka Tara, Komal Gandhar, Awaara, Baiju Bawra, Mother India, Mughal-e-Azam[117] and Subarnarekha (also tied at No. 11).[112]  Sivaji Ganesan became India's first actor to receive an international award when he won the Best Actor award at the Afro-Asian film festival in 1960 and was awarded the title of Chevalier in the Legion of Honour by the French Government in 1995.[118] Tamil cinema is influenced by Dravidian politics,[119] with prominent film personalities C N Annadurai, M G Ramachandran, M Karunanidhi and Jayalalithaa becoming Chief Ministers of Tamil Nadu.[120][timeframe?]  By 1986, India's annual film output had increased to 833 films annually, making India the world's largest film producer.[121] Hindi film production of Bombay, the largest segment of the industry, became known as \"Bollywood\".  By 1996, the Indian film industry had an estimated domestic cinema viewership of 600 million people, establishing India as one of the largest film markets, with the largest regional industries being Hindi, Telugu, and Tamil films.[122] In 2001, in terms of ticket sales, Indian cinema sold an estimated 3.6 billion tickets annually across the globe, compared to Hollywood's 2.6 billion tickets sold.[123][124]  Realistic parallel cinema continued throughout the 1970s,[125] practised in many Indian film cultures. The FFC's art film orientation came under criticism during a Committee on Public Undertakings investigation in 1976, which accused the body of not doing enough to encourage commercial cinema.[126]  Hindi commercial cinema continued with films such as Aradhana (1969), Sachaa Jhutha (1970), Haathi Mere Saathi (1971), Anand (1971), Kati Patang (1971) Amar Prem (1972), Dushman (1972) and Daag (1973).[importance?]  By the early 1970s, Hindi cinema was experiencing thematic stagnation,[129] dominated by musical romance films.[130] Screenwriter duo Salim–Javed (Salim Khan and Javed Akhtar) revitalised the industry.[129] They established the genre of gritty, violent, Bombay underworld crime films with Zanjeer (1973) and Deewaar (1975).[131][132] They reinterpreted the rural themes of Mother India and Gunga Jumna in an urban context reflecting 1970s India,[129][133] channelling the growing discontent and disillusionment among the masses,[129] unprecedented growth of slums[134] and urban poverty, corruption and crime,[135] as well as anti-establishment themes.[136] This resulted in their creation of the \"angry young man\", personified by Amitabh Bachchan,[136] who reinterpreted Kumar's performance in Gunga Jumna[129][133] and gave a voice to the urban poor.[134]  By the mid-1970s, Bachchan's position as a lead actor was solidified by crime-action films Zanjeer and Sholay (1975).[126] The devotional classic Jai Santoshi Ma (1975) was made on a low budget and became a box office success and a cult classic.[126] Another important film was Deewaar (1975, Yash Chopra),[105] a crime film with brothers on opposite sides of the law which Danny Boyle described as \"absolutely key to Indian cinema\".[137]  The term \"Bollywood\" was coined in the 1970s,[138][139] when the conventions of commercial Bombay-produced Hindi films were established.[140] Key to this was Nasir Hussain and Salim–Javed's creation of the masala film genre, which combines elements of action, comedy, romance, drama, melodrama and musical.[140][141] Their film Yaadon Ki Baarat (1973) has been identified as the first masala film and the first quintessentially Bollywood film.[140][142] Masala films made Bachchan the biggest Bollywood movie star of the period. Another landmark was Amar Akbar Anthony (1977, Manmohan Desai).[142][143] Desai further expanded the genre in the 1970s and 1980s.  Commercial Hindi cinema grew in the 1980s, with films such as Ek Duuje Ke Liye (1981), Disco Dancer (1982),  Himmatwala (1983), Tohfa (1984), Naam (1986), Mr India (1987), and Tezaab (1988).  In the late 1980s,[timeframe?] Hindi cinema experienced another period of stagnation, with a decline in box office turnout, due to increasing violence, decline in musical melodic quality, and rise in video piracy, leading to middle-class family audiences abandoning theatres. The turning point came with Indian blockbuster Disco Dancer (1982) which began the era of disco music in Indian cinema. Lead actor Mithun Chakraborty and music director Bappi Lahiri had the highest number of mainstream Indian hit movies that decade. At the end of the decade, Yash Chopra's Chandni (1989) created a new formula for Bollywood musical romance films, reviving the genre and defining Hindi cinema in the years that followed.[144][145] Commercial Hindi cinema grew in the late 1980s and 1990s, with the release of Mr. India (1987), Qayamat Se Qayamat Tak (1988), Chaalbaaz (1989), Maine Pyar Kiya (1989), Lamhe (1991), Saajan (1991), Khuda Gawah (1992), Khalnayak (1993), Darr (1993),[126] Hum Aapke Hain Koun..! (1994), Dilwale Dulhaniya Le Jayenge (1995), Dil To Pagal Hai (1997), Pyar Kiya Toh Darna Kya (1998) and Kuch Kuch Hota Hai (1998). Cult classic Bandit Queen (1994) directed by Shekhar Kapur received international recognition and controversy.[146][147]  In the late 1990s, there was a resurgence of parallel cinema in Bollywood, largely due to the critical and commercial success of crime films such as Satya (1998) and Vaastav (1999). These films launched a genre known as \"Mumbai noir\",[149] reflecting social problems in the city.[150] Ram Gopal Varma directed the Indian Political Trilogy, and the Indian Gangster Trilogy; film critic Rajeev Masand had labelled the latter series as one of the \"most influential movies of Bollywood.[151][152][153] The first instalment of the trilogy, Satya, was also listed in CNN-IBN's 100 greatest Indian films of all time.[154]  Since the 1990s, the three biggest Bollywood movie stars have been the \"Three Khans\": Aamir Khan, Shah Rukh Khan, and Salman Khan.[155][156] Combined, they starred in the top ten highest-grossing Bollywood films,[155] and have dominated the Indian box office since the 1990s.[157][158] Shah Rukh Khan was the most successful for most of the 1990s and 2000s, while Aamir Khan has been the most successful since the late 2000s;[159] according to Forbes, Shah Rukh Khan is \"arguably the world's biggest movie star\" as of 2017, due to his immense popularity in India and China.[160] Other notable Hindi film stars of recent decades include Arjun Rampal, Sunny Deol, Akshay Kumar, Ajay Devgn, Hrithik Roshan, Anil Kapoor, Sanjay Dutt, Sridevi, Madhuri Dixit, Juhi Chawla, Karisma Kapoor, Kajol, Tabu, Aishwarya Rai, Rani Mukerji and Preity Zinta.  Haider (2014, Vishal Bhardwaj), the third instalment of the Indian Shakespearean Trilogy after Maqbool (2003) and Omkara (2006),[161] won the People's Choice Award at the 9th Rome Film Festival in the Mondo Genere making it the first Indian film to achieve this honour.[162][relevant?]  The 2000s and 2010s also saw the rise of a new generation of popular actors like Shahid Kapoor, Ranbir Kapoor, Ranveer Singh, Ayushmann Khurrana, Varun Dhawan, Sidharth Malhotra, Sushant Singh Rajput, Kartik Aaryan, Arjun Kapoor, Aditya Roy Kapur and Tiger Shroff, as well as actresses like Vidya Balan, Priyanka Chopra, Kareena Kapoor, Katrina Kaif, Kangana Ranaut, Deepika Padukone, Sonam Kapoor, Anushka Sharma, Shraddha Kapoor, Alia Bhatt, Parineeti Chopra and Kriti Sanon with Balan, Ranaut and Bhatt gaining wide recognition for successful female-centric films such as The Dirty Picture (2011), Kahaani (2012), Queen (2014), Highway (2014), Tanu Weds Manu Returns (2015), Raazi (2018) and Gangubai Kathiawadi (2022).  Salim–Javed were highly influential in South Indian cinema. In addition to writing two Kannada films, many of their Bollywood films had remakes produced in other regions, including Tamil, Telugu and Malayalam cinema. While the Bollywood directors and producers held the rights to their films in Northern India, Salim–Javed retained the rights in South India, where they sold remake rights for films such as Zanjeer, Yaadon Ki Baarat and Don.[163] Several of these remakes became breakthroughs for actor Rajinikanth.[130][164]  Sridevi is widely regarded as the first female superstar of Indian cinema due to her pan-Indian appeal with equally successful careers in Hindi, Tamil, Malayalam, Kannada and Telugu cinema. She is the only Bollywood actor to have starred in a top 10 grossing film each year of her active career (1983–1997).[citation needed]  K. V. Reddy's Mayabazar (1957) is a landmark film in Indian cinema, a classic of Telugu cinema that inspired generations of filmmakers. It blends myth, fantasy, romance and humour in a timeless story, captivating audiences with its fantastical elements. The film excelled in various departments like cast performances, production design, music, cinematography and is particularly revered for its use of technology.[165][166] The use of special effects, innovative for the 1950s, like the first illusion of moonlight, showcased technical brilliance.. Powerful performances and relatable themes ensure Mayabazar stays relevant, a classic enjoyed by new generations. On the centenary of Indian cinema in 2013, CNN-IBN included Mayabazar in its list of \"100 greatest Indian films of all time\".[167] In a poll conducted by CNN-IBN among those 100 films, Mayabazar was voted by the public as the \"Greatest Indian film of all time\".[168]  K. Viswanath, one of the prominent auteurs of Indian cinema, he received international recognition for his works, and is known for blending parallel cinema with mainstream cinema. His works such as Sankarabharanam (1980) about revitalisation of Indian classical music won the \"Prize of the Public\" at the Besançon Film Festival of France in the year 1981.[169] Forbes included J. V. Somayajulu's performance in the film on its list of \"25 Greatest Acting Performances of Indian Cinema\".[170] Swathi Muthyam (1986) was India's official entry to the 59th Academy Awards.[169] Swarna Kamalam (1988) the dance film choreographed by Kelucharan Mohapatra, and Sharon Lowen was featured at the Ann Arbor Film Festival, fetching three Indian Express Awards.[171][172]  B. Narsing Rao, K. N. T. Sastry, and A. Kutumba Rao garnered international recognition for their works in new-wave cinema.[173][174] Narsing Rao's Maa Ooru (1992) won the \"Media Wave Award\" of Hungary; Daasi (1988) and Matti Manushulu (1990) won the Diploma of Merit awards at the 16th and 17th MIFF respectively.[175][176] Sastry's Thilaadanam (2000) received \"New Currents Award\" at the 7th Busan;[177][178] Rajnesh Domalpalli's Vanaja (2006) won \"Best First Feature Award\" at the 57th Berlinale.[179][180]  Ram Gopal Varma's Siva (1989), which attained cult following[181] introduced steadicams and new sound recording techniques to Indian films.[182] Siva attracted the young audience during its theatrical run, and its success encouraged filmmakers to explore a variety of themes and make experimental films.[183] Varma introduced road movie and film noir to Indian screen with Kshana Kshanam (1991).[184] Varma experimented with close-to-life performances by the lead actors, which bought a rather fictional storyline a sense of authenticity at a time when the industry was being filled with commercial fillers.[185]  Singeetam Srinivasa Rao introduced time travel to the Indian screen with Aditya 369 (1991). The film dealt with exploratory dystopian and apocalyptic themes, taking the audience through a post-apocalyptic experience via time travel and folklore from 1526 CE, including a romantic subplot.[186] Singeetam Srinivasa Rao was inspired by the classic sci-fi novel The Time Machine.[187][188][189]  Chiranjeevi's works such as the social drama film Swayamkrushi (1987), comedy thriller Chantabbai (1986), the vigilante thriller Kondaveeti Donga (1990),[190] the Western thriller Kodama Simham (1990), and the action thriller, Gang Leader (1991), popularised genre films with the highest estimated cinema footfalls.[191] Sekhar Kammula's Dollar Dreams (2000), which explored the conflict between American dreams and human feelings, re-introduced social realism to Telugu film which had stagnated in formulaic commercialism.[192] War drama Kanche (2015, Krish Jagarlamudi) explored the 1944 Nazi attack on the Indian army in the Italian campaign of the Second World War.[193]  Pan-Indian film is a term related to Indian cinema that originated with Telugu cinema as a mainstream commercial film appealing to audiences across the country with a spread to world markets.[196] S. S. Rajamouli pioneered the pan-Indian films movement with duology of epic action films Baahubali: The Beginning (2015) and Baahubali 2: The Conclusion (2017), that changed the face of Indian cinema. Baahubali: The Beginning became the first Indian film to be nominated for American Saturn Awards.[197] It received national and international acclaim for Rajamouli's direction, story, visual effects, cinematography, themes, action sequences, music, and performances, and became a record-breaking box office success.[198] The sequel Baahubali 2 (2017) went on to win the American \"Saturn Award for Best International Film\" & emerged as the second-highest-grossing Indian film of all time.[199][200]  S.S Rajamouli followed up with the alternate historical film RRR (2022) that received universal critical acclaim for its direction, screenwriting, cast performances, cinematography, soundtrack, action sequences and VFX, which further consolidated the Pan-Indian film market. The film was considered one of the ten best films of the year by the National Board of Review, making it only the seventh non-English language film ever to make it to the list.[201] It also became the first Indian film by an Indian production to win an Academy Award.[202] The film went on to receive several other nominations at the Golden Globe Awards, Critics' Choice Movie Award including Best Foreign Language Film.[203] Films like Pushpa: The Rise, Salaar: Part 1 – Ceasefire and Kalki 2898 AD have further contributed to the pan-Indian film wave.  Actors like Prabhas, Allu Arjun, Ram Charan and N. T. Rama Rao Jr. enjoy a nationwide popularity among the audiences after the release of their respective Pan-Indian films. Film critics, journalists and analysts, such as Baradwaj Rangan and Vishal Menon, have labelled Prabhas as the \"first legit Pan-Indian Superstar\".[204]  Hindi cinema has been remaking Telugu films since the late 1940s, some of which went on to become landmark films. Between 2000 and 2019, one in every three successful films made in Hindi was either a remake or part of a series. And most of the star actors, have starred in the hit remakes of Telugu films.[205]  Tamil cinema established Madras (now Chennai) as a secondary film production centre in India, used by Hindi cinema, other South Indian film industries, and Sri Lankan cinema.[206] Over the last quarter of the 20th century, Tamil films from India established a global presence through distribution to an increasing number of overseas theatres.[207][208] The industry also inspired independent filmmaking in Sri Lanka and Tamil diaspora populations in Malaysia, Singapore, and the Western Hemisphere.[209]  Marupakkam (1991, K. S. Sethumadhavan) and Kanchivaram (2007) each won the National Film Award for Best Feature Film.[210] Tamil films receive significant patronage in neighbouring Indian states Kerala, Karnataka, Andhra Pradesh, Maharashtra, Gujarat and New Delhi. In Kerala and Karnataka the films are directly released in Tamil but in Andhra Pradesh and Telangana they are generally dubbed into Telugu.[211][212]  Tamil films have had international success for decades. Since Chandralekha (1948), Muthu (1995) was the second Tamil film to be dubbed into Japanese (as Mutu: Odoru Maharaja[213]) and grossed a record $1.6 million in 1998.[214] In 2010, Enthiran grossed a record $4 million in North America.[215] Tamil-language films appeared at multiple film festivals. Kannathil Muthamittal (Ratnam), Veyyil (Vasanthabalan) and Paruthiveeran (Ameer Sultan), Kanchivaram (Priyadarshan) premiered at the Toronto International Film Festival. Tamil films were submitted by India for the Academy Award for Best Foreign Language Film on eight occasions.[216] Chennai-based music composer A. R. Rahman achieved global recognition with two Academy Awards and is nicknamed as \"Isai Puyal\" (musical storm) and \"Mozart of Madras\". Nayakan (1987, Kamal Haasan) was included in Time's All-Time 100 Movies list.[217]  Malayalam cinema experienced its Golden Age during this time with works of filmmakers such as Adoor Gopalakrishnan, G. Aravindan, T. V. Chandran and Shaji N. Karun.[218] Gopalakrishnan is often considered to be Ray's spiritual heir.[219] He directed some of his most acclaimed films during this period, including Elippathayam (1981) which won the Sutherland Trophy at the London Film Festival.[citation needed] In 1984 My Dear Kuttichathan, directed by Jijo Punnoose under Navodaya Studio, was released and it was the first Indian film to be filmed in 3D format. Karun's debut film Piravi (1989) won the Caméra d'Or at Cannes, while his second film Swaham (1994) was in competition for the Palme d'Or. Vanaprastham was screened at the Un Certain Regard section of the Cannes Film Festival.[citation needed] Murali Nair's Marana Simhasanam (1999), inspired by the first execution by electrocution in India, the film was screened in the Un Certain Regard section at the 1999 Cannes Film Festival where it won the Caméra d'Or.[220][221] The film received special reception at the British Film Institute.[222][223]  Fazil's Manichitrathazhu (1993), scripted by Madhu Muttam, is inspired by a tragedy that happened in an Ezhava tharavad of Alummoottil meda' (an old traditional house) located at Muttom, Alappuzha district, with a central Travancore Channar family, in the 19th century.[224] It was remade in four languages – in Kannada as Apthamitra, in Tamil as Chandramukhi , in Bengali as Rajmohol and in Hindi as Bhool Bhulaiyaa – all being commercially successful.[225] Jeethu Joseph's Drishyam (2013) was remade into four other Indian languages: Drishya (2014) in Kannada, Drushyam (2014) in Telugu, Papanasam (2015) in Tamil and Drishyam (2015) in Hindi. Internationally, it was remade in Sinhala language as Dharmayuddhaya (2017) and in Chinese as Sheep Without a Shepherd (2019), and also in Indonesian.[226][227][228]  Ethnographic works took prominence such as B. V. Karanth's Chomana Dudi (1975), (based on Chomana Dudi by Shivaram Karanth), Girish Karnad's Kaadu (1973), (based on Kaadu by Srikrishna Alanahalli), Pattabhirama Reddy's Samskara (1970) (based on Samskara by U. R. Ananthamurthy), fetching the Bronze Leopard at Locarno International Film Festival,[229] and T. S. Nagabharana's Mysuru Mallige (based on the works of poet K. S. Narasimhaswamy).[230] Girish Kasaravalli's Ghatashraddha (1977), won the Ducats Award at the Manneham Film Festival Germany,[231] Dweepa (2002), made to Best Film at Moscow International Film Festival,[232][233]  Prashanth Neel's K.G.F (2018, 2022) is a period action series based on the Kolar Gold Fields.[234] Set in the late 1970s and early 1980s the series follows Raja Krishnappa Bairya aka Rocky (Yash), a Mumbai-based high ranking mercenary born in poverty, to his rise to power in the Kolar Gold Fields and the subsequent uprising as one of the biggest gangster and businessman at that time.[235][236] The film gathered cult following becoming the highest-grossing Kannada film.[237] Rishab Shetty's Kantara (2022), received acclaim for showcasing the Bhoota Kola, a native Ceremonial dance performance prevalent among the Hindus of coastal Karnataka.[238]  Marathi cinema also known as Marathi film industry, is a film industry based in Mumbai, Maharashtra. It is the oldest film industry of India. The first Marathi movie, Raja Harishchandra of Dadasaheb Phalke was made in 1912, released in 1913 in Girgaon, it was a silent film with Marathi-English intertitles made with full Marathi actors and crew, after the film emerged successful, Phalke made many movies on Hindu mythology.  In 1932, the first sound film, Ayodhyecha Raja was released, just five years after 1st Hollywood sound film The Jazz Singer (1927). The first Marathi film in colour, Pinjara (1972), was made by V. Shantaram. In 1960s–70s movies was based on rural, social subjects with drama and humour genre, Nilu Phule was prominent villain that time. In 1980s, M. Kothare and Sachin Pilgaonkar made many hit movies on thriller, and comedy genre respectively. Ashok Saraf and Laxmikant Berde starred in many of these and emerged as top actors. Mid-2000s onwards, the industry frequently made hit movies.[44][49][239]  K. Moti Gokulsing and Wimal Dissanayake identified six major influences that have shaped Indian popular cinema:[244]  Sharmistha Gooptu and Bhaumik identify Indo-Persian\/Islamicate culture as another major influence. In the early 20th century, Urdu was the lingua franca of popular performances across northern India, established in performance art traditions such as nautch dancing, Urdu poetry and Parsi theatre. Urdu and related Hindi dialects were the most widely understood across northern India, thus Hindustani became the standardised language of early Indian talkies. One Thousand and One Nights (Arabian Nights) had a strong influence on Parsi theatre, which adapted \"Persianate adventure-romances\" into films, and on early Bombay cinema where \"Arabian Nights cinema\" became a popular genre.[250]  Like mainstream Indian popular cinema, Indian parallel cinema was influenced by a combination of Indian theatre and Indian literature (such as Bengali literature and Urdu poetry), but differs when it comes to foreign influences, where it is influenced more by European cinema (particularly Italian neorealism and French poetic realism) than by Hollywood. Ray cited Vittorio De Sica's Bicycle Thieves (1948) and Jean Renoir's The River (1951), on which he assisted, as influences on his debut film Pather Panchali (1955).  During colonial rule, Indians bought film equipment from Europe.[61] The British funded wartime propaganda films during the Second World War, some of which showed the Indian army pitted against the Axis powers, specifically the Empire of Japan, which had managed to infiltrate India.[251] One such story was Burma Rani, which depicted civilian resistance to Japanese occupation by British and Indian forces in Myanmar.[251] Pre-independence businessmen such as J. F. Madan and Abdulally Esoofally traded in global cinema.[50]  Early Indian films made early inroads into the Soviet Union, Middle East, Southeast Asia[252] and China. Mainstream Indian movie stars gained international fame across Asia[253][254][255] and Eastern Europe.[256] For example, Indian films were more popular in the Soviet Union than Hollywood films[257][258] and occasionally domestic Soviet films.[259] From 1954 to 1991, 206 Indian films were sent to the Soviet Union, drawing higher average audience figures than domestic Soviet productions,[258][260] Films such as Awaara and Disco Dancer drew more than 60 million viewers.[261][262] Films such as Awaara, 3 Idiots and Dangal,[263][264] were among the 20 highest-grossing films in China.[265]  Many Asian and South Asian countries increasingly found Indian cinema more suited to their sensibilities than Western cinema.[252] Jigna Desai holds that by the 21st century, Indian cinema had become 'deterritorialised', spreading to parts of the world where Indian expatriates were present in significant numbers and had become an alternative to other international cinema.[266]  Indian films frequently appeared in international fora and film festivals.[252] This allowed parallel Bengali filmmakers to achieve worldwide fame.[267]  Indian cinema more recently began influencing Western musical films, and played a particularly instrumental role in the revival of the genre in the Western world. Ray's work had a worldwide impact, with filmmakers such as Martin Scorsese,[268] James Ivory,[269] Abbas Kiarostami, François Truffaut,[270] Carlos Saura,[271] Isao Takahata and Gregory Nava[272] citing his influence, and others such as Akira Kurosawa praising his work.[273] The \"youthful coming-of-age dramas that flooded art houses since the mid-fifties owe a tremendous debt to the Apu trilogy\", according to the film critic Michael Sragow.[94] Since the 1980s, overlooked Indian filmmakers such as Ghatak[274] and Dutt[275] posthumously gained international acclaim. Baz Luhrmann stated that his successful musical film Moulin Rouge! (2001) was directly inspired by Bollywood musicals.[276] That film's success renewed interest in the then-moribund Western musical genre, subsequently fuelling a renaissance.[277] Danny Boyle's Slumdog Millionaire (2008) was directly inspired by Indian films,[137][278] and is considered to be an \"homage to Hindi commercial cinema\".[279]  Indian cinema has been recognised repeatedly at the US-based Academy Awards. Indian films Mother India (1957), Salaam Bombay! (1988) and Lagaan (2001), were nominated for the Academy Award for Best Foreign Language Film. Indian Oscar winners include Bhanu Athaiya (costume designer), Ray (filmmaker), A. R. Rahman (music composer), Resul Pookutty (sound editor) and Gulzar (lyricist), M. M. Keeravani (music composer), Chandrabose (lyricist)  Cottalango Leon and Rahul Thakkar Sci-Tech Award.[280][281]  Masala is a style of Indian cinema that mixes multiple genres in one work, pioneered in the early 1970s Bollywood by filmmaker Nasir Hussain,[282][128][142] For example, one film can portray action, comedy, drama, romance and melodrama. These films tend to be musicals with songs filmed in picturesque locations. Plots for such movies may seem illogical and improbable to unfamiliar viewers. The genre is named after masala, a mixture of spices in Indian cuisine.  Parallel cinema, also known as art cinema or the Indian New Wave, is known for its realism and naturalism, addressing the sociopolitical climate. This movement is distinct from mainstream Bollywood cinema and began around the same time as the French and Japanese New Waves. The movement began in Bengal (led by Ray, Sen and Ghatak) and then gained prominence in other regions. The movement was launched by Bimal Roy's Do Bigha Zamin (1953), which was both a commercial and critical success, winning the International Prize at Cannes.[92][283][284]  Ray's films include the three instalments of The Apu Trilogy which won major prizes at the Cannes, Berlin and Venice Film Festivals, and are frequently listed among the greatest films of all time.[285][286][287][288]  Other neo-realist filmmakers were Shyam Benegal, Karun, Gopalakrishnan[87] and Kasaravalli.[289]  Some Indian films are known as \"multilinguals\", filmed in similar but non-identical versions, in different languages. Chittoor Nagayya, was one of the first multilingual filmmakers in India.[67] Alam Ara and Kalidas are earliest examples of bilingual filmmaking in India. According to Ashish Rajadhyaksha and Paul Willemen in the Encyclopedia of Indian Cinema (1994), in its most precise form, a multilingual is  a bilingual or a trilingual [that] was the kind of film made in the 1930s in the studio era, when different but identical takes were made of every shot in different languages, often with different leading stars but identical technical crew and music.[290]: 15  Rajadhyaksha and Willemen note that in seeking to construct their Encyclopedia, they often found it \"extremely difficult to distinguish multilinguals in this original sense from dubbed versions, remakes, reissues or, in some cases, the same film listed with different titles, presented as separate versions in different languages ... it will take years of scholarly work to establish definitive data in this respect\".[290]: 15   Pan-India is a term related to Indian cinema that originated with Telugu cinema as a mainstream commercial cinema appealing to audiences across the country with a spread to world markets. S. S. Rajamouli pioneered the Pan-Indian films movement with his duology of epic action films Baahubali: The Beginning (2015) and Baahubali 2: The Conclusion (2017).[291][292] \"Pan-India film\" is both a style of cinema and a distribution strategy, designed to universally appeal to audiences across the country and simultaneously released in multiple languages.[293]  Music and songs are a big part of Indian cinema and it's not just for entertainment but they play a crucial role in storytelling. Music and dance are a core part of Indian culture, and films weave them in to tell the story. Songs are used to express emotions that spoken dialogue might struggle to convey. Songs often used to move the plot forward. Lyrics might reveal a character's inner thoughts, motivations, or foreshadow future events. Sometimes the song itself can become a turning point in the story. While some may find them disruptive, songs remain a deeply rooted tradition in Indian cinema, reflecting both its culture and what audiences love.  Music is a substantial revenue generator for the Indian film industry, with music rights alone accounting for 4–5% of net revenues.[294] The major film music companies are T-Series at Delhi, Sony Music India at Chennai and Zee Music Company at Mumbai, Aditya Music at Hyderabad and Saregama at Kolkata.[294] Film music accounts for 48% of net music sales in the country.[294] A typical film may feature 5–6 choreographed songs.[295]  The demands of a multicultural, increasingly globalised Indian audience led to a mixing of local and international musical traditions.[295] Local dance and music remain a recurring theme in India and followed the Indian diaspora.[295] Playback singers such as Mohammad Rafi, Lata Mangeshkar, Kishore Kumar, Asha Bhosle, Mukesh, S. Janaki, P. Susheela, K. J. Yesudas, S. P. Balasubrahmanyam, K. S. Chithra, Anuradha Paudwal, Kavita Krishnamurthy, Alka Yagnik, Sadhana Sargam, Shreya Ghoshal ,Sunidhi Chauhan, Kumar Sanu, Udit Narayan, Abhijeet and Sonu Nigam drew crowds to presentations of film music.[295] In the 21st century interaction increased between Indian artists and others.[specify][296]  In 2023, the song \"Naatu Naatu\" composed by M. M. Keeravani for the movie RRR won the Oscar for Best Original Song at the 95th Academy Awards, making it the first song from an Indian film, as well as the first from an Asian film, to win in this category. This made it the first Indian film by an Indian production to win an Academy Award.[202][297]  A filming location is any place where acting and dialogue are recorded. Sites where filming without dialogue takes place are termed a second unit photography site. Filmmakers often choose to shoot on location because they believe that greater realism can be achieved in a \"real\" place. Location shooting is often motivated by budget considerations.[citation needed]  The most popular locations for filming in India are the main cities of their state for regional industry. Other locations include Manali and Shimla in Himachal Pradesh; Srinagar in Jammu and Kashmir; Ladakh; Darjeeling in West Bengal; Ooty and Kodaikanal in Tamil Nadu; Amritsar in Punjab; Udaipur, Jodhpur, Jaisalmer and Jaipur in Rajasthan; Delhi; Ottapalam in Kerala; Goa and Puducherry.[298][299]  More than 1000 production organisations operate in the Indian film industry, but few are successful. AVM Productions is the oldest surviving studio in India. Other major production houses include Salman Khan Films, Yash Raj Films, K Sera Sera Virtual Productions ProductionsVyjayanthi Movies,T-Series, Aamir Khan Productions, Lyca Productions, Madras Talkies, AGS Entertainment, Sun Pictures, Red Chillies Entertainment, Arka Media Works, Dharma Productions, Eros International, Sri Venkateswara Creations, Ajay Devgn FFilms, Balaji Motion Pictures, UTV Motion Pictures, Raaj Kamal Films International, Aashirvad Cinemas, Wunderbar Films, Cape of Good Films, Mythri Movie Makers, Maddock Films and Geetha Arts.[300]  Films are made in many cities and regions in India including Andhra Pradesh and Telangana, Assam, Bengal, Bihar, Gujarat, Haryana, Jammu, Kashmir, Jharkhand, Karnataka, Goa, Kerala, Maharashtra, Manipur, Odisha, Chhattisgarh, Punjab, Rajasthan, Tamil Nadu, Tripura and Mizoram.  The Assamese-language film industry is based in Assam in northeastern India. It is sometimes called Jollywood, for the Jyoti Chitraban Film Studio. Some films have been well received by critics but they have not yet captured national audiences. The 21st century has produced Bollywood-style Assamese movies which have set new box office records for the small industry.[302]  The Bengali-language cinematic tradition of Tollygunge, West Bengal, is also known as Tollywood.[303] When the term was coined in the 1930s, it was the centre of the Indian film industry.[304] West Bengal cinema is historically known for the parallel cinema movement and art films.  Braj-language films present Brij culture mainly to rural people, predominantly in the nebulous Braj region centred around Mathura, Agra, Aligarh and Hathras in Western Uttar Pradesh and Bharatpur and Dholpur in Rajasthan (northern India). It is the predominant language in the central stretch of the Ganges-Yamuna Doab in Uttar Pradesh. The first Brij Bhasha movie was Brij Bhoomi (1982, Shiv Kumar), which was a success throughout the country.[305][306] Later Brij Bhasha cinema saw the production of films like Jamuna Kinare and Brij Kau Birju.[307][308]  Bhojpuri-language films predominantly cater to residents of western Bihar and eastern Uttar Pradesh and also have a large audience in Delhi and Mumbai due to the migration of Bhojpuri speakers to these cities. International markets for these films developed in other Bhojpuri-speaking countries of the West Indies, Oceania and South America.[309]  Bhojpuri film history begins with Ganga Maiyya Tohe Piyari Chadhaibo (Mother Ganges, I will offer you a yellow sari, 1962, Kundan Kumar).[310] Throughout the following decades, few films were produced. The industry experienced a revival beginning with the hit Saiyyan Hamar (My Sweetheart, 2001, Mohan Prasad).[311] Although smaller than other Indian film industries, these successes increased Bhojpuri cinema's visibility, leading to an awards show[312] and a trade magazine, Bhojpuri City.[313]  The Chakma language is spoken in Tripura and Mizoram (Northeast India), as well as in the Chittagong Hill Tracts region of Bangladesh. Films in Chakma include Tanyabi Firti (Tanyabi's Lake, 2005, Satarupa Sanyal).[314]  The Chhattisgarhi-language film industry of Chhattisgarhi state, central India, is known as Chhollywood. Its beginnings are with Kahi Debe Sandesh (In Black and White, 1965, Manu Nayak)[315][316][317] No Chhattisgarhi films were released from 1971[318] until Mor Chhainha Bhuinya (2000).[citation needed]  Indian filmmakers also produce English language films. Deepa Mehta, Anant Balani, Homi Adajania, Vijay Singh, Vierendrra Lalit and Sooni Taraporevala have garnered recognition in Indian English cinema.  The Gujarati-language film industry, also known as Gollywood or Dhollywood, is currently centered in the state of Gujarat. During the silent era, many filmmakers and actors were Gujarati and Parsi, and their films were closely related to Gujarati culture. Twenty film companies and studios, mostly located in Bombay, were owned by Gujaratis and at least 44 major Gujarati directors worked during this era.[319] The first film released in Gujarati was Narsinh Mehta (1932).[319][320][321]  More than one thousand Gujarati films have been released.[322]  Gujarati cinema ranges from mythology to history and from social to political. Gujarati films originally targeted a rural audience, but after its revival (c. 2005) catered to an urban audience.[319]  The Hindi language film industry of Mumbai (formerly Bombay), also known as Bollywood,[324] is the largest and most powerful branch of Hindi cinema.[325] Hindi cinema explores issues of caste and culture in films such as Achhut Kanya (1936) and Sujata (1959).[326] International visibility came to the industry with Raj Kapoor's Awara and later in Shakti Samantha's Aradhana.[327] Art film directors include Kaul, Kumar Shahani, Ketan Mehta, Govind Nihalani, Shyam Benegal,[87] Mira Nair, Nagesh Kukunoor, Sudhir Mishra and Nandita Das. Hindi cinema grew during the 1990s with the release of as many as 215 films annually. Magazines such as Filmfare, Stardust and Cine Blitz popularly cover the industry.[328]  Kannada cinema, also known as Sandalwood or Chandanavana,[329] is the segment of Indian cinema[330] dedicated to the production of motion pictures in the Kannada language, which is widely spoken in Karnataka state.[331][332][333] Sati Sulochana (1934, Y. V. Rao) was the first talkie film in the Kannada language.[334][335][336] Kannada films include adaptations of major literary works[229][337] and experimental films.[231]  Kokborok-language films are mainly produced in Tripura and parts of Bangladesh. These films are also clubbed as 'Tripuri cinema' as a blanket term that alludes to the film industry of Tripura, encompassing films made by and for the people of Tripura and Kokborok speaking people in Bangladesh, regardless of the multitude of languages in which cinema is produced in the region'.[338]  Konkani-language films are mainly produced in Goa, one of India's smallest film regions which produced four films in 2009.[339] The first full-length Konkani film was Mogacho Anvddo (1950, Jerry Braganza).[340] The film's release date, 24 April, is celebrated as Konkani Film Day.[341] An immense body of Konkani literature and art is a resource for filmmakers. Kazar (Marriage, 2009, Richard Castelino) and Ujvaadu (Shedding New Light on Old Age Issues, Kasaragod Chinna) are major releases. The pioneering Mangalorean Konkani film is Mog Ani Maipas.  Maithili cinema is made in the Maithili language. The first full-length film was Kanyadan (1965).[342] There are numerous films made in the Maithili over the years[343] The film Mithila Makhaan (2019) won a National Award in the regional films category.[344]  The Malayalam-language film industry, also known as Mollywood, is India's fourth-largest film industry. It is mainly based at Kochi, Kerala state. Neelakkuyil (1954) was one of the first Malayalam films to get national recognition.[346] Newspaper Boy (1955), made by a group of students, was the first neo-realistic Malayalam film.[347] Chemmeen (1965, Ramu Kariat), based on a story by Thakazhi Sivasankara Pillai, became the first South Indian film to win the National Film Award for Best Feature Film.[348]  Malayalam cinema has been in the forefront of technological innovation in Indian filmmaking. The first neorealistic film (Newspaper Boy),[218] the first CinemaScope film (Thacholi Ambu),[349] the first 70 mm film (Padayottam),[350] the first 3D film (My Dear Kuttichathan),[351] the first Panavision film (Vanaprastham), the first digital film (Moonnamathoral),[352] the first Smartphone film (Jalachhayam),[353] and the first 8K film (Villain)[354] in India were made in Malayalam.  The period from 1986 to 1990 is regarded as the Golden Age of Malayalam cinema,[355] with four Malayalam films recognised by selection at the Cannes Film Festival—Shaji N. Karun-directed Piravi (1989), Swaham (1994) and Vanaprastham (1999), and Murali Nair-directed Marana Simhasanam (1999). Piravi (1989) won the Caméra d'Or — Mention Spéciale and Marana Simhasanam has won the Caméra d'Or.[citation needed]  The Kerala State Film Awards established by the Government of Kerala recognises the best works in Malayalam cinema every year, along with J. C. Daniel Award for lifetime achievement in Malayalam cinema. K. R. Narayanan National Institute of Visual Science and Arts (KRNNIVSA) is a training and research centre for film and video technology.[356]  Manipuri cinema is a small film industry of Manipur, encompassing Meitei language and other languages of the state. It began in the 1970s and gained momentum following a 2002 state ban on Hindi films. 80–100 movies are made each year. Among the notable Manipuri films are Imagi Ningthem (1982, Aribam Syam Sharma), Ishanou, Yenning Amadi Likla, Phijigee Mani, Leipaklei, Loktak Lairembee, Eikhoishibu Kanano, Eikhoigi Yum and Oneness.  Marathi films are produced in the Marathi language in Maharashtra state. It the oldest of India's film industries, which began in Kolhapur, moved to Pune and is now based in old Mumbai.[239]  Some of the more notable films are Sangte Aika, Ek Gaon Bara Bhangadi, Pinjara, Sinhasan, Pathlaag, Jait Re Jait, Saamana, Santh Wahate Krishnamai, Sant Tukaram and Shyamchi Aai.[citation needed]  Nagpuri films are produced in the Nagpuri language in Jharkhand state. The first Nagpuri feature film was Sona Kar Nagpur (1992).[357][358] With a mainly rural population and cinema halls closing, non-traditional distribution models may be used.[359]  Gorkha cinema consists of films produced by Nepali-speaking Indians.  The Odia-language film industry of Bhubaneswar and Cuttack, Odisha state, is also known as Ollywood.[360] The first Odia-language film was Sita Bibaha (1936).[361] The best year for Odia cinema was 1984 when Maya Miriga (Nirad Mohapatra) and Dhare Alua were showcased in Indian Panorama and Maya Miriga was invited to Critics Week at Cannes. The film received the Best Third World Film award at Mannheim Film Festival, Jury Award in Hawaii and was shown at the London Film Festival.  The Punjabi-language film industry, based in Amritsar and Mohali, Punjab, is also known as Pollywood. K. D. Mehra made the first Punjabi film, Sheela (1935). As of 2009, Punjabi cinema had produced between 900 and 1,000 movies.[362]  The cinema of Rajasthan (Rajjywood) refers to films produced in Rajasthan in north-western India. These films are produced in various regional and tribal languages including Rajasthani varieties such as Mewari, Marwari, Hadoti etc.  The Sindhi-language film industry is largely based in Sindh, Pakistan, and with Sindhi speakers in North Gujarat and Southwestern Rajasthan, India, and elsewhere among the Sindhi diaspora. The first Indian-made Sindhi film was Ekta (1940).[363] while the first Sindhi film produced in Pakistan was Umar Marvi (1956).[364] The industry has produced some Bollywood-style films.  The Sindhi film industry produces movies at intervals. The first was Abana (1958),[timeframe?] which was a success throughout the country. Sindhi cinema then produced some Bollywood-style films such as Hal Ta Bhaji Haloon, Parewari, Dil Dije Dil Waran Khe, Ho Jamalo, Pyar Kare Dis: Feel the Power of Love and The Awakening. Additionally, numerous Sindhi have contributed in Bollywood, including G P Sippy, Ramesh Sippy, Nikhil Advani, Tarun Mansukhani, Ritesh Sidhwani and Asrani.[relevant?]  Director Songe Dorjee Thongdok introduced the first Sherdukpen-language film Crossing Bridges (2014). Sherdukpen is native to the north-eastern state of Arunachal Pradesh.[365][relevant?]  The Tamil-language film industry based in Chennai, also known as Kollywood, once served as a hub for all South Indian film industries.[366] The first South Indian talkie film Kalidas (1931, H. M. Reddy) was shot in Tamil. Sivaji Ganesan became India's first actor to receive an international award when he won Best Actor at the Afro-Asian film festival in 1960 and the title of Chevalier in the Legion of Honour by the French Government in 1995.[118]  Tamil cinema is influenced by Dravidian politics[119] and has a tradition of addressing social issues. Many of Tamil Nadu's prominent Chief Ministers previously worked in cinema: Dravidian stalwarts C N Annadurai and M Karunanidhi were scriptwriters and M G Ramachandran and Jayalalithaa gained a political base through their fan followings.[120]  Tamil films are distributed to Tamil diaspora populations in various parts of Asia, Southern Africa, Northern America, Europe, and Oceania.[367] The industry-inspired Tamil film-making in Sri Lanka, Malaysia, Singapore and Canada.[citation needed]  The Film and Television Institute of Telangana, Film and Television Institute of Andhra Pradesh, Ramanaidu Film School and Annapurna International School of Film and Media are among the largest film schools in India.[370][371] The Telugu states are home to approximately 2800 theatres, more than any single state in India.[372] Being commercially consistent, Telugu cinema had its influence over commercial cinema in India.[373]  The industry holds the Guinness World Record for the largest film production facility in the world, Ramoji Film City.[374] The Prasads IMAX located in Hyderabad is one of the largest 3D IMAX screens, and is the most attended cinema screen in the world.[240][375][376] As per the CBFC report of 2014, the industry is placed first in India, in terms of films produced yearly.[377] In the years 2005, 2006, 2008, and 2014 the industry has produced the largest number of films in India, exceeding the number of films produced in Bollywood.[378][379]  The Tulu-language film industry based in the port city of Mangalore, Karnataka, is also known as Coastalwood. A small industry, its origins trace to the release of Enna Thangadi (1971) with about one release per year until growth was spurred by the commercial success of Oriyardori Asal (2011). Films are released across the Tulu Nadu cultural region, with some recent films having a simultaneous release in Mumbai, Bangalore, and Arabian Gulf countries.[citation needed]  PVR Cinemas, INOX Leisure etc. are some top multiplexes chains in India, which have cinemas across the nation. Book My Show is the leading tickets selling mobile android application in India, it have tie-up with many such multiplexes. Although PVR and INOX also sell tickets through their application- websites. Due to the convince in tickets booking online most of the viewers pre-book tickets through mobile application. Since advancement of internet service in the country online ticket selling business having robust growth here.[380] 2010 decade onward online platform gained popularity in the nation thus Many film-makers many time prefer to release their films online through one of paid app : Netflix, WFCN, Amazon Prime, JioCinema, SonyLIV, ZEE5, Disney+ Hotstar etc. and avoiding theatrical release.[381]  The Dadasaheb Phalke Award, named for \"father of Indian cinema\" Dadasaheb Phalke,[45][46][47][48] is given in recognition of lifetime contribution to cinema. It was established by the government of India in 1969, and is the country's most prestigious film award.[382]  T. Subbarami Reddy[citation needed]  Government-run and private institutes provide formal education in various aspects of filmmaking. Some of the prominent ones include: "},"meta":{},"created_at":"2025-03-22T14:25:42.276677Z","updated_at":"2025-03-22T14:25:42.276677Z","inner_id":39,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":48,"annotations":[{"id":48,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.311403Z","updated_at":"2025-03-22T14:25:42.311403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"0900dc34-d51a-4701-aa16-959d6c2ef393","import_id":null,"last_action":null,"bulk_created":false,"task":48,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Risk management is the identification, evaluation, and prioritization of risks,[1] followed by the minimization, monitoring, and control of the impact or probability of those risks occurring.[2] Risks can come from various sources (i.e, threats) including uncertainty in international markets, political instability, dangers of project failures (at any phase in design, development, production, or sustaining of life-cycles), legal liabilities, credit risk, accidents, natural causes and disasters, deliberate attack from an adversary, or events of uncertain or unpredictable root-cause.[3]  There are two types of events viz. Risks and Opportunities. Negative events can be classified as risks while positive events are classified as opportunities. Risk management standards have been developed by various institutions, including the Project Management Institute, the National Institute of Standards and Technology, actuarial societies, and International Organization for Standardization.[4][5][6] Methods, definitions and goals vary widely according to whether the risk management method is in the context of project management, security, engineering, industrial processes, financial portfolios, actuarial assessments, or public health and safety. Certain risk management standards have been criticized for having no measurable improvement on risk, whereas the confidence in estimates and decisions seems to increase.[2]  Strategies to manage threats (uncertainties with negative consequences) typically include avoiding the threat, reducing the negative effect or probability of the threat, transferring all or part of the threat to another party, and even retaining some or all of the potential or actual consequences of a particular threat. The opposite of these strategies can be used to respond to opportunities (uncertain future states with benefits).[7]  As a professional role, a risk manager[8] will \"oversee the organization's comprehensive insurance and risk management program, assessing and identifying risks that could impede the reputation, safety, security, or financial success of the organization\", and then develop plans to minimize and \/ or mitigate any negative (financial) outcomes. Risk Analysts [9]  support the technical side of the organization's risk management approach: once risk data has been compiled and evaluated, analysts share their findings with their managers, who use those insights to decide among possible solutions.  See also Chief Risk Officer, internal audit, and Financial risk management § Corporate finance.  Risk is defined as the possibility that an event will occur that adversely affects the achievement of an objective. Uncertainty, therefore, is a key aspect of risk.[10] Risk management appears in scientific and management literature since the 1920s.[11] It became a formal science in the 1950s, when articles and books with \"risk management\" in the title also appear in library searches.[12] Most of research was initially related to finance and insurance.[13][14] One popular standard clarifying vocabulary used in risk management is ISO Guide 31073:2022, \"Risk management — Vocabulary\".[4]  Ideally in risk management, a prioritization process is followed.[15] Whereby the risks with the greatest loss (or impact) and the greatest probability of occurring are handled first. Risks with lower probability of occurrence and lower loss are handled in descending order. In practice the process of assessing overall risk can be tricky, and organisation has to balance resources used to mitigate between risks with a higher probability but lower loss, versus a risk with higher loss but lower probability. Opportunity cost represents a unique challenge for risk managers. It can be difficult to determine when to put resources toward risk management and when to use those resources elsewhere. Again, ideal risk management optimises resource usage (spending, manpower etc), and also minimizes the negative effects of risks.  Opportunities first appear in academic research or management books in the 1990s. The first PMBoK Project Management Body of Knowledge draft of 1987 doesn't mention opportunities at all.  Modern project management school recognize the importance of opportunities. Opportunities have been included in project management literature since the 1990s, e.g. in PMBoK, and became a significant part of project risk management in the years 2000s,[16] when articles titled \"opportunity management\" also begin to appear in library searches. Opportunity management thus became an important part of risk management.  Modern risk management theory deals with any type of external events, positive and negative. Positive risks are called opportunities. Similarly to risks, opportunities have specific mitigation strategies: exploit, share, enhance, ignore.  In practice, risks are considered \"usually negative\". Risk-related research and practice focus significantly more on threats than on opportunities. This can lead to negative phenomena such as target fixation.[17]  For the most part, these methods consist of the following elements, performed, more or less, in the following order:  The Risk management knowledge area, as defined by the Project Management Body of Knowledge PMBoK, consists of the following processes:  The International Organization for Standardization (ISO) identifies the following principles for risk management:[5]  Benoit Mandelbrot distinguished between \"mild\" and \"wild\" risk and argued that risk assessment and management must be fundamentally different for the two types of risk.[19] Mild risk follows normal or near-normal probability distributions, is subject to regression to the mean and the law of large numbers, and is therefore relatively predictable. Wild risk follows fat-tailed distributions, e.g., Pareto or power-law distributions, is subject to regression to the tail (infinite mean or variance, rendering the law of large numbers invalid or ineffective), and is therefore difficult or impossible to predict. A common error in risk assessment and management is to underestimate the wildness of risk, assuming risk to be mild when in fact it is wild, which must be avoided if risk assessment and management are to be valid and reliable, according to Mandelbrot.  According to the standard ISO 31000, \"Risk management – Guidelines\", the process of risk management consists of several steps as follows:[5]  This involves:  After establishing the context, the next step in the process of managing risk is to identify potential risks. Risks are about events that, when triggered, cause problems or benefits. Hence, risk identification can start with the source of problems and those of competitors (benefit), or with the problem's consequences.  Some examples of risk sources are: stakeholders of a project, employees of a company or the weather over an airport.  When either source or problem is known, the events that a source may trigger or the events that can lead to a problem can be investigated. For example: stakeholders withdrawing during a project may endanger funding of the project; confidential information may be stolen by employees even within a closed network; lightning striking an aircraft during takeoff may make all people on board immediate casualties.  The chosen method of identifying risks may depend on culture, industry practice and compliance. The identification methods are formed by templates or the development of templates for identifying source, problem or event. Common risk identification methods are:  Once risks have been identified, they must then be assessed as to their potential severity of impact (generally a negative impact, such as damage or loss) and to the probability of occurrence.  [25] These quantities can be either simple to measure, in the case of the value of a lost building, or impossible to know for sure in the case of an unlikely event, the probability of occurrence of which is unknown. Therefore, in the assessment process it is critical to make the best educated decisions in order to properly prioritize the implementation of the risk management plan.  Even a short-term positive improvement can have long-term negative impacts.  Take the \"turnpike\" example. A highway is widened to allow more traffic. More traffic capacity leads to greater development in the areas surrounding the improved traffic capacity. Over time, traffic thereby increases to fill available capacity. Turnpikes thereby need to be expanded in a seemingly endless cycles. There are many other engineering examples where expanded capacity (to do any function) is soon filled by increased demand. Since expansion comes at a cost, the resulting growth could become unsustainable without forecasting and management.  The fundamental difficulty in risk assessment is determining the rate of occurrence since statistical information is not available on all kinds of past incidents and is particularly scanty in the case of catastrophic events, simply because of their infrequency. Furthermore, evaluating the severity of the consequences (impact) is often quite difficult for intangible assets. Asset valuation is another question that needs to be addressed. Thus, best educated opinions and available statistics are the primary sources of information. Nevertheless, risk assessment should produce such information for senior executives of the organization that the primary risks are easy to understand and that the risk management decisions may be prioritized within overall company goals. Thus, there have been several theories and attempts to quantify risks. Numerous different risk formulae exist, but perhaps the most widely accepted formula for risk quantification is: \"Rate (or probability) of occurrence multiplied by the impact of the event equals risk magnitude.\"[vague]  Risk mitigation measures are usually formulated according to one or more of the following major risk options, which are:  Later research[26] has shown that the financial benefits of risk management are less dependent on the formula used but are more dependent on the frequency and how risk assessment is performed.  In business it is imperative to be able to present the findings of risk assessments in financial, market, or schedule terms. Robert Courtney Jr. (IBM, 1970) proposed a formula for presenting risks in financial terms. The Courtney formula was accepted as the official risk analysis method for the US governmental agencies. The formula proposes calculation of ALE (annualized loss expectancy) and compares the expected loss value to the security control implementation costs (cost–benefit analysis).  Planning for risk management uses four essential techniques. Under the acceptance technique, the business intentionally assumes risks without financial protections in the hopes that possible gains will exceed prospective losses. The transfer approach shields the business from losses by shifting risks to a third party, frequently in exchange for a fee, while the third-party benefits from the project. By choosing not to participate in high-risk ventures, the avoidance strategy avoids losses but also loses out on possibilities. Last but not least, the reduction approach lowers risks by implementing strategies like insurance, which provides protection for a variety of asset classes and guarantees reimbursement in the event of losses.[27]  Once risks have been identified and assessed, all techniques to manage the risk fall into one or more of these four major categories:[28]  Ideal use of these risk control strategies may not be possible.  Some of them may involve trade-offs that are not acceptable to the organization or person making the risk management decisions. Another source, from the US Department of Defense (see link), Defense Acquisition University, calls these categories ACAT, for Avoid, Control, Accept, or Transfer.  This use of the ACAT acronym is reminiscent of another ACAT (for Acquisition Category) used in US Defense industry procurements, in which Risk Management figures prominently in decision making and planning.  Similarly to risks, opportunities have specific mitigation strategies: exploit, share, enhance, ignore.  This includes not performing an activity that could present risk. Refusing to purchase a property or business to avoid legal liability is one such example. Avoiding airplane flights for fear of hijacking.  Avoidance may seem like the answer to all risks, but avoiding risks also means losing out on the potential gain that accepting (retaining) the risk may have allowed. Not entering a business to avoid the risk of loss also avoids the possibility of earning profits. Increasing risk regulation in hospitals has led to avoidance of treating higher risk conditions, in favor of patients presenting with lower risk.[29]  Risk reduction or \"optimization\" involves reducing the severity of the loss or the likelihood of the loss from occurring. For example, sprinklers are designed to put out a fire to reduce the risk of loss by fire.  This method may cause a greater loss by water damage and therefore may not be suitable. Halon fire suppression systems may mitigate that risk, but the cost may be prohibitive as a strategy.  Acknowledging that risks can be positive or negative, optimizing risks means finding a balance between negative risk and the benefit of the operation or activity; and between risk reduction and effort applied. By effectively applying Health, Safety and Environment (HSE) management standards, organizations can achieve tolerable levels of residual risk.[30]  Modern software development methodologies reduce risk by developing and delivering software incrementally. Early methodologies suffered from the fact that they only delivered software in the final phase of development; any problems encountered in earlier phases meant costly rework and often jeopardized the whole project. By developing in iterations, software projects can limit effort wasted to a single iteration.  Outsourcing could be an example of risk sharing strategy if the outsourcer can demonstrate higher capability at managing or reducing risks.[31] For example, a company may outsource only its software development, the manufacturing of hard goods, or customer support needs to another company, while handling the business management itself. This way, the company can concentrate more on business development without having to worry as much about the manufacturing process, managing the development team, or finding a physical location for a center. Also, implanting controls can also be an option in reducing risk. Controls that either detect causes of unwanted events prior to the consequences occurring during use of the product, or detection of the root causes of unwanted failures that the team can then avoid. Controls may focus on management or decision-making processes. All these may help to make better decisions concerning risk.[32]  Briefly defined as \"sharing with another party the burden of loss or the benefit of gain, from a risk, and the measures to reduce a risk.\"  The term 'risk transfer' is often used in place of risk-sharing in the mistaken belief that you can transfer a risk to a third party through insurance or outsourcing. In practice, if the insurance company or contractor go bankrupt or end up in court, the original risk is likely to still revert to the first party. As such, in the terminology of practitioners and scholars alike, the purchase of an insurance contract is often described as a \"transfer of risk.\" However, technically speaking, the buyer of the contract generally retains legal responsibility for the losses \"transferred\", meaning that insurance may be described more accurately as a post-event compensatory mechanism. For example, a personal injuries insurance policy does not transfer the risk of a car accident to the insurance company. The risk still lies with the policyholder namely the person who has been in the accident. The insurance policy simply provides that if an accident (the event) occurs involving the policyholder then some compensation may be payable to the policyholder that is commensurate with the suffering\/damage.  Methods of managing risk fall into multiple categories. Risk-retention pools are technically retaining the risk for the group, but spreading it over the whole group involves transfer among individual members of the group. This is different from traditional insurance, in that no premium is exchanged between members of the group upfront, but instead, losses are assessed to all members of the group.  Risk retention involves accepting the loss, or benefit of gain, from a risk when the incident occurs.  True self-insurance falls in this category. Risk retention is a viable strategy for small risks where the cost of insuring against the risk would be greater over time than the total losses sustained. All risks that are not avoided or transferred are retained by default. This includes risks that are so large or catastrophic that either they cannot be insured against or the premiums would be infeasible. War is an example since most property and risks are not insured against war, so the loss attributed to war is retained by the insured. Also any amounts of potential loss (risk) over the amount insured is retained risk.  This may also be acceptable if the chance of a very large loss is small or if the cost to insure for greater coverage amounts is so great that it would hinder the goals of the organization too much.  Select appropriate controls or countermeasures to mitigate each risk. Risk mitigation needs to be approved by the appropriate level of management. For instance, a risk concerning the image of the organization should have top management decision behind it whereas IT management would have the authority to decide on computer virus risks.  The risk management plan should propose applicable and effective security controls for managing the risks. For example, an observed high risk of computer viruses could be mitigated by acquiring and implementing antivirus software. A good risk management plan should contain a schedule for control implementation and responsible persons for those actions. There are four basic steps of risk management plan, which are threat assessment, vulnerability assessment, impact assessment and risk mitigation strategy development.[33]  According to ISO\/IEC 27001, the stage immediately after completion of the risk assessment phase consists of preparing a Risk Treatment Plan, which should document the decisions about how each of the identified risks should be handled. Mitigation of risks often means selection of security controls, which should be documented in a Statement of Applicability, which identifies which particular control objectives and controls from the  standard have been selected, and why.  Implementation follows all of the planned methods for mitigating the effect of the risks. Purchase insurance policies for the risks that it has been decided to transferred to an insurer, avoid all risks that can be avoided without sacrificing the entity's goals, reduce others, and retain the rest.  Initial risk management plans will never be perfect. Practice, experience, and actual loss results will necessitate changes in the plan and contribute information to allow possible different decisions to be made in dealing with the risks being faced.  Risk analysis results and management plans should be updated periodically. There are two primary reasons for this:   Enterprise risk management (ERM) defines risk as those possible events or circumstances that can have negative influences on the enterprise in question,   where the impact can be on the very existence, the resources (human and capital), the products and services, or the customers of the enterprise, as well as external impacts on society, markets, or the environment.  There are various defined frameworks here, where every probable risk can have a pre-formulated plan to deal with its possible consequences (to ensure contingency if the risk becomes a liability). Managers thus analyze and monitor both the internal and external environment facing the enterprise, addressing business risk generally, and any impact on the enterprise achieving its strategic goals. ERM thus overlaps various other disciplines - operational risk management, financial risk management etc. - but is differentiated by its strategic and long-term focus.[34] ERM systems usually focus on safeguarding reputation, acknowledging its significant role in comprehensive risk management strategies.[35]  As applied to finance, risk management concerns the techniques and practices for measuring, monitoring and controlling the  market- and credit risk (and operational risk) on a firm's balance sheet, due to a bank's credit and trading exposure, or re a fund manager's portfolio value; for an overview see Finance § Risk management.  The concept of \"contractual risk management\" emphasises the use of risk management techniques in contract deployment, i.e. managing the risks which are accepted through entry into a contract. Norwegian academic Petri Keskitalo defines \"contractual risk management\" as \"a practical, proactive and systematical contracting method that uses contract planning and governance to manage risks connected to business activities\".[36] In an article by Samuel Greengard published in 2010, two US legal cases are mentioned which emphasise the importance of having a strategy for dealing with risk:[37]  Greengard recommends using industry-standard contract language as much as possible to reduce risk as much as possible and rely on clauses which have been in use and subject to established court interpretation over a number of years.[37]  Customs risk management is concerned with the risks which arise within the context of international trade and have a bearing on safety and security, including the risk that illicit drugs and counterfeit goods can pass across borders and the risk that shipments and their contents are incorrectly declared.[40] The European Union has adopted a Customs Risk Management Framework (CRMF) applicable across the union and throughout its member states, whose aims include establishing a common level of customs control protection and a balance between the objectives of safe customs control and the facilitation of legitimate trade.[41] Two events which prompted the European Commission to review customs risk management policy in 2012-13 were the September 11 attacks of 2001 and the 2010 transatlantic aircraft bomb plot involving packages being sent from Yemen to the United States, referred to by the Commission as \"the October 2010 (Yemen) incident\".[42]  ESRM is a security program management approach that links security activities to an enterprise's mission and business goals through risk management methods. The security leader's role in ESRM is to manage risks of harm to enterprise assets in partnership with the business leaders whose assets are exposed to those risks. ESRM involves educating business leaders on the realistic impacts of identified risks, presenting potential strategies to mitigate those impacts, then enacting the option chosen by the business in line with accepted levels of business risk tolerance[43]  For medical devices, risk management is a process for identifying, evaluating and mitigating risks associated with harm to people and damage to property or the environment. Risk management is an integral part of medical device design and development, production processes and evaluation of field experience, and is applicable to all types of medical devices. The evidence of its application is required by most regulatory bodies such as the US FDA. The management of risks for medical devices is described by the International Organization for Standardization (ISO) in ISO 14971:2019, Medical Devices—The application of risk management to medical devices, a product safety standard. The standard provides a process framework and associated requirements for management responsibilities, risk analysis and evaluation, risk controls and lifecycle risk management. Guidance on the application of the standard is available via ISO\/TR 24971:2020.  The European version of the risk management standard was updated in 2009 and again in 2012 to refer to the Medical Devices Directive (MDD) and Active Implantable Medical Device Directive (AIMDD) revision in 2007, as well as the In Vitro Medical Device Directive (IVDD). The requirements of EN 14971:2012 are nearly identical to ISO 14971:2007. The differences include three \"(informative)\" Z Annexes that refer to the new MDD, AIMDD, and IVDD. These annexes indicate content deviations that include the requirement for risks to be reduced as far as possible, and the requirement that risks be mitigated by design and not by labeling on the medical device (i.e., labeling can no longer be used to mitigate risk).  Typical risk analysis and evaluation techniques adopted by the medical device industry include hazard analysis, fault tree analysis (FTA), failure mode and effects analysis (FMEA), hazard and operability study (HAZOP), and risk traceability analysis for ensuring risk controls are implemented and effective (i.e. tracking risks identified to product requirements, design specifications, verification and validation results etc.). FTA analysis requires diagramming software. FMEA analysis can be done using a spreadsheet program. There are also integrated medical device risk management solutions.  Through a draft guidance, the FDA has introduced another method named \"Safety Assurance Case\" for medical device safety assurance analysis. The safety assurance case is structured argument reasoning about systems appropriate for scientists and engineers, supported by a body of evidence, that provides a compelling, comprehensible and valid case that a system is safe for a given application in a given environment. With the guidance, a safety assurance case is expected for safety critical devices (e.g. infusion devices) as part of the pre-market clearance submission, e.g. 510(k). In 2013, the FDA introduced another draft guidance expecting medical device manufacturers to submit cybersecurity risk analysis information.  Project risk management must be considered at the different phases of acquisition. At the beginning of a project, the advancement of technical developments, or threats presented by a competitor's projects, may cause a risk or threat assessment and subsequent evaluation of alternatives (see Analysis of Alternatives). Once a decision is made, and the project begun, more familiar project management applications can be used:[44][45][46]  Megaprojects (sometimes also called \"major programs\") are large-scale investment projects, typically costing more than $1 billion per project. Megaprojects include major bridges, tunnels, highways, railways, airports, seaports, power plants, dams, wastewater projects, coastal flood protection schemes, oil and natural gas extraction projects, public buildings, information technology systems, aerospace projects, and defense systems. Megaprojects have been shown to be particularly risky in terms of finance, safety, and social and environmental impacts. Risk management is therefore particularly pertinent for megaprojects and special methods and special education have been developed for such risk management.[47]  It is important to assess risk in regard to natural disasters like floods, earthquakes, and so on. Outcomes of natural disaster risk assessment are valuable when considering future repair costs, business interruption losses and other downtime, effects on the environment, insurance costs, and the proposed costs of reducing the risk.[48][49] The Sendai Framework for Disaster Risk Reduction is a 2015 international accord that has set goals and targets for disaster risk reduction in response to natural disasters.[50] There are regular International Disaster and Risk Conferences in Davos to deal with integral risk management.  Several tools can be used to assess risk and risk management of natural disasters and other climate events, including geospatial modeling, a key component of land change science. This modeling requires an understanding of geographic distributions of people as well as an ability to calculate the likelihood of a natural disaster occurring.  The management of risks to persons and property in wilderness and remote natural areas has developed with increases in outdoor recreation participation and decreased social tolerance for loss.  Organizations providing commercial wilderness experiences can now align with national and international consensus standards for training and equipment such as ANSI\/NASBLA 101-2017 (boating),[51] UIAA 152 (ice climbing tools),[52] and European Norm 13089:2015 + A1:2015 (mountaineering equipment).[53][54] The Association for Experiential Education offers accreditation for wilderness adventure programs.[55] The Wilderness Risk Management Conference provides access to best practices, and specialist organizations provide wilderness risk management consulting and training.[56]  The text Outdoor Safety – Risk Management for Outdoor Leaders,[57] published by the New Zealand Mountain Safety Council, provides a view of wilderness risk management from the New Zealand perspective, recognizing the value of national outdoor safety legislation and devoting considerable attention to the roles of judgment and decision-making processes in wilderness risk management.  One popular models for risk assessment is the Risk Assessment and Safety Management (RASM) Model developed by Rick Curtis, author of The Backpacker's Field Manual.[58] The formula for the RASM Model is: Risk = Probability of Accident × Severity of Consequences. The RASM Model weighs negative risk—the potential for loss, against positive risk—the potential for growth.  IT risk is a risk related to information technology. This is a relatively new term due to an increasing awareness that information security is simply one facet of a multitude of risks that are relevant to IT and the real world processes it supports. \"Cybersecurity is tied closely to the advancement of technology. It lags only long enough for incentives like black markets to evolve and new exploits to be discovered. There is no end in sight for the advancement of technology, so we can expect the same from cybersecurity.\"[59]  ISACA's Risk IT framework ties IT risk to enterprise risk management. Duty of Care Risk Analysis (DoCRA) evaluates risks and their safeguards and considers the interests of all parties potentially affected by those risks.[60] The Verizon Data Breach Investigations Report (DBIR) features how organizations can leverage the Veris Community Database (VCDB) to estimate risk. Using HALOCK methodology within CIS RAM and data from VCDB, professionals can determine threat likelihood for their industries.  IT risk management includes \"incident handling\", an action plan for dealing with intrusions, cyber-theft, denial of service, fire, floods, and other security-related events. According to the SANS Institute, it is a six step process: Preparation, Identification, Containment, Eradication, Recovery, and Lessons Learned.[61]  Operational risk management (ORM) is the oversight of operational risk, including the risk of loss resulting from: inadequate or failed internal processes and systems; human factors; or external events. Given the nature of operations, ORM is typically a \"continual\" process, and will include ongoing risk assessment, risk decision making, and the implementation of risk controls.  For the offshore oil and gas industry, operational risk management is regulated by the safety case regime in many countries. Hazard identification and risk assessment tools and techniques are described in the international standard ISO 17776:2000, and organisations such as the IADC (International Association of Drilling Contractors) publish guidelines for Health, Safety and Environment (HSE) Case development which are based on the ISO standard. Further, diagrammatic representations of hazardous events are often expected by governmental regulators as part of risk management in safety case submissions; these are known as bow-tie diagrams (see Network theory in risk assessment). The technique is also used by organisations and regulators in mining, aviation, health, defence, industrial and finance.  The principles and tools for quality risk management are increasingly being applied to different aspects of pharmaceutical quality systems. These aspects include development, manufacturing, distribution, inspection, and submission\/review processes throughout the lifecycle of drug substances, drug products, biological and biotechnological products (including the use of raw materials, solvents, excipients, packaging and labeling materials in drug products, biological and biotechnological products). Risk management is also applied to the assessment of microbiological contamination in relation to pharmaceutical products and cleanroom manufacturing environments.[62]  Supply chain risk management (SCRM) aims at maintaining supply chain continuity in the event of scenarios or incidents which could interrupt normal business and hence profitability. Risks to the supply chain range from everyday to exceptional, including unpredictable natural events (such as tsunamis and pandemics) to counterfeit products, and reach across quality, security, to resiliency and product integrity. Mitigation of these risks can involve various elements of the business including logistics and cybersecurity, as well as the areas of finance and operations.  Travel risk management is concerned with how organisations assess the risks to their staff when travelling, especially when travelling overseas. In the field of international standards, ISO 31030:2021 addresses good practice in travel risk management.[63]  The Global Business Travel Association's education and research arm, the GBTA Foundation. found in 2015 that most businesses covered by their research employed travel risk management protocols aimed at ensuring the safety and well-being of their business travelers.[64] Six key principles of travel risk awareness put forward by the association are preparation, awareness of surroundings and people, keeping a low profile, adopting an unpredictable routine, communications and layers of protection.[65] Traveler tracking using mobile tracking and messaging technologies had by 2015 become a widely used aspect of travel risk management.[64]  Risk communication is a complex cross-disciplinary academic field that is part of risk management and related to fields like crisis communication. The goal is to make sure that targeted audiences understand how risks affect them or their communities by appealing to their values.[66][67]  Risk communication is particularly important in disaster preparedness,[68] public health,[69] and preparation for major global catastrophic risk.[68] For example, the impacts of climate change and climate risk effect every part of society, so communicating that risk is an important climate communication practice, in order for societies to plan for climate adaptation.[70] Similarly, in pandemic prevention, understanding of risk helps communities stop the spread of disease and improve responses.[71]  Risk communication deals with possible risks and aims to raise awareness of those risks to encourage or persuade changes in behavior to relieve threats in the long term. On the other hand, crisis communication is aimed at raising awareness of a specific type of threat, the magnitude, outcomes, and specific behaviors to adopt to reduce the threat.[72]  Risk communication in food safety is part of the risk analysis framework. Together with risk assessment and risk management, risk communication aims to reduce foodborne illnesses. Food safety risk communication is an obligatory activity for food safety authorities[73] in countries, which adopted the Agreement on the Application of Sanitary and Phytosanitary Measures. "},"meta":{},"created_at":"2025-03-22T14:25:42.276677Z","updated_at":"2025-03-22T14:25:42.276677Z","inner_id":40,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":49,"annotations":[{"id":49,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.312403Z","updated_at":"2025-03-22T14:25:42.312403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"2b6d02e2-99d1-4df2-8bc8-2a6ddc0be1ef","import_id":null,"last_action":null,"bulk_created":false,"task":49,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A film director or filmmaker is a person who controls a film's artistic and dramatic aspects and visualizes the screenplay (or script) while guiding the film crew and actors in the fulfillment of that vision. The director has a key role in choosing the cast members, production design and all the creative aspects of filmmaking in cooperation with the producer.[1]  The film director gives direction to the cast and crew and creates an overall vision through which a film eventually becomes realized or noticed. Directors need to be able to mediate differences in creative visions and stay within the budget.  There are many pathways to becoming a film director. Some film directors started as screenwriters, cinematographers, producers, film editors or actors. Other film directors have attended film school. Directors use different approaches. Some outline a general plotline and let the actors improvise dialogue, while others control every aspect and demand that the actors and crew follow instructions precisely. Some directors also write their own screenplays or collaborate on screenplays with long-standing writing partners. Other directors edit or appear in their films or compose music score for their films.[2]  A film director's task is to envisage a way to translate a screenplay into a fully formed film, and then to realize this vision.[3] To do this, they oversee the artistic and technical elements of film production.[2][4] This entails organizing the film crew in such a way to achieve their vision of the film and communicating with the actors.[5][6] This requires skills of group leadership, as well as the ability to maintain a singular focus even in the stressful, fast-paced environment of a film set.[7] Moreover, it is necessary to have an artistic eye to frame shots and to give precise feedback to cast and crew,[8] thus, excellent communication skills are a must.[9]  Because the film director depends on the successful cooperation of many different creative individuals with possibly strongly contradicting artistic ideals and visions, they also need to possess conflict-resolution skills to mediate whenever necessary.[10] Thus the director ensures that all individuals involved in the film production are working towards an identical vision for the completed film.[5] The set of varying challenges they have to tackle has been described as \"a multi-dimensional jigsaw puzzle with egos and weather thrown in for good measure\".[11] It adds to the pressure that the success of a film can influence when and how they will work again, if at all.[12]  Generally, the sole superiors of the director are the producers and the studio that is financing the film, although sometimes the director can also be a producer of the same film.[3][13] The role of a director differs from producers in that producers typically manage the logistics and business operations of the production, whereas the director is tasked with making creative decisions. The director must work within the restrictions of the film's budget[14] and the demands of the producer and studio (such as the need to get a particular age rating).[15]  Directors also play an important role in post-production. While the film is still in production, the director sends \"dailies\" to the film editor and explains their overall vision for the film, allowing the editor to assemble an editor's cut. In post-production, the director works with the editor to edit the material into the director's cut. Well-established directors have the \"final cut privilege\", meaning that they have the final say on which edit of the film is released. For other directors, the studio can order further edits without the director's permission.  The director is one of the few positions that requires intimate involvement during every stage of film production. Thus, the position of film director is widely considered to be a highly stressful and demanding one.[16] It has been said that \"20-hour days are not unusual\".[3] Some directors also take on additional roles, such as producing, writing or editing.  Under European Union law, the film director is considered the \"author\" or one of the authors of a film, largely as a result of the influence of auteur theory.[17] Auteur theory is a film criticism concept that holds that a film director's film reflects the director's personal creative vision, as if they were the primary \"auteur\" (the French word for \"author\").[18] In spite of—and sometimes even because of—the production of the film as part of an industrial process, the auteur's creative voice is distinct enough to shine through studio interference and the collective process.[citation needed]  Some film directors started as screenwriters, film editors, producers, actors, or film critics, as well as directing for similar media like television and commercials.[19][20] Several American cinematographers have become directors, including Barry Sonnenfeld, originally the Coen brothers' Director of Photography; Wally Pfister, cinematographer on Christopher Nolan's three Batman films made his directorial debut with Transcendence (2014). Despite the misnomer, assistant director has become a completely separate career path and is not typically a position for aspiring directors, but there are exceptions in some countries such as India where assistant directors are indeed directors-in-training.[21][22]  Many film directors have attended a film school to get a bachelor's degree studying film or cinema.[23] Film students generally study the basic skills used in making a film.[24] This includes, for example, preparation, shot lists and storyboards, blocking, communicating with professional actors, communicating with the crew, and reading scripts.[25] Some film schools are equipped with sound stages and post-production facilities.[26] Besides basic technical and logistical skills, students also receive education on the nature of professional relationships that occur during film production.[27] A full degree course can be designed for up to five years of studying.[28] Future directors usually complete short films during their enrollment.[16] The National Film School of Denmark has the student's final projects presented on national TV.[29] Some film schools retain the rights for their students' works.[30] Many directors successfully prepared for making feature films by working in television.[31] The German Film and Television Academy Berlin consequently cooperates with the Berlin\/Brandenburg TV station RBB (Berlin-Brandenburg Broadcasting) and ARTE.[32]  In recent decades American directors have primarily been coming out of USC, UCLA, AFI, Columbia University, and NYU, each of which is known for cultivating a certain style of filmmaking.[20] Notable film schools outside of the United States include Beijing Film Academy, Centro de Capacitación Cinematográfica in Mexico City, Dongseo University in South Korea, FAMU in Prague, Film and Television Institute of India, HFF Munich, La Fémis in Paris, Tel Aviv University, and Vancouver Film School.[33]  Film directors usually are self-employed and hired per project based on recommendations and industry reputation.[34] Compensation might be arranged as a flat fee for the project, as a weekly salary, or as a daily rate.  A handful of top Hollywood directors made from $133.3 million to $257.95 million in 2011, such as James Cameron and Steven Spielberg,[35] but the average United States film directors and producers made $89,840 in 2018.[36] A new Hollywood director typically gets paid around $400,000 for directing their first studio film.[37]  The average annual salary in England is £50,440,[38] in Canada is $62,408,[39] and in Western Australia it can range from $75,230 to $97,119.[40] In France, the average salary is €4000 per month, paid per project.[41] Luc Besson was the highest paid French director in 2017, making €4.44 million for Valerian and the City of a Thousand Planets. That same year, the top ten French directors' salaries in total represented 42% of the total directors' salaries in France.[42]  Film directors in Japan average a yearly salary from ¥4 million to ¥10 million,[43] and the Directors Guild of Japan requires a minimum payment of ¥3.5 million.[44] Korean directors make 300 million to 500 million won for a film, and beginning directors start out making around 50 million won. A Korean director who breaks into the Chinese market might make 1 billion won for a single film.[45]  According to a 2018 report from UNESCO, the film industry throughout the world has a disproportionately higher number of male directors compared to female directors, and they provide as an example the fact that only 20% of films in Europe are directed by women.[46] 44% of graduates from a sample of European films schools are women, and yet women are only 24% of working film directors in Europe. However only a fraction of film school graduates aspire to direct with the majority entering the industry in other roles.[47] In Hollywood, women make up only 12.6 percent of film directors, as reported by a UCLA study of the 200 top theatrical films of 2017,[48] but that number is a significant increase from 6.9% in 2016.[49] As of 2014, there were only 20 women in the Directors Guild of Japan out of the 550 total members.[50] Indian film directors are also greatly underrepresented by women, even compared to other countries, but there has been a recent trend of more attention to women directors in India, brought on partly by Amazon and Netflix moving into the industry.[51] Of the movies produced in Nollywood, women direct only 2%.[52]  There are many different awards for film directing, run by various academies, critics associations, film festivals, and guilds.[53] The Academy Award for Best Director and Cannes Film Festival Award for Best Director are considered among the most prestigious awards for directing,[54][55][56][57] and there is even an award for worst directing given out during the Golden Raspberry Awards.   "},"meta":{},"created_at":"2025-03-22T14:25:42.276677Z","updated_at":"2025-03-22T14:25:42.276677Z","inner_id":41,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":50,"annotations":[{"id":50,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.312403Z","updated_at":"2025-03-22T14:25:42.312403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"c68ec6dd-cf8e-4d2f-a2d0-e7723d006b84","import_id":null,"last_action":null,"bulk_created":false,"task":50,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Heterodox  A central bank, reserve bank, national bank, or monetary authority is an institution that manages the monetary policy of a country or monetary union.[1] In contrast to a commercial bank, a central bank possesses a monopoly on increasing the monetary base. Many central banks also have supervisory or regulatory powers to ensure the stability of commercial banks in their jurisdiction, to prevent bank runs, and, in some cases, to enforce policies on financial consumer protection, and against bank fraud, money laundering, or terrorism financing. Central banks play a crucial role in macroeconomic forecasting, which is essential for guiding monetary policy decisions, especially during times of economic turbulence.[2]  Central banks in most developed nations are usually set up to be institutionally independent from political interference,[3][4][5] even though governments typically have governance rights over them, legislative bodies exercise scrutiny, and central banks frequently do show responsiveness to politics.[6][7][8]  Issues like central bank independence, central bank policies, and rhetoric in central bank governors' discourse or the premises of macroeconomic policies[9] (monetary and fiscal policy) of the state, are a focus of contention and criticism by some policymakers,[10] researchers,[11] and specialized business, economics, and finance media.[12][13]  The notion of central banks as a separate category from other banks has emerged gradually, and only fully coalesced in the 20th century. In the aftermath of World War I, leading central bankers of the United Kingdom and the United States respectively, Montagu Norman and Benjamin Strong, agreed on a definition of central banks that was both positive and normative.[14]: 4-5  Since that time, central banks have been generally distinguishable from other financial institutions, except under Communism in so-called single-tier banking systems such as Hungary's between 1950 and 1987, where the Hungarian National Bank operated alongside three other major state-owned banks.[15] For earlier periods, what institutions do or do not count as central banks is often not univocal.  Correlatively, different scholars have held different views about the timeline of emergence of the first central banks. A widely held view in the second half of the 20th century has been that Stockholms Banco (est. 1657), as the original issuer of banknotes, counted as the oldest central bank, and that consequently its successor the Sveriges Riksbank was the oldest central bank in continuous operation, with the Bank of England as second-oldest and direct or indirect model for all subsequent central banks.[16] That view has persisted in some early-21st-century publications.[17] In more recent scholarship, however, the issuance of banknotes has often been viewed as just one of several techniques to provide central bank money, defined as financial money (in contrast to commodity money) of the highest quality. Under that definition, municipal banks of the late medieval and early modern periods, such as the Taula de canvi de Barcelona (est. 1401) or Bank of Amsterdam (est. 1609), issued central bank money and count as early central banks.[18]  There is no universal terminology for the name of a central bank. Early central banks were often the only or principal formal financial institution in their jurisdiction, and were consequently often named \"bank of\" the relevant city's or country's name, e.g. the Bank of Amsterdam, Bank of Hamburg, Bank of England, or Wiener Stadtbank. Naming practices subsequently evolved as more central banks were established. The expression \"central bank\" itself only appeared in the early 19th century, but at that time it referred to the head office of a multi-branched bank, and was still used in that sense by Walter Bagehot in his seminal 1873 essay Lombard Street.[19]: 9  During that era, what is now known as a central bank was often referred to as a bank of issue (French: institut d'émission, German: Notenbank). The reference to central banking in the current sense only became widespread in the early 20th century.  Names of individual central banks include, with references to the date when the bank acquired its current name:   In some cases, the local-language name is used in English-language practice, e.g. Sveriges Riksbank (est. 1668, current name in use since 1866), De Nederlandsche Bank (est. 1814), Deutsche Bundesbank (est. 1957), or Bangko Sentral ng Pilipinas (est. 1993).  Some commercial banks have names suggestive of central banks, even if they are not: examples are the State Bank of India and Central Bank of India, National Bank of Greece, Banco do Brasil, National Bank of Pakistan, Bank of China, Bank of Cyprus, or Bank of Ireland, as well as Deutsche Bank. Some but not all of these institutions had assumed central banking roles in the past.  The leading executive of a central bank is usually known as the Governor, President, or Chair.  The widespread adoption of central banking is a rather recent phenomenon. At the start of the 20th century, approximately two-thirds of sovereign states did not have a central bank. Waves of central bank adoption occurred in the interwar period and in the aftermath of World War II.[20]  In the 20th century, central banks were often created with the intent to attract foreign capital, as bankers preferred to lend to countries with a central bank on the gold standard.[20]  The use of money as a unit of account predates history. Government control of money is documented in the ancient Egyptian economy (2750–2150 BCE).[21] The Egyptians measured the value of goods with a central unit called shat. Like many other currencies, the shat was linked to gold. The value of a shat in terms of goods was defined by government administrations. Other cultures in Asia Minor later materialized their currencies in the form of gold and silver coins.[22]  The mere issuance of paper currency or other types of financial money by a government is not the same as central banking. The difference is that government-issued financial money, as present e.g. in China during the Yuan dynasty in the form of paper currency, is typically not freely convertible and thus of inferior quality, occasionally leading to hyperinflation.  From the 12th century, a network of professional banks emerged primarily in Southern Europe (including Southern France, with the Cahorsins).[23] Banks could use book money to create deposits for their customers. Thus, they had the possibility to issue, lend and transfer money autonomously without direct control from political authorities.  The Taula de canvi de Barcelona, established in 1401, is the first example of municipal, mostly public banks which pioneered central banking on a limited scale. It was soon emulated by the Bank of Saint George in the Republic of Genoa, first established in 1407, and significantly later by the Banco del Giro in the Republic of Venice and by a network of institutions in Naples that later consolidated into Banco di Napoli. Notable municipal central banks were established in the early 17th century in leading northwestern European commercial centers, namely the Bank of Amsterdam in 1609[24] and the Hamburger Bank in 1619.[25] These institutions offered a public infrastructure for cashless international payments.[26] They aimed to increase the efficiency of international trade and to safeguard monetary stability. These municipal public banks thus fulfilled comparable functions to modern central banks.[27]  The Swedish central bank, known since 1866 as Sveriges Riksbank, was founded in Stockholm in 1664 from the remains of the failed Stockholms Banco and answered to the Riksdag of the Estates, Sweden's early modern parliament.[28] One role of the Swedish central bank was lending money to the government.[29]  The establishment of the Bank of England was devised by Charles Montagu, 1st Earl of Halifax, following a 1691 proposal by William Paterson.[30] A royal charter was granted on 27 July 1694 through the passage of the Tonnage Act.[31] The bank was given exclusive possession of the government's balances, and was the only limited-liability corporation allowed to issue banknotes.[32][page needed] The early modern Bank of England, however, did not have all the functions of a today's central banks, e.g. to regulate the value of the national currency, to finance the government, to be the sole authorized distributor of banknotes, or to function as a lender of last resort to banks suffering a liquidity crisis.  In the early 18th century, a major experiment in national central banking failed in France with John Law's Banque Royale in 1720–1721. Later in the century, France had other attempts with the Caisse d'Escompte first created in 1767, and King Charles III established the Bank of Spain in 1782. The Russian Assignation Bank, established in 1769 by Catherine the Great, was an outlier from the general pattern of early national central banks in that it was directly owned by the Imperial Russian government, rather than private individual shareholders. In the nascent United States, Alexander Hamilton, as Secretary of the Treasury in the 1790s, set up the First Bank of the United States despite heavy opposition from Jeffersonian Republicans.[33]  Central banks were established in many European countries during the 19th century.[34][35] Napoleon created the Banque de France in 1800, in order to stabilize and develop the French economy and to improve the financing of his wars.[36] The Bank of France remained the most important Continental European central bank throughout the 19th century.[37] The Bank of Finland was founded in 1812, soon after Finland had been taken over from Sweden by Russia to become a grand duchy.[38] Simultaneously, a quasi-central banking role was played by a small group of powerful family-run banking networks, typified by the House of Rothschild, with branches in major cities across Europe, as well as Hottinguer in Switzerland and Oppenheim in Germany.[39][40]  The theory of central banking, even though the name was not yet widely used, evolved in the 19th century. Henry Thornton, an opponent of the real bills doctrine, was a defender of the bullionist position and a significant figure in monetary theory. Thornton's process of monetary expansion anticipated the theories of Knut Wicksell regarding the \"cumulative process which restates the Quantity Theory in a theoretically coherent form\". As a response to a currency crisis in 1797, Thornton wrote in 1802 An Enquiry into the Nature and Effects of the Paper Credit of Great Britain, in which he argued that the increase in paper credit did not cause the crisis. The book also gives a detailed account of the British monetary system as well as a detailed examination of the ways in which the Bank of England should act to counteract fluctuations in the value of the pound.[41]  In the United Kingdom until the mid-nineteenth century, commercial banks were able to issue their own banknotes, and notes issued by provincial banking companies were commonly in circulation.[42] Many consider the origins of the central bank to lie with the passage of the Bank Charter Act 1844.[16] Under the 1844 Act, bullionism was institutionalized in Britain,[43] creating a ratio between the gold reserves held by the Bank of England and the notes that the bank could issue.[44] The Act also placed strict curbs on the issuance of notes by the country banks.[44] The Bank of England took over a role of lender of last resort in the 1870s after criticism of its lacklustre response to the failure of Overend, Gurney and Company. The journalist Walter Bagehot wrote on the subject in Lombard Street: A Description of the Money Market, in which he advocated for the bank to officially become a lender of last resort during a credit crunch, sometimes referred to as \"Bagehot's dictum\".  The 19th and early 20th centuries central banks in most of Europe and Japan developed under the international gold standard. Free banking or currency boards were common at the time.[citation needed] Problems with collapses of banks during downturns, however, led to wider support for central banks in those nations which did not as yet possess them, for example in Australia.[citation needed] In the United States, the role of a central bank had been ended in the so-called Bank War of the 1830s by President Andrew Jackson.[45] In 1913, the U.S. created the Federal Reserve System through the passing of The Federal Reserve Act.[46]  Following World War I, the Economic and Financial Organization (EFO) of the League of Nations, influenced by the ideas of Montagu Norman and other leading policymakers and economists of the time, took an active role to promote the independence of central banks, a key component of the economic orthodoxy the EFO fostered at the Brussels Conference (1920). The EFO thus directed the creation of the Oesterreichische Nationalbank in Austria, Hungarian National Bank, Bank of Danzig, and Bank of Greece, as well as comprehensive reforms of the Bulgarian National Bank and Bank of Estonia. Similar ideas were emulated in other newly independent European countries, e.g. for the National Bank of Czechoslovakia.[14]  Brazil established a central bank in 1945, which was a precursor to the Central Bank of Brazil created twenty years later. After gaining independence, numerous African and Asian countries also established central banks or monetary unions. The Reserve Bank of India, which had been established during British colonial rule as a private company, was nationalized in 1949 following India's independence. By the early 21st century, most of the world's countries had a national central bank set up as a public sector institution, albeit with widely varying degrees of independence.  Before the near-generalized adoption of the model of national public-sector central banks, a number of economies relied on a central bank that was effectively or legally run from outside their territory. The first colonial central banks, such as the Bank of Java (est. 1828 in Batavia), Banque de l'Algérie (est. 1851 in Algiers), or Hongkong and Shanghai Banking Corporation (est. 1865 in Hong Kong), operated from the colony itself. Following the generalization of the transcontinental use of the electrical telegraph using submarine communications cable, however, new colonial banks were typically headquartered in the colonial metropolis; prominent examples included the Paris-based Banque de l'Indochine (est. 1875), Banque de l'Afrique Occidentale (est. 1901), and Banque de Madagascar (est. 1925). The Banque de l'Algérie's head office was relocated from Algiers to Paris in 1900.  In some cases, independent countries which did not have a strong domestic base of capital accumulation and were critically reliant on foreign funding found advantage in granting a central banking role to banks that were effectively or even legally foreign. A seminal case was the Imperial Ottoman Bank established in 1863 as a French-British joint venture, and a particularly egregious one was the Paris-based National Bank of Haiti (est. 1881) which captured significant financial resources from the economically struggling albeit independent nation of Haiti.[47] Other cases include the London-based Imperial Bank of Persia, established in 1885, and the Rome-based National Bank of Albania, established in 1925. The State Bank of Morocco was established in 1907 with international shareholding and headquarters functions distributed between Paris and Tangier, a half-decade before the country lost its independence. In other cases, there have been organized currency unions such as the Belgium–Luxembourg Economic Union established in 1921, under which Luxembourg had no central bank, but that was managed by a national central bank (in that case the National Bank of Belgium) rather than a supranational one. The present-day Common Monetary Area of Southern Africa has comparable features.  Yet another pattern was set in countries where federated or otherwise sub-sovereign entities had wide policy autonomy that was echoed to varying degrees in the organization of the central bank itself. These included, for example, the Austro-Hungarian Bank from 1878 to 1918, the U.S. Federal Reserve in its first two decades, the Bank deutscher Länder between 1948 and 1957, or the National Bank of Yugoslavia between 1972 and 1993. Conversely, some countries that are politically organized as federations, such as today's Canada, Mexico, or Switzerland, rely on a unitary central bank.  In the second half of the 20th century, the dismantling of colonial systems left some groups of countries using the same currency even though they had achieved national independence. In contrast to the unraveling of Austria-Hungary and the Ottoman Empire after World War I, some of these countries decided to keep using a common currency, thus forming a monetary union, and to entrust its management to a common central bank. Examples include the Eastern Caribbean Currency Authority, the Central Bank of West African States, and the Bank of Central African States.  The concept of supranational central banking took a globally significant dimension with the Economic and Monetary Union of the European Union and the establishment of the European Central Bank (ECB) in 1998. In 2014, the ECB took an additional role of banking supervision as part of the newly established policy of European banking union.  The primary role of central banks is usually to maintain price stability, as defined as a specific level of inflation. Inflation is defined either as the devaluation of a currency or equivalently the rise of prices relative to a currency. Most central banks currently have an inflation target close to 2%.  Since inflation lowers real wages, Keynesians view inflation as the solution to involuntary unemployment. However, \"unanticipated\" inflation leads to lender losses as the real interest rate will be lower than expected. Thus, Keynesian monetary policy aims for a steady rate of inflation.  Central banks as monetary authorities in representative states are intertwined through globalized financial markets. As a regulator of one of the most widespread currencies in the global economy, the US Federal Reserve plays an outsized role in the international monetary market. Being the main supplier and rate adjusted for US dollars, the Federal Reserve implements a set of requirements to control inflation and unemployment in the US.[48]  Frictional unemployment is the time period between jobs when a worker is searching for, or transitioning from one job to another. Unemployment beyond frictional unemployment is classified as unintended unemployment. For example, structural unemployment is a form of unintended unemployment resulting from a mismatch between demand in the labour market and the skills and locations of the workers seeking employment. Macroeconomic policy generally aims to reduce unintended unemployment.  Keynes labeled any jobs that would be created by a rise in wage-goods (i.e., a decrease in real-wages) as involuntary unemployment:  Economic growth can be enhanced by investment in capital, such as more or better machinery. A low interest rate implies that firms can borrow money to invest in their capital stock and pay less interest for it. Lowering the interest is therefore considered to encourage economic growth and is often used to alleviate times of low economic growth. On the other hand, raising the interest rate is often used in times of high economic growth as a contra-cyclical device to keep the economy from overheating and avoid market bubbles.  Further goals of monetary policy are stability of interest rates, of the financial market, and of the foreign exchange market. Goals frequently cannot be separated from each other and often conflict. Costs must therefore be carefully weighed before policy implementation.  In the aftermath of the Paris agreement on climate change, a debate is now underway on whether central banks should also pursue environmental goals as part of their activities. In 2017, eight central banks formed the Network for Greening the Financial System (NGFS)[49] to evaluate the way in which central banks can use their regulatory and monetary policy tools to support climate change mitigation. Today more than 70 central banks are part of the NGFS.[50]  In January 2020, the European Central Bank has announced[51] it will consider climate considerations when reviewing its monetary policy framework.  Proponents of \"green monetary policy\" are proposing that central banks include climate-related criteria in their collateral eligibility frameworks, when conducting asset purchases and also in their refinancing operations.[52] But critics such as Jens Weidmann are arguing it is not central banks' role to conduct climate policy.[53] China is among the most advanced central banks when it comes to green monetary policy.[54] It has given green bonds preferential status to lower their yield[55] and uses window policy to direct green lending.[56]  The implications of potential stranded assets in the economy highlights one example of the embedded transition risk to climate change with potential cascade effects throughout the financial system.[57][58][59] In response, four broad types of interventions including methodology development, investor encouragement, financial regulation and policy toolkits have been adopted by or suggested for central banks.[20]  Achieving the 2°C threshold revolve in part around the development of climate-aligned financial regulations. A significant challenge lies in the lack of awareness among corporations and investors, driven by poor information flow and insufficient disclosure.[20] To address this issue, regulators and central banks are promoting transparency, integrated reporting, and exposure specifications, with the goal of promoting long-term, low-carbon emission goals, rather than short-term financial objectives.[20][60] These regulations aim to assess risk comprehensively, identifying carbon-intensive assets and increasing their capital requirements. This should result in high-carbon assets becoming less attractive while favoring low-carbon assets, which have historically been perceived as high-risk, and low volatility investment vehicles.[20][61][62]  Quantitative easing is a potential measure that could be applied by Central banks to achieve a low-carbon transition.[20] Although there is a historical bias toward high-carbon companies, included in Central banks portfolios due to their high credit ratings, innovative approaches to quantitative easing could invert this trend to favor low-carbon assets.[20][63][64]  Considering the potential impact of central banks on climate change, it is important to consider the mandates of central banks. The mandate of a central bank can be narrow, meaning only a few objectives are given, limiting the ability of a central bank to include climate change in its policies.[20] However, central bank mandates may not necessarily have to be modified to accommodate climate change-related activities.[20] For example, the European Central Bank has incorporated carbon-emissions into its asset purchase criteria, despite its relatively narrow mandate that focuses on price stability.[65]  The functions of a central bank may include:  Central banks implement a country's chosen monetary policy.  At the most basic level, monetary policy involves establishing what form of currency the country may have, whether a fiat currency, gold-backed currency (disallowed for countries in the International Monetary Fund), currency board or a currency union. When a country has its own national currency, this involves the issue of some form of standardized currency, which is essentially a form of promissory note: \"money\" under certain circumstances. Historically, this was often a promise to exchange the money for precious metals in some fixed amount. Now, when many currencies are fiat money, the \"promise to pay\" consists of the promise to accept that currency to pay for taxes.  A central bank may use another country's currency either directly in a currency union, or indirectly on a currency board. In the latter case, exemplified by the Bulgarian National Bank, Hong Kong and Latvia (until 2014), the local currency is backed at a fixed rate by the central bank's holdings of a foreign currency. Similar to commercial banks, central banks hold assets (government bonds, foreign exchange, gold, and other financial assets) and incur liabilities (currency outstanding). Central banks create money by issuing banknotes and loaning them to the government in exchange for interest-bearing assets such as government bonds. When central banks decide to increase the money supply by an amount which is greater than the amount their national governments decide to borrow, the central banks may purchase private bonds or assets denominated in foreign currencies.  The European Central Bank remits its interest income to the central banks of the member countries of the European Union. The US Federal Reserve remits most of its profits to the U.S. Treasury. This income, derived from the power to issue currency, is referred to as seigniorage, and usually belongs to the national government. The state-sanctioned power to create currency is called the Right of Issuance. Throughout history, there have been disagreements over this power, since whoever controls the creation of currency controls the seigniorage income. The expression \"monetary policy\" may also refer more narrowly to the interest-rate targets and other active measures undertaken by the monetary authority.  The primary monetary policy tool available to central banks is the administered interest rate paid on qualifying deposits held with them. Adjusting this rate up or down influences the rate commercial banks pay on their own customer deposits, which in turn influences the rate that commercial banks charge customers for loans.  A central bank affects the monetary base through open market operations, if its country has a well developed market for its government bonds. This entails managing the quantity of money in circulation through the buying and selling of various financial instruments, such as treasury bills, repurchase agreements or \"repos\", company bonds, or foreign currencies, in exchange for money on deposit at the central bank. Those deposits are convertible to currency, so all of these purchases or sales result in more or less base currency entering or leaving market circulation.  If the central bank wishes to decrease interest rates, it reduces its administered rates (Bank Rate, the reverse repurchase agreement rate and the discount rate). This results in commercial banks bidding down the rate they pay customers on their deposits and, subsequently, loan rates are reduced commensurately. Cheaper credit can increase consumer spending or business investment, stimulating output growth. On the other hand, cheaper interest income can reduce spending, suppressing output. Additionally, when business loans are more affordable, companies can expand to keep up with consumer demand. They ultimately hire more workers, whose incomes increase, which in its turn also increases the demand. This method is usually enough to stimulate demand and drive economic growth to a higher rate. In other instances, monetary policy might instead entail the targeting of a specific exchange rate relative to some foreign currency or else relative to gold. For example, in the case of the United States, the Federal Reserve targets the federal funds rate, the rate at which member banks lend to one another overnight; however, the monetary policy of China (since 2014) is to target the exchange rate between the Chinese renminbi and a basket of foreign currencies.  A third alternative is to change reserve requirements. The reserve requirement refers to the proportion of total liabilities that banks must keep on hand overnight, either in its vaults or at the central bank. Banks only maintain a small portion of their assets as cash available for immediate withdrawal; the rest is invested in illiquid assets like mortgages and loans. Lowering the reserve requirement frees up funds for banks to buy other profitable assets. However, even though this tool immediately increases liquidity, central banks rarely change the reserve requirement because doing so frequently adds uncertainty to banks' planning. Most modern central banks now have zero formal reserve requirement.  Other forms of monetary policy, particularly used when interest rates are at or near 0% and there are concerns about deflation or deflation is occurring, are referred to as unconventional monetary policy. These include credit easing, quantitative easing, forward guidance, and signalling.[66] In credit easing, a central bank purchases private sector assets to improve liquidity and improve access to credit. Signaling can be used to lower market expectations for lower interest rates in the future. For example, during the credit crisis of 2008, the US Federal Reserve indicated rates would be low for an \"extended period\", and the Bank of Canada made a \"conditional commitment\" to keep rates at the lower bound of 25 basis points (0.25%) until the end of the second quarter of 2010.  Some have envisaged the use of what Milton Friedman once called \"helicopter money\" whereby the central bank would make direct transfers to citizens[67] in order to lift inflation up to the central bank's intended target. Such policy option could be particularly effective at the zero lower bound.[68]  Since 2017, prospect of implementing Central Bank Digital Currency (CBDC) has been in discussion.[69] As of the end of 2018, at least 15 central banks were considering to implementing CBDC.[70] Since 2014, the People's Bank of China has been working on a project for digital currency to make its own digital currency and electronic payment systems.[71][72]  In some countries a central bank, through its subsidiaries, controls and monitors the banking sector. In other countries banking supervision is carried out by a government department such as the UK Treasury, or by an independent government agency, for example, UK's Financial Conduct Authority. It examines the banks' balance sheets and behaviour and policies toward consumers.[clarification needed] Apart from refinancing, it also provides banks with services such as transfer of funds, bank notes and coins or foreign currency. Thus it is often described as the \"bank of banks\".  Many countries will monitor and control the banking sector through several different agencies and for different purposes. The Bank regulation in the United States for example is highly fragmented with 3 federal agencies, the Federal Deposit Insurance Corporation, the Federal Reserve Board, or Office of the Comptroller of the Currency and numerous others on the state and the private level. There is usually significant cooperation between the agencies. For example, money center banks, deposit-taking institutions, and other types of financial institutions may be subject to different (and occasionally overlapping) regulation. Some types of banking regulation may be delegated to other levels of government, such as state or provincial governments.  Any cartel of banks is particularly closely watched and controlled. Most countries control bank mergers and are wary of concentration in this industry due to the danger of groupthink and runaway lending bubbles based on a single point of failure, the credit culture of the few large banks.  Central banks have increasingly engaged in public communication to ensure accountability, build trust, and manage inflation expectations.[73] Various aspects of central bank communication are also analyzed, including textual content through text mining techniques,[74] facial expressions during press conferences,[75] vocal characteristics,[76] and the clarity and readability of monetary policy announcements.[77]  Numerous governments have opted to make central banks independent. The economic logic behind central bank independence is that when governments delegate monetary policy to an independent central bank (with an anti-inflationary purpose) and away from elected politicians, monetary policy will not reflect the interests of the politicians. When governments control monetary policy, politicians may be tempted to boost economic activity in advance of an election to the detriment of the long-term health of the economy and the country. As a consequence, financial markets may not consider future commitments to low inflation to be credible when monetary policy is in the hands of elected officials, which increases the risk of capital flight. An alternative to central bank independence is to have fixed exchange rate regimes.[80][81][82]  Governments generally have some degree of influence over even \"independent\" central banks; the aim of independence is primarily to prevent short-term interference. In 1951, the Deutsche Bundesbank became the first central bank to be given full independence, leading this form of central bank to be referred to as the \"Bundesbank model\", as opposed, for instance, to the New Zealand model, which has a goal (i.e. inflation target) set by the government.  Central bank independence is usually guaranteed by legislation and the institutional framework governing the bank's relationship with elected officials, particularly the minister of finance. Central bank legislation will enshrine specific procedures for selecting and appointing the head of the central bank. Often the minister of finance will appoint the governor in consultation with the central bank's board and its incumbent governor. In addition, the legislation will specify banks governor's term of appointment. The most independent central banks enjoy a fixed non-renewable term for the governor in order to eliminate pressure on the governor to please the government in the hope of being re-appointed for a second term.[83] Generally, independent central banks enjoy both goal and instrument independence.[84]  Despite their independence, central banks are usually accountable at some level to government officials, either to the finance ministry or to parliament. For example, the Board of Governors of the U.S. Federal Reserve are nominated by the U.S. president and confirmed by the Senate,[85] publishes verbatim transcripts, and balance sheets are audited by the Government Accountability Office.[86]  In the 1990s there was a trend towards increasing the independence of central banks as a way of improving long-term economic performance.[87] While a large volume of economic research has been done to define the relationship between central bank independence and economic performance, the results are ambiguous.[88]  The literature on central bank independence has defined a cumulative and complementary number of aspects:[89][90]  There is very strong consensus among economists that an independent central bank can run a more credible monetary policy, making market expectations more responsive to signals from the central bank.[92] Both the Bank of England (1997) and the European Central Bank have been made independent and follow a set of published inflation targets so that markets know what to expect.[citation needed] Populism can reduce de facto central bank independence.[93]  International organizations such as the World Bank, the Bank for International Settlements (BIS) and the International Monetary Fund (IMF) strongly support central bank independence. This results, in part, from a belief in the intrinsic merits of increased independence. The support for independence from the international organizations also derives partly from the connection between increased independence for the central bank and increased transparency in the policy-making process. The IMF's Financial Services Action Plan (FSAP) review self-assessment, for example, includes a number of questions about central bank independence in the transparency section. An independent central bank will score higher in the review than one that is not independent.[citation needed]  Central bank independence indices allow a quantitative analysis of central bank independence for individual countries over time. One central bank independence index is the Garriga CBI,[94] where a higher index indicates higher central bank independence, shown below for individual countries.  Collectively, central banks purchase less than 500 tonnes of gold each year, on average (out of an annual global production of 2,500–3,000 tonnes).[95] In 2018, central banks collectively hold over 33,000 metric tons of the gold, about a fifth of all the gold ever mined, according to Bloomberg News.[96]  In 2016, 75% of the world's central-bank assets were controlled by four centers in China, the United States, Japan and the eurozone. The central banks of Brazil, Switzerland, Saudi Arabia, the U.K., India and Russia, each account for an average of 2.5 percent. The remaining 107 central banks hold less than 13 percent. According to data compiled by Bloomberg News, the top 10 largest central banks owned $21.4 trillion in assets, a 10 percent increase from 2015.[97] Following is a ranking of the 5 biggest: "},"meta":{},"created_at":"2025-03-22T14:25:42.276677Z","updated_at":"2025-03-22T14:25:42.276677Z","inner_id":42,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":51,"annotations":[{"id":51,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.312403Z","updated_at":"2025-03-22T14:25:42.312403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"4bf92457-15ba-4ba9-b44b-2bcb6720b8b7","import_id":null,"last_action":null,"bulk_created":false,"task":51,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Heterodox  In economics, inflation is an increase in the average price of goods and services in terms of money.[3]: 579  This is usually measured using a consumer price index (CPI).[4][5][6][7] When the general price level rises, each unit of currency buys fewer goods and services; consequently, inflation corresponds to a reduction in the purchasing power of money.[8][9] The opposite of CPI inflation is deflation, a decrease in the general price level of goods and services. The common measure of inflation is the inflation rate, the annualized percentage change in a general price index.[10]: 22–32  As prices faced by households do not all increase at the same rate, the consumer price index (CPI) is often used for this purpose.  Changes in inflation are widely attributed to fluctuations in real demand for goods and services (also known as demand shocks, including changes in fiscal or monetary policy), changes in available supplies such as during energy crises (also known as supply shocks), or changes in inflation expectations, which may be self-fulfilling.[11] Moderate inflation affects economies in both positive and negative ways. The negative effects would include an increase in the opportunity cost of holding money; uncertainty over future inflation, which may discourage investment and savings; and, if inflation were rapid enough, shortages of goods as consumers begin hoarding out of concern that prices will increase in the future. Positive effects include reducing unemployment due to nominal wage rigidity,[10]: 238–255  allowing the central bank greater freedom in carrying out monetary policy, encouraging loans and investment instead of money hoarding, and avoiding the inefficiencies associated with deflation.  Today, some economists favour a low and steady rate of inflation, though inflation is less popular with the general public than with economists, since \"inflation simultaneously transfers some of [the] people's income into the hands of government.\"[12] Low (as opposed to zero or negative) inflation reduces the probability of economic recessions by enabling the labor market to adjust more quickly in a downturn and reduces the risk that a liquidity trap prevents monetary policy from stabilizing the economy while avoiding the costs associated with high inflation.[13] The task of keeping the rate of inflation low and stable is usually given to central banks that control monetary policy, normally through the setting of interest rates and by carrying out open market operations.[11]  The term originates from the Latin inflare (to blow into or inflate). Conceptually, inflation refers to the general trend of prices, not changes in any specific price. For example, if people choose to buy more cucumbers than tomatoes, cucumbers consequently become more expensive and tomatoes less expensive. These changes are not related to inflation; they reflect a shift in tastes. Inflation is related to the value of currency itself. When currency was linked with gold, if new gold deposits were found, the price of gold and the value of currency would fall, and consequently, prices of all other goods would become higher.[14]  By the nineteenth century, economists categorised three separate factors that cause a rise or fall in the price of goods: a change in the value or production costs of the good, a change in the price of money which then was usually a fluctuation in the commodity price of the metallic content in the currency, and currency depreciation resulting from an increased supply of currency relative to the quantity of redeemable metal backing the currency. Following the proliferation of private banknote currency printed during the American Civil War, the term \"inflation\" started to appear as a direct reference to the currency depreciation that occurred as the quantity of redeemable banknotes outstripped the quantity of metal available for their redemption. At that time, the term inflation referred to the devaluation of the currency, and not to a rise in the price of goods.[15] This relationship between the over-supply of banknotes and a resulting depreciation in their value was noted by earlier classical economists such as David Hume and David Ricardo, who would go on to examine and debate what effect a currency devaluation has on the price of goods.[16]  Other economic concepts related to inflation include: deflation – a fall in the general price level;[17] disinflation – a decrease in the rate of inflation;[18] hyperinflation – an out-of-control inflationary spiral;[19] stagflation – a combination of inflation, slow economic growth and high unemployment;[20] reflation – an attempt to raise the general level of prices to counteract deflationary pressures;[21] and asset price inflation – a general rise in the prices of financial assets without a corresponding increase in the prices of goods or services;[22] agflation – an advanced increase in the price for food and industrial agricultural crops when compared with the general rise in prices.[23]  More specific forms of inflation refer to sectors whose prices vary semi-independently from the general trend. \"House price inflation\" applies to changes in the house price index[24] while \"energy inflation\" is dominated by the costs of oil and gas.[25]  Inflation has been a feature of history during the entire period when money has been used as a means of payment. One of the earliest documented inflations occurred in Alexander the Great's empire 330 BCE.[26] Historically, when commodity money was used, periods of inflation and deflation would alternate depending on the condition of the economy. However, when large, prolonged infusions of gold or silver into an economy occurred, this could lead to long periods of inflation.  The adoption of fiat currency by many countries, from the 18th century onwards, made much larger variations in the supply of money possible.[27] Rapid increases in the money supply have taken place a number of times in countries experiencing political crises, producing hyperinflations – episodes of extreme inflation rates much higher than those observed in earlier periods of commodity money. The hyperinflation in the Weimar Republic of Germany is a notable example. The hyperinflation in Venezuela is the highest in the world, with an annual inflation rate of 833,997% as of October 2018.[28]  Historically, inflations of varying magnitudes have occurred, interspersed with corresponding deflationary periods,[26] from the price revolution of the 16th century, which was driven by the flood of gold and particularly silver seized and mined by the Spaniards in Latin America, to the largest paper money inflation of all time in Hungary after World War II.[29]  However, since the 1980s, inflation has been held low and stable in countries with independent central banks. This has led to a moderation of the business cycle and a reduction in variation in most macroeconomic indicators – an event known as the Great Moderation.[30]  Alexander the Great's conquest of the Persian Empire in 330 BCE was followed by one of the earliest documented inflation periods in the ancient world.[26] Rapid increases in the quantity of money or in the overall money supply have occurred in many different societies throughout history, changing with different forms of money used.[31][32] For instance, when silver was used as currency, the government could collect silver coins, melt them down, mix them with other, less valuable metals such as copper or lead and reissue them at the same nominal value, a process known as debasement. At the ascent of Nero as Roman emperor in AD 54, the denarius contained more than 90% silver, but by the 270s hardly any silver was left. By diluting the silver with other metals, the government could issue more coins without increasing the amount of silver used to make them. When the cost of each coin is lowered in this way, the government profits from an increase in seigniorage.[33] This practice would increase the money supply but at the same time the relative value of each coin would be lowered. As the relative value of the coins becomes lower, consumers would need to give more coins in exchange for the same goods and services as before. These goods and services would experience a price increase as the value of each coin is reduced.[34] Again at the end of the third century CE during the reign of Diocletian, the Roman Empire experienced rapid inflation.[26]  Song dynasty China introduced the practice of printing paper money to create fiat currency.[35] During the Mongol Yuan dynasty, the government spent a great deal of money fighting costly wars, and reacted by printing more money, leading to inflation.[36] Fearing the inflation that plagued the Yuan dynasty, the Ming dynasty initially rejected the use of paper money, and reverted to using copper coins.[37]  During the Malian king Mansa Musa's hajj to Mecca in 1324, he was reportedly accompanied by a camel train that included thousands of people and nearly a hundred camels. When he passed through Cairo, he spent or gave away so much gold that it depressed its price in Egypt for over a decade,[38] reducing its purchasing power. A contemporary Arab historian remarked about Mansa Musa's visit:  Gold was at a high price in Egypt until they came in that year. The mithqal did not go below 25 dirhams and was generally above, but from that time its value fell and it cheapened in price and has remained cheap till now. The mithqal does not exceed 22 dirhams or less. This has been the state of affairs for about twelve years until this day by reason of the large amount of gold which they brought into Egypt and spent there [...]. There is no reliable evidence of inflation in Europe for the thousand years that followed the fall of the Roman Empire, but from the Middle Ages onwards reliable data do exist. Mostly, the medieval inflation episodes were modest, and there was a tendency that inflationary periods were followed by deflationary periods.[26]  From the second half of the 15th century to the first half of the 17th, Western Europe experienced a major inflationary cycle referred to as the \"price revolution\",[40][41] with prices on average rising perhaps sixfold over 150 years. This is often attributed to the influx of gold and silver from the New World into Habsburg Spain,[42] with wider availability of silver in previously cash-starved Europe causing widespread inflation.[43][44] European population rebound from the Black Death began before the arrival of New World metal, and may have begun a process of inflation that New World silver compounded later in the 16th century.[45]  A pattern of intermittent inflation and deflation periods persisted for centuries until the Great Depression in the 1930s, which was characterized by major deflation. Since the Great Depression, however, there has been a general tendency for prices to rise every year. In the 1970s and early 1980s, annual inflation in most industrialized countries reached two digits (ten percent or more). The double-digit inflation era was of short duration, however, inflation by the mid-1980s returned to more modest levels. Amid this, general trends there have been spectacular high-inflation episodes in individual countries in interwar Europe, towards the end of the Nationalist Chinese government in 1948–1949, and later in some Latin American countries, in Israel, and in Zimbabwe. Some of these episodes are considered hyperinflation periods, normally designating inflation rates that surpass 50 percent monthly.[26]  Given that there are many possible measures of the price level, there are many possible measures of price inflation. Most frequently, the term \"inflation\" refers to a rise in a broad price index representing the overall price level for goods and services in the economy. The consumer price index (CPI), the personal consumption expenditures price index (PCEPI) and the GDP deflator are some examples of broad price indices. However, \"inflation\" may also be used to describe a rising price level within a narrower set of assets, goods or services within the economy, such as commodities (including food, fuel, metals), tangible assets (such as real estate), services (such as entertainment and health care), or labor. Although the values of capital assets are often casually said to \"inflate,\" this should not be confused with inflation as a defined term; a more accurate description for an increase in the value of a capital asset is appreciation. The FBI (CCI), the producer price index, and employment cost index (ECI) are examples of narrow price indices used to measure price inflation in particular sectors of the economy. Core inflation is a measure of inflation for a subset of consumer prices that excludes food and energy prices, which rise and fall more than other prices in the short term. The Federal Reserve Board pays particular attention to the core inflation rate to get a better estimate of long-term future inflation trends overall.[47]  The inflation rate is most widely calculated by determining the movement or change in a price index, typically the consumer price index.[48]  The inflation rate is the percentage change of a price index over time. The Retail Prices Index is also a measure of inflation that is commonly used in the United Kingdom. It is broader than the CPI and contains a larger basket of goods and services. Inflation is politically driven, and policy can directly influence the trend of inflation.  The RPI is indicative of the experiences of a wide range of household types, particularly low-income households.[49]  To illustrate the method of calculation, in January 2007, the U.S. Consumer Price Index was 202.416, and in January 2008 it was 211.080. The formula for calculating the annual percentage rate inflation in the CPI over the course of the year is:      (    211.080 − 202.416  202.416   )  × 100 % = 4.28 %   {\\displaystyle \\left({\\frac {211.080-202.416}{202.416}}\\right)\\times 100\\%=4.28\\%}    The resulting inflation rate for the CPI in this one-year period is 4.28%, meaning the general level of prices for typical U.S. consumers rose by approximately four percent in 2007.[50]  Other widely used price indices for calculating price inflation include the following:  Other common measures of inflation are:  ∴       GDP Deflator   =    Nominal GDP   Real GDP      {\\displaystyle {\\mbox{GDP Deflator}}={\\frac {\\mbox{Nominal GDP}}{\\mbox{Real GDP}}}}    In some cases, the measures are meant to be more humorous or to reflect a single place.  This includes:  Measuring inflation in an economy requires objective means of differentiating changes in nominal prices on a common set of goods and services, and distinguishing them from those price shifts resulting from changes in value such as volume, quality, or performance. For example, if the price of a can of corn changes from $0.90 to $1.00 over the course of a year, with no change in quality, then this price difference represents inflation. This single price change would not, however, represent general inflation in an overall economy. Overall inflation is measured as the price change of a large \"basket\" of representative goods and services. This is the purpose of a price index, which is the combined price of a \"basket\" of many goods and services. The combined price is the sum of the weighted prices of items in the \"basket\". A weighted price is calculated by multiplying the unit price of an item by the number of that item the average consumer purchases. Weighted pricing is necessary to measure the effect of individual unit price changes on the economy's overall inflation. The consumer price index, for example, uses data collected by surveying households to determine what proportion of the typical consumer's overall spending is spent on specific goods and services, and weights the average prices of those items accordingly. Those weighted average prices are combined to calculate the overall price. To better relate price changes over time, indexes typically choose a \"base year\" price and assign it a value of 100. Index prices in subsequent years are then expressed in relation to the base year price.[56] While comparing inflation measures for various periods one has to take into consideration the base effect as well.  Inflation measures are often modified over time, either for the relative weight of goods in the basket, or in the way in which goods and services from the present are compared with goods and services from the past. Basket weights are updated regularly, usually every year, to adapt to changes in consumer behavior. Sudden changes in consumer behavior can still introduce a weighting bias in inflation measurement. For example, during the COVID-19 pandemic it has been shown that the basket of goods and services was no longer representative of consumption during the crisis, as numerous goods and services could no longer be consumed due to government containment measures (\"lock-downs\").[57][58]  Over time, adjustments are also made to the type of goods and services selected to reflect changes in the sorts of goods and services purchased by 'typical consumers'. New products may be introduced, older products disappear, the quality of existing products may change, and consumer preferences can shift. Different segments of the population may naturally consume different \"baskets\" of goods and services and may even experience different inflation rates. It is argued that companies have put more innovation into bringing down prices for wealthy families than for poor families.[59]  Inflation numbers are often seasonally adjusted to differentiate expected cyclical cost shifts. For example, home heating costs are expected to rise in colder months, and seasonal adjustments are often used when measuring inflation to compensate for cyclical energy or fuel demand spikes. Inflation numbers may be averaged or otherwise subjected to statistical techniques to remove statistical noise and volatility of individual prices.[60][61]  When looking at inflation, economic institutions may focus only on certain kinds of prices, or special indices, such as the core inflation index which is used by central banks to formulate monetary policy.[62]  Most inflation indices are calculated from weighted averages of selected price changes. This necessarily introduces distortion, and can lead to legitimate disputes about what the true inflation rate is. This problem can be overcome by including all available price changes in the calculation, and then choosing the median value.[63] In some other cases, governments may intentionally report false inflation rates; for instance, during the presidency of Cristina Kirchner (2007–2015) the government of Argentina was criticised for manipulating economic data, such as inflation and GDP figures, for political gain and to reduce payments on its inflation-indexed debt.[64][65]  The true inflation is one percentage point lower than the official one, according to research. Therefore, the 2% inflation target is needed to prevent the true inflation being close to zero or even deflation. The reasons are the following:[66]  Nevertheless, people overestimate the inflation even vs. the measured inflation. This is because they focus more on commonly-bought items than on durable goods, and more on price increases than on price decreases.[68]  On the other hand, different people have different shopping baskets and hence face different inflation rates.[68]  Inflation expectations or expected inflation is the rate of inflation that is anticipated for some time in the foreseeable future. There are two major approaches to modeling the formation of inflation expectations. Adaptive expectations models them as a weighted average of what was expected one period earlier and the actual rate of inflation that most recently occurred. Rational expectations models them as unbiased, in the sense that the expected inflation rate is not systematically above or systematically below the inflation rate that actually occurs.  A long-standing survey of inflation expectations is the University of Michigan survey.[69]  Inflation expectations affect the economy in several ways. They are more or less built into nominal interest rates, so that a rise (or fall) in the expected inflation rate will typically result in a rise (or fall) in nominal interest rates, giving a smaller effect if any on real interest rates. In addition, higher expected inflation tends to be built into the rate of wage increases, giving a smaller effect if any on the changes in real wages. Moreover, the response of inflationary expectations to monetary policy can influence the division of the effects of policy between inflation and unemployment (see monetary policy credibility).  Theories of the origin and causes of inflation have existed since at least the 16th century. Two competing theories, the quantity theory of money and the real bills doctrine, appeared in various disguises during century-long debates on recommended central bank behaviour. In the 20th century, Keynesian, monetarist and new classical (also known as rational expectations) views on inflation dominated post-World War II macroeconomics discussions, which were often heated intellectual debates, until some kind of synthesis of the various theories was reached by the end of the century.  The price revolution from ca. 1550–1700 caused several thinkers to present what is now considered to be early formulations of the quantity theory of money (QTM). Other contemporary authors attributed rising price levels to the debasement of national coinages. Later research has shown that also growing output of Central European silver mines and an increase in the velocity of money because of innovations in the payment technology, in particular the increased use of bills of exchange, contributed to the price revolution.[70]  An alternative theory, the real bills doctrine (RBD), originated in the 17th and 18th century, receiving its first authoritative exposition in Adam Smith's The Wealth of Nations.[71] It asserts that banks should issue their money in exchange for short-term real bills of adequate value. As long as banks only issue a dollar in exchange for assets worth at least a dollar, the issuing bank's assets will naturally move in step with its issuance of money, and the money will hold its value. Should the bank fail to get or maintain assets of adequate value, then the bank's money will lose value, just as any financial security will lose value if its asset backing diminishes. The real bills doctrine (also known as the backing theory) thus asserts that inflation results when money outruns its issuer's assets. The quantity theory of money, in contrast, claims that inflation results when money outruns the economy's production of goods.  During the 19th century, three different schools debated these questions: The British Currency School upheld a quantity theory view, believing that the Bank of England's issues of bank notes should vary one-for-one with the bank's gold reserves. In contrast to this, the British Banking School followed the real bills doctrine, recommending that the bank's operations should be governed by the needs of trade: Banks should be able to issue currency against bills of trading, i.e. \"real bills\" that they buy from merchants. A third group, the Free Banking School, held that competitive private banks would not overissue, even though a monopolist central bank could be believed to do it.[72]  The debate between currency, or quantity theory, and banking schools during the 19th century prefigures current questions about the credibility of money in the present. In the 19th century, the banking schools had greater influence in policy in the United States and Great Britain, while the currency schools had more influence \"on the continent\", that is in non-British countries, particularly in the Latin Monetary Union and the Scandinavian Monetary Union.  During the Bullionist Controversy during the Napoleonic Wars, David Ricardo argued that the Bank of England had engaged in over-issue of bank notes, leading to commodity price increases. In the late 19th century, supporters of the quantity theory of money led by Irving Fisher debated with supporters of bimetallism. Later, Knut Wicksell sought to explain price movements as the result of real shocks rather than movements in money supply, resounding statements from the real bills doctrine.[70]  In 2019, monetary historians Thomas M. Humphrey and Richard Timberlake published \"Gold, the Real Bills Doctrine, and the Fed: Sources of Monetary Disorder 1922–1938\".[73]  John Maynard Keynes in his 1936 main work The General Theory of Employment, Interest and Money emphasized that wages and prices were sticky in the short run, but gradually responded to aggregate demand shocks. These could arise from many different sources, e.g. autonomous movements in investment or fluctuations in private wealth or interest rates.[26] Economic policy could also affect demand, monetary policy by affecting interest rates and fiscal policy either directly through the level of government final consumption expenditure or indirectly by changing disposable income via tax changes.  The various sources of variations in aggregate demand will cause cycles in both output and price levels. Initially, a demand change will primarily affect output because of the price stickiness, but eventually prices and wages will adjust to reflect the change in demand. Consequently, movements in real output and prices will be positively, but not strongly, correlated.[26]  Keynes' propositions formed the basis of Keynesian economics which came to dominate macroeconomic research and economic policy in the first decades after World War II.[11]: 526  Other Keynesian economists developed and reformed several of Keynes' ideas. Importantly, Alban William Phillips in 1958 published indirect evidence of a negative relation between inflation and unemployment, confirming the Keynesian emphasis on a positive correlation between increases in real output (normally accompanied by a fall in unemployment) and rising prices, i.e. inflation. Phillips' findings were confirmed by other empirical analyses and became known as a Phillips curve. It quickly became central to macroeconomic thinking, apparently offering a stable trade-off between price stability and employment. The curve was interpreted to imply that a country could achieve low unemployment if it were willing to tolerate a higher inflation rate or vice versa.[11]: 173   The Phillips curve model described the U.S. experience well in the 1960s, but failed to describe the stagflation experienced in the 1970s.  During the 1960s the Keynesian view of inflation and macroeconomic policy altogether were challenged by monetarist theories, led by Milton Friedman.[11]: 528–529  Friedman famously stated that \"Inflation is always and everywhere a monetary phenomenon.\"[74] He revived the quantity theory of money by Irving Fisher and others, making it into a central tenet of monetarist thinking, arguing that the most significant factor influencing inflation or deflation is how fast the money supply grows or shrinks.[75]  The quantity theory of money, simply stated, says that any change in the amount of money in a system will change the price level. This theory begins with the equation of exchange:  where  In this formula, the general price level is related to the level of real economic activity (Q), the quantity of money (M) and the velocity of money (V). The formula itself is simply an uncontroversial accounting identity because the velocity of money (V) is defined residually from the equation to be the ratio of final nominal expenditure (    P Q   {\\displaystyle PQ}  ) to the quantity of money (M).[10]: 81–107   Monetarists assumed additionally that the velocity of money is unaffected by monetary policy (at least in the long run), that the real value of output is also exogenous in the long run, its long-run value being determined independently by the productive capacity of the economy, and that money supply is exogenous and can be controlled by the monetary authorities. Under these assumptions, the primary driver of the change in the general price level is changes in the quantity of money.[10]: 81–107  Consequently, monetarists contended that monetary policy, not fiscal policy, was the most potent instrument to influence aggregate demand, real output and eventually inflation. This was contrary to Keynesian thinking which in principle recognized a role for monetary policy, but in practice believed that the effect from interest rate changes to the real economy was slight, making monetary policy an ineffective instrument, preferring fiscal policy.[11]: 528  Conversely, monetarists considered fiscal policy, or government spending and taxation, as ineffective in controlling inflation.[75]  Friedman also took issue with the traditional Keynesian view concerning the Phillips curve. He, together with Edmund Phelps, contended that the trade-off between inflation and unemployment implied by the Phillips curve was only temporary, but not permanent. If politicians tried to exploit it, it would eventually disappear because higher inflation would over time be built into the economic expectations of households and firms.[11]: 528–529  This line of thinking led to the concept of potential output (sometimes called the \"natural gross domestic product\"), a level of GDP where the economy is stable in the sense that inflation will neither decrease nor increase. This level may itself change over time when institutional or natural constraints change. It corresponds to the Non-Accelerating Inflation Rate of Unemployment, NAIRU, or the \"natural\" rate of unemployment (sometimes called the \"structural\" level of unemployment).[11] If GDP exceeds its potential (and unemployment consequently is below the NAIRU), the theory says that inflation will accelerate as suppliers increase their prices. If GDP falls below its potential level (and unemployment is above the NAIRU), inflation will decelerate as suppliers attempt to fill excess capacity, cutting prices and undermining inflation.[76]  In the early 1970s, rational expectations theory led by economists like Robert Lucas, Thomas Sargent and Robert Barro transformed macroeconomic thinking radically. They held that economic actors look rationally into the future when trying to maximize their well-being, and do not respond solely to immediate opportunity costs and pressures.[11]: 529–530  In this view, future expectations and strategies are important for inflation as well. One implication was that agents would anticipate the likely behaviour of central banks and base their own actions on these expectations. A central bank having a reputation of being \"soft\" on inflation will generate high inflation expectations, which again will be self-fulfilling when all agents build expectations of future high inflation into their nominal contracts like wage agreements. On the other hand, if the central bank has a reputation of being \"tough\" on inflation, then such a policy announcement will be believed and inflationary expectations will come down rapidly, thus allowing inflation itself to come down rapidly with minimal economic disruption. The implication is that credibility becomes very important for central banks in fighting inflation.[11]: 467–469   Events during the 1970s proved Milton Friedman and other critics of the traditional Phillips curve right: The relation between the inflation rate and the unemployment rate broke down. Eventually, a consensus was established that the break-down was due to agents changing their inflation expectations, confirming Friedman's theory. As a consequence, the notion of a natural rate of unemployment (alternatively called the structural rate of unemployment) was accepted by most economists, meaning that there is a specific level of unemployment that is compatible with stable inflation. Stabilization policy must therefore try to steer economic activity so that the actual unemployment rate converges towards that level.[11]: 176–189  The trade-off between the unemployment rate and inflation implied by Phillips thus holds in the short term, but not in the long term.[77] Also the oil crises of the 1970s causing at the same time rising unemployment and rising inflation (i.e. stagflation) led to a broad recognition by economists that supply shocks could independently affect inflation.[26][11]: 529   During the 1980s a group of researchers named new Keynesians emerged who accepted many originally non-Keynesian concepts like the importance of monetary policy, the existence of a natural level of unemployment and the incorporation of rational expectations formation as a reasonable benchmark. At the same time they believed, like Keynes did, that various market imperfections in different markets like labour markets and financial markets were also important to study to understand both inflation generation and business cycles.[11]: 533–534  During the 1980s and 1990s, there were often heated intellectual debates between new Keynesians and new classicals, but by the 2000s, a synthesis gradually emerged. The result has been called the new Keynesian model,[11]: 535  the \"new neoclassical synthesis\"[78][79] or simply the \"new consensus\" model.[78]  A common view beginning around the year 2000 and holding through to the present time on inflation and its causes can be illustrated by a modern Phillips curve including a role for supply shocks and inflation expectations beside the original role of aggregate demand (determining employment and unemployment fluctuations) in influencing the inflation rate.[11] Consequently, demand shocks, supply shocks and inflation expectations are all potentially important determinants of inflation,[80] confirming the basis of the older triangle model by Robert J. Gordon:[81]  The important role of rational expectations is recognized by the emphasis on credibility on the part of central banks and other policy-makers.[78] The monetarist assertion that monetary policy alone could successfully control inflation formed part of the new consensus which recognized that both monetary and fiscal policy are important tools for influencing aggregate demand.[78][11]: 528  Indeed, monetary policy is under normal circumstances considered to be the preferable instrument to contain inflation.[80][11] At the same time, most central banks have abandoned trying to target money growth as originally advocated by the monetarists. Instead, most central banks in developed countries focus on adjusting interest rates to achieve an explicit inflation target.[3][11]: 505–509  The reason for central bank reluctance in following money growth targets is that the money stock measures that central banks can control tightly, e.g. the monetary base, are not very closely linked to aggregate demand, whereas conversely money supply measures like M2, which are in some cases more closely correlated with aggregate demand, are difficult to control for the central bank. Also, in many countries the relationship between aggregate demand and all money stock measures have broken down in recent decades, weakening further the case for monetary policy rules focusing on the money supply.[3]: 608   However, while more disputed in the 1970s, surveys of members of the American Economic Association (AEA) since the 1990s have shown that most professional American economists generally agree with the statement \"Inflation is caused primarily by too much growth in the money supply\", while the same surveys have shown a lack of consensus by AEA members since the 1990s that \"In the short run, a reduction in unemployment causes the rate of inflation to increase\" has developed despite more agreement with the statement in the 1970s.[90]  Housing shortages[91][92][93][94] and climate change[95][96][97][98] have both been cited as significant drivers of inflation in the 21st century.  In 2021–2022, most countries experienced a considerable increase in inflation, peaking in 2022 and declining in 2023. The causes are believed to be a mixture of demand and supply shocks, whereas inflation expectations generally seem to remain anchored (as per May 2023).[99] Possible causes on the demand side include expansionary fiscal and monetary policy in the wake of the global COVID-19 pandemic, whereas supply shocks include supply chain problems also caused by the pandemic[99] and exacerbated by energy price rises following the Russian invasion of Ukraine in 2022.   The term sellers' inflation was coined during this period to describe the effect of corporate profits as a possible cause of inflation: Price inelasticity can contribute to inflation when firms consolidate, tending to support monopoly or monopsony conditions anywhere along the supply chain for goods or services. When this occurs, firms can provide greater shareholder value by taking a larger proportion of profits than by investing in providing greater volumes of their outputs.[100][101] Shortly after initial energy price shocks caused by the Russian invasion of Ukraine had subsided, oil companies found that supply chain constrictions, already exacerbated by the ongoing global pandemic, supported price inelasticity, i.e., they began lowering prices to match the price of oil when it fell much more slowly than they had increased their prices when costs rose.[102]  The quantity theory of money has long been popular with libertarian-conservative critics of the Federal Reserve. During the COVID pandemic and its immediate aftermath, the M2 money supply increased at the fastest rate in decades, leading some to link the growth to the 2021-2023 inflation surge. Fed chairman Jerome Powell said in December 2021 that the once-strong link between the money supply and inflation \"ended about 40 years ago,\" due to financial innovations and deregulation. Previous Fed chairs Ben Bernanke and Alan Greenspan, had previously concurred with this position. The broadest measure of money supply, M3, increased about 45% from 2010 through 2015, far faster than GDP growth, yet the inflation rate declined during that period — the opposite of what monetarism would have predicted. A lower velocity of money than was historically the case[103] was also cited for a diminished effect of growth in the money supply on inflation.[104][105]  Additionally, there are theories about inflation accepted by economists outside of the mainstream. The Austrian School stresses that inflation is not uniform over all assets, goods, and services. Inflation depends on differences in markets and on where newly created money and credit enter the economy. Ludwig von Mises said that inflation should refer to an increase in the quantity of money, that is not offset by a corresponding increase in the need for money, and that price inflation will necessarily follow, always leaving a poorer nation.[106][107][108]  Inflation is the decrease in the purchasing power of a currency. That is, when the general level of prices rise, each monetary unit can buy fewer goods and services in aggregate. The effect of inflation differs on different sectors of the economy, with some sectors being adversely affected while others benefitting. For example, with inflation, those segments in society which own physical assets, such as property, stock etc., benefit from the price\/value of their holdings going up, when those who seek to acquire them will need to pay more for them. Their ability to do so will depend on the degree to which their income is fixed. For example, increases in payments to workers and pensioners often lag behind inflation, and for some people income is fixed. Also, individuals or institutions with cash assets will experience a decline in the purchasing power of the cash. Increases in the price level (inflation) erode the real value of money (the functional currency) and other items with an underlying monetary nature.  Debtors who have debts with a fixed nominal rate of interest will see a reduction in the \"real\" interest rate as the inflation rate rises. The real interest on a loan is the nominal rate minus the inflation rate. The formula R = N-I approximates the correct answer as long as both the nominal interest rate and the inflation rate are small. The correct equation is r = n\/i where r, n and i are expressed as ratios (e.g. 1.2 for +20%, 0.8 for −20%). As an example, when the inflation rate is 3%, a loan with a nominal interest rate of 5% would have a real interest rate of approximately 2% (in fact, it's 1.94%). Any unexpected increase in the inflation rate would decrease the real interest rate. Banks and other lenders adjust for this inflation risk either by including an inflation risk premium to fixed interest rate loans, or lending at an adjustable rate.  High or unpredictable inflation rates are regarded as harmful to an overall economy. They add inefficiencies in the market, and make it difficult for companies to budget or plan long-term. Inflation can act as a drag on productivity as companies are forced to shift resources away from products and services to focus on profit and losses from currency inflation.[56] Uncertainty about the future purchasing power of money discourages investment and saving.[109] Inflation hurts asset prices such as stock performance in the short-run, as it erodes non-energy corporates' profit margins and leads to central banks' policy tightening measures.[110] Inflation can also impose hidden tax increases. For instance, inflated earnings push taxpayers into higher income tax rates unless the tax brackets are indexed to inflation.  With high inflation, purchasing power is redistributed from those on fixed nominal incomes, such as some pensioners whose pensions are not indexed to the price level, towards those with variable incomes whose earnings may better keep pace with the inflation.[56] This redistribution of purchasing power will also occur between international trading partners. Where fixed exchange rates are imposed, higher inflation in one economy than another will cause the first economy's exports to become more expensive and affect the balance of trade. There can also be negative effects to trade from an increased instability in currency exchange prices caused by unpredictable inflation.  The real purchasing power of fixed payments is eroded by inflation unless they are inflation-adjusted to keep their real values constant. In many countries, employment contracts, pension benefits, and government entitlements (such as social security) are tied to a cost-of-living index, typically to the consumer price index.[127] A cost-of-living adjustment (COLA) adjusts salaries based on changes in a cost-of-living index.[128] It does not control inflation, but rather seeks to mitigate the consequences of inflation for those on fixed incomes. Salaries are typically adjusted annually in low inflation economies. During hyperinflation they are adjusted more often.[127] They may also be tied to a cost-of-living index that varies by geographic location if the employee moves.  Annual escalation clauses in employment contracts can specify retroactive or future percentage increases in worker pay which are not tied to any index. These negotiated increases in pay are colloquially referred to as cost-of-living adjustments (\"COLAs\") or cost-of-living increases because of their similarity to increases tied to externally determined indexes.  Monetary policy is the policy enacted by the monetary authorities (most frequently the central bank of a nation) to accomplish their objectives.[129] Among these, keeping inflation at a low and stable level is often a prominent objective, either directly via inflation targeting or indirectly, e.g. via a fixed exchange rate against a low-inflation currency area.  Historically, central banks and governments have followed various policies to achieve low inflation, employing various nominal anchors. Before World War I, the gold standard was prevalent, but was eventually found to be detrimental to economic stability and employment, not least during the Great Depression in the 1930s.[130] For the first decades after World War II, the Bretton Woods system initiated a fixed exchange rate system for most developed countries, tying their currencies to the US dollar, which again was directly convertible to gold.[131] The system disintegrated in the 1970s, however, after which the major currencies started floating against each other.[132] During the 1970s many central banks turned to a money supply target recommended by Milton Friedman and other monetarists, aiming for a stable growth rate of money to control inflation. However, it was found to be impractical because of the unstable relationship between monetary aggregates and other macroeconomic variables, and was eventually abandoned by all major economies.[130] In 1990, New Zealand as the first country ever adopted an official inflation target as the basis of its monetary policy, continually adjusting interest rates to steer the country's inflation rate towards its official target. The strategy was generally considered to work well, and central banks in most developed countries have over the years adapted a similar strategy.[133] As of 2023, the central banks of all G7 member countries can be said to follow an inflation target, including the European Central Bank and the Federal Reserve, who have adopted the main elements of inflation targeting without officially calling themselves inflation targeters.[133] In emerging countries fixed exchange rate regimes are still the most common monetary policy.[134]  From its first inception in New Zealand in 1990, direct inflation targeting as a monetary policy strategy has spread to become prevalent among developed countries. The basic idea is that the central bank perpetually adjusts interest rates to steer the country's inflation rate towards its official target. Via the monetary transmission mechanism interest rate changes affect aggregate demand in various ways, causing output and employment to respond.[135] Changes in employment and unemployment rates affect wage setting, leading to larger or smaller wage increases, depending on the direction of the interest rate adjustment. A changed rate of wage increases will transmit into changes in price setting – i.e. a change in the inflation rate. The relation between (un)employment and inflation is known as the Phillips curve.  In most OECD countries, the inflation target is usually about 2% to 3% (in developing countries like Armenia, the inflation target is higher, at around 4%).[136] Low (as opposed to zero or negative) inflation reduces the severity of economic recessions by enabling the labor market to adjust more quickly in a downturn, and reduces the risk that a liquidity trap prevents monetary policy from stabilizing the economy.[12][13]  Under a fixed exchange rate currency regime, a country's currency is tied in value to another single currency or to a basket of other currencies. A fixed exchange rate is usually used to stabilize the value of a currency, vis-a-vis the currency it is pegged to. It can also be used as a means to control inflation if the currency area tied to itself maintains low and stable inflation. However, as the value of the reference currency rises and falls, so does the currency pegged to it. This essentially means that the inflation rate in the fixed exchange rate country is determined by the inflation rate of the country the currency is pegged to. In addition, a fixed exchange rate prevents a government from using domestic monetary policy to achieve macroeconomic stability.[137]  As of 2023, Denmark is the only OECD country which maintains a fixed exchange rate (against the euro), but it is frequently used as a monetary policy strategy in developing countries.[134]  The gold standard is a monetary system in which a region's common medium of exchange is paper notes (or other monetary token) that are normally freely convertible into pre-set, fixed quantities of gold. The standard specifies how the gold backing would be implemented, including the amount of specie per currency unit. The currency itself has no innate value but is accepted by traders because it can be redeemed for the equivalent value of the commodity (specie). A U.S. silver certificate, for example, could be redeemed for an actual piece of silver.  Under a gold standard, the long term rate of inflation (or deflation) would be determined by the growth rate of the supply of gold relative to total output.[138] Critics argue that this will cause arbitrary fluctuations in the inflation rate, and that monetary policy would essentially be determined by an intersection of however much new gold was produced by mining and changing demand for gold for practical uses.[139][140] The gold standard was historically found to make it more difficult to stabilize employment levels and avoid recessions and was eventually abandoned everywhere.[130][141]  Another method attempted in the past have been wage and price controls (\"incomes policies\"). Temporary price controls may be used as a complement to other policies to fight inflation; price controls may make disinflation faster, while reducing the need for unemployment to reduce inflation. If price controls are used during a recession, the kinds of distortions that price controls cause may be lessened. However, economists generally advise against the imposition of price controls.[142][143][144]  Wage and price controls, in combination with rationing, have been used successfully in wartime environments. However, their use in other contexts is far more mixed. Notable failures of their use include the 1972 imposition of wage and price controls by Richard Nixon. More successful examples include the Prices and Incomes Accord in Australia and the Wassenaar Agreement in the Netherlands.  In general, wage and price controls are regarded as a temporary and exceptional measures, only effective when coupled with policies designed to reduce the underlying causes of inflation during the wage and price control regime, for example, winning the war being fought. "},"meta":{},"created_at":"2025-03-22T14:25:42.276677Z","updated_at":"2025-03-22T14:25:42.276677Z","inner_id":43,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":52,"annotations":[{"id":52,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.312403Z","updated_at":"2025-03-22T14:25:42.312403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"4f425bc7-2a9f-476c-a180-d23eb51180f8","import_id":null,"last_action":null,"bulk_created":false,"task":52,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  A sovereign wealth fund (SWF), or sovereign investment fund, is a state-owned investment fund that invests in real and financial assets such as stocks, bonds, real estate, precious metals, or in alternative investments such as private equity funds or hedge funds. Sovereign wealth funds invest globally. Most SWFs are funded by revenues from commodity exports or from foreign exchange reserves held by the central bank.  Some sovereign wealth funds may be held by a central bank, which accumulates the funds in the course of its management of a nation's banking system; this type of fund is usually of major economic and fiscal importance.  Other sovereign wealth funds are simply the state savings that are invested by various entities for investment return, and that may not have a significant role in fiscal management.  The accumulated funds may have their origin in, or may represent, foreign currency deposits, gold, special drawing rights (SDRs) and International Monetary Fund (IMF) reserve positions held by central banks and monetary authorities, along with other national assets such as pension investments, oil funds, or other industrial and financial holdings. These are assets of the sovereign nations that are typically held in domestic and different reserve currencies (such as the dollar, euro, pound, and yen). Such investment management entities may be set up as official investment companies, state pension funds, or sovereign funds, among others.  There have been attempts to distinguish funds held by sovereign entities from foreign-exchange reserves held by central banks. Sovereign wealth funds can be characterized as maximizing long-term return, with foreign exchange reserves serving short-term \"currency stabilization\", and liquidity management. Many central banks in recent years possess reserves massively in excess of needs for liquidity or foreign exchange management.  Moreover, it is widely believed most have diversified hugely into assets other than short-term, highly liquid monetary ones, though almost no data is publicly available to back up this assertion.  The term \"sovereign wealth fund\" was first used in 2005 by Andrew Rozanov in an article entitled, \"Who holds the wealth of nations?\" in the Central Banking Journal.[1] The previous edition of the journal described the shift from traditional reserve management to sovereign wealth management; subsequently the term gained widespread use as the spending power of global officialdom has rocketed upward.[citation needed]  China's sovereign wealth funds entered global markets in 2007.[2]: 4  Since then, their scale and scope have expanded significantly.[2]: 4   SWFs were the first institutions to use sovereign capital in an effort to contain the financial damage in the early stages of the 2007-2008 global financial crisis.[2]: 1–2  SWFs are able to react quickly in such circumstances because unlike regulators, SWFs actively participate in the market.[2]: 2   SWFs grew rapidly between 2008 and 2021, with global assets under management by these funds increasing from approximately $4 trillion to more than $10 trillion.[2]: 3   SWFs invest in a variety of asset classes such as stocks, bonds, real estate, private equity and hedge funds.  Many sovereign funds are directly investing in institutional real estate.  According to the Sovereign Wealth Fund Institute's transaction database around US$9.26 billion in direct sovereign wealth fund transactions were recorded in institutional real estate for the last half of 2012.[3] In the first half of 2014, global sovereign wealth fund direct deals amounted to $50.02 billion according to the SWFI.[4]  Sovereign wealth funds have existed for more than a century, but since 2000, the number of sovereign wealth funds has increased dramatically. The first SWFs were non-federal U.S. state funds established in the mid-19th century to fund specific public services.[5] The U.S. state of Texas was thus the first to establish such a scheme, to fund public education. The Permanent School Fund (PSF) was created in 1854 to benefit primary and secondary schools, with the Permanent University Fund (PUF) following in 1876 to benefit universities. The PUF was endowed with public lands, the ownership of which the state retained by terms of the 1845 annexation treaty between the Republic of Texas and the United States. While the PSF was first funded by an appropriation from the state legislature, it also received public lands at the same time that the PUF was created. The first SWF established for a sovereign state is the Kuwait Investment Authority, a commodity SWF created in 1953 from oil revenues before Kuwait gained independence from the United Kingdom. As of July 2023, Kuwait's Sovereign Wealth Fund, or locally known as Ajyal Fund, is now worth $853 billion.[6]  Another early registered SWF is the Revenue Equalization Reserve Fund of Kiribati. Since its creation in 1956, when the British administration of the Gilbert Islands in Micronesia put a levy on the export of phosphates used in fertilizer, the fund has grown to $520 million.[7]  SWFs are typically created when governments have budgetary surpluses and have little or no international debt.[dubious – discuss] It is not always possible or desirable to hold this excess liquidity as money or to channel it into immediate consumption. This is especially the case when a nation depends on raw material exports like oil, copper or diamonds. In such countries, the main reason for creating a SWF is because of the properties of resource revenue: high volatility of resource prices, unpredictability of extraction, and exhaustibility of resources.  SWFs are primarily commodity-based and many have been established by oil-rich states.[2]: 5  SWFs of China are a notable exception to this more typical model.[2]: 5   Stabilization SWFs are created to reduce the volatility of government revenues, to counter the boom-bust cycles' adverse effect on government spending and the national economy.  Savings SWFs build up savings for future generations.  One such fund is the Government Pension Fund of Norway. It is believed that SWFs in resource-rich countries can help avoid resource curse, but the literature on this question is controversial. Governments may be able to spend the money immediately, but risk causing the economy to overheat, e.g., in Hugo Chávez's Venezuela or Shah-era Iran. In such circumstances, saving money to spend during a period of low inflation is often desirable.  Other reasons for creating SWFs may be economic, or strategic, such as war chests for uncertain times. For example, the Kuwait Investment Authority during the Gulf War managed excess reserves above the level needed for currency reserves (although many central banks do that now). The Government of Singapore Investment Corporation, Temasek Holdings, or Mubadala are partially the expression of a desire to bolster their countries' standing as an international financial centre. The Korea Investment Corporation has since been similarly managed.  Sovereign wealth funds invest in all types of companies and assets, including startups like Xiaomi and renewable energy companies like Bloom Energy.[8]  According to a 2014 study, SWFs are not created for reasons related to reserve accumulation and commodity-export specialization. Rather, the diffusion of SWF can best be understood as a fad whereby certain governments consider it fashionable to create SWFs and are influenced by what their peers are doing.[9]  As market participants, SWFs influence other institutional investors, who may see investments made alongside SWFs as inherently safer.[2]: 9  This effect can be seen with increasing frequency, especially with regard to investments made by the Government Pension Fund of Norway, Abu Dhabi Investment Authority, and Temasek Holdings, and China Investment Corporation.[2]: 9  SLFs help facilitate a state's ability to use its selective equity investments to promote its industrial policies and strategic interests.[2]: 9   The growth of sovereign wealth funds is attracting close attention because:  The governments of SWFs commit to follow certain rules:  A number of transparency indices sprang up before the Santiago Principles, some more stringent than others.[citation needed]  To address these concerns, some of the world's main SWFs came together in a summit in Santiago, Chile, on 2–3 September 2008. Under the leadership of the IMF, they formed a temporary International Working Group of Sovereign Wealth Funds. This working group then drafted the 24 Santiago Principles, to set out a common global set of international standards regarding transparency, independence, and accountability in the way that SWFs operate.[18][19] These were published after being presented to the IMF International Monetary Financial Committee on 11 October 2008.[19] They also considered a standing committee to represent them, and so a new organisation, the International Forum of Sovereign Wealth Funds was set up to maintain the new standards going forward and represent them in international policy debates.[20]  As of 2016, 30 funds[21] have formally signed up to the Principles, representing collectively 80% of the assets managed by sovereign funds globally or US$5.5 trillion.[22]  Natural resource-rich developing economies are typically encouraged to adopt good governance standards for sovereign wealth funds, such as the Santiago Principles, which emphasize transparency, accountability, and sound investment practices. This approach is often preferred over local content policies, which can foster corruption and rent-seeking behavior in contexts with weak governance.[23]  Assets under management of SWFs amounted to $7.94 trillion as of 24 December 2020.[24]  Countries with SWFs funded by oil and gas exports, totaled $5.4 trillion as of 2020.[25] Non-commodity SWFs are typically funded by transfer of assets from official foreign exchange reserves, and in some cases from government budget surpluses and privatization revenues. Middle Eastern and Asian countries account for 77% of all SWFs.  Numerous SWFs have gone bust throughout history. The most notable ones have been Algeria's FRR, Brazil's FSB, Ecuador's numerous SWF arrangements, Papua New Guinea's MRSF, and Venezuela's FIEM and FONDEN. The main reason why these funds have been exhausted is due to political instability, while economic determinants generally play a less important role.[26]  SWFs in unstable countries may provoke risks for recipient states of SWF investments, given that the instability in SWF-sponsor countries makes those investments uncertain and likely to be disinvested to weather political risk in the short-term.[citation needed]  Highly stable countries, such as Denmark, Qatar, China, or Australia are less likely to experience SWF depletion precisely because of their political stability.[citation needed] "},"meta":{},"created_at":"2025-03-22T14:25:42.276677Z","updated_at":"2025-03-22T14:25:42.276677Z","inner_id":44,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":53,"annotations":[{"id":53,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.312403Z","updated_at":"2025-03-22T14:25:42.312403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"a5374e92-6fd7-4c89-b96e-ced38e1f33b8","import_id":null,"last_action":null,"bulk_created":false,"task":53,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Investment banking is an advisory-based financial service for institutional investors, corporations, governments, and similar clients. Traditionally associated with corporate finance, such a bank might assist in raising financial capital by underwriting or acting as the client's agent in the issuance of debt or equity securities. An investment bank may also assist companies involved in mergers and acquisitions (M&A) and provide ancillary services such as market making, trading of derivatives and equity securities, FICC services (fixed income instruments, currencies, and commodities) or research (macroeconomic, credit or equity research). Most investment banks maintain prime brokerage and asset management departments in conjunction with their investment research businesses. As an industry, it is broken up into the Bulge Bracket (upper tier), Middle Market (mid-level businesses), and boutique market (specialized businesses).  Unlike commercial banks and retail banks, investment banks do not take deposits. The revenue model of an investment bank comes mostly from the collection of fees for advising on a transaction, contrary to a commercial or retail bank. From the passage of Glass–Steagall Act in 1933 until its repeal in 1999 by the Gramm–Leach–Bliley Act, the United States maintained a separation between investment banking and commercial banks. Other industrialized countries, including G7 countries, have historically not maintained such a separation. As part of the Dodd–Frank Wall Street Reform and Consumer Protection Act of 2010 (Dodd–Frank Act of 2010), the Volcker Rule asserts some institutional separation of investment banking services from commercial banking.[1]  All investment banking activity is classed as either \"sell side\" or \"buy side\". The \"sell side\" involves trading securities for cash or for other securities (e.g. facilitating transactions, market-making), or the promotion of securities (e.g. underwriting, research, etc.). The \"buy side\" involves the provision of advice to institutions that buy investment services. Private equity funds, mutual funds, life insurance companies, unit trusts, and hedge funds are the most common types of buy-side entities.  An investment bank can also be split into private and public functions with a screen separating the two to prevent information from crossing. The private areas of the bank deal with private insider information that may not be publicly disclosed, while the public areas, such as stock analysis, deal with public information. An advisor who provides investment banking services in the United States must be a licensed broker-dealer and subject to U.S. Securities and Exchange Commission (SEC) and Financial Industry Regulatory Authority (FINRA) regulation.[2]  The Dutch East India Company was the first company to issue bonds and shares of stock to the general public. It was also the first publicly traded company, being the first company to be publicly listed.[3][4]  Investment banking has changed over the years, beginning as a partnership firm focused on underwriting security issuance, i.e. initial public offerings (IPOs) and secondary market offerings, brokerage, and mergers and acquisitions, and evolving into a \"full-service\" range including securities research, proprietary trading, and investment management.[5] In the 21st century, the SEC filings of the major independent investment banks such as Goldman Sachs and Morgan Stanley reflect three product segments:  In the United States, commercial banking and investment banking were separated by the Glass–Steagall Act, which was repealed in 1999. The repeal led to more \"universal banks\" offering an even greater range of services. Many large commercial banks have therefore developed investment banking divisions through acquisitions and hiring. Notable full-service investment banks with a significant investment banking division (IBD) include JPMorgan Chase, Bank of America, Citigroup, Deutsche Bank, UBS (Acquired Credit Suisse), and Barclays.  After the financial crisis of 2007–08 and the subsequent passage of the Dodd-Frank Act of 2010, regulations have limited certain investment banking operations, notably with the Volcker Rule's restrictions on proprietary trading.[7]  The traditional service of underwriting security issues has declined as a percentage of revenue. As far back as 1960, 70% of Merrill Lynch's revenue was derived from transaction commissions while \"traditional investment banking\" services accounted for 5%. However, Merrill Lynch was a relatively \"retail-focused\" firm with a large brokerage network.[7]  Investment banking is split into front office, middle office, and back office activities. While large service investment banks offer all lines of business, both \"sell side\" and \"buy side\", smaller sell-side advisory firms such as boutique investment banks and small broker-dealers focus on niche segments within investment banking and sales\/trading\/research, respectively.  For example, Evercore (NYSE:EVR) acquired ISI International Strategy & Investment (ISI) in 2014 to expand their revenue into research-driven equity sales and trading.[8]  Investment banks offer services to both corporations issuing securities and investors buying securities. For corporations, investment bankers offer information on when and how to place their securities on the open market, a highly regulated process by the SEC to ensure transparency is provided to investors. Therefore, investment bankers play a very important role in issuing new security offerings.[7][9]  Front office is generally described as a revenue-generating role. There are two main areas within front office: investment banking and markets.[10]  Corporate finance is the aspect of investment banks which involves helping customers raise funds in capital markets and giving advice on mergers and acquisitions (M&A);[12] transactions in which capital is raised for the corporation include those listed aside.[12] This work may involve, i.a., subscribing investors to a security issuance, coordinating with bidders, or negotiating with a merger target.  A pitch book, also called a confidential information memorandum (CIM), is a document that highlights the relevant financial information, past transaction experience, and background of the deal team to market the bank to a potential M&A client; if the pitch is successful, the bank arranges the deal for the client.[13]  Recent legal and regulatory developments in the U.S. will likely alter the makeup of the group of arrangers and financiers willing to arrange and provide financing for certain highly leveraged transactions.[14][15]  On behalf of the bank and its clients, a large investment bank's primary function is buying and selling products.[16]  Sales is the term for the investment bank's sales force, whose primary job is to call on institutional and high-net-worth investors to suggest trading ideas (on a caveat emptor basis) and take orders. Sales desks then communicate their clients' orders to the appropriate bank department, which can price and execute trades, or structure new products that fit a specific need.  Sales make deals tailored to their corporate customers' needs, that is, their terms are often specific. Focusing on their customer relationship, they may deal on the whole range of asset types.  (In distinction, trades negotiated by market-makers usually bear standard terms; in market making, traders will buy and sell financial products with the goal of making money on each trade. See under trading desk.)  Structuring has been a relatively recent activity as derivatives have come into play, with highly technical and numerate employees working on creating complex financial products which typically offer much greater margins and returns than underlying cash securities, so-called \"yield enhancement\". In 2010, investment banks came under pressure as a result of selling complex derivatives contracts to local municipalities in Europe and the US.[17]  Strategists advise external as well as internal clients on the strategies that can be adopted in various markets. Ranging from derivatives to specific industries, strategists place companies and industries in a quantitative framework with full consideration of the macroeconomic scene. This strategy often affects the way the firm will operate in the market, the direction it would like to take in terms of its proprietary and flow positions, the suggestions salespersons give to clients, as well as the way structurers create new products.   Banks also undertake risk through proprietary trading, performed by a special set of traders who do not interface with clients and through \"principal risk\"—risk undertaken by a trader after he buys or sells a product to a client and does not hedge his total exposure.  Here, and in general, banks seek to maximize profitability for a given amount of risk on their balance sheet.  Note here that the FRTB framework has underscored the distinction between the \"Trading book\" and the \"Banking book\"  - i.e. assets intended for active trading, as opposed to assets expected to be held to maturity -  and market risk capital requirements will differ accordingly.  The necessity for numerical ability in sales and trading has created jobs for physics, computer science, mathematics, and engineering PhDs who act as \"front office\" quantitative analysts.  The securities research division reviews companies and writes reports about their prospects, often with \"buy\", \"hold\", or \"sell\" ratings. Investment banks typically have sell-side analysts which cover various industries. Their sponsored funds or proprietary trading offices will also have buy-side research. Research also covers credit risk, fixed income, macroeconomics, and quantitative analysis, all of which are used internally and externally to advise clients; alongside \"Equity\", these may be separate \"groups\".  The research group(s) typically provide a key service in terms of advisory and strategy.  While the research division may or may not generate revenue (based on the specific compliance policies at different banks), its resources are used to assist traders in trading, the sales force in suggesting ideas to customers, and investment bankers by covering their clients.[18] Research also serves outside clients with investment advice (such as institutional investors and high-net-worth individuals) in the hopes that these clients will execute suggested trade ideas through the sales and trading division of the bank, and thereby generate revenue for the firm.  With MiFID II requiring sell-side research teams in banks to charge for research, the business model for research is increasingly becoming revenue-generating. External rankings of researchers are becoming increasingly important, and banks have started the process of monetizing research publications, client interaction times, meetings with clients etc.  There is a potential conflict of interest between the investment bank and its analysis, in that published analysis can impact the performance of a security (in the secondary markets or an initial public offering) or influence the relationship between the banker and its corporate clients, and vice versa regarding material non-public information (MNPI), thereby affecting the bank's profitability.[19] See also Chinese wall § Finance.  This area of the bank includes treasury management, internal controls (such as Risk), and internal corporate strategy.  Corporate treasury is responsible for an investment bank's funding, capital structure management, and liquidity risk monitoring; it is (co)responsible for the bank's funds transfer pricing (FTP) framework.  Internal control tracks and analyzes the capital flows of the firm, the finance division is the principal adviser to senior management on essential areas such as controlling the firm's global risk exposure and the profitability and structure of the firm's various businesses via dedicated trading desk product control teams. In the United States and United Kingdom, a comptroller (or financial controller) is a senior position, often reporting to the chief financial officer.  Risk management involves analyzing the market and credit risk that an investment bank or its clients take onto their balance sheet during transactions or trades.   Middle office \"Credit Risk\" focuses around capital markets activities, such as syndicated loans, bond issuance, restructuring, and leveraged finance.  These are not considered \"front office\" as they tend not to be client-facing and rather 'control' banking functions from taking too much risk.  \"Market Risk\" is the control function for the Markets' business and conducts review of sales and trading activities utilizing the VaR model.  Other Middle office \"Risk Groups\" include country risk, operational risk, and counterparty risks which may or may not exist on a bank to bank basis.  Front office risk teams, on the other hand, engage in revenue-generating activities involving debt structuring, restructuring, syndicated loans, and securitization for clients such as corporates, governments, and hedge funds.  Here \"Credit Risk Solutions\", are a key part of capital market transactions, involving debt structuring, exit financing, loan amendment, project finance, leveraged buy-outs, and sometimes portfolio hedging.  The \"Market Risk Team\" provides services to investors via derivative solutions, portfolio management, portfolio consulting, and risk advisory.  Well-known \"Risk Groups\" are at JPMorgan Chase, Morgan Stanley, Goldman Sachs and Barclays.  J.P. Morgan IB Risk works with investment banking to execute transactions and advise investors, although its Finance & Operation risk groups focus on middle office functions involving internal, non-revenue generating, operational risk controls.[20][21][22] The credit default swap, for instance, is a famous credit risk hedging solution for clients invented by J.P. Morgan's Blythe Masters during the 1990s.  The Loan Risk Solutions group[23] within Barclays' investment banking division and Risk Management and Financing group[24] housed in Goldman Sach's securities division are client-driven franchises.  Risk management groups such as credit risk, operational risk, internal risk control, and legal risk are restrained to internal business functions — including firm balance-sheet risk analysis and assigning the trading cap — that are independent of client needs, even though these groups may be responsible for deal approval that directly affects capital market activities.  Similarly, the Internal corporate strategy group, tackling firm management and profit strategy, unlike corporate strategy groups that advise clients, is non-revenue regenerating yet a key functional role within investment banks.  This list is not a comprehensive summary of all middle-office functions within an investment bank, as specific desks within front and back offices may participate in internal functions.[25]  The back office data-checks trades that have been conducted, ensuring that they are not wrong, and transacts the required transfers. Many banks have outsourced operations. It is, however, a critical part of the bank.[citation needed]  Every major investment bank has considerable amounts of in-house software, created by the technology team, who are also responsible for technical support. Technology has changed considerably in the last few years as more sales and trading desks are using electronic processing. Some trades are initiated by complex algorithms for hedging purposes.  Firms are responsible for compliance with local and foreign government regulations and internal regulations.  The investment banking industry can be broken up into Bulge Bracket (upper tier), Middle Market (mid-level businesses), and boutique market (specialized businesses) categories. There are various trade associations throughout the world which represent the industry in lobbying, facilitate industry standards, and publish statistics. The International Council of Securities Associations (ICSA) is a global group of trade associations.  In the United States, the Securities Industry and Financial Markets Association (SIFMA) is likely the most significant; however, several of the large investment banks are members of the American Bankers Association Securities Association (ABASA),[27] while small investment banks are members of the National Investment Banking Association (NIBA).  In Europe, the European Forum of Securities Associations was formed in 2007 by various European trade associations.[28] Several European trade associations (principally the London Investment Banking Association and the European SIFMA affiliate) combined in November 2009 to form the Association for Financial Markets in Europe (AFME).[29]  In the securities industry in China, the Securities Association of China is a self-regulatory organization whose members are largely investment banks.  Global investment banking revenue increased for the fifth year running in 2007, to a record US$84 billion, which was up 22% on the previous year and more than double the level in 2003.[30] Subsequent to their exposure to United States sub-prime securities investments, many investment banks have experienced losses. As of late 2012, global revenues for investment banks were estimated at $240 billion, down about a third from 2009, as companies pursued less deals and traded less.[31] Differences in total revenue are likely due to different ways of classifying investment banking revenue, such as subtracting proprietary trading revenue.  In terms of total revenue, SEC filings of the major independent investment banks in the United States show that investment banking (defined as M&A advisory services and security underwriting) made up only about 15–20% of total revenue for these banks from 1996 to 2006, with the majority of revenue (60+% in some years) brought in by \"trading\" which includes brokerage commissions and proprietary trading; the proprietary trading is estimated to provide a significant portion of this revenue.[6]  The United States generated 46% of global revenue in 2009, down from 56% in 1999. Europe (with Middle East and Africa) generated about a third, while Asian countries generated the remaining 21%.[30]: 8  The industry is heavily concentrated in a small number of major financial centers, including New York City, City of London, Frankfurt, Hong Kong, Singapore, and Tokyo. The majority of the world's largest Bulge Bracket investment banks and their investment managers are headquartered in New York and are also important participants in other financial centers.[32] The city of London has historically served as a hub of European M&A activity, often facilitating the most capital movement and corporate restructuring in the area.[33][34] Meanwhile, Asian cities are receiving a growing share of M&A activity.  According to estimates published by the International Financial Services London, for the decade prior to the financial crisis in 2008, M&A was a primary source of investment banking revenue, often accounting for 40% of such revenue, but dropped during and after the financial crisis.[30]: 9  Equity underwriting revenue ranged from 30% to 38%, and fixed-income underwriting accounted for the remaining revenue.[30]: 9   Revenues have been affected by the introduction of new products with higher margins; however, these innovations are often copied quickly by competing banks, pushing down trading margins. For example, brokerages commissions for bond and equity trading is a commodity business, but structuring and trading derivatives have higher margins because each over-the-counter contract has to be uniquely structured and could involve complex pay-off and risk profiles. One growth area is private investment in public equity (PIPEs, otherwise known as Regulation D or Regulation S). Such transactions are privately negotiated between companies and accredited investors.  Banks also earned revenue by securitizing debt, particularly mortgage debt prior to the financial crisis. Investment banks have become concerned that lenders are securitizing in-house, driving the investment banks to pursue vertical integration by becoming lenders, which has been allowed in the United States since the repeal of the Glass–Steagall Act in 1999.[35]  According to The Wall Street Journal, in terms of total M&A advisory fees for the whole of 2020, the top ten investment banks were as listed in the table below.[36] Many of these firms belong either to the Bulge Bracket (upper tier), Middle Market (mid-level businesses), or are elite boutique investment banks (independent advisory investment banks).  The above list is just a ranking of the advisory arm (M&A advisory, syndicated loans, equity capital markets, and debt capital markets) of each bank and does not include the generally much larger portion of revenues from sales & trading and asset management. Mergers and acquisitions and capital markets are also often covered by The Wall Street Journal and Bloomberg.  The financial crisis of 2007–2008 led to the collapse of several notable investment banks, such as the bankruptcy of Lehman Brothers (one of the largest investment banks in the world) and the hurried fire sale of Merrill Lynch and the much smaller Bear Stearns to much larger banks, which effectively rescued them from bankruptcy. The entire financial services industry, including numerous investment banks, was bailed out by government taxpayer funded loans through the Troubled Asset Relief Program (TARP). Surviving U.S. investment banks such as Goldman Sachs and Morgan Stanley converted to traditional bank holding companies to accept TARP relief.[38] Similar situations have occurred across the globe with countries rescuing their banking industry. Initially, banks received part of a $700 billion TARP intended to stabilize the economy and thaw the frozen credit markets.[39] Eventually, taxpayer assistance to banks reached nearly $13 trillion—most without much scrutiny—[40] lending did not increase,[41] and credit markets remained frozen.[42]  The crisis led to questioning of the investment banking business model[43] without the regulation imposed on it by Glass–Steagall.[neutrality is disputed] Once Robert Rubin, a former co-chairman of Goldman Sachs, became part of the Clinton administration and deregulated banks, the previous conservatism of underwriting established companies and seeking long-term gains was replaced by lower standards and short-term profit.[44] Formerly, the guidelines said that in order to take a company public, it had to be in business for a minimum of five years and it had to show profitability for three consecutive years. After deregulation, those standards were gone, but small investors did not grasp the full impact of the change.[44]  A number of former Goldman Sachs top executives, such as Henry Paulson and Ed Liddy, were in high-level positions in government and oversaw the controversial taxpayer-funded bank bailout.[44] The TARP Oversight Report released by the Congressional Oversight Panel found that the bailout tended to encourage risky behavior and \"corrupt[ed] the fundamental tenets of a market economy\".[45]  Under threat of a subpoena, Goldman Sachs revealed that it received $12.9 billion in taxpayer aid, $4.3 billion of which was then paid out to 32 entities, including many overseas banks, hedge funds, and pensions.[46] The same year it received $10 billion in aid from the government, it also paid out multimillion-dollar bonuses; the total paid in bonuses was $4.82 billion.[47][48] Similarly, Morgan Stanley received $10 billion in TARP funds and paid out $4.475 billion in bonuses.[49]  The investment banking industry, including boutique investment banks, have come under criticism for a variety of reasons, including perceived conflicts of interest, overly large pay packages, cartel-like or oligopolistic behavior, taking both sides in transactions, and more.[50] Investment banking has also been criticized for its opacity.[51] However, the lack of transparency inherent to the investment banking industry is largely due to the necessity to abide by the non-disclosure agreement (NDA) signed with the client. The accidental leak of confidential client data can cause a bank to incur significant monetary losses.  Conflicts of interest may arise between different parts of a bank, creating the potential for market manipulation, according to critics. Authorities that regulate investment banking, such as the Financial Conduct Authority (FCA) in the United Kingdom and the SEC in the United States, require that banks impose a \"Chinese wall\" to prevent communication between investment banking on one side and equity research and trading on the other. However, critics say such a barrier does not always exist in practice. Independent advisory firms that exclusively provide corporate finance advice argue that their advice is not conflicted, unlike bulge bracket banks.  Conflicts of interest often arise in relation to investment banks' equity research units, which have long been part of the industry. A common practice is for equity analysts to initiate coverage of a company to develop relationships that lead to highly profitable investment banking business. In the 1990s, many equity researchers allegedly traded positive stock ratings for investment banking business. Alternatively, companies may threaten to divert investment banking business to competitors unless their stock was rated favorably. Laws were passed to criminalize such acts, and increased pressure from regulators and a series of lawsuits, settlements, and prosecutions curbed this business to a large extent following the 2001 stock market tumble after the dot-com bubble.  Philip Augar, author of The Greed Merchants, said in an interview that, \"You cannot simultaneously serve the interest of issuer clients and investing clients. And it’s not just underwriting and sales; investment banks run proprietary trading operations that are also making a profit out of these securities.\"[50]  Many investment banks also own retail brokerages. During the 1990s, some retail brokerages sold consumers securities which did not meet their stated risk profile. This behavior may have led to investment banking business or even sales of surplus shares during a public offering to keep public perception of the stock favorable.  Since investment banks engage heavily in trading for their own account, there is always the temptation for them to engage in some form of front running—the illegal practice whereby a broker executes orders for their own account before filling orders previously submitted by their customers, thereby benefiting from any changes in prices induced by those orders.  Documents under seal in a decade-long lawsuit concerning eToys.com's IPO but obtained by New York Times' Wall Street Business columnist Joe Nocera alleged that IPOs managed by Goldman Sachs and other investment bankers involved asking for kickbacks from their institutional clients who made large profits flipping IPOs which Goldman had intentionally undervalued. Depositions in the lawsuit alleged that clients willingly complied with these demands because they understood it was necessary to participate in future hot issues.[52] Reuters Wall Street correspondent Felix Salmon retracted his earlier, more conciliatory statements on the subject and said he believed that the depositions show that companies going public and their initial consumer stockholders are both defrauded by this practice, which may be widespread throughout the IPO finance industry.[53] The case is ongoing, and the allegations remain unproven.  Nevertheless, the controversy around investment banks intentionally underpricing IPOs for their self-interest has become a highly debated subject. The cause for concern is that the investment banks advising on the IPOs have the incentive to serve institutional investors on the buy-side, creating a valid reason for a potential conflict of interest.[54]  The post-IPO spike in the stock price of newly listed companies has only worsened the problem, with one of the leading critics being high-profile venture capital (VC) investor, Bill Gurley.[55]  Investment banking has been criticized for the enormous pay packages awarded to those who work in the industry. According to Bloomberg Wall Street's five biggest firms paid over $3 billion to their executives from 2003 to 2008, \"while they presided over the packaging and sale of loans that helped bring down the investment-banking system\".[56]  In 2003-2007, pay packages included $172 million for Merrill Lynch CEO Stanley O'Neal before the bank was bought by Bank of America, and $161 million for Bear Stearns' James Cayne before the bank collapsed and was sold to JPMorgan Chase.[56]Such pay arrangements attracted the ire of Democrats and Republicans in the United States Congress, who demanded limits on executive pay in 2008 when the U.S. government was bailing out the industry with a $700 billion financial rescue package.[56]  Writing in the Global Association of Risk Professionals journal, Aaron Brown, a vice president at Morgan Stanley, says \"By any standard of human fairness, of course, investment bankers make obscene amounts of money.\"[50] "},"meta":{},"created_at":"2025-03-22T14:25:42.276677Z","updated_at":"2025-03-22T14:25:42.276677Z","inner_id":45,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":54,"annotations":[{"id":54,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.312403Z","updated_at":"2025-03-22T14:25:42.312403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"c552c0e4-925c-4035-acb3-87c60f87214d","import_id":null,"last_action":null,"bulk_created":false,"task":54,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Heterodox  Supply-side economics is a macroeconomic theory postulating that economic growth can be most effectively fostered by lowering taxes, decreasing regulation, and allowing free trade.[1][2] According to supply-side economics theory, consumers will benefit from greater supply of goods and services at lower prices, and employment will increase.[3] Supply-side fiscal policies are designed to increase aggregate supply, as opposed to aggregate demand, thereby expanding output and employment while lowering prices. Such policies are of several general varieties:  A basis of supply-side economics is the Laffer curve, a theoretical relationship between rates of taxation and government revenue.[5][6][7][8]  The Laffer curve suggests that when the tax level is too high, lowering tax rates will boost government revenue through higher economic growth, though the level at which rates are deemed \"too high\" is disputed.[9][10][11] A 2012 poll of leading economists found none agreed that reducing the US federal income tax rate would result in higher annual tax revenue within five years.[12] Critics also argue that several large tax cuts in the United States over the last 40 years have not increased revenue.[13][14][15]  The term \"supply-side economics\" was thought for some time to have been coined by the journalist Jude Wanniski in 1975; according to Robert D. Atkinson, the term \"supply side\" was first used in 1976 by Herbert Stein (a former economic adviser to President Richard Nixon) and only later that year was this term repeated by Jude Wanniski.[16] The term alludes to ideas of the economists Robert Mundell and Arthur Laffer. The term is contrasted with demand-side economics.  Supply-side economics developed in response to the stagflation of the 1970s.[18] It drew on a range of non-Keynesian economic thought, including the Chicago School and New Classical School.[19][20] Bruce Bartlett, an advocate of supply-side economics, traced the school of thought's intellectual descent from the philosophers Ibn Khaldun and David Hume, satirist Jonathan Swift, political economist Adam Smith and United States Secretary of the Treasury Alexander Hamilton.[21] In 2007, Bartlett stated:  Today, hardly any economist believes what the Keynesians believed in the 1970s and most accept the basic ideas of supply-side economics – that incentives matter, that high tax rates are bad for growth, and that inflation is fundamentally a monetary phenomenon. Consequently, there is no longer any meaningful difference between supply-side economics and mainstream economics.  ...   Today, supply-side economics has become associated with an obsession for cutting taxes under any and all circumstances. No longer do its advocates in Congress and elsewhere confine themselves to cutting marginal tax rates – the tax on each additional dollar earned – as the original supply-siders did. Rather, they support even the most gimmicky, economically dubious tax cuts with the same intensity. ... today it is common to hear tax cutters claim, implausibly, that all tax cuts raise revenue.[22] Current day advocates of supply-side economic policies claim that lower tax rates produce macroeconomic benefits and emphasize this benefit rather than their traditional ideological Classical liberals opposition to taxation  because they opposed government in general. Their traditional claim was that each man had a right to himself and his property and therefore taxation was immoral and of questionable legal grounding.[23] On the other hand, supply-side economists argued that the alleged collective benefit (i.e. increased economic output and efficiency) provided the main impetus for tax cuts. As in classical economics, supply-side economics proposed that production or supply is the key to economic prosperity and that consumption or demand is merely a secondary consequence. Early on, this idea had been summarized in Say's law of markets, which states: \"A product is no sooner created, than it, from that instant, affords a market for other products to the full extent of its own value.\" or, in other words, production (supply) must first occur to enable economic activity or trade.[citation needed]  Supply-side economics rose in popularity among Republican Party politicians from 1977 onwards. Prior to 1977, Republicans were more split on tax reduction, with some worrying that tax cuts would fuel inflation and exacerbate deficits.[24] In 1978, Jude Wanniski published The Way the World Works in which he laid out the central thesis of supply-side economics[25] and detailed the failure of high tax rate progressive income tax systems and United States monetary policy under Richard Nixon and Jimmy Carter in the 1970s. Wanniski advocated lower tax rates and a return to some kind of gold standard, similar to the 1944–1971 Bretton Woods System that Nixon abandoned.  James D. Gwartney and Richard L. Stroup provide a definition of supply-side economics as the belief that adjustments in marginal tax rates have significant effects on the total supply.[26] Gwartney and Stroup said \"that the supply-side argument provided the foundation for the Reagan tax policy, which led to significant reductions in marginal tax rates in the United States during the 1980s\".[26]  Barry P. Bosworth has provided another definition by presenting the supply-side economics from two perspectives:  Supply-side economics has originated as an alternative to Keynesian economics, which focused macroeconomic policy on management of final demand.[28] Demand-side economics relies on a fixed-price view of the economy, where the demand plays a key role in defining the future supply growth, which also allows for incentive implications of investment.[27]  The Keynesian policy approaches focus on demand management as a major instrument to affect aggregate production and GNP, while Monetarism focuses on management of monetary aggregates and credit. Unlike supply-side economics, demand-side economics is based on the assumption that increases in GNP result from increased spending.[29]  Traditional policy approaches were challenged by the theory of supply-side economics in the Reagan Administration of the 1980s. It claims that fiscal policy may lead to changes in supply as well as in demand.[30] So, when marginal tax rates are high, consumers pursue additional leisure and current consumption instead of pursuing current income and extra income in the future. Therefore, there is a decline in work effort and investment, which in turn causes a decrease of production and GNP, regardless of the total demand levels.  On these assumptions, supply side economists formulate the idea that a cut in marginal tax rates has a positive effect on economic growth.  The main focus of supply-side economics is promotion of economic growth. In this regard, some studies have suggested to consider two relative prices.  The first one influences decisions of individuals on the distribution of their income between consumption and savings.[31]: 36  The cost of individual's decision to assign a unit of income to either consumption or savings is a future value of the unit, which has been given up by choosing either to consume or to save. The unit of income value is defined by the marginal tax rates. Therefore, higher tax rates would decrease the cost of consumption, which would cause a fall in investment and savings. At the same time, lower tax rates would cause the investment and savings levels to rise, while the consumption levels would fall.[29]  The second price influences decisions of individuals on the distribution of their time between work and leisure.[31] The cost of individual's decision to allocate a unit of time either to work or leisure stands for current income, which was given up by choosing either work or leisure. The cost also includes the future income, which was given up for leisure instead of enhancing the professional skills. The value of lost income is defined by the tax rate assigned to the additional income. Therefore, the increase in marginal tax rates leads to a decrease in the price of leisure. However, if the marginal tax rate decline, the cost of leisure increases.[29]  Both the amount of retained and taxed income is determined by the marginal tax rate.[29] That is why, from a supply-side economist's standpoint, marginal tax rates play a significant role in determining the development of the economy. Due to crucial role in determining how much time workers will spend on work and leisure or how much income will be spent on consumption and for savings, supply-side economists insist on decreasing tax rates as they believe it could improve the growth rates of the economy.  Laffer curve illustrates a mathematical relationship between tax revenues and tax rates, which was popularized by economist Arthur B. Laffer in 1974.[29] The Laffer Curve posits the existence of a maximum point when tax revenue is maximized at a specific (unknown) tax rate. Many interpret the Laffer Curve as higher tax rates can sometimes decrease the tax base, which will lead to the decrease in tax revenues even if the tax rates are high.[26] Due to the effect exerted by taxes on the taxed income, the adjustment of tax rates may not lead to proportional changes in tax revenues. That is why, some supply-side economists insist decreasing high tax rates can result in an increase of tax revenues.  The Laffer curve embodies a postulate of supply-side economics: that tax rates and tax revenues are distinct, with government tax revenues the same at a 100% tax rate as they are at a 0% tax rate and maximum revenue somewhere in between these two values. Supply-siders argued that in a high tax rate environment lowering tax rates would result in either increased revenues or smaller revenue losses than one would expect relying on only static estimates of the previous tax base.[32]  This led supply-siders to advocate large reductions in marginal income and capital gains tax rates to encourage greater investment, which would produce more supply. Jude Wanniski and many others advocate a zero capital gains rate.[33][34]  Defunct  Newspapers  Journals  TV channels  Websites  Other  Congressional caucuses  Economics  Gun rights  Identity politics  Nativist  Religion  Watchdog groups  Youth\/student groups  Social media  Miscellaneous  Other  In the United States, commentators[who?] frequently equate supply-side economics with Reaganomics.[citation needed] The administration of Republican president Ronald Reagan promoted its fiscal policies as being based on supply-side economics. Reagan made supply-side economics a household phrase and promised an across-the-board reduction in income tax rates and an even larger reduction in capital gains tax rates.[35] During Reagan's 1980 presidential campaign, the key economic concern was double digit inflation, which Reagan described as \"[t]oo many dollars chasing too few goods\", but rather than the usual dose of tight money, recession and layoffs, with their consequent loss of production and wealth, he promised a gradual and painless way to fight inflation by \"producing our way out of it\".[36]  Switching from earlier monetary policy, Federal Reserve chair Paul Volcker implemented tighter monetary policies including lower money supply growth to break the inflationary psychology and squeeze inflationary expectations out of the economic system.[37] Therefore, supply-side supporters argue that Reaganomics was only partially based on supply-side economics.[citation needed]  Congress under Reagan passed a plan that would slash taxes by $749 billion over five years. Critics claim that the tax cuts increased budget deficits while Reagan supporters credit them with helping the 1980s economic expansion and argued that the budget deficit would have decreased if not for massive increases in military spending.[better source needed][38] As a result, Jason Hymowitz cited Reagan—along with Jack Kemp—as a great advocate for supply-side economics in politics and repeatedly praised his leadership.[39]  Critics of Reaganomics claim it failed to produce much of the exaggerated gains some supply-siders had promised. Paul Krugman later summarized the situation: \"When Ronald Reagan was elected, the supply-siders got a chance to try out their ideas. Unfortunately, they failed.\" Although he credited supply-side economics for being more successful than monetarism which he claimed \"left the economy in ruins\", he stated that supply-side economics produced results which fell \"so far short of what it promised\", describing the supply-side theory as \"free lunches\".[40]  Clinton signed the Omnibus Budget Reconciliation Act of 1993 into law, which raised income taxes rates on incomes above $115,000, created additional higher tax brackets for corporate income over $335,000, removed the cap on Medicare taxes, raised fuel taxes and increased the portion of Social Security income subject to tax, among other tax increases. Frankel and Orszag described the \"progressive fiscal conservatism\" of the 1993 package: \"Such progressive fiscal conservatism combines modest attempts at redistribution (the progressive component) and budget discipline (the fiscal conservative component). Thus the 1993 package included significant spending reductions and tax increases. But it concentrated the tax increases on upper-income taxpayers, while substantially expanding the Earned Income Tax Credit, Head Start, and other government programs aimed at lower earners.\" The tax increases led to greater revenue (relative to a baseline without a tax increase).[44]  The bill was strongly opposed by Republicans, vigorously attacked by John Kasich and Minority Whip Newt Gingrich as destined to cause job losses and lower revenue.[45]  Economist Paul Krugman wrote in 2017 that Clinton's tax increases on the rich provided counter-example to the supply-side tax cut doctrine: \"Bill Clinton provided a clear test, by raising taxes on the rich. Republicans predicted disaster, but instead the economy boomed, creating more jobs than under Reagan.\"   Supply-side economist Alan Reynolds argued that the Clinton era represented a continuation of a low tax policy (from the 1980s): In reality, tax policy was not unambiguously better in the eighties than in the nineties. The highest income tax rate was 50 percent from 1983 to 1986, but below 40 percent after 1993. And the capital gains tax was 28 percent from 1987 to [1997], but only 20 percent in the booming years of 1997-2000. On balance, there were good and bad things about both periods. But both the eighties and the nineties had much wiser tax policies than we had from 1968 to 1982.[46] In May 2012, Sam Brownback, Governor of the state of Kansas, signed into law the \"Kansas Senate Bill Substitute HB 2117\",[47][48] which cut the number of individual income tax brackets from three to two, and cut the top income tax rates from 6.45% and 6.25% to 4.9% and the bottom rate from 3.5% to 3%.[49][citation needed] It also eliminated the 7% tax on \"pass-through\" income, income that businesses — such as sole proprietorships, partnerships, limited liability companies, and subchapter S corporations — pass on to their owners instead of paying corporate income tax on, for the owners of almost 200,000 businesses[48][50]: 1 [51] The law cut taxes by US$231 million in its first year, and cuts were projected to increase to US$934 million annually after six years.[51][52]  The cuts were based on model legislation published by the conservative American Legislative Exchange Council (ALEC),[53][54] and were supported by The Wall Street Journal,[citation needed] supply-side economist Arthur Laffer,[55] economics commentator Stephen Moore[56] and anti-tax leader Grover Norquist.[57] The tax cuts have been called the \"Kansas experiment\",[50] and was described by the Brookings Institution as \"one of the cleanest experiments for how tax cuts affect economic growth in the U.S.\"[58]  Brownback compared his tax cut policies with those of Ronald Reagan, but also described them as \"a real live experiment ... We'll see how it works.\",[49][59] Brownback forecast his cuts would create an additional 23,000 jobs in Kansas by 2020, and was intended to generate rapid economic growth, which he said would be \"like a shot of adrenaline into the heart of the Kansas economy.\"[48][60] On the other hand, the Kansas Legislature's research staff warned of the possibility of a deficit of nearly US$2.5 billion by July 2018.[51]  By 2017, state revenues had fallen by hundreds of millions of dollars[61]  causing spending on roads, bridges, and education to be slashed,[62][63] but instead of boosting economic growth, growth in Kansas remained consistently below average.[64] A working paper by two economists at Oklahoma State University (Dan Rickman and Hongbo Wang) using historical data from several other states with economies structured similarly to Kansas found that the Kansas economy grew about 7.8% less and employment about 2.6% less than it would have had Brownback not cut taxes.[65][66] In 2017, the Republican Legislature of Kansas voted to roll back the cuts, and after Brownback vetoed the repeal, overrode his veto.[67]  According to Max Ehrenfreund, economists generally agree that an explanation for the reduction instead of increase in economic growth from the tax cuts is that \"any\" benefits from tax cuts come over the long, not short run, but what does come in the short run is a major decline in demand for goods and services. In the Kansas economy cuts in state government expenditures cut incomes of state government \"employees, suppliers and contractors\" who spent much or most of their incomes locally. In addition, concern over the state's large budget deficits \"might have deterred businesses from making major new investments\".[65]  One problem Kansas encountered is that while studies have shown that tax cuts increase economic growth, the increased revenue from that growth at the new lower tax rates are only enough to make up for 10-30% of the tax cuts, meaning that to avoid deficits, spending cuts must also be made.[50][68]  Supply-side advocates Laffer and economics commentators Stephen Moore and Larry Kudlow played prominent roles in formulating Trump's economic policies by advising him on his tax cut, as well as encouraging him to lower trade barriers.[69] Laffer and Moore wrote a 2018 book about the policy, Trumponomics, with a foreword by Kudlow. Economist Gregory Mankiw reviewed the book in Foreign Affairs, and characterized the statements around Trump's policies as \"snake-oil economics\".[70][11] He criticized the authors for un-apologetically parroting the president's claimed annual growth rates spawned by his tax cut to be 1–4%, when the highest reasonable estimates were around 0.5%, but also credits them for continuing to support the consensus view that free trade is good for all, against the president's mercantilist views.[70][11] He also criticized them for following a simplistic \"economic growth will solve all problems\" approach, when previous presidential economic advisors had been more nuanced, recognizing the unavoidable tradeoff between equity and efficiency in their approaches to managing the economy.[11] Trump implemented individual and corporate income tax cuts which took effect in 2018. Rutgers economics professor Farrokh Langdana argued that the Trump tax cuts were an example of supply-side tax policy, citing a letter from economists long-associated with the supply-side theory describing them as such.[71]  One benefit of a supply-side policy is that shifting the aggregate supply curve outward means prices can be lowered along with expanding output and employment. This is in contrast to demand-side policies (e.g., higher government spending), which even if successful tend to create inflationary pressures (i.e., raise the aggregate price level) as the aggregate demand curve shifts outward. Infrastructure investment is an example of a policy that has both demand-side and supply-side elements.[4]  Supply-side economics holds that increased taxation steadily reduces economic activity within a nation and discourages investment. Taxes act as a type of trade barrier or tariff that causes economic participants to revert to less efficient means of satisfying their needs. As such, higher taxation leads to lower levels of specialization and lower economic efficiency. The idea is said to be illustrated by the Laffer curve.[72]  Supply-side economists have less to say on the effects of deficits and sometimes cite Robert Barro's work that states that rational economic actors will buy bonds in sufficient quantities to reduce long-term interest rates.[73]  Bruce Bartlett stated in 2007 that \"The original supply-siders suggested that some tax cuts, under very special circumstances, might actually raise federal revenues. ... But today it is common to hear tax cutters claim, implausibly, that all tax cuts raise revenue.\"[22]  Some contemporary economists do not consider supply-side economics a tenable economic theory, with Alan Blinder calling it an \"ill-fated\" and perhaps \"silly\" school on the pages of a 2006 textbook.[74] Greg Mankiw, former chairman of President President George W. Bush's Council of Economic Advisers, offered similarly sharp criticism of the school in the early editions of his introductory economics textbook. \"Tax cuts rarely pay for themselves. My reading of the academic literature leads me to believe that about one-third of the cost of a typical tax cut is recouped with faster economic growth.\"[75]  In a 1992 article for the Harvard International Review, James Tobin wrote: \"The 'Laffer curve' idea that tax cuts would actually increase revenues turned out to deserve the ridicule.\"[76]  Karl Case and Ray Fair wrote in Principles of Economics, \"The extreme promises of supply-side economics did not materialize. President Reagan argued that because of the effect depicted in the Laffer curve, the government could maintain expenditures, cut tax rates, and balance the budget. This was not the case. Government revenues fell sharply from levels that would have been realized without the tax cuts.\"[77]  Supply side proponents Trabandt and Uhlig argue that \"static scoring overestimates the revenue loss for labor and capital tax cuts\" and that \"dynamic scoring\" is a better predictor for the effects of tax cuts.[78]  A 1999 study by University of Chicago economist Austan Goolsbee examined major changes in high-income tax rates in the United States from the 1920s onwards. It concluded that there were only modest changes in the reported income of high-income individuals, indicating that the tax changes had little effect on how much people work.[79][80] He concluded that the notion that governments could raise more money by cutting rates \"is unlikely to be true at anything like today's marginal tax rates.\"[79] In 2015, one study found that in the past several decades, tax cuts in the U.S. seldom recouped revenue losses and had minimal impact on GDP growth.[81][82]  A 2008 working paper found that in the case of Russia, \"tax rate cuts can increase revenues by improving tax compliance.\"[83]  The New Palgrave Dictionary of Economics reports that estimates of revenue-maximizing tax rates have varied widely, with a mid-range of around 70%.[84]  According to a 2012 study, \"the U.S. marginal top [tax] rate is far from the top of the Laffer curve.\"[85] A 2012 survey found a consensus among leading economists that reducing the US federal income tax rate would raise GDP but would not increase tax revenue.[86]  John Quiggin distinguishes between the Laffer curve and Laffer's analysis of tax rates. The Laffer curve was \"correct but unoriginal\", but Laffer's analysis that the United States was on the wrong side of the Laffer curve \"was original but incorrect.\"[87]  Proponents of supply-side economics have sometimes cited tax cuts enacted in the 1920s as evidence that tax cuts can increase tax revenue. After World War I, the highest tax bracket, which was for those earning over $100,000 a year (worth at least $1 million a year now), was over 70 percent.[88] According to The Heritage Foundation, revenue acts of 1921, 1924 and 1926 reduced this tax rate to less than 25 percent, yet tax revenues actually went up significantly.[89] Tax historian Joseph Thorndike argues that the tax cuts helped \"bolster\" growth but did not \"cover the full cost of those tax cuts\".[90]  Proponents of supply-side economics sometimes cite tax cuts enacted by President Lyndon B. Johnson with the Revenue Act of 1964. John F. Kennedy had the year prior advocated a drastic tax-rate cut in 1963 when the top income tax rate was 91%, arguing that \"[t]ax rates are too high today and tax revenues too low, and the soundest way to raise revenues in the long run is to cut rates now\".[91] The CBO concluded in 1978 that the tax cuts reduced tax revenue by $12 billion and that only between $3 billion to $9 billion were recaptured due to bolstered economic growth. According to the CBO, \"most of this rise [in revenues] was due to economic growth that would have taken place even without the tax cut.\"[90]  At the same time, some studies have found a relatively robust response to tax cuts from the top 5% of tax returns.[92] There has been identified an increase of 7.7% in revenues from the top 5%, from $17.17 billion US in 1963 to $18.49 billion in 1965. Hereby, the data have provided evidence that the group has been in the prohibitive part of the Laffer curve, because its input to total tax revenues have increased despite the tax rates decreasing significantly.[92]  Supply-siders justified Reagan's tax cuts during the 1980s by claiming they would result in net increases in tax revenue, yet tax revenues declined (relative to a baseline without the cuts) due to Reagan's tax cuts, and the deficit ballooned during Reagan's term in office.[93][94][95][96] The Treasury Department studied the Reagan tax cuts and concluded they significantly reduced tax revenues relative to a baseline without them.[97] The 1990 budget by the Reagan administration concluded that the 1981 tax cuts had caused a reduction in tax revenue.[90]  Both CBO and the Reagan Administration forecast that individual and business income tax revenues would be lower if the Reagan tax cut proposals were implemented, relative to a policy baseline without those cuts, by about $50 billion in 1982 and $210 billion by 1986.[98] FICA tax revenue increased because in 1983 FICA tax rates were increased from 6.7% to 7% and the ceiling was raised by $2,100. For the self-employed, the FICA tax rate went from 9.35% to 14%.[99] The FICA tax rate increased throughout Reagan's term and rose to 7.51% in 1988 and the ceiling was raised by 61% through Reagan's two terms. Those tax hikes on wage earners, along with inflation, were the source of revenue gains in the early 1980s.[100]  It has been contended by some supply-side critics that the argument to lower taxes to increase revenues was a smokescreen for \"starving\" the government of revenues in the hope that the tax cuts would lead to a corresponding drop in government spending, but this did not turn out to be the case. Paul Samuelson called this notion \"the tape worm theory—the idea that the way to get rid of a tape worm is [to] stab your patient in the stomach\".[101]   There is frequent confusion on the meaning of the term \"supply-side economics\" between the related ideas of the existence of the Laffer Curve and the belief that decreasing tax rates can increase tax revenues. Many supply-side economists doubt the latter claim while still supporting the general policy of tax cuts. Economist Gregory Mankiw used the term \"fad economics\" to describe the notion of tax rate cuts increasing revenue in the third edition of his 2007 Principles of Macroeconomics textbook in a section entitled \"Charlatans and Cranks\":  An example of fad economics occurred in 1980, when a small group of economists advised Presidential candidate, Ronald Reagan, that an across-the-board cut in income tax rates would raise tax revenue. They argued that if people could keep a higher fraction of their income, people would work harder to earn more income. Even though tax rates would be lower, income would rise by so much, they claimed, that tax revenues would rise. Almost all professional economists, including most of those who supported Reagan's proposal to cut taxes, viewed this outcome as far too optimistic. Lower tax rates might encourage people to work harder and this extra effort would offset the direct effects of lower tax rates to some extent, but there was no credible evidence that work effort would rise by enough to cause tax revenues to rise in the face of lower tax rates. [...] People on fad diets put their health at risk but rarely achieve the permanent weight loss they desire. Similarly, when politicians rely on the advice of charlatans and cranks, they rarely get the desirable results they anticipate. After Reagan's election, Congress passed the cut in tax rates that Reagan advocated, but the tax cut did not cause tax revenues to rise.[102][103] In 1986, Martin Feldstein — a self-described \"traditional supply sider\" who served as Reagan's chairman of the Council of Economic Advisors from 1982 to 1984 — characterized the \"new supply siders\" who emerged circa 1980:  What distinguished the new supply siders from the traditional supply siders as the 1980s began was not the policies they advocated but the claims that they made for those policies ... The \"new\" supply siders were much more extravagant in their claims. They projected rapid growth, dramatic increases in tax revenue, a sharp rise in saving, and a relatively painless reduction in inflation. The height of supply side hyperbole was the \"Laffer curve\" proposition that the tax cut would actually increase tax revenue because it would unleash an enormously depressed supply of effort. Another remarkable proposition was the claim that even if the tax cuts did lead to an increased budget deficit, that would not reduce the funds available for investment in plant and equipment because tax changes would raise the saving rate by enough to finance the increased deficit ... Nevertheless, I have no doubt that the loose talk of the supply side extremists gave fundamentally good policies a bad name and led to quantitative mistakes that not only contributed to subsequent budget deficits but that also made it more difficult to modify policy when those deficits became apparent.[28] During his presidency, President Bush signed the Economic Growth and Tax Relief Reconciliation Act of 2001 and Jobs and Growth Tax Relief Reconciliation Act of 2003, which entailed significant tax cuts. In 2003, the Congressional Budget Office conducted a dynamic scoring analysis of tax cuts advocated by supply advocates, and found that the Bush tax cuts would not pay for themselves. Two of the nine models used in the study predicted a large improvement in the deficit over the next ten years resulting from tax cuts, but only by making the assumption that people would work harder from 2004 to 2014 because they believed that tax rates would increase again in 2014, and they wanted to make more money before the tax cuts expired.[104]  In 2006, the CBO released a study titled \"A Dynamic Analysis of Permanent Extension of the President's Tax Relief\".[105] This study found that under the best possible scenario making tax cuts permanent would increase the economy \"over the long run\" by 0.7%. This study was criticized by many economists, including Harvard Economics Professor Greg Mankiw, who pointed out that the CBO used a very low value for the earnings-weighted compensated labor supply elasticity of 0.14.[106] In a paper published in the Journal of Public Economics, Mankiw and Matthew Weinzierl noted that the current economics research would place an appropriate value for labor supply elasticity at around 0.5.[107]  The Congressional Budget Office (CBO) estimated that extending the Bush tax cuts beyond their 2010 expiration would increase the deficit by $1.8 trillion over 10 years.[108] The CBO also completed a study in 2005 analyzing a hypothetical 10% income tax cut and concluded that under various scenarios there would be minimal offsets to the loss of revenue. In other words, deficits would increase by nearly the same amount as the tax cut in the first five years with limited feedback revenue thereafter.[109]  Nobel laureate economist Milton Friedman agreed the tax cuts would reduce tax revenues and result in intolerable deficits, though he supported them as a means to restrain federal spending.[110] Friedman characterized the reduced government tax revenue as \"cutting their allowance\".  Douglas Holtz-Eakin was a Bush administration economist who was appointed director of the Congressional Budget Office in 2003. Under his leadership, the CBO undertook a study of income tax rates which found that any new revenue from tax cuts paled in comparison to their cost.[111][112][113]  Dartmouth economics professor Andrew Samwick was the chief staff economist for the Bush Council of Economic Advisers from July 2003 to July 2004. Writing on his blog in 2007, Samwick urged his former colleagues in the Bush administration to avoid asserting that the Bush tax cuts paid for themselves, because \"No thoughtful person believes it...Not a single one.\"[114]  The New York Times reported in November 2018 that the Trump tax overhaul \"has fattened the paychecks of most American workers, padded the profits of large corporations and sped economic growth.\" Cautioning that \"its still early but ten months after the law took effect, the promised 'supply side' bump is harder to find than the sugar-high stimulus.\" The writers explained that \"It's highly unusual for deficits...to grow this much during periods of prosperity\" and that \"the fiscal health of the U.S. is deteriorating fast, as revenues have declined sharply\" (nearly $200 billion or about 6%) relative to the CBO forecast prior to the tax cuts. Results for 2018 included:  Analysis conducted by the Congressional Research Service on the first-year effect of the tax cut found that little if any economic growth in 2018 could be attributed to it.[116][117] Growth in GDP, employment, worker compensation and business investment slowed during the second year following enactment of the tax cut, prior to the emergence of the COVID-19 pandemic.[118][119][120]  Following the Trump tax cut, top White House economic advisor Larry Kudlow falsely asserted that federal revenues had increased about 10% since the tax cut, though they had actually declined.[121] He also falsely asserted that the CBO had found the \"entire $1.5 trillion tax cut is virtually paid for by higher revenues and better nominal GDP.\"[122][123][124]  Beginning in 2012, China's economic performance entered a \"new normal,\" in which the growth of the economy slowed to a medium pace for the first time since the broad economic reforms of Chinese leader Deng Xiaoping. In response, Xi Jinping, General Secretary of the Chinese Communist Party, announced supply-side structural reforms (SSSR) in 2015 in an effort to combat the slowing economic growth, moving away from the export-oriented economy and toward supply and production driven growth.[125] The focus of the reforms correspond to increase in total factor productivity (TFP) through an increase in investment in technological improvements as a replacement for the labor and capital-intensive emphasis of the previous growth model. China's supply-side structural reforms focus on the reduction of excess capacity across various economic sectors. The reform plan centers around four key areas: cutting excess industrial capacity, reducing leverage in the corporate sector, reducing stock of property inventories, and lowering costs for new enterprises.[126] The former two areas corresponds to short-term initiatives related to the state-owned sectors, while the former initiatives correspond to longer-term solutions within the private sector.[127] Cutting excess industrial capacity focuses heavily on sectors like coal, steel, and electricity generation. The targets for coal production reductions implemented by the National Development and Reform Commission (NDRC) amounted to 250 million tonnes per annum (Mta) in 2016 as well as a 100-150 Mta reduction in steel production capacity over a five-year period.[126] The result in the steel industry was more pricing power for the remaining large firms, as well as higher profits due to the increase in price. The increase in profits due to excess capacity reductions has also led to an increased capacity of firms to settle outstanding debts and reduce leverage, part of the second pillar of China's SSSR. As part of the deleveraging initiative, the government also encouraged mergers and acquisitions, direct financing, and debt-to-equity swaps, resulting in the stabilization of corporate debt to GDP ratio. Additional reforms include increased incentives for private sector investment, development of modern service industries, and increasing public goods and services supply.[126] The longer-term initiatives have also been accompanied by large-scale tax cuts as well as a transition from business tax to value-added tax (VAT) which produced positive results for service industry growth. Policies targeted toward new growth engine creation include Made in China 2025 and the Internet Plus agenda, both of which have been attributed in part to the rapid growth of China's industrial and innovation competitiveness. China's supply-side structural reforms are ongoing and oriented toward the long term. The adjustments to the industrial sector as a result of the early reform policies have been attributed to a nominal increase in GDP growth. However, the economic effects of the COVID-19 pandemic impacted demand growth in China's domestic consumer market, which has slowed the effects of continued supply-side reforms.[128]  Increasing the supply of housing is a way to drive down prices, in contrast to demand-side economics which believes in subsidizing buyers or reducing demand with tight monetary policy.[129][130]  Critics of supply-side policies emphasize the growing federal deficits, increased income inequality and lack of growth.[132] They argue that the Laffer curve only measures the rate of taxation, not tax incidence, which may be a stronger predictor of whether a tax code change is stimulative or dampening.[133]  Writing in 2010, John Quiggin said, \"To the extent that there was an economic response to the Reagan tax cuts, and to those of George W. Bush twenty years later, it seems largely to have been a Keynesian demand-side response, to be expected when governments provide households with additional net income in the context of a depressed economy.\"[87]  Cutting marginal tax rates can also be perceived as primarily beneficial to the wealthy, which some see as politically rather than economically motivated:[134]  Back in 1980 George H. W. Bush famously described supply-side economics — the claim that cutting taxes on rich people will conjure up an economic miracle, so much so that revenues will actually rise — as \"voodoo economic policy.\" Yet it soon became the official doctrine of the Republican Party, and still is. That shows an impressive level of commitment. But what makes this commitment even more impressive is that it's a doctrine that has been tested again and again — and has failed every time...In other words, supply-side economics is a classic example of a zombie doctrine: a view that should have been killed by the evidence long ago, but just keeps shambling along, eating politicians' brains. — Paul Krugman[135] Mr. David Stockman has said that supply-side economics was merely a cover for the trickle-down approach to economic policy—what an older and less elegant generation called the horse-and-sparrow theory: If you feed the horse enough oats, some will pass through to the road for the sparrows.— John Kenneth Galbraith[136] Studies, which have analysed the tax cuts in 2001 (EGTRRA), provided controversial conclusions: the decrease in taxes have provided a generally positive impact on the future output from the effect of the lower tax rates on human capital accumulation, private saving and investment, labor supply; however, the tax cuts have produced adverse effects such as higher deficits and reduced national savings.[82] Thus, Gale and Potter (2002) concluded that these tax cuts could not affect the GDP levels in any significant way in the next 10 years.[137] "},"meta":{},"created_at":"2025-03-22T14:25:42.277677Z","updated_at":"2025-03-22T14:25:42.277677Z","inner_id":46,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":55,"annotations":[{"id":55,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.312403Z","updated_at":"2025-03-22T14:25:42.312403Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"c066be2c-2630-406d-be62-909481bf4e1a","import_id":null,"last_action":null,"bulk_created":false,"task":55,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"The money market is a component of the economy that provides short-term funds. The money market deals in short-term loans, generally for a period of a year or less.  As short-term securities became a commodity, the money market became a component of the financial market for assets involved in short-term borrowing, lending, buying and selling with original maturities of one year or less. Trading in money markets is done over the counter and is wholesale.  There are several money market instruments in most Western countries, including treasury bills, commercial paper, banker's acceptances, deposits, certificates of deposit, bills of exchange, repurchase agreements, federal funds, and short-lived mortgage- and asset-backed securities.[1] The instruments bear differing maturities, currencies, credit risks, and structures.[2] A market can be described as a money market if it is composed of highly liquid, short-term assets. Money market funds typically invest in government securities, certificates of deposit, commercial paper of companies, and other highly liquid, low-risk securities. The four most relevant types of money are commodity money, fiat money, fiduciary money (cheques, banknotes), and commercial bank money.[3]  Commodity money relies on intrinsically valuable commodities that act as a medium of exchange. Fiat money, on the other hand, gets its value from a government order.  Money markets, which provide liquidity for the global financial system including for capital markets, are part of the broader system of financial markets.  The money market consists of financial institutions and dealers in money or credit who wish to either borrow or lend. Participants borrow and lend for short periods, typically up to twelve months.  Money market trades in short-term financial instruments commonly called \"paper\". This contrasts with the capital market for longer-term funding, which is supplied by bonds and equity.  The heart of the money market revolves around the concept of interbank lending, where banks lend and borrow from each other using financial instruments such as commercial paper and repurchase agreements. These instruments are often valued with reference to the London Interbank Offered Rate (LIBOR) for the specific term and currency.  Finance companies usually secure their funding by issuing substantial amounts of asset-backed commercial paper (ABCP). This paper is backed by the commitment of valuable assets placed into an ABCP conduit. These assets can include things like auto loans, credit card receivables, residential or commercial mortgage loans, mortgage-backed securities, and other financial assets. Some large, financially stable corporations even issue their own commercial paper, while others prefer to have banks issue it on their behalf.  In the United States, federal, state and local governments all issue paper to meet funding needs. States and local governments issue municipal paper, while the U.S. Treasury issues Treasury bills to fund the U.S. public debt:  Money markets serve five functions—to finance trade, finance industry, invest profitably, enhance commercial banks' self-sufficiency, and lubricate central bank policies.[4][5]  The money market plays a crucial role in financing domestic and international trade. Commercial finance is made available to the traders through bills of exchange, which are discounted by the bill market. The acceptance houses and discount markets help in financing foreign trade.  The money market contributes to the growth of industries in two ways:  The money market enables commercial banks to use their excess reserves in profitable investments. The main objective of commercial banks is to earn income from its reserves as well as maintain liquidity to meet the uncertain cash demand of its depositors. In the money market, the excess reserves of commercial banks are invested in near money assets (e.g., short-term bills of exchange), which are easily converted into cash. Thus, commercial banks earn profits without sacrificing liquidity.  Developed money markets help commercial banks to become self-sufficient. In an emergency, when commercial banks have scarcity of funds, they need not approach the central bank and borrow at a higher interest rate. They can instead meet their requirements by recalling their old short-run loans[clarify] from the money market.  Though the central bank can function and influence the banking system in the absence of a money market, the existence of a developed money market significantly enhances monetary policy transmission and central bank efficiency.  Money markets help central banks in three primary ways:  Interest Rate Signaling: Short-term interest rates in money markets serve as immediate indicators of monetary and banking conditions, guiding central bank policy decisions. As Chen & Valcarcel (2021) demonstrate, money market rates respond quickly to policy changes, providing real-time feedback on policy effectiveness.[6] Bech & Klee (2011) further show how segmentation in money markets can affect this transmission mechanism.[7]  Policy Implementation: Well-integrated money markets enable central banks to achieve widespread influence across financial sub-markets efficiently. When the central bank adjusts its policy rate, these changes transmit rapidly through interbank markets to other financial instruments and ultimately to the broader economy. Carpenter & Demiralp (2012) highlight how this transmission has evolved beyond traditional money multiplier frameworks.[8]  Liquidity Management: Money markets facilitate efficient distribution of liquidity among financial institutions, reducing the need for direct central bank intervention. This market-based approach to liquidity allocation improves the overall efficiency of monetary policy operations. However, as Brunnermeier & Koby (2018) note, there are limits to the effectiveness of monetary policy through these channels, particularly in low-interest-rate environments.[9]  There are two types of instruments in the fixed income market that pay interest at maturity, instead of as coupons—discount instruments and accrual instruments. Discount instruments, like repurchase agreements, are issued at a discount of face value, and their maturity value is the face value. Accrual instruments are issued at face value and mature at face value plus interest. "},"meta":{},"created_at":"2025-03-22T14:25:42.277677Z","updated_at":"2025-03-22T14:25:42.277677Z","inner_id":47,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":56,"annotations":[{"id":56,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.319900Z","updated_at":"2025-03-22T14:25:42.319900Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"dc60f600-53bb-4312-8217-64cee0bf8f89","import_id":null,"last_action":null,"bulk_created":false,"task":56,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"    The foreign exchange market (forex, FX (pronounced \"fix\"), or currency market) is a global decentralized or over-the-counter (OTC) market for the trading of currencies. This market determines foreign exchange rates for every currency. It includes all aspects of buying, selling and exchanging currencies at current or determined prices. In terms of trading volume, it is by far the largest market in the world, followed by the credit market.[1]  The main participants in this market are the larger international banks. Financial centers around the world function as anchors of trading between a wide range of multiple types of buyers and sellers around the clock, with the exception of weekends. Since currencies are always traded in pairs, the foreign exchange market does not set a currency's absolute value but rather determines its relative value by setting the market price of one currency if paid for with another. Ex: 1 USD is worth X CAD, or CHF, or JPY, etc.  The foreign exchange market works through financial institutions and operates on several levels. Behind the scenes, banks turn to a smaller number of financial firms known as \"dealers\", who are involved in large quantities of foreign exchange trading. Most foreign exchange dealers are banks, so this behind-the-scenes market is sometimes called the \"interbank market\" (although a few insurance companies and other kinds of financial firms are involved). Trades between foreign exchange dealers can be very large, involving hundreds of millions of dollars. Because of the sovereignty issue when involving two currencies, Forex has little (if any) supervisory entity regulating its actions.  The foreign exchange market assists international trade and investments by enabling currency conversion. For example, it permits a business in the United States to import goods from  European Union member states, especially Eurozone members, and pay Euros, even though its income is in United States dollars. It also supports direct speculation and evaluation relative to the value of currencies and the carry trade speculation, based on the differential interest rate between two currencies.[2]  In a typical foreign exchange transaction, a party purchases some quantity of one currency by paying with some quantity of another currency.  The modern foreign exchange market began forming during the 1970s. This followed three decades of government restrictions on foreign exchange transactions under the Bretton Woods system of monetary management, which set out the rules for commercial and financial relations among the world's major industrial states after World War II. Countries gradually switched to floating exchange rates from the previous exchange rate regime, which remained fixed per the Bretton Woods system.\t  The foreign exchange market is unique because of the following characteristics:  As such, it has been referred to as the market closest to the ideal of perfect competition, notwithstanding currency intervention by central banks.  According to the Bank for International Settlements, the preliminary global results from the 2022 Triennial Central Bank Survey of Foreign Exchange and OTC Derivatives Markets Activity show that trading in foreign exchange markets averaged US$7.5 trillion per day in April 2022. This is up from US$6.6 trillion in April 2019. Measured by value, foreign exchange swaps were traded more than any other instrument in April 2022, at US$3.8 trillion per day, followed by spot trading at US$2.1 trillion.[3]  The $7.5 trillion break-down is as follows:  Currency trading and exchange first occurred in ancient times.[4] Money-changers (people helping others to change money and also taking a commission or charging a fee) were living in the Holy Land in the times of the Talmudic writings (Biblical times). These people (sometimes called \"kollybistẻs\") used city stalls, and at feast times the Temple's Court of the Gentiles instead.[5] Money-changers were also the silversmiths and\/or goldsmiths[6] of more recent ancient times.  During the 4th century AD, the Byzantine government kept a monopoly on the exchange of currency.[7]  Papyri PCZ I 59021 (c.259\/8 BC), shows the occurrences of exchange of coinage in Ancient Egypt.[8]  Currency and exchange were important elements of trade in the ancient world, enabling people to buy and sell items like food, pottery, and raw materials.[9] If a Greek coin held more gold than an Egyptian coin due to its size or content, then a merchant could barter fewer Greek gold coins for more Egyptian ones, or for more material goods. This is why, at some point in their history, most world currencies in circulation today had a value fixed to a specific quantity of a recognized standard like silver and gold.  During the 15th century, the Medici family were required to open banks at foreign locations in order to exchange currencies to act on behalf of textile merchants.[10][11] To facilitate trade, the bank created the nostro (from Italian, this translates to \"ours\") account book which contained two columned entries showing amounts of foreign and local currencies; information pertaining to the keeping of an account with a foreign bank.[12][13][14][15] During the 17th (or 18th) century, Amsterdam maintained an active Forex market.[16] In 1704, foreign exchange took place between agents acting in the interests of the Kingdom of England and the County of Holland.[17]  Alex. Brown & Sons traded foreign currencies around 1850 and was a leading currency trader in the USA.[18] In 1880, J.M. do Espírito Santo de Silva (Banco Espírito Santo) applied for and was given permission to engage in a foreign exchange trading business.[19][20]  The year 1880 is considered by at least one source to be the beginning of modern foreign exchange: the gold standard began in that year.[21]  Prior to the First World War, there was a much more limited control of international trade. Motivated by the onset of war, countries abandoned the gold standard monetary system.[22]  From 1899 to 1913, holdings of countries' foreign exchange increased at an annual rate of 10.8%, while holdings of gold increased at an annual rate of 6.3% between 1903 and 1913.[23]  At the end of 1913, nearly half of the world's foreign exchange was conducted using the pound sterling.[24] The number of foreign banks operating within the boundaries of London increased from 3 in 1860, to 71 in 1913. In 1902, there were just two London foreign exchange brokers.[25] At the start of the 20th century, trades in currencies was most active in Paris, New York City and Berlin; Britain remained largely uninvolved until 1914. Between 1919 and 1922, the number of foreign exchange brokers in London increased to 17; and in 1924, there were 40 firms operating for the purposes of exchange.[26]  During the 1920s, the Kleinwort family were known as the leaders of the foreign exchange market, while Japheth, Montagu & Co. and Seligman still warrant recognition as significant FX traders.[27] The trade in London began to resemble its modern manifestation. By 1928, Forex trade was integral to the financial functioning of the city. However, during the 1930s, London's pursuit of widespread trade prosperity was hindered by continental exchange controls and additional factors in Europe and Latin America.[28] Some of these additional factors include tariff rates and quota,[29] protectionist policies, trade barriers and taxes, economic depression and agricultural overproduction, and impact of protection on trade.  In 1944, the Bretton Woods Accord was signed, allowing currencies to fluctuate within a range of ±1% from the currency's par exchange rate.[30] In Japan, the Foreign Exchange Bank Law was introduced in 1954. As a result, the Bank of Tokyo became a center of foreign exchange by September 1954. Between 1954 and 1959, Japanese law was changed to allow foreign exchange dealings in many more Western currencies.[31]  U.S. President, Richard Nixon is credited with ending the Bretton Woods Accord and fixed rates of exchange, eventually resulting in a free-floating currency system. After the Accord ended in 1971,[32] the Smithsonian Agreement allowed rates to fluctuate by up to ±2%. In 1961–62, the volume of foreign operations by the U.S. Federal Reserve was relatively low.[33][34] Those responsible for managing exchange rates then found the boundaries of the Agreement unrealistic. As a result, it led to its discontinuation in March 1973. Afterwards, none of the major currencies (such as the US dollar, the British pound, or the Japanese yen) were maintained with a capacity for conversion to gold. Instead, organizations relied on reserves of currency to facilitate international trade and back the value of their own currency.[35][36] From 1970 to 1973, the volume of trading in the market increased three-fold.[37][38][39] At some time (according to Gandolfo during February–March 1973) some of the markets were \"split\", and a two-tier currency market was subsequently introduced, with dual currency rates. This was abolished in March 1974.[40][41][42]  Reuters introduced computer monitors during June 1973, replacing the telephones and telex used previously for trading quotes.[43]  Due to the ultimate ineffectiveness of the Bretton Woods Accord and the European Joint Float, the forex markets were forced to close sometime during 1972 and March 1973.[44] This was a result of the collapse of the Bretton Woods System, as major currencies began to float against each other, ultimately leading to the abandonment of the fixed exchange rate system.[45][46] Meanwhile, the largest purchase of US dollars in the history of 1976[47] was when the West German government achieved an almost 3 billion dollar acquisition (a figure is given as 2.75 billion in total by The Statesman: Volume 18 1974). This event indicated the impossibility of balancing of exchange rates by the measures of control used at the time, and the monetary system and the foreign exchange markets in West Germany and other countries within Europe closed for two weeks (during February and, or, March 1973. Giersch, Paqué, & Schmieding state closed after purchase of \"7.5 million Dmarks\" Brawley states \"... Exchange markets had to be closed. When they re-opened ... March 1 \" that is a large purchase occurred after the close).[48][49][50][51]  In developed nations, state control of foreign exchange trading ended in 1973 when complete floating and relatively free market conditions of modern times began.[52] Other sources claim that the first time a currency pair was traded by U.S. retail customers was during 1982, with additional currency pairs becoming available by the next year.[53][54]  On 1 January 1981, as part of changes beginning during 1978, the People's Bank of China allowed certain domestic \"enterprises\" to participate in foreign exchange trading.[55][56] Sometime during 1981, the South Korean government ended Forex controls and allowed free trade to occur for the first time. During 1988, the country's government accepted the IMF quota for international trade.[57]  Intervention by European banks (especially the Bundesbank) influenced the Forex market on 27 February 1985.[58] The greatest proportion of all trades worldwide during 1987 were within the United Kingdom (slightly over one quarter). The United States had the second highest involvement in trading.[59]  During 1991, Iran changed international agreements with some countries from oil-barter to foreign exchange.[60]  The foreign exchange market is the most liquid financial market in the world. Traders include governments and central banks, commercial banks, other institutional investors and financial institutions, currency speculators, other commercial corporations, and individuals. According to the 2019 Triennial Central Bank Survey, coordinated by the Bank for International Settlements, average daily turnover was $7.5 trillion in April 2022 (compared to $1.9 trillion in 2004).[3] Of this $6.6 trillion, $2.1 trillion was spot transactions and $5.4 trillion was traded in outright forwards, swaps, and other derivatives.  Foreign exchange is traded in an over-the-counter market where brokers\/dealers negotiate directly with one another, so there is no central exchange or clearing house. The biggest geographic trading center is the United Kingdom, primarily London. In April 2022, trading in the United Kingdom accounted for 38.1% of the total, making it by far the most important center for foreign exchange trading in the world.  Owing to London's dominance in the market, a particular currency's quoted price is usually the London market price. For instance, when the International Monetary Fund calculates the value of its special drawing rights every day, they use the London market prices at noon that day. Trading in the United States accounted for 19.4%, Singapore and Hong Kong account for 9.4% and 7.1%, respectively, and Japan accounted for 4.4%.[3]  Turnover of exchange-traded foreign exchange futures and options was growing rapidly in 2004–2013, reaching $145 billion in April 2013 (double the turnover recorded in April 2007).[61] As of April 2022, exchange-traded currency derivatives represent 2% of OTC foreign exchange turnover. Foreign exchange futures contracts were introduced in 1972 at the Chicago Mercantile Exchange and are traded more than to most other futures contracts.  Most developed countries permit the trading of derivative products (such as futures and options on futures) on their exchanges. All these developed countries already have fully convertible capital accounts. Some governments of emerging markets do not allow foreign exchange derivative products on their exchanges because they have capital controls. The use of derivatives is growing in many emerging economies.[62] Countries such as South Korea, South Africa, and India have established currency futures exchanges, despite having some capital controls.  Foreign exchange trading increased by 20% between April 2007 and April 2010 and has more than doubled since 2004.[63] The increase in turnover is due to a number of factors: the growing importance of foreign exchange as an asset class, the increased trading activity of high-frequency traders, and the emergence of retail investors as an important market segment. The growth of electronic execution and the diverse selection of execution venues has lowered transaction costs, increased market liquidity, and attracted greater participation from many customer types. In particular, electronic trading via online portals has made it easier for retail traders to trade in the foreign exchange market. By 2010, retail trading was estimated to account for up to 10% of spot turnover, or $150 billion per day (see below: Retail foreign exchange traders).  Unlike a stock market, the foreign exchange market is divided into levels of access. At the top is the interbank foreign exchange market, which is made up of the largest commercial banks and securities dealers. Within the interbank market, spreads represent the gap between the bid (the highest price a buyer is willing to pay) and ask (the lowest price a seller is willing to accept) prices in trading.[65] Relationships play a role in a bank's access to interbank market liquidity. Banks with reserve imbalances may prefer to borrow from banks with established relationships and can sometimes secure loans at more favorable interest rates compared to other sources.[65]  The difference between the bid and ask prices widens (for example from 0 to 1 pip to 1–2 pips for currencies such as the EUR) as you go down the levels of access. This is due to volume. If a trader can guarantee large numbers of transactions for large amounts, they can demand a smaller difference between the bid and ask price, which is referred to as a better spread. The levels of access that make up the foreign exchange market are determined by the size of the \"line\" (the amount of money with which they are trading). The top-tier interbank market accounts for 51% of all transactions.[66] After that, smaller banks, large multinational corporations (requiring risk hedging and cross-border payroll), major hedge funds, and even a few retail market makers come into play. According to Galati and Melvin, “Pension funds, insurance companies, mutual funds, and other institutional investors have played an increasingly important role in financial markets in general, and in FX markets in particular, since the early 2000s.” (2004) In addition, he notes, “Hedge funds have grown markedly over the 2001–2004 period in terms of both number and overall size”.[67] Central banks also participate in the foreign exchange market to align currencies to their economic needs.  An important part of the foreign exchange market comes from the financial activities of companies seeking foreign exchange to pay for goods or services. Commercial companies often trade fairly small amounts compared to those of banks or speculators, and their trades often have a little short-term impact on market rates. Nevertheless, trade flows are an important factor in the long-term direction of a currency's exchange rate. Some multinational corporations (MNCs) can have an unpredictable impact when very large positions are covered due to exposures that are not widely known by other market participants.  National central banks play an important role in the foreign exchange markets. They try to control the money supply, inflation, and\/or interest rates and often have official or unofficial target rates for their currencies. They can use their often substantial foreign exchange reserves to stabilize the market. Nevertheless, the effectiveness of central bank \"stabilizing speculation\" is doubtful because central banks do not go bankrupt if they make large losses as other traders would. There is also no convincing evidence that they actually make a profit from trading.  Foreign exchange fixing is the daily monetary exchange rate fixed by the national bank of each country. The idea is that central banks use the fixing time and exchange rate to evaluate the behavior of their currency. Fixing exchange rates reflect the real value of equilibrium in the market. Banks, dealers, and traders use fixing rates as a market trend indicator.  The mere expectation or rumor of a central bank foreign exchange intervention might be enough to stabilize the currency. However, aggressive intervention might be used several times each year in countries with a dirty float currency regime. Central banks do not always achieve their objectives. The combined resources of the market can easily overwhelm any central bank.[68] Several scenarios of this nature were seen in the 1992–93 European Exchange Rate Mechanism collapse, and in more recent times in Asia.  Investment management firms (who typically manage large accounts on behalf of customers such as pension funds and endowments) use the foreign exchange market to facilitate transactions in foreign securities. For example, an investment manager bearing an international equity portfolio needs to purchase and sell several pairs of foreign currencies to pay for foreign securities purchases.  Some investment management firms also have more speculative specialist currency overlay operations, which manage clients' currency exposures with the aim of generating profits as well as limiting risk. While the number of this type of specialist firms is quite small, many have a large value of assets under management and can, therefore, generate large trades.  Individual retail speculative traders constitute a growing segment of this market. Currently, they participate indirectly through brokers or banks. Retail brokers, while largely controlled and regulated in the US by the Commodity Futures Trading Commission and National Futures Association, have previously been subjected to periodic foreign exchange fraud.[69][70] To deal with the issue, in 2010 the NFA required its members that deal in the Forex markets to register as such (i.e., Forex CTA instead of a CTA).  Those NFA members that would traditionally be subject to minimum net capital requirements, FCMs and IBs, are subject to greater minimum net capital requirements if they deal in Forex. A number of the foreign exchange brokers operate from the UK under Financial Services Authority regulations where foreign exchange trading using margin is part of the wider over-the-counter derivatives trading industry that includes contracts for difference and financial spread betting.  There are two main types of retail FX brokers offering the opportunity for speculative currency trading: brokers and dealers or market makers. Brokers serve as an agent of the customer in the broader FX market, by seeking the best price in the market for a retail order and dealing on behalf of the retail customer. They charge a commission or \"mark-up\" in addition to the price obtained in the market. Dealers or market makers, by contrast, typically act as principals in the transaction versus the retail customer, and quote a price they are willing to deal at.  Non-bank foreign exchange companies offer currency exchange and international payments to private individuals and companies. These are also known as \"foreign exchange brokers\" but are distinct in that they do not offer speculative trading but rather currency exchange with payments (i.e., there is usually a physical delivery of currency to a bank account).  It is estimated that in the UK, 14% of currency transfers\/payments are made via Foreign Exchange Companies.[71] These companies' selling point is usually that they will offer better exchange rates or cheaper payments than the customer's bank.[72] These companies differ from Money Transfer\/Remittance Companies in that they generally offer higher-value services. The volume of transactions done through Foreign Exchange Companies in India amounts to about US$2 billion[73] per day. This does not compete favorably with any well developed foreign exchange market of international repute, but with the entry of online Foreign Exchange Companies the market is steadily growing. Around 25% of currency transfers\/payments in India are made via non-bank Foreign Exchange Companies.[74]  Most of these companies use the USP of better exchange rates than the banks. They are regulated by FEDAI and any transaction in foreign Exchange is governed by the Foreign Exchange Management Act, 1999 (FEMA).  Money transfer companies\/remittance companies perform high-volume low-value transfers generally by economic migrants back to their home country. In 2007, the Aite Group estimated that there were $369 billion of remittances (an increase of 8% on the previous year). The four largest foreign markets (India, China, Mexico, and the Philippines) receive $95 billion. The largest and best-known provider is Western Union with 345,000 agents globally, followed by UAE Exchange.[75]  Bureaux de change or currency transfer companies provide low-value foreign exchange services for travelers. These are typically located at airports and stations or at tourist locations and allow physical notes to be exchanged from one currency to another. They access foreign exchange markets via banks or non-bank foreign exchange companies.  There is no unified or centrally cleared market for the majority of trades, and there is very little cross-border regulation. Due to the over-the-counter (OTC) nature of currency markets, there are rather a number of interconnected marketplaces, where different currencies instruments are traded. This implies that there is not a single exchange rate but rather a number of different rates (prices), depending on what bank or market maker is trading, and where it is. In practice, the rates are quite close due to arbitrage. Due to London's dominance in the market, a particular currency's quoted price is usually the London market price. Major trading exchanges include Electronic Broking Services (EBS) and Thomson Reuters Dealing, while major banks also offer trading systems. A joint venture of the Chicago Mercantile Exchange and Reuters, called Fxmarketspace opened in 2007 and aspired but failed to the role of a central market clearing mechanism.[78]  The main trading centers are London and New York City, though Tokyo, Hong Kong, and Singapore are all important centers as well. Banks throughout the world participate. Currency trading happens continuously throughout the day; as the Asian trading session ends, the European session begins, followed by the North American session and then back to the Asian session.  Fluctuations in exchange rates are usually caused by actual monetary flows as well as by expectations of changes in monetary flows. These are caused by changes in gross domestic product (GDP) growth, inflation (purchasing power parity theory), interest rates (interest rate parity, Domestic Fisher effect, International Fisher effect), budget and trade deficits or surpluses, large cross-border M&A deals and other macroeconomic conditions. Major news is released publicly, often on scheduled dates, so many people have access to the same news at the same time. However, large banks have an important advantage; they can see their customers' order flow.  Currencies are traded against one another in pairs. Each currency pair thus constitutes an individual trading product and is traditionally noted XXXYYY or XXX\/YYY, where XXX and YYY are the ISO 4217 international three-letter code of the currencies involved. The first currency (XXX) is the base currency that is quoted relative to the second currency (YYY), called the counter currency (or quote currency). For instance, the quotation EURUSD (EUR\/USD) 1.5465 is the price of the Euro expressed in US dollars, meaning 1 euro = 1.5465 dollars. The market convention is to quote most exchange rates against the USD with the US dollar as the base currency (e.g. USDJPY, USDCAD, USDCHF). The exceptions are the British pound (GBP), Australian dollar (AUD), the New Zealand dollar (NZD) and the euro (EUR) where the USD is the counter currency (e.g. GBPUSD, AUDUSD, NZDUSD, EURUSD).[citation needed]  The factors affecting XXX will affect both XXXYYY and XXXZZZ. This causes a positive currency correlation between XXXYYY and XXXZZZ.  On the spot market, according to the 2022 Triennial Survey, the most heavily traded bilateral currency pairs were:  The U.S. currency was involved in 88.5% of transactions, followed by the euro (30.5%), the yen (16.7%), and sterling (12.9%) (see table). Volume percentages for all individual currencies should add up to 200%, as each transaction involves two currencies.  Trading in the euro has grown considerably since the currency's creation in January 1999, and how long the foreign exchange market will remain dollar-centered is open to debate. Until recently, trading the euro versus a non-European currency ZZZ would have usually involved two trades: EURUSD and USDZZZ. The exception to this is EURJPY, which is an established traded currency pair in the interbank spot market.  In a fixed exchange rate regime, exchange rates are decided by the government, while a number of theories have been proposed to explain (and predict) the fluctuations in exchange rates in a floating exchange rate regime, including:  None of the models developed so far succeed to explain exchange rates and volatility in the longer time frames. For shorter time frames (less than a few days), algorithms can be devised to predict prices. It is understood from the above models that many macroeconomic factors affect the exchange rates and in the end currency prices are a result of dual forces of supply and demand. The world's currency markets can be viewed as a huge melting pot: in a large and ever-changing mix of current events, supply and demand factors are constantly shifting, and the price of one currency in relation to another shifts accordingly. No other market encompasses (and distills) as much of what is going on in the world at any given time as foreign exchange.[79]  Supply and demand for any given currency, and thus its value, are not influenced by any single element, but rather by several. These elements generally fall into three categories: economic factors, political conditions, and market psychology.  Economic factors include: (a) economic policy, disseminated by government agencies and central banks, (b) economic conditions, generally revealed through economic reports, and other economic indicators.  Internal, regional, and international political conditions and events can have a profound effect on currency markets.  All exchange rates are susceptible to political instability and anticipations about the new ruling party. Political upheaval and instability can have a negative impact on a nation's economy. For example, destabilization of coalition governments in Pakistan and Thailand can negatively affect the value of their currencies. Similarly, in a country experiencing financial difficulties, the rise of a political faction that is perceived to be fiscally responsible can have the opposite effect. Also, events in one country in a region may spur positive\/negative interest in a neighboring country and, in the process, affect its currency.  Market psychology and trader perceptions influence the foreign exchange market in a variety of ways:  A spot transaction is a two-day delivery transaction (except in the case of trades between the US dollar, Canadian dollar, Turkish lira, euro and Russian ruble, which settle the next business day), as opposed to the futures contracts, which are usually three months. This trade represents a “direct exchange” between two currencies, has the shortest time frame, involves cash rather than a contract, and interest is not included in the agreed-upon transaction.  Spot trading is one of the most common types of forex trading. Often, a forex broker will charge a small fee to the client to roll-over the expiring transaction into a new identical transaction for a continuation of the trade. This roll-over fee is known as the \"swap\" fee.  One way to deal with the foreign exchange risk is to engage in a forward transaction. In this transaction, money does not actually change hands until some agreed upon future date. A buyer and seller agree on an exchange rate for any date in the future, and the transaction occurs on that date, regardless of what the market rates are then. The duration of the trade can be one day, a few days, months or years. Usually the date is decided by both parties. Then the forward contract is negotiated and agreed upon by both parties.  Forex banks, ECNs, and prime brokers offer NDF contracts, which are derivatives that have no real deliver-ability.  NDFs are popular for currencies with restrictions such as the Argentinian peso.  In fact, a forex hedger can only hedge such risks with NDFs, as currencies such as the Argentinian peso cannot be traded on open markets like major currencies.[84]  The most common type of forward transaction is the foreign exchange swap. In a swap, two parties exchange currencies for a certain length of time and agree to reverse the transaction at a later date. These are not standardized contracts and are not traded through an exchange. A deposit is often required in order to hold the position open until the transaction is completed.  Futures are standardized forward contracts and are usually traded on an exchange created for this purpose. The average contract length is roughly 3 months. Futures contracts are usually inclusive of any interest amounts.  Currency futures contracts are contracts specifying a standard volume of a particular currency to be exchanged on a specific settlement date. Thus the currency futures contracts are similar to forward contracts in terms of their obligation, but differ from forward contracts in the way they are traded. In addition, Futures are daily settled removing credit risk that exist in Forwards.[85] They are commonly used by MNCs to hedge their currency positions. In addition they are traded by speculators who hope to capitalize on their expectations of exchange rate movements.  A foreign exchange option (commonly shortened to just FX option) is a derivative where the owner has the right but not the obligation to exchange money denominated in one currency into another currency at a pre-agreed exchange rate on a specified date. The FX options market is the deepest, largest and most liquid market for options of any kind in the world.  Controversy about currency speculators and their effect on currency devaluations and national economies recurs regularly. Economists, such as Milton Friedman, have argued that speculators ultimately are a stabilizing influence on the market, and that stabilizing speculation performs the important function of providing a market for hedgers and transferring risk from those people who don't wish to bear it, to those who do.[86] Other economists, such as Joseph Stiglitz, consider this argument to be based more on politics and a free market philosophy than on economics.[87]  Large hedge funds and other well capitalized \"position traders\" are the main professional speculators. According to some economists, individual traders could act as \"noise traders\" and have a more destabilizing role than larger and better informed actors.[88]  Currency speculation is considered a highly suspect activity in many countries, such as Thailand.[89] While investment in traditional financial instruments like bonds or stocks often is considered to contribute positively to economic growth by providing capital, currency speculation does not; according to this view, it is simply gambling that often interferes with economic policy. For example, in 1992, currency speculation forced Sweden's central bank, the Riksbank, to raise interest rates for a few days to 500% per annum, and later to devalue the krona.[90] Mahathir Mohamad, one of the former Prime Ministers of Malaysia, is one well-known proponent of this view. He blamed the devaluation of the Malaysian ringgit in 1997 on George Soros and other speculators.  Gregory Millman reports on an opposing view, comparing speculators to \"vigilantes\" who simply help \"enforce\" international agreements and anticipate the effects of basic economic \"laws\" in order to profit.[91] In this view, countries may develop unsustainable economic bubbles or otherwise mishandle their national economies, and foreign exchange speculators made the inevitable collapse happen sooner. A relatively quick collapse might even be preferable to continued economic mishandling, followed by an eventual, larger, collapse. Mahathir Mohamad and other critics of speculation are viewed as trying to deflect the blame from themselves for having caused the unsustainable economic conditions.  Risk aversion is a kind of trading behavior exhibited by the foreign exchange market when a potentially adverse event happens that may affect market conditions. This behavior is caused when risk averse traders liquidate their positions in risky assets and shift the funds to less risky assets due to uncertainty.  In the context of the foreign exchange market, traders liquidate their positions in various currencies to take up positions in safe-haven currencies, such as the US dollar.[92] Sometimes, the choice of a safe haven currency is more of a choice based on prevailing sentiments rather than one of economic statistics. An example would be the financial crisis of 2008. The value of equities across the world fell while the US dollar strengthened (see Fig.1). This happened despite the strong focus of the crisis in the US.[93]  Currency carry trade refers to the act of borrowing one currency that has a low interest rate in order to purchase another with a higher interest rate. A large difference in rates can be highly profitable for the trader, especially if high leverage is used. However, with all levered investments this is a double edged sword, and large exchange rate price fluctuations can suddenly swing trades into huge losses. "},"meta":{},"created_at":"2025-03-22T14:25:42.277677Z","updated_at":"2025-03-22T14:25:42.277677Z","inner_id":48,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":57,"annotations":[{"id":57,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.319900Z","updated_at":"2025-03-22T14:25:42.319900Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"744aa008-d785-45ad-8807-3a2778c2af3c","import_id":null,"last_action":null,"bulk_created":false,"task":57,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Personal finance is the financial management that an individual or a family unit performs to budget, save, and spend monetary resources in a controlled manner, taking into account various financial risks and future life events.  When planning personal finances, the individual would take into account the suitability of various banking products (checking accounts, savings accounts, credit cards, and loans), insurance products (health insurance, disability insurance, life insurance, etc.), and investment products (bonds, stocks, real estate, etc.), as well as participation in monitoring and management of credit scores, income taxes, retirement funds and pensions.  Before a specialty in personal finance was developed, various disciplines which are closely related to it, such as family economics, and consumer economics, were taught in various colleges as part of home economics for over 100 years.[1]  The earliest known research in personal finance was done in 1920 by Hazel Kyrk. Her dissertation at University of Chicago laid the foundation of consumer economics and family economics.[1] Margaret Reid, a professor of Home Economics at the same university, is recognized as one of the pioneers in the study of consumer behavior and Household behavior.[1][2]  In 1947, Herbert A. Simon, a Nobel laureate, suggested that a decision-maker did not always make the best financial decision because of limited educational resources and personal inclinations.[1] In 2009, Dan Ariely suggested the 2008 financial crisis showed that human beings do not always make rational financial decisions, and the market is not necessarily automated and corrective of any imbalances in the economy.[1][3]  Research into personal finance is based on several theories, such as social exchange theory and andragogy (adult learning theory). In America, professional bodies such as American Association of Family and Consumer Sciences and the American Council on Consumer Interests started to play an important role in developing this field from the 1950s to the 1970s. The establishment of the Association for Financial Counseling and Planning Education (AFCPE) in 1984 at Iowa State University and the Academy of Financial Services (AFS) in 1985 marked an important milestone in personal finance history in the US. Attendances of the two societies mainly come from faculty and graduates from business and home economics colleges. AFCPE started to offered several certifications for professionals in this field, such as Accredited Financial Counselor (AFC) and Certified Housing Counselor (CHC). Meanwhile, AFS cooperates with Certified Financial Planner (CFP Board).[1]  Before 1990, the study of personal finance received little attention from mainstream economists and business faculties. However, several American universities such as Brigham Young University, Iowa State University, and San Francisco State University started to offer financial educational programs in both undergraduate and graduate programs since the 1990s. These institutions published several works in journals such as The Journal of Financial Counseling and Planning and the Journal of Personal Finance.  As the concerns about consumers' financial capability increased during the early 2000s, various education programs emerged, catering to a broad audience or a specific group of people, such as youth and women. The educational programs are frequently known as \"financial literacy\". However, there was no standardized curriculum for personal finance education until after the 2008 financial crisis. The United States President's Advisory Council on Financial Capability was set up in 2008 to encourage financial literacy among the American people. It also stressed the importance of developing a standard in financial education.[1]  It is hard to define universal personal finance principles because:  A financial advisor can offer personalized advice in complicated situations and for high-wealth individuals. Still, University of Chicago professor Harold Pollack and personal finance writer Helaine Olen argue that in the United States, good personal finance advice boils down to a few simple points:[4]  The limits stated by laws may be different in each country; in any case personal finance should not disregard correct behavioral principles and the diligence of a \"good family father\": people should not develop attachment to the idea of money, morally reprehensible, and, when investing, should maintain the medium-long-term horizon avoiding hazards in the expected return of investment.  The key component of personal finance is financial planning which is a dynamic process requiring regular monitoring and re-evaluation. In general, it involves five steps:[5][6]  Typical goals that most adults and young adults have are paying off credit card\/student loan\/housing\/car loan debt, investing for retirement, investing for college costs for children, and paying medical expenses.[7]  In the modern world, there is a growing need for people to understand and take control of their finances because of the following reasons:  1. Lack of comprehensive formal education: Although many countries have some formal education for personal finance, the Organization for Economic Co-operation and Development (OECD) studies show low financial literacy in areas it is not required, even in developed countries like Japan.  This illustrates the importance of learning personal finance from an early stage,[9] to differentiate between needs vs. wants,[10] improve financial literacy, and to build financial planning skills..  2. Shortened employable age: Over the years, with the advent of automation [11] and changing needs; it has been witnessed across the globe that several jobs that require manual intervention or that are mechanical are increasingly becoming redundant.  These are some of the reasons why individuals should start planning for their retirement and systematically build on their retirement corpus,[15] hence the need for personal finance.  3. Increased life expectancy:[16] With the developments in healthcare, people today live till a much older age than previous generations. The average life expectancy has increased even in developing economies. The average life expectancy has gradually shifted from 60 to 81[16] and upwards, which coupled with a shorter employable age reinforces the need for a large enough retirement corpus and the importance of personal finance.  4. Rising medical expenses:[17] Medical expenses including cost of prescription medicine, hospital admission care and charges, nursing care, specialized care, geriatric care have all seen an exponential rise over the years. Many of these medical expenses are not covered through insurance policies that might either be private\/individual insurance coverage or through federal or national insurance coverage.  These reasons illustrate the need to have medical, accidental, critical illness, life coverage insurance for oneself and one's family as well as the need for emergency corpus;.[20]  Critical areas of personal financial planning, as suggested by the Financial Planning Standards Board, are:[21]  Although credit can provide a variety of benefits and opportunities to the borrower, it is important to fully understand the advantages and disadvantages of borrowing to ensure sound financial decisions.[22] Using credit indiscriminately and lack of sufficient education can land an individual into debt and disadvantaged situations. Typical downsides of using credit are:  According to a survey done by Harris Interactive, 99% of the adults agreed that personal finance should be taught in schools.[24] Financial authorities and the American federal government had offered free educational materials online to the public. However, a Bank of America poll found that 42% of adults were discouraged. In comparison, 28% of adults thought that personal finance is difficult because of the vast amount of online information. As of 2015, 17 out of 50 states in the United States require high school students to study personal finance before graduation.[25][26] The effectiveness of financial education on general audience is controversial. For example, a study by Bell, Gorin, and Hogarth (2009) stated that financial education graduates were more likely to use a formal spending plan. Financially educated high school students are more likely to have a savings account with regular savings, fewer overdrafts, and more likely to pay off their credit card balances. However, another study done by Cole and Shastry (Harvard Business School, 2009) found that there were no differences in saving behaviors of people in American states with financial literacy mandate enforced and the states without a literacy mandate.[1]  Kiplinger publishes magazines on personal finance.[27] "},"meta":{},"created_at":"2025-03-22T14:25:42.277677Z","updated_at":"2025-03-22T14:25:42.277677Z","inner_id":49,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":58,"annotations":[{"id":58,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.319900Z","updated_at":"2025-03-22T14:25:42.319900Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"0b5a7b8a-9884-4e34-b635-75d693824631","import_id":null,"last_action":null,"bulk_created":false,"task":58,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Insurance is a means of protection from financial loss in which, in exchange for a fee, a party agrees to compensate another party in the event of a certain loss, damage, or injury. It is a form of risk management, primarily used to protect against the risk of a contingent or uncertain loss.  An entity which provides insurance is known as an insurer, insurance company, insurance carrier, or underwriter. A person or entity who buys insurance is known as a policyholder, while a person or entity covered under the policy is called an insured. The insurance transaction involves the policyholder assuming a guaranteed, known, and relatively small loss in the form of a payment to the insurer (a premium) in exchange for the insurer's promise to compensate the insured in the event of a covered loss. The loss may or may not be financial, but it must be reducible to financial terms. Furthermore, it usually involves something in which the insured has an insurable interest established by ownership, possession, or pre-existing relationship.  The insured receives a contract, called the insurance policy, which details the conditions and circumstances under which the insurer will compensate the insured, or their designated beneficiary or assignee. The amount of money charged by the insurer to the policyholder for the coverage set forth in the insurance policy is called the premium. If the insured experiences a loss which is potentially covered by the insurance policy, the insured submits a claim to the insurer for processing by a claims adjuster. A mandatory out-of-pocket expense required by an insurance policy before an insurer will pay a claim is called a deductible (or if required by a health insurance policy, a copayment). The insurer may hedge its own risk by taking out reinsurance, whereby another insurance company agrees to carry some of the risks, especially if the primary insurer deems the risk too large for it to carry.  Methods for transferring or distributing risk were practiced by Chinese and Indian traders as long ago as the 3rd and 2nd millennia BC, respectively.[1][2] Chinese merchants travelling treacherous river rapids would redistribute their wares across many vessels to limit the loss due to any single vessel capsizing.  Codex Hammurabi Law 238 (c. 1755–1750 BC) stipulated that a sea captain, ship-manager, or ship charterer that saved a ship from total loss was only required to pay one-half the value of the ship to the ship-owner.[3][4][5] In the Digesta seu Pandectae (533), the second volume of the codification of laws ordered by Justinian I (527–565), a legal opinion written by the Roman jurist Paulus in 235 AD was included about the Lex Rhodia (\"Rhodian law\"). It articulates the general average principle of marine insurance established on the island of Rhodes in approximately 1000 to 800 BC, plausibly by the Phoenicians during the proposed Dorian invasion and emergence of the purported Sea Peoples during the Greek Dark Ages (c. 1100–c. 750).[6][7][8]  The law of general average is the fundamental principle that underlies all insurance.[7] In 1816, an archeological excavation in Minya, Egypt produced a Nerva–Antonine dynasty-era tablet from the ruins of the Temple of Antinous in Antinoöpolis, Aegyptus. The tablet prescribed the rules and membership dues of a burial society collegium established in Lanuvium, Italia in approximately 133 AD during the reign of Hadrian (117–138) of the Roman Empire.[7] In 1851 AD, future U.S. Supreme Court Associate Justice Joseph P. Bradley (1870–1892 AD), once employed as an actuary for the Mutual Benefit Life Insurance Company, submitted an article to the Journal of the Institute of Actuaries. His article detailed an historical account of a Severan dynasty-era life table compiled by the Roman jurist Ulpian in approximately 220 AD that was also included in the Digesta.[9]  Concepts of insurance has been also found in 3rd century BC Hindu scriptures such as Dharmasastra, Arthashastra and Manusmriti.[10] The ancient Greeks had marine loans. Money was advanced on a ship or cargo, to be repaid with large interest if the voyage prospers. However, the money would not be repaid at all if the ship were lost, thus making the rate of interest high enough to pay for not only for the use of the capital but also for the risk of losing it (fully described by Demosthenes). Loans of this character have ever since been common in maritime lands under the name of bottomry and respondentia bonds.[11]  The direct insurance of sea-risks for a premium paid independently of loans began in Belgium about 1300 AD.[11]  Separate insurance contracts (i.e., insurance policies not bundled with loans or other kinds of contracts) were invented in Genoa in the 14th century, as were insurance pools backed by pledges of landed estates. The first known insurance contract dates from Genoa in 1347. In the next century, maritime insurance developed widely, and premiums were varied with risks.[12] These new insurance contracts allowed insurance to be separated from investment, a separation of roles that first proved useful in marine insurance.  The earliest known policy of life insurance was made in the Royal Exchange, London, on 18 June 1583, for £383, 6s. 8d. for twelve months on the life of William Gibbons.[11]  Insurance became far more sophisticated in Enlightenment-era Europe, where specialized varieties developed.  Property insurance as we know it today can be traced to the Great Fire of London, which in 1666 devoured more than 13,000 houses. The devastating effects of the fire converted the development of insurance \"from a matter of convenience into one of urgency, a change of opinion reflected in Sir Christopher Wren's inclusion of a site for \"the Insurance Office\" in his new plan for London in 1667.\"[13] A number of attempted fire insurance schemes came to nothing, but in 1681, economist Nicholas Barbon and eleven associates established the first fire insurance company, the \"Insurance Office for Houses\", at the back of the Royal Exchange to insure brick and frame homes. Initially, 5,000 homes were insured by his Insurance Office.[14]  At the same time, the first insurance schemes for the underwriting of business ventures became available. By the end of the seventeenth century, London's growth as a centre for trade was increasing due to the demand for marine insurance. In the late 1680s, Edward Lloyd opened a coffee house, which became the meeting place for parties in the shipping industry wishing to insure cargoes and ships, including those willing to underwrite such ventures. These informal beginnings led to the establishment of the insurance market Lloyd's of London and several related shipping and insurance businesses.[15]  Life insurance policies were taken out in the early 18th century. The first company to offer life insurance was the Amicable Society for a Perpetual Assurance Office, founded in London in 1706 by William Talbot and Sir Thomas Allen.[16][17] Upon the same principle, Edward Rowe Mores established the Society for Equitable Assurances on Lives and Survivorship in 1762.  It was the world's first mutual insurer and it pioneered age based premiums based on mortality rate laying \"the framework for scientific insurance practice and development\" and \"the basis of modern life assurance upon which all life assurance schemes were subsequently based.\"[18]  In the late 19th century \"accident insurance\" began to become available.[19] The first company to offer accident insurance was the Railway Passengers Assurance Company, formed in 1848 in England to insure against the rising number of fatalities on the nascent railway system.   The first international insurance rule was the York Antwerp Rules (YAR) for the distribution of costs between ship and cargo in the event of general average. In 1873 the \"Association for the Reform and Codification of the Law of Nations\", the forerunner of the International Law Association (ILA), was founded in Brussels. It published the first YAR in 1890, before switching to the present title of the \"International Law Association\" in 1895.[20][21]  By the late 19th century governments began to initiate national insurance programs against sickness and old age. Germany built on a tradition of welfare programs in Prussia and Saxony that began as early as in the 1840s. In the 1880s Chancellor Otto von Bismarck introduced old age pensions, accident insurance and medical care that formed the basis for Germany's welfare state.[22][23] In Britain more extensive legislation was introduced by the Liberal government in the National Insurance Act 1911. This gave the British working classes the first contributory system of insurance against illness and unemployment.[24] This system was greatly expanded after the Second World War under the influence of the Beveridge Report, to form the first modern welfare state.[22][25]  In 2008, the International Network of Insurance Associations (INIA), then an informal network, became active and it has been succeeded by the Global Federation of Insurance Associations (GFIA), which was formally founded in 2012 to aim to increase insurance industry effectiveness in providing input to international regulatory bodies and to contribute more effectively to the international dialogue on issues of common interest. It consists of its 40 member associations and 1 observer association in 67 countries, which companies account for around 89% of total insurance premiums worldwide.[26]  Insurance involves pooling funds from many insured entities (known as exposures) to pay for the losses that only some insureds may incur. The insured entities are therefore protected from risk for a fee, with the fee being dependent upon the frequency and severity of the event occurring. In order to be an insurable risk, the risk insured against must meet certain characteristics. Insurance as a financial intermediary is a commercial enterprise and a major part of the financial services industry, but individual entities can also self-insure through saving money for possible future losses.[27]  Risk which can be insured by private companies typically share seven common characteristics:[28]  When a company insures an individual entity, there are basic legal requirements and regulations. Several commonly cited legal principles of insurance include:[29]  To \"indemnify\" means to make whole again, or to be reinstated to the position that one was in, to the extent possible, prior to the happening of a specified event or peril. Accordingly, life insurance is generally not considered to be indemnity insurance, but rather \"contingent\" insurance (i.e., a claim arises on the occurrence of a specified event). There are generally three types of insurance contracts that seek to indemnify an insured:  From an insured's standpoint, the result is usually the same: the insurer pays the loss and claims expenses.  If the Insured has a \"reimbursement\" policy, the insured can be required to pay for a loss and then be \"reimbursed\" by the insurance carrier for the loss and out of pocket costs including, with the permission of the insurer, claim expenses.[30][note 1]  Under a \"pay on behalf\" policy, the insurance carrier would defend and pay a claim on behalf of the insured who would not be out of pocket for anything. Most modern liability insurance is written on the basis of \"pay on behalf\" language, which enables the insurance carrier to manage and control the claim.  Under an \"indemnification\" policy, the insurance carrier can generally either \"reimburse\" or \"pay on behalf of\", whichever is more beneficial to it and the insured in the claim handling process.  An entity seeking to transfer risk (an individual, corporation, or association of any type, etc.) becomes the \"insured\" party once risk is assumed by an \"insurer\", the insuring party, by means of a contract, called an insurance policy. Generally, an insurance contract includes, at a minimum, the following elements: identification of participating parties (the insurer, the insured, the beneficiaries), the premium, the period of coverage, the particular loss event covered, the amount of coverage (i.e., the amount to be paid to the insured or beneficiary in the event of a loss), and exclusions (events not covered). An insured is thus said to be \"indemnified\" against the loss covered in the policy.  When insured parties experience a loss for a specified peril, the coverage entitles the policyholder to make a claim against the insurer for the covered amount of loss as specified by the policy. The fee paid by the insured to the insurer for assuming the risk is called the premium. Insurance premiums from many insureds are used to fund accounts reserved for later payment of claims – in theory for a relatively few claimants – and for overhead costs. So long as an insurer maintains adequate funds set aside for anticipated losses (called reserves), the remaining margin is an insurer's profit.  Policies typically include a number of exclusions, for example:  Insurers may prohibit certain activities which are considered dangerous and therefore excluded from coverage. One system for classifying activities according to whether they are authorised by insurers refers to \"green light\" approved activities and events, \"yellow light\" activities and events which require insurer consultation and\/or waivers of liability, and \"red light\" activities and events which are prohibited and outside the scope of insurance cover.[33]  Insurance can have various effects on society through the way that it changes who bears the cost of losses and damage. On one hand it can increase fraud; on the other it can help societies and individuals prepare for catastrophes and mitigate the effects of catastrophes on both households and societies.  Insurance can influence the probability of losses through moral hazard, insurance fraud, and preventive steps by the insurance company. Insurance scholars have typically used moral hazard to refer to the increased loss due to unintentional carelessness and insurance fraud to refer to increased risk due to intentional carelessness or indifference.[34] Insurers attempt to address carelessness through inspections, policy provisions requiring certain types of maintenance, and possible discounts for loss mitigation efforts. While in theory insurers could encourage investment in loss reduction, some commentators have argued that in practice insurers had historically not aggressively pursued loss control measures—particularly to prevent disaster losses such as hurricanes—because of concerns over rate reductions and legal battles. However, since about 1996 insurers have begun to take a more active role in loss mitigation, such as through building codes.[35]  According to the study books of The Chartered Insurance Institute, there are variant methods of insurance as follows:  Insurers may use the subscription business model, collecting premium payments periodically in return for on-going and\/or compounding benefits offered to policyholders.  Insurers' business model aims to collect more in premium and investment income than is paid out in losses, and to also offer a competitive price which consumers will accept. Profit can be reduced to a simple equation:  Insurers make money in two ways:  The most complicated aspect of insuring is the actuarial science of ratemaking (price-setting) of policies, which uses statistics and probability to approximate the rate of future claims based on a given risk. After producing rates, the insurer will use discretion to reject or accept risks through the underwriting process.  At the most basic level, initial rate-making involves looking at the frequency and severity of insured perils and the expected average payout resulting from these perils. Thereafter an insurance company will collect historical loss-data, bring the loss data to present value, and compare these prior losses to the premium collected in order to assess rate adequacy.[36] Loss ratios and expense loads are also used. Rating for different risk characteristics involves—at the most basic level—comparing the losses with \"loss relativities\"—a policy with twice as many losses would, therefore, be charged twice as much. More complex multivariate analyses are sometimes used when multiple characteristics are involved and a univariate analysis could produce confounded results. Other statistical methods may be used in assessing the probability of future losses.  Upon termination of a given policy, the amount of premium collected minus the amount paid out in claims is the insurer's underwriting profit on that policy. Underwriting performance is measured by something called the \"combined ratio\", which is the ratio of expenses\/losses to premiums.[37] A combined ratio of less than 100% indicates an underwriting profit, while anything over 100 indicates an underwriting loss. A company with a combined ratio over 100% may nevertheless remain profitable due to investment earnings.  Insurance companies earn investment profits on \"float\". Float, or available reserve, is the amount of money on hand at any given moment that an insurer has collected in insurance premiums but has not paid out in claims. Insurers start investing insurance premiums as soon as they are collected and continue to earn interest or other income on them until claims are paid out. The Association of British Insurers (grouping together 400 insurance companies and 94% of UK insurance services) has almost 20% of the investments in the London Stock Exchange.[38] In 2007, U.S. industry profits from float totaled $58 billion. In a 2009 letter to investors, Warren Buffett wrote, \"we were paid $2.8 billion to hold our float in 2008\".[39]  In the United States, the underwriting loss of property and casualty insurance companies was $142.3 billion in the five years ending 2003. But overall profit for the same period was $68.4 billion, as the result of float. Some insurance-industry insiders, most notably Hank Greenberg, do not believe that it is possible to sustain a profit from float forever without an underwriting profit as well, but this opinion is not universally held. Reliance on float for profit has led some industry experts to call insurance companies \"investment companies that raise the money for their investments by selling insurance\".[40]  Naturally, the float method is difficult to carry out in an economically depressed period. Bear markets do cause insurers to shift away from investments and to toughen up their underwriting standards, so a poor economy generally means high insurance-premiums. This tendency to swing between profitable and unprofitable periods over time is commonly known as the underwriting, or insurance, cycle.[41]  Claims and loss handling is the materialized utility of insurance; it is the actual \"product\" paid for. Claims may be filed by insureds directly with the insurer or through brokers or agents. The insurer may require that the claim be filed on its own proprietary forms, or may accept claims on a standard industry form, such as those produced by ACORD.  Insurance company claims departments employ a large number of claims adjusters, supported by a staff of records management and data entry clerks. Incoming claims are classified based on severity and are assigned to adjusters, whose settlement authority varies with their knowledge and experience. An adjuster undertakes an investigation of each claim, usually in close cooperation with the insured, determines if coverage is available under the terms of the insurance contract (and if so, the reasonable monetary value of the claim), and authorizes payment.  Policyholders may hire their own public adjusters to negotiate settlements with the insurance company on their behalf. For policies that are complicated, where claims may be complex, the insured may take out a separate insurance policy add-on, called loss recovery insurance, which covers the cost of a public adjuster in the case of a claim.  Adjusting liability insurance claims is particularly difficult because they involve a third party, the plaintiff, who is under no contractual obligation to cooperate with the insurer and may in fact regard the insurer as a deep pocket. The adjuster must obtain legal counsel for the insured—either inside (\"house\") counsel or outside (\"panel\") counsel, monitor litigation that may take years to complete, and appear in person or over the telephone with settlement authority at a mandatory settlement conference when requested by a judge.  If a claims adjuster suspects underinsurance, the condition of average may come into play to limit the insurance company's exposure.  In managing the claims-handling function, insurers seek to balance the elements of customer satisfaction, administrative handling expenses, and claims overpayment leakages. In addition to this balancing act, fraudulent insurance practices are a major business risk that insurers must manage and overcome. Disputes between insurers and insureds over the validity of claims or claims-handling practices occasionally escalate into litigation (see insurance bad faith).  Insurers will often use insurance agents to initially market or underwrite their customers. Agents can be captive, meaning they write only for one company, or independent, meaning that they can issue policies from several companies. The existence and success of companies using insurance agents is likely due to the availability of improved and personalised services. Companies also use Broking firms, Banks and other corporate entities (like Self Help Groups, Microfinance Institutions, NGOs, etc.) to market their products.[42]  Any risk that can be quantified can potentially be insured. Specific kinds of risk that may give rise to claims are known as perils. An insurance policy will set out in detail which perils are covered by the policy and which are not. Below are non-exhaustive lists of the many different types of insurance that exist. A single policy may cover risks in one or more of the categories set out below. For example, vehicle insurance would typically cover both the property risk (theft or damage to the vehicle) and the liability risk (legal claims arising from an accident). A home insurance policy in the United States typically includes coverage for damage to the home and the owner's belongings, certain legal claims against the owner, and even a small amount of coverage for medical expenses of guests who are injured on the owner's property.  Business insurance can take a number of different forms, such as the various kinds of professional liability insurance, also called professional indemnity (PI), which are discussed below under that name; and the business owner's policy (BOP), which packages into one policy many of the kinds of coverage that a business owner needs, in a way analogous to how homeowners' insurance packages the coverages that a homeowner needs.[43]  Vehicle insurance protects the policyholder against financial loss in the event of an incident involving a vehicle they own, such as in a traffic collision.  Coverage typically includes:  GAP (Guaranteed Asset Protection) insurance covers the excess amount on an auto loan in an instance where the policyholder's insurance company does not cover the entire loan. Depending on the company's specific policies it might or might not cover the deductible as well. This coverage is marketed for those who put low down payments, have high interest rates on their loans, and those with 60-month or longer terms. Gap insurance is typically offered by a finance company when the vehicle owner purchases their vehicle, but many auto insurance companies offer this coverage to consumers as well.  Health insurance policies cover the cost of medical treatments. Dental insurance, like medical insurance, protects policyholders for dental costs. In most developed countries, all citizens receive some health coverage from their governments, paid through taxation. In most countries, health insurance is often part of an employer's benefits.  Casualty insurance insures against accidents, not necessarily tied to any specific property. It is a broad spectrum of insurance that a number of other types of insurance could be classified, such as auto, workers compensation, and some liability insurances.  Life insurance provides a monetary benefit to a decedent's family or other designated beneficiary, and may specifically provide for income to an insured person's family, burial, funeral and other final expenses. Life insurance policies often allow the option of having the proceeds paid to the beneficiary either in a lump sum cash payment or an annuity. In most states, a person cannot purchase a policy on another person without their knowledge.  Annuities provide a stream of payments and are generally classified as insurance because they are issued by insurance companies, are regulated as insurance, and require the same kinds of actuarial and investment management expertise that life insurance requires. Annuities and pensions that pay a benefit for life are sometimes regarded as insurance against the possibility that a retiree will outlive his or her financial resources. In that sense, they are the complement of life insurance and, from an underwriting perspective, are the mirror image of life insurance.  Certain life insurance contracts accumulate cash values, which may be taken by the insured if the policy is surrendered or which may be borrowed against. Some policies, such as annuities and endowment policies, are financial instruments to accumulate or liquidate wealth when it is needed.  In many countries, such as the United States and the UK, the tax law provides that the interest on this cash value is not taxable under certain circumstances. This leads to widespread use of life insurance as a tax-efficient method of saving as well as protection in the event of early death.  In the United States, the tax on interest income on life insurance policies and annuities is generally deferred. However, in some cases the benefit derived from tax deferral may be offset by a low return. This depends upon the insuring company, the type of policy and other variables (mortality, market return, etc.). Moreover, other income tax saving vehicles (e.g., IRAs, 401(k) plans, Roth IRAs) may be better alternatives for value accumulation.  Burial insurance is an old type of life insurance which is paid out upon death to cover final expenses, such as the cost of a funeral. The Greeks and Romans introduced burial insurance c. 600 CE when they organized guilds called \"benevolent societies\" which cared for the surviving families and paid funeral expenses of members upon death. Guilds in the Middle Ages served a similar purpose, as did friendly societies during Victorian times.  Property insurance provides protection against risks to property, such as fire, theft or weather damage. This may include specialized forms of insurance such as fire insurance, flood insurance, earthquake insurance, home insurance, inland marine insurance or boiler insurance. The term property insurance may, like casualty insurance, be used as a broad category of various subtypes of insurance, some of which are listed below:  Liability insurance is a broad superset that covers legal claims against the insured. Many types of insurance include an aspect of liability coverage. For example, a homeowner's insurance policy will normally include liability coverage which protects the insured in the event of a claim brought by someone who slips and falls on the property; automobile insurance also includes an aspect of liability insurance that indemnifies against the harm that a crashing car can cause to others' lives, health, or property. The protection offered by a liability insurance policy is twofold: a legal defense in the event of a lawsuit commenced against the policyholder and indemnification (payment on behalf of the insured) with respect to a settlement or court verdict. Liability policies typically cover only the negligence of the insured, and will not apply to results of wilful or intentional acts by the insured.  Often a commercial insured's liability insurance program consists of several layers. The first layer of insurance generally consists of primary insurance, which provides first dollar indemnity for judgments and settlements up to the limits of liability of the primary policy. Generally, primary insurance is subject to a deductible and obligates the insurer to defend the insured against lawsuits, which is normally accomplished by assigning counsel to defend the insured. In many instances, a commercial insured may elect to self-insure. Above the primary insurance or self-insured retention, the insured may have one or more layers of excess insurance to provide coverage additional limits of indemnity protection. There are a variety of types of excess insurance, including \"stand-alone\" excess policies (policies that contain their own terms, conditions, and exclusions), \"follow form\" excess insurance (policies that follow the terms of the underlying policy except as specifically provided), and \"umbrella\" insurance policies (excess insurance that in some circumstances could provide coverage that is broader than the underlying insurance).[50]  Credit insurance repays some or all of a loan when the borrower is insolvent.  Cyber-insurance is a business lines insurance product intended to provide coverage to corporations from Internet-based risks, and more generally from risks relating to information technology infrastructure, information privacy, information governance liability, and activities related thereto.  Some communities prefer to create virtual insurance among themselves by other means than contractual risk transfer, which assigns explicit numerical values to risk. A number of religious groups, including the Amish and some Muslim groups, depend on support provided by their communities when disasters strike. The risk presented by any given person is assumed collectively by the community who all bear the cost of rebuilding lost property and supporting people whose needs are suddenly greater after a loss of some kind. In supportive communities where others can be trusted to follow community leaders, this tacit form of insurance can work. In this manner the community can even out the extreme differences in insurability that exist among its members. Some further justification is also provided by invoking the moral hazard of explicit insurance contracts.  In the United Kingdom, The Crown (which, for practical purposes, meant the civil service) did not insure property such as government buildings. If a government building was damaged, the cost of repair would be met from public funds because, in the long run, this was cheaper than paying insurance premiums. Since many UK government buildings have been sold to property companies and rented back, this arrangement is now less common.  In the United States, the most prevalent form of self-insurance is governmental risk management pools. They are self-funded cooperatives, operating as carriers of coverage for the majority of governmental entities today, such as county governments, municipalities, and school districts. Rather than these entities independently self-insure and risk bankruptcy from a large judgment or catastrophic loss, such governmental entities form a risk pool. Such pools begin their operations by capitalization through member deposits or bond issuance. Coverage (such as general liability, auto liability, professional liability, workers compensation, and property) is offered by the pool to its members, similar to coverage offered by insurance companies. However, self-insured pools offer members lower rates (due to not needing insurance brokers), increased benefits (such as loss prevention services) and subject matter expertise. Of approximately 91,000 distinct governmental entities operating in the United States, 75,000 are members of self-insured pools in various lines of coverage, forming approximately 500 pools. Although a relatively small corner of the insurance market, the annual contributions (self-insured premiums) to such pools have been estimated up to 17 billion dollars annually.[57]  Insurance companies may provide any combination of insurance types, but are often classified into three groups:[58]  General insurance companies can be further divided into these sub categories.  In most countries, life and non-life insurers are subject to different regulatory regimes and different tax and accounting rules. The main reason for the distinction between the two types of company is that life, annuity, and pension business is long-term in nature – coverage for life assurance or a pension can cover risks over many decades. By contrast, non-life insurance cover usually covers a shorter period, such as one year.  Insurance companies are commonly classified as either mutual or proprietary companies.[59] Mutual companies are owned by the policyholders, while shareholders (who may or may not own policies) own proprietary insurance companies.  Demutualization of mutual insurers to form stock companies, as well as the formation of a hybrid known as a mutual holding company, became common in some countries, such as the United States, in the late 20th century. However, not all states permit mutual holding companies.  Reinsurance companies are insurance companies that provide policies to other insurance companies, allowing them to reduce their risks and protect themselves from substantial losses.[60] The reinsurance market is dominated by a few large companies with huge reserves. A reinsurer may also be a direct writer of insurance risks as well.  Captive insurance companies can be defined as limited-purpose insurance companies established with the specific objective of financing risks emanating from their parent group or groups. This definition can sometimes be extended to include some of the risks of the parent company's customers. In short, it is an in-house self-insurance vehicle. Captives may take the form of a \"pure\" entity, which is a 100% subsidiary of the self-insured parent company; of a \"mutual\" captive, which insures the collective risks of members of an industry; and of an \"association\" captive, which self-insures individual risks of the members of a professional, commercial or industrial association. Captives represent commercial, economic and tax advantages to their sponsors because of the reductions in costs they help create and for the ease of insurance risk management and the flexibility for cash flows they generate. Additionally, they may provide coverage of risks which is neither available nor offered in the traditional insurance market at reasonable prices.  The types of risk that a captive can underwrite for their parents include property damage, public and product liability, professional indemnity, employee benefits, employers' liability, motor and medical aid expenses. The captive's exposure to such risks may be limited by the use of reinsurance.  Captives are becoming an increasingly important component of the risk management and risk financing strategy of their parent. This can be understood against the following background:  Other possible forms for an insurance company include reciprocals, in which policyholders reciprocate in sharing risks, and Lloyd's organizations.[61]  Admitted insurance companies are those in the United States that have been admitted or licensed by the state licensing agency. The insurance they provide is called admitted insurance. Non-admitted companies have not been approved by the state licensing agency, but are allowed to provide insurance under special circumstances when they meet an insurance need that admitted companies cannot or will not meet.[62]  There are also companies known as \"insurance consultants\". Like a mortgage broker, these companies are paid a fee by the customer to shop around for the best insurance policy among many companies. Similar to an insurance consultant, an \"insurance broker\" also shops around for the best insurance policy among many companies. However, with insurance brokers, the fee is usually paid in the form of commission from the insurer that is selected rather than directly from the client.  Neither insurance consultants nor insurance brokers are insurance companies and no risks are transferred to them in insurance transactions. Third party administrators are companies that perform underwriting and sometimes claims handling services for insurance companies. These companies often have special expertise that the insurance companies do not have.  The financial stability and strength of an insurance company is a consideration when buying an insurance contract. An insurance premium paid currently provides coverage for losses that might arise many years in the future. For that reason, a more financially stable insurance carrier reduces the risk of the insurance company becoming insolvent, leaving their policyholders with no coverage (or coverage only from a government-backed insurance pool or other arrangements with less attractive payouts for losses). A number of independent rating agencies provide information and rate the financial viability of insurance companies.  Insurance companies are rated by various agencies such as AM Best. The ratings include the company's financial strength, which measures its ability to pay claims. It also rates financial instruments issued by the insurance company, such as bonds, notes, and securitization products.  Advanced economies account for the bulk of the global insurance industry. According to Swiss Re, the global insurance market wrote $7.186 trillion in direct premiums in 2023.[63]  (\"Direct premiums\" means premiums written directly by insurers before accounting for ceding of risk to reinsurers.) As usual, the United States was the country with the largest insurance market with $3.226 trillion (44.9%) of direct premiums written, with the People's Republic of China coming in second at only $723 billion (10.1%), the United Kingdom coming in third at $374 billion (5.2%), and Japan coming in fourth at $362 billion (5.0%).[63] However, the European Union's single market is the actual second largest market, with 16 percent market share.[63]  In the United States, insurance is regulated by the states under the McCarran–Ferguson Act, with \"periodic proposals for federal intervention\", and a nonprofit coalition of state insurance agencies called the National Association of Insurance Commissioners works to harmonize the country's different laws and regulations.[64] The National Conference of Insurance Legislators (NCOIL) also works to harmonize the different state laws.[65] 1988 California Proposition 103 is claimed to reduce home insurance rates,[66] while it is blamed by some for reduced availability of home insurance in wildfire-distressed neighborhoods.[67]  In the European Union, the Third Non-Life Directive and the Third Life Directive, both passed in 1992 and effective 1994, created a single insurance market in Europe and allowed insurance companies to offer insurance anywhere in the EU (subject to permission from authority in the head office) and allowed insurance consumers to purchase insurance from any insurer in the EU.[68] As far as insurance in the United Kingdom, the Financial Services Authority took over insurance regulation from the General Insurance Standards Council in 2005;[69] laws passed include the Insurance Companies Act 1973 and another in 1982,[70] and reforms to warranty and other aspects under discussion as of 2012[update].[71]  The insurance industry in China was nationalized in 1949 and thereafter offered by only a single state-owned company, the People's Insurance Company of China, which was eventually suspended as demand declined in a communist environment. In 1978, market reforms led to an increase in the market and by 1995 a comprehensive Insurance Law of the People's Republic of China[72] was passed, followed in 1998 by the formation of China Insurance Regulatory Commission (CIRC), which has broad regulatory authority over the insurance market of China.[73]  In India IRDA is insurance regulatory authority. As per the section 4 of IRDA Act 1999, Insurance Regulatory and Development Authority (IRDA), which was constituted by an act of parliament. National Insurance Academy, Pune is apex insurance capacity builder institute promoted with support from Ministry of Finance and by LIC, Life & General Insurance companies.  In 2017, within the framework of the joint project of the Bank of Russia and Yandex, a special check mark (a green circle with a tick and 'Реестр ЦБ РФ' (Unified state register of insurance entities) text box) appeared in the search for Yandex system, informing the consumer that the company's financial services are offered on the marked website, which has the status of an insurance company, a broker or a mutual insurance association.[74]  Insurance is just a risk transfer mechanism wherein the financial burden which may arise due to some fortuitous event is transferred to a bigger entity (i.e., an insurance company) by way of paying premiums. This only reduces the financial burden and not the actual chances of happening of an event. Insurance is a risk for both the insurance company and the insured. The insurance company understands the risk involved and will perform a risk assessment when writing the policy.    As a result, the premiums may go up if they determine that the policyholder will file a claim. However, premiums might reduce if the policyholder commits to a risk management program as recommended by the insurer.[75] It is therefore important that insurers view risk management as a joint initiative between policyholder and insurer since a robust risk management plan minimizes the possibility of a large claim for the insurer while stabilizing or reducing premiums for the policyholder. University of Tennessee research published in 2014 found that all company staff in the businesses they surveyed recognised the importance of insurance but largely they were too distant within their organization from the provision or cost of insurance to be able to relate to company insurance needs.[76]  If a person is financially stable and plans for life's unexpected events, they may be able to go without insurance. However, they must have enough to cover a total and complete loss of employment and of their possessions. Some states will accept a surety bond, a government bond, or even making a cash deposit with the state.[citation needed]  An insurance company may inadvertently find that its insureds may not be as risk-averse as they might otherwise be (since, by definition, the insured has transferred the risk to the insurer), a concept known as moral hazard. This 'insulates' many from the true costs of living with risk, negating measures that can mitigate or adapt to risk and leading some to describe insurance schemes as potentially maladaptive.[77]  Insurance policies can be complex and some policyholders may not understand all the fees and coverages included in a policy. As a result, people may buy policies on unfavorable terms. In response to these issues, many countries have enacted detailed statutory and regulatory regimes governing every aspect of the insurance business, including minimum standards for policies and the ways in which they may be advertised and sold.  For example, most insurance policies in the English language today have been carefully drafted in plain English; the industry learned the hard way that many courts will not enforce policies against insureds when the judges themselves cannot understand what the policies are saying. Typically, courts construe ambiguities in insurance policies against the insurance company and in favor of coverage under the policy.  Many institutional insurance purchasers buy insurance through an insurance broker. While on the surface it appears the broker represents the buyer (not the insurance company), and typically counsels the buyer on appropriate coverage and policy limitations, in the vast majority of cases a broker's compensation comes in the form of a commission as a percentage of the insurance premium, creating a conflict of interest in that the broker's financial interest is tilted toward encouraging an insured to purchase more insurance than might be necessary at a higher price. A broker generally holds contracts with many insurers, thereby allowing the broker to \"shop\" the market for the best rates and coverage possible.  Insurance may also be purchased through an agent. A tied agent, working exclusively with one insurer, represents the insurance company from whom the policyholder buys (while a free agent sells policies of various insurance companies). Just as there is a potential conflict of interest with a broker, an agent has a different type of conflict. Because agents work directly for the insurance company, if there is a claim the agent may advise the client to the benefit of the insurance company. Agents generally cannot offer as broad a range of selection compared to an insurance broker.  An independent insurance consultant advises insureds on a fee-for-service retainer, similar to an attorney, and thus offers completely independent advice, free of the financial conflict of interest of brokers or agents. However, such a consultant must still work through brokers or agents in order to secure coverage for their clients.  In the United States, economists and consumer advocates generally consider insurance to be worthwhile for low-probability, catastrophic losses, but not for high-probability, small losses. Because of this, consumers are advised to select high deductibles and to not insure losses which would not cause a disruption in their life. However, consumers have shown a tendency to prefer low deductibles and to prefer to insure relatively high-probability, small losses over low-probability, perhaps due to not understanding or ignoring the low-probability risk. This is associated with reduced purchasing of insurance against low-probability losses, and may result in increased inefficiencies from moral hazard.[78]  Redlining is the practice of denying insurance coverage in specific geographic areas, supposedly because of a high likelihood of loss, while the alleged motivation is unlawful discrimination. Racial profiling or redlining has a long history in the property insurance industry in the United States. From a review of industry underwriting and marketing materials, court documents, and research by government agencies, industry and community groups, and academics, it is clear that race has long affected and continues to affect the policies and practices of the insurance industry.[79]  In July 2007, the US Federal Trade Commission (FTC) released a report presenting the results of a study concerning credit-based insurance scores in automobile insurance. The study found that these scores are effective predictors of risk. It also showed that African-Americans and Hispanics are substantially overrepresented in the lowest credit scores, and substantially underrepresented in the highest, while Caucasians and Asians are more evenly spread across the scores. The credit scores were also found to predict risk within each of the ethnic groups, leading the FTC to conclude that the scoring models are not solely proxies for redlining. The FTC indicated little data was available to evaluate benefit of insurance scores to consumers.[80] The report was disputed by representatives of the Consumer Federation of America, the National Fair Housing Alliance, the National Consumer Law Center, and the Center for Economic Justice, for relying on data provided by the insurance industry.[81]  All states have provisions in their rate regulation laws or in their fair trade practice acts that prohibit unfair discrimination, often called redlining, in setting rates and making insurance available.[82]  In determining premiums and premium rate structures, insurers consider quantifiable factors, including location, credit scores, gender, occupation, marital status, and education level. However, the use of such factors is often considered to be unfair or unlawfully discriminatory, and the reaction against this practice has in some instances led to political disputes about the ways in which insurers determine premiums and regulatory intervention to limit the factors used.  An insurance underwriter's job is to evaluate a given risk as to the likelihood that a loss will occur. Any factor that causes a greater likelihood of loss should theoretically be charged a higher rate. This basic principle of insurance must be followed if insurance companies are to remain solvent.[citation needed] Thus, \"discrimination\" against (i.e., negative differential treatment of) potential insureds in the risk evaluation and premium-setting process is a necessary by-product of the fundamentals of insurance underwriting.[citation needed] For instance, insurers charge older people significantly higher premiums than they charge younger people for term life insurance. Older people are thus treated differently from younger people (i.e., a distinction is made, discrimination occurs). The rationale for the differential treatment goes to the heart of the risk a life insurer takes: older people are likely to die sooner than young people, so the risk of loss (the insured's death) is greater in any given period of time and therefore the risk premium must be higher to cover the greater risk.[citation needed] However, treating insureds differently when there is no actuarially sound reason for doing so is unlawful discrimination.  New assurance products can now[when?] be protected from copying with a business method patent in the United States.  A recent example of a new insurance product that is patented is Usage Based auto insurance. Early versions were independently invented and patented by a major US auto insurance company, Progressive Auto Insurance (U.S. patent 5,797,134) and a Spanish independent inventor, Salvador Minguijon Perez.[83]  Many independent inventors are in favor of patenting new insurance products since it gives them protection from big companies when they bring their new insurance products to market. Independent inventors account for 70% of the new U.S. patent applications in this area.[citation needed]  Patenting new insurance products can be risky, as it is practically impossible for insurance companies to determine if their product will infringe on a pre-existing patent.[84] For example, in 2004, The Hartford insurance company had to pay $80 million to an independent inventor, Bancorp Services, in order to settle a patent infringement and theft of trade secret lawsuit for a type of corporate owned life insurance product invented and patented by Bancorp.[85]  There are currently about 150 new patent applications on insurance inventions filed per year in the United States. The rate at which patents have been issued has steadily risen from 15 in 2002 to 44 in 2006.[86]  The first US insurance patent was granted in 2005, which concerned coverage of data transferred over the internet.[87] Another example of an application posted was posted in 2009.[88][dead link‍] This patent application describes a method for increasing the ease of changing insurance companies.[89]  Insurance on demand (also IoD) is an insurance service that provides clients with coverage for a specific occasion or event when needed; i.e. only episodic rather than on a 24\/7 basis as is typically provided by traditional policies. For example, air travelers can purchase a policy for one single plane flight, rather than a longer-lasting travel insurance plan.[citation needed]  Certain insurance products and practices have been described as rent-seeking by critics.[citation needed] That is, some insurance products or practices are useful primarily because of legal benefits, such as reducing taxes, as opposed to providing protection against risks of adverse events.[citation needed]  Muslim scholars have varying opinions about life insurance. Life insurance policies that earn interest (or guaranteed bonus\/NAV) are generally considered to be a form of riba (usury) and some consider even policies that do not earn interest to be a form of gharar (speculation). Some argue that gharar is not present due to the actuarial science behind the underwriting.[citation needed] Jewish rabbinical scholars also have expressed reservations regarding insurance as an avoidance of God's will but most find it acceptable in moderation.[90]  Some Christians believe insurance represents a lack of faith.[91][92] There is a long history of resistance to commercial insurance in Anabaptist communities (Mennonites, Amish, Hutterites, Brethren in Christ), but many participate in community-based self-insurance programs that spread risk within their communities.[93][94][95]  Country-specific articles: "},"meta":{},"created_at":"2025-03-22T14:25:42.277677Z","updated_at":"2025-03-22T14:25:42.277677Z","inner_id":50,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":59,"annotations":[{"id":59,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.319900Z","updated_at":"2025-03-22T14:25:42.319900Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"fe63028a-6573-4ccc-9673-2621a9ffcc1d","import_id":null,"last_action":null,"bulk_created":false,"task":59,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Investment is traditionally defined as the \"commitment of resources to achieve later benefits\". If an investment involves money, then it can be defined as a \"commitment of money to receive more money later\". From a broader viewpoint, an investment can be defined as \"to tailor the pattern of expenditure and receipt of resources to optimise the desirable patterns of these flows\". When expenditures and receipts are defined in terms of money, then the net monetary receipt in a time period is termed cash flow, while money received in a series of several time periods is termed cash flow stream.  In finance, the purpose of investing is to generate a return on the invested asset. The return may consist of a capital gain (profit) or loss, realised if the  investment is sold, unrealised capital appreciation (or depreciation) if yet unsold. It may also consist of periodic income such as dividends, interest, or rental income. The return may also include currency gains or losses due to changes in foreign currency exchange rates.  Investors generally expect higher returns from riskier investments. When a low-risk investment is made, the return is also generally low. Similarly, high risk comes with a chance of high losses. Investors, particularly novices, are often advised to diversify their portfolio. Diversification has the statistical effect of reducing overall risk.  In modern economies, traditional investments include:  Alternative investments include:  An investor may bear a risk of loss of some or all of their capital invested. Investment differs from arbitrage, in which profit is generated without investing capital or bearing risk.  Savings bear the (normally remote) risk that the financial provider may default.  Foreign currency savings also bear foreign exchange risk: if the currency of a savings account differs from the account holder's home currency, then there is the risk that the exchange rate between the two currencies will move unfavourably so that the value of the savings account decreases, measured in the account holder's home currency.  Even investing in tangible assets like property has its risk. And similar to most risks, property buyers can seek to mitigate any potential risk by taking out mortgage and by borrowing at a lower loan to security ratio.  In contrast with savings, investments tend to carry more risk, in the form of both a wider variety of risk factors and a greater level of uncertainty.  Industry to industry volatility is more or less of a risk depending. In biotechnology, for example, investors look for big profits on companies that have small market capitalizations but can be worth hundreds of millions quite quickly.[1] The risk is high because approximately 90% of biotechnology products researched do not make it to market due to regulations and the complex demands within pharmacology as the average prescription drug takes 10 years and US$2.5 billion worth of capital.[2]  In the medieval Islamic world, the qirad was a major financial instrument. This was an arrangement between one or more investors and an agent where the investors entrusted capital to an agent who then traded with it in hopes of making a profit.  Both parties then received a previously settled portion of the profit, though the agent was not liable for any losses. Many will notice that the qirad is similar to the institution of the commenda later used in western Europe, though whether the qirad transformed into the commenda or the two institutions evolved independently cannot be stated with certainty.[3]  In the early 1900s, purchasers of stocks, bonds, and other securities were described in media, academia, and commerce as speculators. Since the Wall Street crash of 1929, and particularly by the 1950s, the term \"investment\" had come to denote the more conservative end of the securities spectrum, while \"speculation\" was applied by financial brokers and their advertising agencies to higher risk securities much in vogue at that time.[4] Since the last half of the 20th century, the terms \"speculation\" and \"speculator\" have specifically referred to higher risk ventures.  A value investor buys assets that they believe to be undervalued (and sells overvalued ones). To identify undervalued securities, a value investor uses analysis of the financial reports of the issuer to evaluate the security. Value investors employ accounting ratios, such as earnings per share and sales growth, to identify securities trading at prices below their worth.  Warren Buffett and Benjamin Graham are notable examples of value investors. Graham and Dodd's seminal work, Security Analysis, was written in the wake of the Wall Street Crash of 1929.[5]  The price to earnings ratio (P\/E), or earnings multiple, is a particularly significant and recognized fundamental ratio, with a function of dividing the share price of the stock, by its earnings per share. This will provide the value representing the sum investors are prepared to expend for each dollar of company earnings. This ratio is an important aspect, due to its capacity as measurement for the comparison of valuations of various companies. A stock with a lower P\/E ratio will cost less per share than one with a higher P\/E, taking into account the same level of financial performance; therefore, it essentially means a low P\/E is the preferred option.[6]  An instance in which the price to earnings ratio has a lesser significance is when companies in different industries are compared. For example, although it is reasonable for a telecommunications stock to show a P\/E in the low teens, in the case of hi-tech stock, a P\/E in the 40s range is not unusual. When making comparisons, the P\/E ratio can give you a refined view of a particular stock valuation.  For investors paying for each dollar of a company's earnings, the P\/E ratio is a significant indicator, but the price-to-book ratio (P\/B) is also a reliable indication of how much investors are willing to spend on each dollar of company assets. In the process of the P\/B ratio, the share price of a stock is divided by its net assets; any intangibles, such as goodwill, are not taken into account. It is a crucial factor of the price-to-book ratio, due to it indicating the actual payment for tangible assets and not the more difficult valuation of intangibles. Accordingly, the P\/B could be considered a comparatively conservative metric.  Growth investors seek investments they believe are likely to have higher earnings or greater value in the future. To identify such stocks, growth investors often evaluate measures of current stock value as well as predictions of future financial performance.[7] Growth investors seek profits through capital appreciation – the gains earned when a stock is sold at a higher price than what it was purchased for. The price-to-earnings (P\/E) multiple is also used for this type of investment; growth stock are likely to have a P\/E higher than others in its industry.[8] According to Investopedia author Troy Segal and U.S. Department of State Fulbright fintech research awardee Julius Mansa, growth investing is best suited for investors who prefer relatively shorter investment horizons, higher risks, and are not seeking immediate cash flow through dividends.[7]  Some investors attribute the introduction of the growth investing strategy to investment banker Thomas Rowe Price Jr., who tested and popularized the method in 1950 by introducing his mutual fund, the T. Rowe Price Growth Stock Fund. Price asserted that investors could reap high returns by \"investing in companies that are well-managed in fertile fields.\"[9]  A new form of investing that seems to have caught the attention of investors is Venture Capital. Venture Capital is independently managed dedicated pools of capital that focus on equity or equity-linked investments in privately held, high growth companies.[10]  Momentum investors generally seek to buy stocks that are currently experiencing a short-term uptrend, and they usually sell them once this momentum starts to decrease. Stocks or securities purchased for momentum investing are often characterized by demonstrating consistently high returns for the past three to twelve months.[11] However, in a bear market, momentum investing also involves short-selling securities of stocks that are experiencing a downward trend, because it is believed that these stocks will continue to decrease in value. Essentially, momentum investing generally relies on the principle that a consistently up-trending stock will continue to grow, while a consistently down-trending stock will continue to fall.  Economists and financial analysts have not reached a consensus on the effectiveness of using the momentum investing strategy. Rather than evaluating a company's operational performance, momentum investors instead utilize trend lines, moving averages, and the Average Directional Index (ADX) to determine the existence and strength of trends.[12]  Dollar cost averaging (DCA), also known in the UK as pound-cost averaging, is the process of consistently investing a certain amount of money across regular increments of time, and the method can be used in conjunction with value investing, growth investing, momentum investing, or other strategies. For example, an investor who practices dollar-cost averaging could choose to invest $200 a month for the next 3 years, regardless of the share price of their preferred stock(s), mutual funds, or exchange-traded funds.  Many investors believe that dollar-cost averaging helps minimize short-term volatility by spreading risk out across time intervals and avoiding market timing.[12] Research also shows that DCA can help reduce the total average cost per share in an investment because the method enables the purchase of more shares when their price is lower, and less shares when the price is higher.[12] However, dollar-cost averaging is also generally characterized by more brokerage fees, which could decrease an investor's overall returns.  The term \"dollar-cost averaging\" is believed to have first been coined in 1949 by economist and author Benjamin Graham in his book, The Intelligent Investor. Graham asserted that investors that use DCA are \"likely to end up with a satisfactory overall price for all [their] holdings.\"[13]  Micro-investing is a type of investment strategy that is designed to make investing regular, accessible and affordable, especially for those who may not have a lot of money to invest or who are new to investing.[14][15]  Investments are often made indirectly through intermediary financial institutions. These intermediaries include pension funds, banks, and insurance companies. They may pool money received from a number of individual end investors into funds such as investment trusts, unit trusts, and SICAVs to make large-scale investments. Each individual investor holds an indirect or direct claim on the assets purchased, subject to charges levied by the intermediary, which may be large and varied.  Approaches to investment sometimes referred to in marketing of collective investments include dollar cost averaging and market timing.  Free cash flow measures the cash a company generates which is available to its debt and equity investors, after allowing for reinvestment in working capital and capital expenditure. High and rising free cash flow, therefore, tend to make a company more attractive to investors.  The debt-to-equity ratio is an indicator of capital structure. A high proportion of debt, reflected in a high debt-to-equity ratio, tends to make a company's earnings, free cash flow, and ultimately the returns to its investors, riskier or volatile. Investors compare a company's debt-to-equity ratio with those of other companies in the same industry, and examine trends in debt-to-equity ratios and free cashflow. "},"meta":{},"created_at":"2025-03-22T14:25:42.277677Z","updated_at":"2025-03-22T14:25:42.277677Z","inner_id":51,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":60,"annotations":[{"id":60,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.319900Z","updated_at":"2025-03-22T14:25:42.319900Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"8d920792-8a0d-484a-a460-b2aa421a8110","import_id":null,"last_action":null,"bulk_created":false,"task":60,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  In finance, margin is the collateral that a holder of a financial instrument has to deposit with a counterparty (most often their broker or an exchange) to cover some or all of the credit risk the holder poses for the counterparty. This risk can arise if the holder has done any of the following:  The collateral for a margin account can be the cash deposited in the account or securities provided, and represents the funds available to the account holder for further share trading. On United States futures exchanges, margins were formerly called performance bonds. Most of the exchanges today use SPAN (\"Standard Portfolio Analysis of Risk\") methodology, which was developed by the Chicago Mercantile Exchange in 1988, for calculating margins for options and futures.  A margin account is a loan account with a broker which can be used for share trading. The funds available under the margin loan are determined by the broker based on the securities owned and provided by the trader, which act as collateral for the loan. The broker usually has the right to change the percentage of the value of each security it will allow toward further advances to the trader, and may consequently make a margin call if the balance available falls below the amount actually utilised. In any event, the broker will usually charge interest and other fees on the amount drawn on the margin account.  If the cash balance of a margin account is negative, the amount is owed to the broker, and usually attracts interest. If the cash balance is positive, the money is available to the account holder to reinvest, or may be withdrawn by the holder or left in the account and may earn interest. In terms of futures and cleared derivatives, the margin balance would refer to the total value of collateral pledged to the CCP (central counterparty clearing) and or futures commission merchants.  Margin buying refers to the buying of securities with cash borrowed from a broker, using the bought securities as collateral. This has the effect of magnifying any profit or loss made on the securities. The securities serve as collateral for the loan. The net value—the difference between the value of the securities and the loan—is initially equal to the amount of one's own cash used. This difference has to stay above a minimum margin requirement, the purpose of which is to protect the broker against a fall in the value of the securities to the point that the investor can no longer cover the loan.  Margin lending became popular in the late 1800s as a means to finance railroads.[1] In the 1920s, margin requirements were loose. In other words, brokers required investors to put in very little of their own money, whereas today, the Federal Reserve's margin requirement (under Regulation T) limits debt to 50 percent. During the 1920s leverage rates of up to 90 percent debt were not uncommon.[2] When the stock market started to contract, many individuals received margin calls. They had to deliver more money to their brokers or their shares would be sold. Since many individuals did not have the equity to cover their margin positions, their shares were sold, causing further market declines and further margin calls. This was one of the major contributing factors which led to the stock market crash of 1929, which in turn contributed to the Great Depression.[2] However, as reported in Peter Rappoport and Eugene N. White's 1994 paper published in The American Economic Review, \"Was the Crash of 1929 Expected\",[3] all sources indicate that beginning in either late 1928 or early 1929, \"margin requirements began to rise to historic new levels. The typical peak rates on brokers' loans were 40–50 percent. Brokerage houses followed suit and demanded higher margin from investors\".  For example, Jane buys a share in a company for $100 using $20 of her own money and $80 borrowed from her broker. The net value (the share price minus the amount borrowed) is $20. The broker has a minimum margin requirement of $10. Suppose the share price drops to $85. The net value is now only $5 (the previous net value of $20 minus the share's $15 drop in price), so, to maintain the broker's minimum margin, Jane needs to increase this net value to $10 or more, either by selling the share or repaying part of the loan.  Short selling refers to the selling of securities that the trader does not own, borrowing them from a broker, and using the cash as collateral. This has the effect of reversing any profit or loss made on the securities. The initial cash deposited by the trader, together with the amount obtained from the sale, serve as collateral for the loan. The net value—the difference between the cash amount and the value of loan security—is initially equal to the amount of one's own cash used. This difference has to stay above a minimum margin requirement, the purpose of which is to protect the broker against a rise in the value of the borrowed securities to the point that the investor can no longer cover the loan.  For example, Jane sells a share of stock she does not own for $100 and puts $20 of her own money as collateral, resulting $120 cash in the account. The net value (the cash amount minus the share price) is $20. The broker has a minimum margin requirement of $10. Suppose the share price rises to $115. The net value is now only $5 (the previous net value of $20 minus the share's $15 rise in price), so, to maintain the broker's minimum margin, Jane needs to increase this net value to $10 or more, either by buying the share back or depositing additional cash.  Enhanced leverage is a strategy offered by some brokers that provides 4:1 or 6+:1 leverage. This requires maintaining two sets of accounts, long and short.  The initial margin requirement is the amount of collateral required to open a position. Thereafter, the collateral required until the position is closed is the maintenance requirement. The maintenance requirement is the minimum amount of collateral required to keep the position open and is generally lower than the initial requirement. This allows the price to move against the margin without forcing a margin call immediately after the initial transaction. When the total value of the collateral dips below the maintenance margin requirement, the position holder must pledge additional collateral to bring their total balance back up to or above the initial margin requirement.  On instruments determined to be especially risky, however, either regulators, the exchange, or the broker may set the maintenance requirement higher than normal or equal to the initial requirement to reduce their exposure to the risk accepted by the trader. For speculative futures and derivatives clearing accounts futures commission merchants may charge a premium or margin multiplier to exchange requirements. This is typically an additional 10%–25%.  The broker may at any time revise the value of the collateral securities (margin) after the estimation of the risk, based, for example, on market factors. If this results in the market value of the collateral securities for a margin account falling below the revised margin, the broker or exchange immediately issues a \"margin call\", requiring the investor to bring the margin account back into line. To do so, the investor must either pay funds (the call) into the margin account, provide additional collateral, or dispose of some of the securities. If the investor fails to bring the account back into line, the broker can sell the investor's collateral securities to bring the account back into line.  If a margin call occurs unexpectedly, it can cause a domino effect of selling, which will lead to other margin calls and so forth, effectively crashing an asset class or group of asset classes. The \"Bunker Hunt Day\" crash of the silver market on Silver Thursday, 27 March 1980, is one such example. This situation most frequently happens as a result of an adverse change in the market value of the leveraged asset or contract. It could also happen when the margin requirement is raised, either due to increased volatility or due to legislation. In extreme cases, certain securities may cease to qualify for margin trading; in such a case, the brokerage will require the trader to either fully fund their position, or to liquidate it.  The minimum margin requirement, sometimes called the maintenance margin requirement, is the ratio of (stock equity − leveraged dollars) to stock equity, where \"stock equity\" is the stock price multiplied by the number of shares bought and \"leveraged dollars\" is the amount borrowed in the margin account.  For instance, assume that an investor bought 1,000 shares of a company each priced at $50. If the initial margin requirement were 60%, then stock equity = $50 × 1,000 = $50,000 and leveraged dollars (or amount borrowed) = $50,000 × (100% − 60%) = $20,000.  If the maintenance margin changed to 25%, then the customer would have to maintain a net value equal to 25% of the total stock equity. That means that he or she would have to maintain net equity of $50,000 × 0.25 = $12,500. At what price would the investor get a margin call? For stock price P the stock equity would be (in this example) 1,000P.  So if the stock price dropped from $50 to $26.67, then the investor would be called to add additional funds to the account to make up for the loss in stock equity.  Alternatively, one can calculate P using      P =  P  0      ( 1 −  initial margin requirement  )   ( 1 −  maintenance margin requirement  )       {\\displaystyle \\textstyle P=P_{0}{\\frac {(1-{\\text{initial margin requirement}})}{(1-{\\text{maintenance margin requirement}})}}}   where P0 is the initial price of the stock. Using the same example to demonstrate this:      P = $ 50    ( 1 − 0.6 )   ( 1 − 0.25 )    = $ 26.66.   {\\displaystyle P=\\$50{\\frac {(1-0.6)}{(1-0.25)}}=\\$26.66.}    Margin requirements are reduced for positions that offset each other. For instance spread traders who have offsetting futures contracts do not have to deposit collateral both for their short position and their long position. The exchange calculates the loss in a worst-case scenario of the total position. Similarly an investor who creates a collar has reduced risk since any loss on the call is offset by a gain in the stock, and a large loss in the stock is offset by a gain on the put; in general, covered calls have less strict requirements than naked call writing.  The margin-equity ratio is a term used by speculators, representing the amount of their trading capital that is being held as margin at any particular time. Traders would rarely (and unadvisedly) hold 100% of their capital as margin. The probability of losing their entire capital at some point would be high. By contrast, if the margin-equity ratio is so low as to make the trader's capital equal to the value of the futures contract itself, then they would not profit from the inherent leverage implicit in futures trading. A conservative trader might hold a margin-equity ratio of 15%, while a more aggressive trader might hold 40%.  Return on margin (ROM) is often used to judge performance because it represents the net gain or net loss compared to the exchange's perceived risk as reflected in required margin. ROM may be calculated (realized return) \/ (initial margin). The annualized ROM is equal to  For example, if a trader earns 10% on margin in two months, that would be about 77% annualized  that is, Annualized ROM = 1.16 - 1 = 77%  Sometimes, return on margin will also take into account peripheral charges such as brokerage fees and interest paid on the sum borrowed.  The margin interest rate is usually based on the broker's call. "},"meta":{},"created_at":"2025-03-22T14:25:42.277677Z","updated_at":"2025-03-22T14:25:42.277677Z","inner_id":52,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":61,"annotations":[{"id":61,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.319900Z","updated_at":"2025-03-22T14:25:42.319900Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"1dd3ce06-6d65-4ce7-863b-586aa0b337dc","import_id":null,"last_action":null,"bulk_created":false,"task":61,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  An alternative investment, also known as an alternative asset or alternative investment fund (AIF),[1] is an investment in any asset class excluding capital stocks, bonds, and cash.[2]  The term is a relatively loose one and includes tangible assets such as precious metals,[3] collectibles (art,[4] wine, antiques, vintage cars, coins, watches, musical instruments, or stamps[5]) and some financial assets such as real estate, commodities, private equity, distressed securities, hedge funds, exchange funds, carbon credits,[6] venture capital, film production,[7] financial derivatives, cryptocurrencies, non-fungible tokens, and Tax Receivable Agreements.[8] Investments in real estate, forestry and shipping are also often termed \"alternative\" despite the ancient use of such real assets to enhance and preserve wealth.[9] Alternative investments are to be contrasted with traditional investments.  As the definition of alternative investments is broad, data and research vary widely across the investment classes. For example, art and wine investments may lack high-quality data.[10] The Goizueta Business School at Emory University has established the Emory Center for Alternative Investments to provide research and a forum for discussion regarding private equity, hedge fund, and venture capital investments.  In recent years, the growth of alternative finance has opened up new avenues to investing in alternatives such as the following:  In a 1986 paper, William Baumol used the repeat sale method and compared prices of 500 paintings sold over 410 years before concluding that the average real annual return on art was 0.55%.[11] Another study of high-quality oil paintings sold in Sweden between 1985 and 2016 determined the average return to be 0.6% annually.[12][13] However, art gallerists are sometimes ambivalent to the idea of treating artwork as an investment.[14] Art is also notoriously difficult to value, and there are quite a few factors to bear in mind for art valuation.  Equity crowdfunding platforms allow \"the crowd\" to review early-stage investment opportunities presented by entrepreneurs and take an equity stake in the business. Typically an online platform acts as a broker between investors and founders. These platforms differ greatly in the types of opportunities they will offer up to investors, how much due diligence is performed, degree of investor protections available, minimum investment size and so on. Equity crowdfunding platforms have seen a significant amount of success in the UK and, with the passing of JOBS Act Title III in early 2016, are now picking up steam in the United States.  The notion of “infrastructure as an asset class” has grown steadily in the past seven years.[15][16] But, so far, this development has been the preserve of institutional investors: pension funds, insurance companies and sovereign wealth funds, with very limited access to high-net-worth investors (except a few large family offices).  Only available in the UK, SEIS funds and EIS funds present a tax-efficient way of investing in early-stage ventures. These work much like venture capital funds, with the added bonus of receiving government tax incentives for investing and loss relief protection should the companies invested in fail. Such funds help to diversify investor exposure by investing in multiple early ventures. Fees are normally charged by the management team for participating in the fund, and these can end up totaling anywhere between 15% and 40% of the fund value over the course of its life.  Lease investing platforms provide investors with options to co-invest and have partial ownership in physical assets that earn lease income. Through these platforms, investors can have fractional ownership of a particular asset leased to an organization and earn fixed returns.  Private equity consists of large-scale private investments into unlisted companies in return for equity. Private funds are typically formed by combining funds from institutional investors such as high-net-worth individuals, insurance companies, university endowment funds and pension funds. Funds are used alongside borrowed money and the money of the private equity firm itself to invest in businesses they believe to have high growth potential.[17]  Venture capital consists of private investments made into young start-up companies in exchange for equity. Venture capital funds are typically formed by drawing capital from seed money, or angel investors. Nowadays, crowdfunding is also used by start-up companies for capital. Accredited investors such as high-net worth individuals, banks, and other companies will also invest in a start-up company if it grows to a large enough scale.  The 2003 Capgemini World Wealth Report, based on 2002 data, showed high-net-worth individuals, as defined in the report, to have 10% of their financial assets in alternative investments. For the purposes of the report, alternative investments included \"structured products, luxury valuables and collectibles, hedge funds, managed futures, and precious metals\".[18] By 2007, this had reduced to 9%.[19] No recommendations were made in either report about the amount of money investors should place in alternative investments. As of 2019, the global breakdown of financial assets included a 13% allocation to alternative investments.[20]  Alternative investments are sometimes used as a way of reducing overall investment risk through diversification.  Some of the characteristics of alternative investments may include:  Alternatives may be offered by traditional investment companies or specialized companies. Among companies which specialize in alternative investments, some offer a variety of strategies others offer only a specific type.  In 2023, Blackstone, which specializes in only alternative investments including  private equity, private debt, real assets, hedge funds, and hedge funds of funds, became the first alternative investment manager to reach $1 trillion in assets under management (AUM).[22] Other notable alternative asset managers include Apollo, Brookfield, KKR, and Carlyle, each of which have hundreds of billions in AUM.[22]  As of 2023, traditional asset management companies had begun to offer alternatives including BlackRock, T. Rowe Price, and Franklin Templeton Investments.[23]  Liquid alternatives (\"alts\") are alternative investments that provide daily liquidity.[example needed] Liquid alternative investments should[according to whom?] produce returns uncorrelated to GDP growth, must have protection against systemic market risk and should be too small to create new systemic risks for the market.[24] Hedge funds may be included in this category; however, traditional hedge funds may have liquidity limitations, and the term is usually used for registered mutual funds which use hedge fund strategies such as long-short equity investments.[25]  Liquid alternatives became popular in the late 2000s, growing from $124 billion in assets under management 2010 to $310 billion in 2014.[26] However, in 2015 only $85 million was added, with 31 closed funds and a high-profile underperformance by the largest long-short equity fund at the time, Marketfield Fund.[26]  In 2014 there were an estimated 298 liquid alternative funds with strategies such as long-short equity funds; event-driven, relative value, tactical trading (including managed futures), and multi-strategy. This number does not include option income funds, tactical shorting and leveraged indexed funds.[25]  There has been expressed skepticism over the complexity of liquid alts and the lack of able portfolio managers.[27] One of the world's largest hedge fund managers, AQR Capital, began offering funds in 2009,[28] and grew from $33 billion in assets under management (AUM) in 2010 to $185 billion in AUM in 2017 driven in part by marketing mutual-fund like products with lower fees.[29] As of 2016, AQR Capital was the largest manager of liquid alts.[30] "},"meta":{},"created_at":"2025-03-22T14:25:42.277677Z","updated_at":"2025-03-22T14:25:42.277677Z","inner_id":53,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":62,"annotations":[{"id":62,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.319900Z","updated_at":"2025-03-22T14:25:42.319900Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"fdd79846-a4e0-4f0a-b4a5-b41478d280aa","import_id":null,"last_action":null,"bulk_created":false,"task":62,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Heterodox  Business cycles are intervals of general expansion followed by recession in economic performance. The changes in economic activity that characterize business cycles have important implications for the welfare of the general population, government institutions, and private sector firms.   There are many definitions of a business cycle. The simplest defines recessions as two consecutive quarters of negative GDP growth. More satisfactory classifications are provided by, first including more economic indicators and second by looking for more data patterns than the two quarter definition. In the United States, the National Bureau of Economic Research oversees a Business Cycle Dating Committee that defines a recession as \"a significant decline in economic activity spread across the market, lasting more than a few months, normally visible in real GDP, real income, employment, industrial production, and wholesale-retail sales.\"[1]  Business cycles are usually thought of as medium term evolution. They are less related to long-term trends, coming from slowly-changing factors like technological advances. Further, a one period change, that is unusual over the course of one or two years, is often relegated to “noise”; an example is a worker strike or an isolated period of severe weather.   The individual episodes of expansion\/recession occur with changing duration and intensity over time. Typically their periodicity has a wide range from around 2 to 10 years.  There are many sources of business cycle movements such as rapid and significant changes in the price of oil or variation in consumer sentiment that affects overall spending in the macroeconomy and thus investment and firms' profits. Usually such sources are unpredictable in advance and can be viewed as random \"shocks\" to the cyclical pattern, as happened during the 2007–2008 financial crises or the COVID-19 pandemic.  The first systematic exposition of economic crises, in opposition to the existing theory of economic equilibrium, was the 1819 Nouveaux Principes d'économie politique by Jean Charles Léonard de Sismondi.[2] Prior to that point classical economics had either denied the existence of business cycles,[3] blamed them on external factors, notably war,[4] or only studied the long term. Sismondi found vindication in the Panic of 1825, which was the first unarguably international economic crisis, occurring in peacetime.[citation needed]  Sismondi and his contemporary Robert Owen, who expressed similar but less systematic thoughts in 1817 Report to the Committee of the Association for the Relief of the Manufacturing Poor, both identified the cause of economic cycles as overproduction and underconsumption, caused in particular by wealth inequality. They advocated government intervention and socialism, respectively, as the solution. This work did not generate interest among classical economists, though underconsumption theory developed as a heterodox branch in economics until being systematized in Keynesian economics in the 1930s.  Sismondi's theory of periodic crises was developed into a theory of alternating cycles by Charles Dunoyer,[5] and similar theories, showing signs of influence by Sismondi, were developed by Johann Karl Rodbertus. Periodic crises in capitalism formed the basis of the theory of Karl Marx, who further claimed that these crises were increasing in severity and, on the basis of which, he predicted a communist revolution.[citation needed] Though only passing references in Das Kapital (1867) refer to crises, they were extensively discussed in Marx's posthumously published books, particularly in Theories of Surplus Value. In Progress and Poverty (1879), Henry George focused on land's role in crises – particularly land speculation – and proposed a single tax on land as a solution.  Statistical or econometric modelling and theory of business cycle movements can also be used. In this case a time series analysis is used to capture the regularities and the stochastic signals and noise in economic time series such as Real GDP or Investment.  [Harvey and Trimbur, 2003, Review of Economics and Statistics] developed models for describing stochastic or pseudo- cycles, of which business cycles represent a leading case.  As well-formed and compact – and easy to implement – statistical methods may outperform macroeconomic approaches in numerous cases, they provide a solid alternative even for rather complex economic theory.[6]  In 1860 French economist Clément Juglar first identified economic cycles 7 to 11 years long, although he cautiously did not claim any rigid regularity.[7] This interval of periodicity is also commonplace, as an empirical finding, in time series models for stochastic cycles in economic data.  Furthermore, methods like statistical modelling in a Bayesian framework – see e.g. [Harvey, Trimbur, and van Dijk, 2007, Journal of Econometrics] – can incorporate such a range explicitly by setting up priors that concentrate around say 6 to 12 years, such flexible knowledge about the frequency of business cycles can actually be included in their mathematical study, using a Bayesian statistical paradigm.[8]  Later[when?], economist Joseph Schumpeter argued that a Juglar cycle has four stages:  Schumpeter's Juglar model associates recovery and prosperity with increases in productivity, consumer confidence, aggregate demand, and prices.  In the 20th century, Schumpeter and others proposed a typology of business cycles according to their periodicity, so that a number of particular cycles were named after their discoverers or proposers:[9]  Some say interest in the different typologies of cycles has waned since the development of modern macroeconomics, which gives little support to the idea of regular periodic cycles.[12] Further econometric studies such as the two works in 2003 and 2007 cited above demonstrate a clear tendency for cyclical components in macroeconomic times to behave in a stochastic rather than deterministic way.  Others, such as Dmitry Orlov, argue that simple compound interest mandates the cycling of monetary systems. Since 1960, World GDP has increased by fifty-nine times, and these multiples have not even kept up with annual inflation over the same period. Social Contract (freedoms and absence of social problems) collapses may be observed in nations where incomes are not kept in balance with cost-of-living over the timeline of the monetary system cycle.  The Bible (760 BCE) and Hammurabi's Code (1763 BCE) both explain economic remediations for cyclic sixty-year recurring great depressions, via fiftieth-year Jubilee (biblical) debt and wealth resets[citation needed]. Thirty major debt forgiveness events are recorded in history including the debt forgiveness given to most European nations in the 1930s to 1954.[13]  There were great increases in productivity, industrial production and real per capita product throughout the period from 1870 to 1890 that included the Long Depression and two other recessions.[14][15] There were also significant increases in productivity in the years leading up to the Great Depression.  Both the Long and Great Depressions were characterized by overcapacity and market saturation.[16][17]  Over the period since the Industrial Revolution, technological progress has had a much larger effect on the economy than any fluctuations in credit or debt, the primary exception being the Great Depression, which caused a multi-year steep economic decline. The effect of technological progress can be seen by the purchasing power of an average hour's work, which has grown from $3 in 1900 to $22 in 1990, measured in 2010 dollars.[18]  There were similar increases in real wages during the 19th century.  (See: Productivity improving technologies (historical).)  A table of innovations and long cycles can be seen at: Kondratiev wave § Modern modifications of Kondratiev theory. Since surprising news in the economy, which has a random aspect, impact the state of the business cycle, any corresponding descriptions must have a random part at its root that motivates the use of statistical frameworks in this area.  There were frequent crises in Europe and America in the 19th and first half of the 20th century, specifically the period 1815–1939. This period started from the end of the Napoleonic wars in 1815, which was immediately followed by the Post-Napoleonic depression in the United Kingdom (1815–1830), and culminated in the Great Depression of 1929–1939, which led into World War II. See Financial crisis: 19th century for listing and details. The first of these crises not associated with a war was the Panic of 1825.[19]  Business cycles in OECD countries after World War II were generally more restrained than the earlier business cycles. This was particularly true during the Golden Age of Capitalism (1945\/50–1970s), and the period 1945–2008 did not experience a global downturn until the Late-2000s recession.[20] Economic stabilization policy using fiscal policy and monetary policy appeared to have dampened the worst excesses of business cycles, and automatic stabilization due to the aspects of the government's budget also helped mitigate the cycle even without conscious action by policy-makers.[21]  In this period, the economic cycle – at least the problem of depressions – was twice declared dead. The first declaration was in the late 1960s, when the Phillips curve was seen as being able to steer the economy. However, this was followed by stagflation in the 1970s, which discredited the theory. The second declaration was in the early 2000s, following the stability and growth in the 1980s and 1990s in what came to be known as the Great Moderation. Notably, in 2003, Robert Lucas Jr., in his presidential address to the American Economic Association, declared that the \"central problem of depression-prevention [has] been solved, for all practical purposes.\"[22]  Various regions have experienced prolonged depressions, most dramatically the economic crisis in former Eastern Bloc countries following the end of the Soviet Union in 1991. For several of these countries the period 1989–2010 has been an ongoing depression, with real income still lower than in 1989.[23]  In 1946, economists Arthur F. Burns and Wesley C. Mitchell provided the now standard definition of business cycles in their book Measuring Business Cycles:[24]  Business cycles are a type of fluctuation found in the aggregate economic activity of nations that organize their work mainly in business enterprises: a cycle consists of expansions occurring at about the same time in many economic activities, followed by similarly general recessions, contractions, and revivals which merge into the expansion phase of the next cycle; in duration, business cycles vary from more than one year to ten or twelve years; they are not divisible into shorter cycles of similar characteristics with amplitudes approximating their own. According to A. F. Burns:[25]  Business cycles are not merely fluctuations in aggregate economic activity. The critical feature that distinguishes them from the commercial convulsions of earlier centuries or from the seasonal and other short term variations of our own age is that the fluctuations are widely diffused over the economy – its industry, its commercial dealings, and its tangles of finance. The economy of the western world is a system of closely interrelated parts. He who would understand business cycles must master the workings of an economic system organized largely in a network of free enterprises searching for profit. The problem of how business cycles come about is therefore inseparable from the problem of how a capitalist economy functions. In the United States, it is generally accepted that the National Bureau of Economic Research (NBER) is the final arbiter of the dates of the peaks and troughs of the business cycle. An expansion is the period from a trough to a peak and a recession as the period from a peak to a trough. The NBER identifies a recession as \"a significant decline in economic activity spread across the economy, lasting more than a few months, normally visible in real GDP, real income, employment, industrial production\".[26]  There is often a close timing relationship between the upper turning points of the business cycle, commodity prices, and freight rates, which is shown to be particularly tight in the grand peak years of 1873, 1889, 1900 and 1912.[27] Hamilton expressed that in the post war era, a majority of recessions are connected to an increase in oil price.[28]  Commodity price shocks are considered to be a significant driving force of the US business cycle.[29]  Along these lines, the research in [Trimbur, 2010, International Journal of Forecasting] shows empirical results for the relation between oil-prices and real GDP. The methodology uses a statistical model that incorporate level shifts in the price of crude oil; hence the approach describes the possibility of oil price shocks and forecasts the likelihood of such events.[30]  Economic indicators are used to measure the business cycle: consumer confidence index, retail trade index, unemployment and industry\/service production index. Stock and Watson claim that financial indicators' predictive ability is not stable over different time periods because of economic shocks, random fluctuations and development in financial systems.[31] Ludvigson believes consumer confidence index is a coincident indicator as it relates to consumer's current situations.[32] Winton & Ralph state that retail trade index is a benchmark for the current economic level because its aggregate value counts up for two-thirds of the overall GDP and reflects the real state of the economy.[33] According to Stock and Watson, unemployment claim can predict when the business cycle is entering a downward phase.[34] Banbura and Rüstler argue that industry production's GDP information can be delayed as it measures real activity with real number, but it provides an accurate prediction of GDP.[35]  Series used to infer the underlying business cycle fall into three categories: lagging, coincident, and leading. They are described as main elements of an analytic system to forecast peaks and troughs in the business cycle.[36]  For almost 30 years, these economic data series are considered as \"the leading index\" or \"the leading indicators\"-were compiled and published by the U.S. Department of Commerce.  A prominent coincident, or real-time, business cycle indicator is the Aruoba-Diebold-Scotti Index.  Recent research employing spectral analysis has confirmed the presence of Kondratiev waves in the world GDP dynamics at an acceptable level of statistical significance.[37] Korotayev & Tsirel also detected shorter business cycles, dating the Kuznets to about 17 years and calling it the third sub-harmonic of the Kondratiev, meaning that there are three Kuznets cycles per Kondratiev.[jargon]  Recurrence quantification analysis has been employed to detect the characteristic of business cycles and economic development. To this end, Orlando et al.[38] developed the so-called recurrence quantification correlation index to test correlations of RQA on a sample signal and then investigated the application to business time series. The said index has been proven to detect hidden changes in time series. Further, Orlando et al.,[39] over an extensive dataset, shown that recurrence quantification analysis may help in anticipating transitions from laminar (i.e. regular) to turbulent (i.e. chaotic) phases such as USA GDP in 1949, 1953, etc. Last but not least, it has been demonstrated that recurrence quantification analysis can detect differences between macroeconomic variables and highlight hidden features of economic dynamics.[39]  The Business Cycle follows changes in stock prices which are mostly caused by external factors such as socioeconomic conditions, inflation, exchange rates. Intellectual capital does not affect a company stock's current earnings. Intellectual capital contributes to a stock's return growth.[40]  Unlike long-term trends, medium-term data fluctuations are connected to the monetary policy transmission mechanism and its role in regulating inflation during an economic cycle. At the same time, the presence of nominal restrictions in price setting behavior might impact the short-term course of inflation.[41]  In recent years economic theory has moved towards the study of economic fluctuation rather than a \"business cycle\"[42] – though some economists use the phrase 'business cycle' as a convenient shorthand. For example, Milton Friedman said that calling the business cycle a \"cycle\" is a misnomer, because of its non-cyclical nature. Friedman believed that for the most part, excluding very large supply shocks, business declines are more of a monetary phenomenon.[43] Arthur F. Burns and Wesley C. Mitchell define business cycle as a form of fluctuation. In economic activities, a cycle of expansions happening, followed by recessions, contractions, and revivals. All of which combine to form the next cycle's expansion phase; this sequence of change is repeated but not periodic.[44]  The explanation of fluctuations in aggregate economic activity is one of the primary concerns of macroeconomics and a variety of theories have been proposed to explain them.  Within economics, it has been debated as to whether or not the fluctuations of a business cycle are attributable to external (exogenous) versus internal (endogenous) causes. In the first case shocks are stochastic, in the second case shocks are deterministically chaotic and embedded in the economic system.[45] The classical school (now neo-classical) argues for exogenous causes and the underconsumptionist (now Keynesian) school argues for endogenous causes. These may also broadly be classed as \"supply-side\" and \"demand-side\" explanations: supply-side explanations may be styled, following Say's law, as arguing that \"supply creates its own demand\", while demand-side explanations argue that effective demand may fall short of supply, yielding a recession or depression.  This debate has important policy consequences: proponents of exogenous causes of crises such as neoclassicals largely argue for minimal government policy or regulation (laissez faire), as absent these external shocks, the market functions, while proponents of endogenous causes of crises such as Keynesians largely argue for larger government policy and regulation, as absent regulation, the market will move from crisis to crisis. This division is not absolute – some classicals (including Say) argued for government policy to mitigate the damage of economic cycles, despite believing in external causes, while Austrian School economists argue against government involvement as only worsening crises, despite believing in internal causes.  The view of the economic cycle as caused exogenously dates to Say's law, and much debate on endogeneity or exogeneity of causes of the economic cycle is framed in terms of refuting or supporting Say's law; this is also referred to as the \"general glut\" (supply in relation to demand) debate.  Until the Keynesian Revolution in mainstream economics in the wake of the Great Depression, classical and neoclassical explanations (exogenous causes) were the mainstream explanation of economic cycles; following the Keynesian revolution, neoclassical macroeconomics was largely rejected. There has been some resurgence of neoclassical approaches in the form of real business cycle (RBC) theory.  The debate between Keynesians and neo-classical advocates was reawakened following the recession of 2007.  Mainstream economists working in the neoclassical tradition, as opposed to the Keynesian tradition, have usually viewed the departures of the harmonic working of the market economy as due to exogenous influences, such as the State or its regulations, labor unions, business monopolies, or shocks due to technology or natural causes.  Contrarily, in the heterodox tradition of Jean Charles Léonard de Sismondi, Clément Juglar, and Marx the recurrent upturns and downturns of the market system are an endogenous characteristic of it.[46]  The 19th-century school of under consumptionism also posited endogenous causes for the business cycle, notably the paradox of thrift, and today this previously heterodox school has entered the mainstream in the form of Keynesian economics via the Keynesian revolution.  Mainstream economics views business cycles as essentially \"the random summation of random causes\". In 1927, Eugen Slutzky observed that summing random numbers, such as the last digits of the Russian state lottery, could generate patterns akin to that we see in business cycles, an observation that has since been repeated many times. This caused economists to move away from viewing business cycles as a cycle that needed to be explained and instead viewing their apparently cyclical nature as a methodological artefact. This means that what appear to be cyclical phenomena can actually be explained as just random events that are fed into a simple linear model. Thus business cycles are essentially random shocks that average out over time. Mainstream economists have built models of business cycles based on the idea that they are caused by random shocks.[47][48][49] Due to this inherent randomness, recessions can sometimes not occur for decades; for example, Australia did not experience any recession between 1991 and 2020.[50]  While economists have found it difficult to forecast recessions or determine their likely severity, research indicates that longer expansions do not cause following recessions to be more severe.[51]  According to Keynesian economics, fluctuations in aggregate demand cause the economy to come to short run equilibrium at levels that are different from the full employment rate of output. These fluctuations express themselves as the observed business cycles. Keynesian models do not necessarily imply periodic business cycles. However, simple Keynesian models involving the interaction of the Keynesian multiplier and accelerator give rise to cyclical responses to initial shocks. Paul Samuelson's \"oscillator model\"[52] is supposed to account for business cycles thanks to the multiplier and the accelerator. The amplitude of the variations in economic output depends on the level of the investment, for investment determines the level of aggregate output (multiplier), and is determined by aggregate demand (accelerator).  In the Keynesian tradition, Richard Goodwin[53] accounts for cycles in output by the distribution of income between business profits and workers' wages. The fluctuations in wages are almost the same as in the level of employment (wage cycle lags one period behind the employment cycle), for when the economy is at high employment, workers are able to demand rises in wages, whereas in periods of high unemployment, wages tend to fall. According to Goodwin, when unemployment and business profits rise, the output rises.  Exports and imports are large components of an economy's aggregate expenditure, especially one that is oriented toward international trade. Income is an essential determinant of the level of imported goods. A higher GDP reflects a higher level of spending on imported goods and services, and vice versa. Therefore, expenditure on imported goods and services falls during a recession and rises during an economic expansion or boom.[54]  Import expenditures are commonly considered to be procyclical and cyclical in nature, coincident with the business cycle.[54] Domestic export expenditures give a good indication of foreign business cycles as foreign import expenditures are coincident with the foreign business cycle.  One alternative theory is that the primary cause of economic cycles is due to the credit cycle: the net expansion of credit (increase in private credit, equivalently debt, as a percentage of GDP) yields economic expansions, while the net contraction causes recessions, and if it persists, depressions. In particular, the bursting of speculative bubbles is seen as the proximate cause of depressions, and this theory places finance and banks at the center of the business cycle.  A primary theory in this vein is the debt deflation theory of Irving Fisher, which he proposed to explain the Great Depression. A more recent complementary theory is the Financial Instability Hypothesis of Hyman Minsky, and the credit theory of economic cycles is often associated with Post-Keynesian economics such as Steve Keen.  Post-Keynesian economist Hyman Minsky has proposed an explanation of cycles founded on fluctuations in credit, interest rates and financial frailty, called the Financial Instability Hypothesis. In an expansion period, interest rates are low and companies easily borrow money from banks to invest. Banks are not reluctant to grant them loans, because expanding economic activity allows business increasing cash flows and therefore they will be able to easily pay back the loans. This process leads to firms becoming excessively indebted, so that they stop investing, and the economy goes into recession.  While credit causes have not been a primary theory of the economic cycle within the mainstream, they have gained occasional mention, such as (Eckstein & Sinai 1990), cited approvingly by (Summers 1986).  Within mainstream economics, Keynesian views have been challenged by real business cycle models in which fluctuations are due to random changes in the total productivity factor (which are caused by changes in technology as well as the legal and regulatory environment). This theory is most associated with Finn E. Kydland and Edward C. Prescott, and more generally the Chicago school of economics (freshwater economics). They consider that economic crisis and fluctuations cannot stem from a monetary shock, only from an external shock, such as an innovation.[47]  This theory explains the nature and causes of economic cycles from the viewpoint of life-cycle of marketable goods.[55] The theory originates from the work of Raymond Vernon, who described the development of international trade in terms of product life-cycle – a period of time during which the product circulates in the market. Vernon stated that some countries specialize in the production and export of technologically new products, while others specialize in the production of already known products. The most developed countries are able to invest large amounts of money in the technological innovations and produce new products, thus obtaining a dynamic comparative advantage over developing countries.  Recent research by Georgiy Revyakin proved initial Vernon theory and showed economic cycles in developed countries overran economic cycles in developing countries.[56] He also presumed economic cycles with different periodicity can be compared to the products with various life-cycles. In case of Kondratiev waves such products correlate with fundamental discoveries implemented in production (inventions which form the technological paradigm: Richard Arkwright's machines, steam engines, industrial use of electricity, computer invention, etc.); Kuznets cycles describe such products as infrastructural components (roadways, transport, utilities, etc.); Juglar cycles may go in parallel with enterprise fixed capital (equipment, machinery, etc.), and Kitchin cycles are characterized by change in the society preferences (tastes) for consumer goods, and time, which is necessary to start the production.  Highly competitive market conditions would determine simultaneous technological updates of all economic agents (as a result, cycle formation): in case if a manufacturing technology at an enterprise does not meet the current technological environment – such company loses its competitiveness and eventually goes bankrupt.  Another set of models tries to derive the business cycle from political decisions. The political business cycle theory is strongly linked to the name of Michał Kalecki who discussed \"the reluctance of the 'captains of industry' to accept government intervention in the matter of employment\".[57]  Persistent full employment would mean increasing workers' bargaining power to raise wages and to avoid doing unpaid labor, potentially hurting profitability. However, he did not see this theory as applying under fascism, which would use direct force to destroy labor's power.  In recent years, proponents of the \"electoral business cycle\" theory have argued that incumbent politicians encourage prosperity before elections in order to ensure re-election – and make the citizens pay for it with recessions afterwards.[58] The political business cycle is an alternative theory stating that when an administration of any hue is elected, it initially adopts a contractionary policy to reduce inflation and gain a reputation for economic competence. It then adopts an expansionary policy in the lead up to the next election, hoping to achieve simultaneously low inflation and unemployment on election day.[59]  The partisan business cycle suggests that cycles result from the successive elections of administrations with different policy regimes. Regime A adopts expansionary policies, resulting in growth and inflation, but is voted out of office when inflation becomes unacceptably high. The replacement, Regime B, adopts contractionary policies reducing inflation and growth, and the downwards swing of the cycle. It is voted out of office when unemployment is too high, being replaced by Party A.  For Marx, the economy based on production of commodities to be sold in the market is intrinsically prone to crisis. In the heterodox Marxian view, profit is the major engine of the market economy, but business (capital) profitability has a tendency to fall that recurrently creates crises in which mass unemployment occurs, businesses fail, remaining capital is centralized and concentrated and profitability is recovered. In the long run, these crises tend to be more severe and the system will eventually fail.[60]  Some Marxist authors such as Rosa Luxemburg viewed the lack of purchasing power of workers as a cause of a tendency of supply to be larger than demand, creating crisis, in a model that has similarities with the Keynesian one. Indeed, a number of modern authors have tried to combine Marx's and Keynes's views. Henryk Grossman[61] reviewed the debates and the counteracting tendencies and Paul Mattick subsequently emphasized the basic differences between the Marxian and the Keynesian perspective. While Keynes saw capitalism as a system worth maintaining and susceptible to efficient regulation, Marx viewed capitalism as a historically doomed system that cannot be put under societal control.[62]  The American mathematician and economist Richard M. Goodwin formalised a Marxist model of business cycles known as the Goodwin Model in which recession was caused by increased bargaining power of workers (a result of high employment in boom periods) pushing up the wage share of national income, suppressing profits and leading to a breakdown in capital accumulation. Later theorists applying variants of the Goodwin model have identified both short and long period profit-led growth and distribution cycles in the United States and elsewhere.[63][64][65][66][67] David Gordon provided a Marxist model of long period institutional growth cycles in an attempt to explain the Kondratiev wave. This cycle is due to the periodic breakdown of the social structure of accumulation, a set of institutions which secure and stabilize capital accumulation.  Economists of the heterodox Austrian School argue that business cycles are caused by excessive issuance of credit by banks in fractional reserve banking systems. According to Austrian economists, excessive issuance of bank credit may be exacerbated if central bank monetary policy sets interest rates too low, and the resulting expansion of the money supply causes a \"boom\" in which resources are misallocated or \"malinvested\" because of artificially low interest rates. Eventually, the boom cannot be sustained and is followed by a \"bust\" in which the malinvestments are liquidated  (sold for less than their original cost) and the money supply contracts.[68][69]  One of the criticisms of the Austrian business cycle theory is based on the observation that the United States suffered recurrent economic crises in the 19th century, notably the Panic of 1873, which occurred prior to the establishment of a U.S. central bank in 1913. Adherents of the Austrian School, such as the historian Thomas Woods, argue that these earlier financial crises were prompted by government and bankers' efforts to expand credit despite restraints imposed by the prevailing gold standard, and are thus consistent with Austrian Business Cycle Theory.[70][71]  The Austrian explanation of the business cycle differs significantly from the mainstream understanding of business cycles and is generally rejected by mainstream economists. Mainstream economists generally do not support Austrian school explanations for business cycles, on both theoretical as well as real-world empirical grounds.[72][73][74][75][76][77] Austrians claim that the boom-and-bust business cycle is caused by government intervention into the economy, and that the cycle would be comparatively rare and mild without central government interference.  The slope of the yield curve is one of the most powerful predictors of future economic growth, inflation, and recessions.[78] One measure of the yield curve slope (i.e. the difference between 10-year Treasury bond rate and the 3-month Treasury bond rate) is included in the Financial Stress Index published by the St. Louis Fed.[79]  A different measure of the slope (i.e. the difference between 10-year Treasury bond rates and the federal funds rate) is incorporated into the Index of Leading Economic Indicators published by The Conference Board.[80]  An inverted yield curve is often a harbinger of recession.  A positively sloped yield curve is often a harbinger of inflationary growth. Work by Arturo Estrella and Tobias Adrian has established the predictive power of an inverted yield curve to signal a recession.  Their models show that when the difference between short-term interest rates (they use 3-month T-bills) and long-term interest rates (10-year Treasury bonds) at the end of a federal reserve tightening cycle is negative or less than 93 basis points positive that a rise in unemployment usually occurs.[81]  The New York Fed publishes a monthly recession probability prediction derived from the yield curve and based on Estrella's work.  All the recessions in the United States since 1970 (up through 2017) have been preceded by an inverted yield curve (10-year vs. 3-month). Over the same time frame, every occurrence of an inverted yield curve has been followed by recession as declared by the NBER business cycle dating committee.[82]  Estrella and others have postulated that the yield curve affects the business cycle via the balance sheet of banks (or bank-like financial institutions).[83]  When the yield curve is inverted banks are often caught paying more on short-term deposits (or other forms of short-term wholesale funding) than they are making on long-term loans leading to a loss of profitability and reluctance to lend resulting in a credit crunch.  When the yield curve is upward sloping, banks can profitably take-in short term deposits and make long-term loans so they are eager to supply credit to borrowers. This eventually leads to a credit bubble.  Henry George claimed land price fluctuations were the primary cause of most business cycles.[84]  Population swings can impact business cycles.[85]  Many social indicators, such as mental health, crimes, and suicides, worsen during economic recessions (though general mortality tends to fall, and it is in expansions when it tends to increase).[86] As periods of economic stagnation are painful for the many who lose their jobs, there is often political pressure for governments to mitigate recessions. Since the 1940s, following the Keynesian Revolution, most governments of developed nations have seen the mitigation of the business cycle as part of the responsibility of government, under the rubric of stabilization policy.[87]  Since in the Keynesian view, recessions are caused by inadequate aggregate demand, when a recession occurs the government should increase the amount of aggregate demand and bring the economy back into equilibrium.  This the government can do in two ways, firstly by increasing the money supply (expansionary monetary policy) and secondly by increasing government spending or cutting taxes (expansionary fiscal policy).  By contrast, some economists, notably New classical economist Robert Lucas, argue that the welfare cost of business cycles are very small to negligible, and that governments should focus on long-term growth instead of stabilization.  However, even according to Keynesian theory, managing economic policy to smooth out the cycle is a difficult task in a society with a complex economy. Some theorists, notably those who believe in Marxian economics, believe that this difficulty is insurmountable. Karl Marx claimed that recurrent business cycle crises were an inevitable result of the operations of the capitalistic system. In this view, all that the government can do is to change the timing of economic crises. The crisis could also show up in a different form, for example as severe inflation or a steadily increasing government deficit. Worse, by delaying a crisis, government policy is seen as making it more dramatic and thus more painful.  Additionally, since the 1960s neoclassical economists have played down the ability of Keynesian policies to manage an economy.  Since the 1960s, economists like Nobel Laureates Milton Friedman and Edmund Phelps have made ground in their arguments that inflationary expectations negate the Phillips curve in the long run.  The stagflation of the 1970s provided striking support for their theories while proving a dilemma for Keynesian policies, which appeared to necessitate both expansionary policies to mitigate recession and contractionary policies to reduce inflation. Friedman has gone so far as to argue that all the central bank of a country should do is to avoid making large mistakes, as he believes they did by contracting the money supply very rapidly in the face of the Wall Street crash of 1929, in which they made what would have been a recession into the Great Depression.[citation needed]  The Hodrick-Prescott [88] and the Christiano-Fitzgerald [89] filters can be implemented using the R package  mFilter, while singular spectrum filters [90][91]  can be implemented using the R package ASSA. "},"meta":{},"created_at":"2025-03-22T14:25:42.277677Z","updated_at":"2025-03-22T14:25:42.277677Z","inner_id":54,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":63,"annotations":[{"id":63,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.319900Z","updated_at":"2025-03-22T14:25:42.319900Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"ea65f8c2-f8f8-48b6-8dbc-449e0866d38c","import_id":null,"last_action":null,"bulk_created":false,"task":63,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"In finance, a trade is an exchange of a security such as stocks, bonds, commodities, currencies, derivatives or any valuable financial instrument for \"cash\". Such a financial transaction is usually done by participants of an exchange such as a stock exchange, commodity exchange or futures exchange with a short-dated promise to pay in the currency of the country where the 'exchange' is located.   The price is agreed between the buyer and seller on the execution of the trade and is guided by the supply and demand for that financial instrument.[1] Once the trade is executed a number of steps take place until the trade is finally settled. There is a pre-defined settlement period for this to happen in each market.  Trading in financial markets is key part of a countries economics, providing liquidity, enabling price discovery, and facilitating efficient capital allocation.[2] When trading in financial markets, financial traders balance risk and potential reward to attempt to make profit from the trades.  The securities trade life cycle involves:  Participants in the financial markets include:   This finance-related article is a stub. You can help Wikipedia by expanding it."},"meta":{},"created_at":"2025-03-22T14:25:42.277677Z","updated_at":"2025-03-22T14:25:42.277677Z","inner_id":55,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":64,"annotations":[{"id":64,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.319900Z","updated_at":"2025-03-22T14:25:42.319900Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"e3659ea0-2447-48d2-87a0-6ae18d11b787","import_id":null,"last_action":null,"bulk_created":false,"task":64,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  A bank is a financial institution that accepts deposits from the public and creates a demand deposit while simultaneously making loans, mobilizing saver surplus to deficit spenders.[1] Lending activities can be directly performed by the bank or indirectly through capital markets.[2]  Whereas banks play an important role in financial stability and the economy of a country, most jurisdictions exercise a high degree of regulation over banks. Most countries have institutionalized a system known as fractional-reserve banking, under which banks hold liquid assets equal to only a portion of their current liabilities.[3] In addition to other regulations intended to ensure liquidity, banks are generally subject to minimum capital requirements based on an international set of capital standards, the Basel Accords.[4]  Banking in its modern sense evolved in the fourteenth century in the prosperous cities of Renaissance Italy but, in many ways, functioned as a continuation of ideas and concepts of credit and lending that had their roots in the ancient world. In the history of banking, a number of banking dynasties –  notably, the Medicis, the Pazzi, the Fuggers, the Welsers, the Berenbergs, and the Rothschilds –  have played a central role over many centuries. The oldest existing retail bank is Banca Monte dei Paschi di Siena (founded in 1472), while the oldest existing merchant bank is Berenberg Bank (founded in 1590).  Banking as an archaic activity (or quasi-banking[5][6]) is thought to have begun as early as the end of the 4th millennium BCE,[7] to the 3rd millennia BCE.[8][9]  The present era of banking can be traced to wealthy medieval Renaissance Italian city-states, whose elite families such as the Bardi and Peruzzi dominated banking in 14th-century Florence before establishing branches in many other parts of Europe.[10] Giovanni di Bicci de' Medici set up one of the most famous Italian banks, the Medici Bank, in 1397.[11] The Republic of Genoa founded the earliest-known state deposit bank, and Banco di San Giorgio (Bank of St. George), in 1407 at Genoa, Italy.[12]  Fractional reserve banking and the issue of banknotes emerged in the 17th and 18th centuries. Merchants started to store their gold with the goldsmiths of London, who possessed private vaults, and who charged a fee for that service. In exchange for each deposit of precious metal, the goldsmiths issued receipts certifying the quantity and purity of the metal they held as a bailee; these receipts could not be assigned, only the original depositor could collect the stored goods.  Gradually the goldsmiths began to lend money out on behalf of the depositor, and promissory notes, which evolved into banknotes, were issued for money deposited as a loan to the goldsmith. Thus, by the 19th century, we find in ordinary cases of deposits, of money with banking corporations, or bankers, the transaction amounts to a mere loan, or mutuum, and the bank is to restore, not the same money, but an equivalent sum, whenever it is demanded[13] and money, when paid into a bank, ceases altogether to be the money of the principal (see Parker v. Marchant, 1 Phillips 360); it is then the money of the banker, who is bound to return an equivalent, by paying a similar sum to that deposited with him, when he is asked for it. [14] The goldsmith paid interest on deposits. Since the promissory notes were payable on demand, and the advances (loans) to the goldsmith's customers were repayable over a longer time-period, this was an early form of fractional reserve banking. The promissory notes developed into an assignable instrument which could circulate as a safe and convenient form of money[15] backed by the goldsmith's promise to pay,[16][need quotation to verify] allowing goldsmiths to advance loans with little risk of default.[17][need quotation to verify] Thus the goldsmiths of London became the forerunners of banking by creating new money based on credit.  The Bank of England originated the permanent issue of banknotes in 1695.[18] The Royal Bank of Scotland established the first overdraft facility in 1728.[19] By the beginning of the 19th century Lubbock's Bank had established a bankers' clearing house in London to allow multiple banks to clear transactions. The Rothschilds pioneered international finance on a large scale,[20][21] financing the purchase of shares in the Suez canal for the British government in 1875.[22][need quotation to verify]  The word bank was taken into Middle English from Middle French banque, from Old Italian banco, meaning \"table\", from Old High German banc, bank \"bench, counter\". Benches were used as makeshift desks or exchange counters during the Renaissance by Florentine bankers, who used to make their transactions atop desks covered by green tablecloths.[23][24]  The definition of a bank varies from country to country. See the relevant country pages for more information.  Under English common law, a banker is defined as a person who carries on the business of banking by conducting current accounts for their customers, paying checks drawn on them as well as collecting them for their customers.[25]  In most common law jurisdictions there is a Bills of Exchange Act that codifies the law in relation to negotiable instruments, including checks, and this Act contains a statutory definition of the term banker: banker includes a body of persons, whether incorporated or not, who carry on the business of banking' (Section 2, Interpretation). Although this definition seems circular, it is actually functional, because it ensures that the legal basis for bank transactions such as checks does not depend on how the bank is structured or regulated.  The business of banking is in many common law countries not defined by statute but by common law, the definition above. In other English common law jurisdictions there are statutory definitions of the business of banking or banking business. When looking at these definitions it is important to keep in mind that they are defining the business of banking for the purposes of the legislation, and not necessarily in general. In particular, most of the definitions are from legislation that has the purpose of regulating and supervising banks rather than regulating the actual business of banking. However, in many cases, the statutory definition closely mirrors the common law one. Examples of statutory definitions:  Since the advent of EFTPOS (Electronic Funds Transfer at Point Of Sale), direct credit, direct debit and internet banking, the check has lost its primacy in most banking systems as a payment instrument. This has led legal theorists to suggest that the check based definition should be broadened to include financial institutions that conduct current accounts for customers and enable customers to pay and be paid by third parties, even if they do not pay and collect checks .[27]  Banks act as payment agents by conducting checking or current accounts for customers, paying checks drawn by customers in the bank, and collecting checks deposited to customers' current accounts. Banks also enable customer payments via other payment methods such as Automated Clearing House (ACH), Wire transfers or telegraphic transfer, EFTPOS, and automated teller machines (ATMs).  Banks borrow money by accepting funds deposited on current accounts, by accepting term deposits, and by issuing debt securities such as banknotes and bonds. Banks lend money by making advances to customers on current accounts, by making installment loans, and by investing in marketable debt securities and other forms of money lending.  Banks provide different payment services, and a bank account is considered indispensable by most businesses and individuals. Nonbanks that provide payment services such as remittance companies are normally not considered as an adequate substitute for a bank account.  Banks issue new money when they make loans. In contemporary banking systems, regulators set a minimum level of reserve funds that banks must hold against the deposit liabilities created by the funding of these loans, in order to ensure that the banks can meet demands for payment of such deposits. These reserves can be acquired through the acceptance of new deposits, sale of other assets, or borrowing from other banks including the central bank.[28]  Activities undertaken by banks include personal banking, corporate banking, investment banking, private banking, transaction banking, insurance, consumer finance, trade finance and other related.  Banks offer many different channels to access their banking and other services:  A bank can generate revenue in a variety of different ways including interest, transaction fees and financial advice. Traditionally, the most significant method is via charging interest on the capital it lends out to customers.[29] The bank profits from the difference between the level of interest it pays for deposits and other sources of funds, and the level of interest it charges in its lending activities.  This difference is referred to as the spread between the cost of funds and the loan interest rate. Historically, profitability from lending activities has been cyclical and dependent on the needs and strengths of loan customers and the stage of the economic cycle. Fees and financial advice constitute a more stable revenue stream and banks have therefore placed more emphasis on these revenue lines to smooth their financial performance.  In the past 20 years, American banks have taken many measures to ensure that they remain profitable while responding to increasingly changing market conditions.  This helps in making a profit and facilitates economic development as a whole.[31]  Recently, as banks have been faced with pressure from fintechs, new and additional business models have been suggested such as freemium, monetization of data, white-labeling of banking and payment applications, or the cross-selling of complementary products.[32]  Banks face a number of risks in order to conduct their business, and how well these risks are managed and understood is a key driver behind profitability, and how much capital a bank is required to hold. Bank capital consists principally of equity, retained earnings and subordinated debt.  Some of the main risks faced by banks include:  The capital requirement is a bank regulation, which sets a framework within which a bank or depository institution must manage its balance sheet. The categorization of assets and capital is highly standardized so that it can be risk weighted.  After the financial crisis of 2007–2008, regulators force banks to issue Contingent convertible bonds (CoCos). These are hybrid capital securities that absorb losses in accordance with their contractual terms when the capital of the issuing bank falls below a certain level. Then debt is reduced and bank capitalization gets a boost. Owing to their capacity to absorb losses, CoCos have the potential to satisfy regulatory capital requirement.[35][36]  The economic functions of banks include:  Banks are susceptible to many forms of risk which have triggered occasional systemic crises.[37] These include liquidity risk (where many depositors may request withdrawals in excess of available funds), credit risk (the chance that those who owe money to the bank will not repay it), and interest rate risk (the possibility that the bank will become unprofitable, if rising interest rates force it to pay relatively more on its deposits than it receives on its loans).  Banking crises have developed many times throughout history when one or more risks have emerged for the banking sector as a whole. Prominent examples include the bank run that occurred during the Great Depression, the U.S. Savings and Loan crisis in the 1980s and early 1990s, the Japanese banking crisis during the 1990s, and the sub-prime mortgage crisis in the 2000s.  The 2023 global banking crisis is the latest of these crises: In March 2023, liquidity shortages and bank insolvencies led to three bank failures in the United States, and within two weeks, several of the world's largest banks failed or were shut down by regulators  Assets of the largest 1,000 banks in the world grew by 6.8% in the 2008–2009 financial year to a record US$96.4 trillion while profits declined by 85% to US$115 billion. Growth in assets in adverse market conditions was largely a result of recapitalization. EU banks held the largest share of the total, 56% in 2008–2009, down from 61% in the previous year. Asian banks' share increased from 12% to 14% during the year, while the share of US banks increased from 11% to 13%. Fee revenue generated by global investment in banking totaled US$66.3 billion in 2009, up 12% on the previous year.[38]  The United States has the most banks in the world in terms of institutions (5,330 as of 2015) and possibly branches (81,607 as of 2015).[39] This is an indicator of the geography and regulatory structure of the US, resulting in a large number of small to medium-sized institutions in its banking system. As of November 2009, China's top four banks have in excess of 67,000 branches (ICBC:18000+, BOC:12000+, CCB:13000+, ABC:24000+) with an additional 140 smaller banks with an undetermined number of branches. Japan had 129 banks and 12,000 branches. In 2004, Germany, France, and Italy each had more than 30,000 branches – more than double the 15,000 branches in the United Kingdom.[38]  Between 1985 and 2018 banks engaged in around 28,798 mergers or acquisitions, either as the acquirer or the target company. The overall known value of these deals cumulates to around 5,169 bil. USD.[40] In terms of value, there have been two major waves (1999 and 2007) which both peaked at around 460 bil. USD followed by a steep decline (−82% from 2007 until 2018).  Here is a list of the largest deals in history in terms of value with participation from at least one bank:  Currently, commercial banks are regulated in most jurisdictions by government entities and require a special bank license to operate.  Usually, the definition of the business of banking for the purposes of regulation is extended to include acceptance of deposits, even if they are not repayable to the customer's order – although money lending, by itself, is generally not included in the definition.  Unlike most other regulated industries, the regulator is typically also a participant in the market, being either publicly or privately governed central bank. Central banks also typically have a monopoly on the business of issuing banknotes. However, in some countries, this is not the case. In the UK, for example, the Financial Services Authority licenses banks, and some commercial banks (such as the Bank of Scotland) issue their own banknotes in addition to those issued by the Bank of England, the UK government's central bank.  Banking law is based on a contractual analysis of the relationship between the bank (defined above) and the customer – defined as any entity for which the bank agrees to conduct an account.  The law implies rights and obligations into this relationship as follows:  These implied contractual terms may be modified by express agreement between the customer and the bank. The statutes and regulations in force within a particular jurisdiction may also modify the above terms or create new rights, obligations, or limitations relevant to the bank-customer relationship.  Some types of financial institutions, such as building societies and credit unions, may be partly or wholly exempt from bank license requirements, and therefore regulated under separate rules.  The requirements for the issue of a bank license vary between jurisdictions but typically include:  Banks' activities can be divided into:  Most banks are profitmaking private enterprises. However, some are owned by the government, or are nonprofits.  The United States banking industry is one of the most heavily regulated and guarded in the world,[43] with multiple specialized and focused regulators. All banks with FDIC-insured deposits have the Federal Deposit Insurance Corporation (FDIC) as a regulator. However, for soundness examinations (i.e., whether a bank is operating in a sound manner), the Federal Reserve is the primary federal regulator for Fed-member state banks; the Office of the Comptroller of the Currency (OCC) is the primary federal regulator for national banks. State non-member banks are examined by the state agencies as well as the FDIC.[44]: 236  National banks have one primary regulator – the OCC.  Each regulatory agency has its own set of rules and regulations to which banks and thrifts must adhere. The Federal Financial Institutions Examination Council (FFIEC) was established in 1979 as a formal inter-agency body empowered to prescribe uniform principles, standards, and report forms for the federal examination of financial institutions. Although the FFIEC has resulted in a greater degree of regulatory consistency between the agencies, the rules and regulations are constantly changing.  In addition to changing regulations, changes in the industry have led to consolidations within the Federal Reserve, FDIC, OTS, and OCC. Offices have been closed, supervisory regions have been merged, staff levels have been reduced and budgets have been cut. The remaining regulators face an increased burden with an increased workload and more banks per regulator. While banks struggle to keep up with the changes in the regulatory environment, regulators struggle to manage their workload and effectively regulate their banks. The impact of these changes is that banks are receiving less hands-on assessment by the regulators, less time spent with each institution, and the potential for more problems slipping through the cracks, potentially resulting in an overall increase in bank failures across the United States.  The changing economic environment has a significant impact on banks and thrifts as they struggle to effectively manage their interest rate spread in the face of low rates on loans, rate competition for deposits and the general market changes, industry trends and economic fluctuations. It has been a challenge for banks to effectively set their growth strategies with the recent economic market. A rising interest rate environment may seem to help financial institutions, but the effect of the changes on consumers and businesses is not predictable and the challenge remains for banks to grow and effectively manage the spread to generate a return to their shareholders.  The management of the banks' asset portfolios also remains a challenge in today's economic environment. Loans are a bank's primary asset category and when loan quality becomes suspect, the foundation of a bank is shaken to the core. While always an issue for banks, declining asset quality has become a big problem for financial institutions.  There are several reasons for this, one of which is the lax attitude some banks have adopted because of the years of \"good times.\" The potential for this is exacerbated by the reduction in the regulatory oversight of banks and in some cases depth of management. Problems are more likely to go undetected, resulting in a significant impact on the bank when they are discovered. In addition, banks, like any business, struggle to cut costs and have consequently eliminated certain expenses, such as adequate employee training programs.  Banks also face a host of other challenges such as aging ownership groups. Across the country, many banks' management teams and boards of directors are aging. Banks also face ongoing pressure from shareholders, both public and private, to achieve earnings and growth projections. Regulators place added pressure on banks to manage the various categories of risk. Banking is also an extremely competitive industry. Competing in the financial services industry has become tougher with the entrance of such players as insurance agencies, credit unions, check cashing services, credit card companies, etc.  As a reaction, banks have developed their activities in financial instruments, through financial market operations such as brokerage and have become big players in such activities.  Another major challenge is the aging infrastructure, also called legacy IT. Backend systems were built decades ago and are incompatible with new applications. Fixing bugs and creating interfaces costs huge sums, as knowledgeable programmers become scarce.[45]  To be able to provide home buyers and builders with the funds needed, banks must compete for deposits. The phenomenon of disintermediation had to dollars moving from savings accounts and into direct market instruments such as U.S. Department of Treasury obligations, agency securities, and corporate debt. One of the greatest factors in recent years in the movement of deposits was the tremendous growth of money market funds whose higher interest rates attracted consumer deposits.[46]  To compete for deposits, US savings institutions offer many different types of plans:[46]  Bank statements are accounting records produced by banks under the various accounting standards of the world. Under GAAP there are two kinds of accounts: debit and credit. Credit accounts are Revenue, Equity and Liabilities. Debit Accounts are Assets and Expenses. The bank credits a credit account to increase its balance, and debits a credit account to decrease its balance.[47]  The customer debits his or her savings\/bank (asset) in his ledger when making a deposit (and the account is normally in debit), while the customer credits a credit card (liability) account in his ledger every time he spends money (and the account is normally in credit). When the customer reads his bank statement, the statement will show a credit to the account for deposits, and debits for withdrawals of funds. The customer with a positive balance will see this balance reflected as a credit balance on the bank statement. If the customer is overdrawn, he will have a negative balance, reflected as a debit balance on the bank statement.  One source of deposits for banks is deposit brokers who deposit large sums of money on behalf of investors through trust corporations. This money will generally go to the banks which offer the most favorable terms, often better than those offered local depositors. It is possible for a bank to engage in business with no local deposits at all, all funds being brokered deposits. Accepting a significant quantity of such deposits, or \"hot money\" as it is sometimes called, puts a bank in a difficult and sometimes risky position, as the funds must be lent or invested in a way that yields a return sufficient to pay the high interest being paid on the brokered deposits. This may result in risky decisions and even in eventual failure of the bank. Banks which failed during 2008 and 2009 in the United States during the global financial crisis had, on average, four times more brokered deposits as a percent of their deposits than the average bank. Such deposits, combined with risky real estate investments, factored into the savings and loan crisis of the 1980s. Regulation of brokered deposits is opposed by banks on the grounds that the practice can be a source of external funding to growing communities with insufficient local deposits.[48] There are different types of accounts: saving, recurring and current accounts.  Custodial accounts are accounts in which assets are held for a third party. For example, businesses that accept custody of funds for clients prior to their conversion, return, or transfer may have a custodial account at a bank for these purposes.  In modern times there have been huge reductions to the barriers of global competition in the banking industry. Increases in telecommunications and other financial technologies, such as Bloomberg, have allowed banks to extend their reach all over the world since they no longer have to be near customers to manage both their finances and their risk. The growth in cross-border activities has also increased the demand for banks that can provide various services across borders to different nationalities. Despite these reductions in barriers and growth in cross-border activities, the banking industry is nowhere near as globalized as some other industries. In the US, for instance, very few banks even worry about the Riegle–Neal Act, which promotes more efficient interstate banking. In the vast majority of nations around the globe, the market share for foreign owned banks is currently less than a tenth of all market shares for banks in a particular nation. One reason the banking industry has not been fully globalized is that it is more convenient to have local banks provide loans to small businesses and individuals. On the other hand, for large corporations, it is not as important in what nation the bank is in since the corporation's financial information is available around the globe.[49]  There have been two significant attempts to overcome the industry's traditional focus on competing at the national level rather than the international level.  In the 1980s, Citigroup and HSBC both began to develop large networks of retail bank branches in numerous countries around the world, in order to become global consumer banking brands.  But in 2021, Citigroup initiated an exit from retail banking outside of its core U.S. market,[50] while in 2022, HSBC initiated an exit from the U.S. retail market (except for its wealth management business)[51] and then in 2023 put its retail operations in a dozen other countries under review for sale or closure.[52]  According to Wells Fargo, both banks were operating on the assumption that globalization would lead to the rise of large numbers of consumers who would regularly travel across borders for both work and play, but that \"global consumer customer never materialized\".[50]  Terms and concepts:  Types of institutions:  Crime:  Related lists:    Banking by country     "},"meta":{},"created_at":"2025-03-22T14:25:42.277677Z","updated_at":"2025-03-22T14:25:42.277677Z","inner_id":56,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":65,"annotations":[{"id":65,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.320914Z","updated_at":"2025-03-22T14:25:42.320914Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"5131b155-4df7-448d-8f71-24cc469fb2cb","import_id":null,"last_action":null,"bulk_created":false,"task":65,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A country's gross government debt (also called public debt or sovereign debt[1]) is the financial liabilities of the government sector.[2]: 81  Changes in government debt over time reflect primarily borrowing due to past government deficits.[3] A deficit occurs when a government's expenditures exceed revenues.[4][2]: 79–82  Government debt may be owed to domestic residents, as well as to foreign residents. If owed to foreign residents, that quantity is included in the country's external debt.[5]  In 2020, the value of government debt worldwide was $87.4 US trillion, or 99% measured as a share of gross domestic product (GDP).[6] Government debt accounted for almost 40% of all debt (which includes corporate and household debt), the highest share since the 1960s.[6] The rise in government debt since 2007 is largely attributable to stimulus measures during the Great Recession, and the COVID-19 recession.[6]  Governments may take on debt when the government's spending desires do not match government revenue flows. Taking debt can allow governments to conduct fiscal policy more effectively, avoid tax increases, and making investments with long-term returns.[7] The ability of government to issue debt has been central to state formation and to state building.[8][9] Public debt has been linked to the rise of democracy, private financial markets, and modern economic growth.[8][9]  Actors that issue sovereign credit include private investors, commercial banks, multilateral development banks (such as the World Bank) and other governments.[7] Low-income, highly indebted states tend to attain loans from multilateral development banks and other governments because they are considered too risky for private investors.[7] Higher-income states tend to issue sovereign bonds, which are subsequently traded by investors in secondary markets.[7] Ratings agencies (e.g. Moody's, Standard & Poor's) issue ratings that measure the credit-worthiness of governments, which may in turn affect the value of sovereign bonds in secondary markets.[7]  Government debt is typically measured as the gross debt of the general government sector that is in the form of liabilities that are debt instruments.[2]: 207  A debt instrument is a financial claim that requires payment of interest and\/or principal by the debtor to the creditor in the future. Examples include debt securities (such as bonds and bills), loans, and government employee pension obligations.[2]: 207   International comparisons usually focus on general government debt because the level of government responsible for programs (for example, health care) differs across countries and the general government comprises central, state, provincial, regional, local governments, and social security funds.[2]: 18, s2.58, s2.59  The debt of public corporations (such as post offices that provide goods or services on a market basis) is not included in general government debt, following the International Monetary Fund's Government Finance Statistics Manual 2014 (GFSM), which describes recommended methodologies for compiling debt statistics to ensure international comparability.[2]: 33, s2.127   The gross debt of the general government sector is the total liabilities that are debt instruments. An alternative debt measure is net debt, which is gross debt minus financial assets in the form of debt instruments.[2]: 208, s7.243  Net debt estimates are not always available since some government assets may be difficult to value, such as loans made at concessional rates.[2]: 208–209, s7.246   Debt can be measured at market value or nominal value. As a general rule, the GFSM says debt should be valued at market value, the value at which the asset could be exchanged for cash.[2]: 55, s3.107  However, the nominal value is useful for a debt-issuing government, as it is the amount that the debtor owes to the creditor.[2]: 191, ft28  If market and nominal values are not available, face value (the undiscounted amount of principal to be repaid at maturity)[2]: 56  is used.[2]: 208, s7.238   A country's general government debt-to-GDP ratio is an indicator of its debt burden since GDP measures the value of goods and services produced by an economy during a period (usually a year). As well, debt measured as a percentage of GDP facilitates comparisons across countries of different size. The OECD views the general government debt-to-GDP ratio as a key indicator of the sustainability of government finance.[3]  An important reason governments borrow is to act as an economic \"shock absorber\". For example, deficit financing can be used to maintain government services during a recession when tax revenues fall and expenses rise for say unemployment benefits.[10] Government debt created to cover costs from major shock events can be particularly beneficial. Such events would include  In the absence of debt financing, when revenues decline during a downturn, a government would need to raise taxes or reduce spending, which would exacerbate the negative event.  While government borrowing may be desirable at times, a \"deficits bias\" can arise when there is disagreement among groups in society over government spending.[12][13] To counter deficit bias, many countries have adopted balanced budget rules or restrictions on government debt. Examples include the \"debt anchor\"[10] in Sweden; a \"debt brake\" in Germany and Switzerland; and the European Union's Stability and Growth Pact agreement to maintain a general government gross debt of no more than 60% of GDP.[14][15]  The ability of government to issue debt has been central to state formation and to state building.[8][9] Public debt has been linked to the rise of democracy, private financial markets, and modern economic growth.[8][9] For example, in the 17th and 18th centuries England established a parliament that included creditors, as part of a larger coalition, whose authorization had to be secured for the country to borrow or raise taxes. This institution improved England's ability to borrow because lenders were more willing to hold the debt of a state with democratic institutions that would support debt repayment, versus a state where the monarch could not be compelled to repay debt.[8][9]  As public debt came to be recognized as a safe and liquid investment, it could be used as collateral for private loans. This created a complementarity between the development of public debt markets and private financial markets.[8] Government borrowing to finance public goods, such as urban infrastructure, has been associated with modern economic growth.[8]: 6   Written records point to public borrowing as long as two thousand years ago when Greek city-states such as Syracuse borrowed from their citizens.[8]: 10–16  But the founding of the Bank of England in 1694 revolutionised public finance and put an end to defaults such as the Great Stop of the Exchequer of 1672, when Charles II had suspended payments on his bills. From then on, the British Government would never fail to repay its creditors.[16] In the following centuries, other countries in Europe and later around the world adopted similar financial institutions to manage their government debt.  In 1815, at the end of the Napoleonic Wars, British government debt reached a peak of more than 200% of GDP,[17] nearly 887 million pounds sterling.[18] The debt was paid off over 90 years by running primary budget surpluses (that is, revenues were greater than spending after payment of interest).[11]  In 1900, the country with the most total debt was France (£1,086,215,525), followed by Russia (£656,000,000) then the United Kingdom (£628,978,782);[18] on a per-capita basis, the highest-debt countries were New Zealand (£58 12s. per person), the Australian colonies (£52 13s.) and Portugal (£35).[18]  In 2018, global government debt reached the equivalent of $66 trillion, or about 80% of global GDP,[19] and by 2020, global government debt reached $87US trillion, or 99% of global GDP.[6] The COVID-19 pandemic caused public debt to soar in 2020, particularly in advanced economies that put in place sweeping fiscal measures.[6]  Government debt accumulation may lead to a rising interest rate,[10] which can crowd out private investment as governments compete with private firms for limited investment funds. Some evidence suggests growth rates are lower for countries with government debt greater than around 80 percent of GDP.[10][20] A World Bank Group report that analyzed debt levels of 100 developed and developing countries from 1980 to 2008 found that debt-to-GDP ratios above 77% for developed countries (64% for developing countries) reduced future annual economic growth by 0.017 (0.02 for developing countries) percentage points for each percentage point of debt above the threshold.[21][22]  Excessive debt levels may make governments more vulnerable to a debt crisis, where a country is unable to make payments on its debt, and it cannot borrow more.[10] Crises can be costly, particularly if a debt crisis is combined with a financial\/banking crisis which leads to economy-wide deleveraging. As firms sell assets to pay off debt, asset prices fall which risks an even greater fall in incomes, further depressing tax revenue and requiring governments to drastically cut government services.[23] Examples of debt crises include the Latin American debt crisis of the early 1980s, and Argentina's debt crisis in 2001. To help avoid a crisis, governments may want to maintain a \"fiscal breathing space\". Historical experience shows that room to double the level of government debt when needed is an approximate guide.[10]  Government debt is built up by borrowing when expenditure exceeds revenue, so government debt generally creates an intergenerational transfer. This is because the beneficiaries of the government's expenditure on goods and services when the debt is created typically differ from the individuals responsible for repaying the debt in the future.  An alternative view of government debt, sometimes called the Ricardian equivalence proposition, is that government debt has no impact on the economy if individuals are altruistic and internalize the impact of the debt on future generations.[24] According to this proposition, while the quantity of government purchases affects the economy, debt financing will have the same impact as tax financing because with debt financing individuals will anticipate the future taxes needed to repay the debt, and so increase their saving and bequests by the amount of government debt. Such higher individual saving means, for example, that private consumption falls one-for-one with the rise in government debt, so the interest rate would not rise and private investment is not crowded out.  In public discourse, politicians and commentators frequently draw parallels between government debt and household debt, as they argue that a government taking on debt is akin to a household taking on debt. However, economists generally challenge this analogy, as the functions and constraints of governments and households are vastly dissimilar.[25][26][27][28] Differences include that governments can print money,[29][30][31] interest rates on government borrowing may be cheaper than individual borrowing,[29][30] governments can increase their budgets through taxation,[29][30] governments have indefinite planning horizons,[32] national debt may be held primarily domestically (the equivalent of household members owing each other),[32] governments typically have greater collateral for borrowing,[33] and contractions in government spending can cause or prolong economic crises and increase the debt of the government.[28] For governments, the main risks of overspending may revolve around inflation rather than the size of the debt per se.[31][32]  Historically, there have been many cases where governments have defaulted on their debts, including Spain in the 16th and 17th centuries, which nullified its government debt several times; the Confederate States of America, whose debt was not repaid after the American Civil War; and revolutionary Russia after 1917, which refused to accept responsibility for Imperial Russia's foreign debt.[34]  If government debt is issued in a country's own fiat money, it is sometimes considered risk free because the debt and interest can be repaid by money creation.[35][36] However, not all governments issue their own currency. Examples include sub-national governments, like municipal, provincial, and state governments; and countries in the eurozone. In the Greek government-debt crisis, one proposed solution was for Greece to leave the eurozone and go back to issuing the drachma[37][38] (although this would have addressed only future debt issuance, leaving substantial existing debt denominated in what would then be a foreign currency).[39]  Debt of a sub-national government is generally viewed as less risky for a lender if it is explicitly or implicitly guaranteed by a regional or national level of government. When New York City declined into what would have been bankrupt status during the 1970s, a bailout came from New York State and the United States national government. U.S. state and local government debt is substantial — in 2016 their debt amounted to $3 trillion, plus another $5 trillion in unfunded liabilities.[40]  A country that issues its own currency may be at low risk of default in local currency, but if a central bank without inflation targeting provides finance by buying government bonds (debt monetization or indirectly quantitative easing), this can lead to price inflation. In an extreme case, in the 1920s Weimar Germany suffered from hyperinflation when the government used money creation to pay off the national debt following World War I.  While U.S. Treasury bonds denominated in U.S. dollars may be considered risk-free to an American purchaser, a foreign investor bears the risk of a fall in the value of the U.S. dollar relative to their home currency. A government can issue debt in foreign currency to eliminate exchange rate risk for foreign lenders, but that means the borrowing government then bears the exchange rate risk. Also, by issuing debt in foreign currency, a country cannot erode the value of the debt by means of inflation.[41] Almost 70% of all debt in a sample of developing countries from 1979 through 2006 was denominated in U.S. dollars.[42]  Most governments have contingent liabilities, which are obligations that do not arise unless a particular event occurs in the future.[2]: 76  An example of an explicit contingent liability is a public sector loan guarantee, where the government is required to make payments only if the debtor defaults.[2]: 210, s.7.252  Examples of implicit contingent liabilities include ensuring the payment of future social security pension benefits, covering the obligations of subnational governments in the event of a default, and spending for natural disaster relief.[2]: 209–210   Explicit contingent liabilities and net implicit social security obligations should be included as memorandum items to a government's balance sheet,[2]: 69, 76–77, 209–212  but they are not included in government debt because they are not contractual obligations.[2]: 210, s.7.252  Indeed, it is not uncommon for governments to change unilaterally the benefit structure of social security schemes, for example (e.g., by changing the circumstances under which the benefits become payable, or the amount of the benefit).[2]: 76, s4.49  In the U.S. and in many countries, there is no money earmarked for future social insurance payments — the system is called a pay-as-you-go scheme. According to the 2018 annual reports from the trustees for the U.S. Social Security and Medicare trust funds, Medicare is facing a $37 trillion unfunded liability over the next 75 years, and Social Security is facing a $13 trillion unfunded liability over the same time frame.[43] Neither of these amounts are included in the U.S. gross general government debt, which in 2024 was $34 trillion.[44]  In 2010 the European Commission required EU Member Countries to publish their debt information in standardized methodology, explicitly including debts that were previously hidden in a number of ways to satisfy minimum requirements on local (national) and European (Stability and Growth Pact) level.[45]  Government finance:  Specific:  General: "},"meta":{},"created_at":"2025-03-22T14:25:42.277677Z","updated_at":"2025-03-22T14:25:42.277677Z","inner_id":57,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":66,"annotations":[{"id":66,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.320914Z","updated_at":"2025-03-22T14:25:42.320914Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"2af24aa7-ad70-4176-b896-ea15f1710c73","import_id":null,"last_action":null,"bulk_created":false,"task":66,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Heterodox  In economics and political science, fiscal policy is the use of government revenue collection (taxes or tax cuts) and expenditure to influence a country's economy. The use of government revenue expenditures to influence macroeconomic variables developed in reaction to the Great Depression of the 1930s, when the previous laissez-faire approach to economic management became unworkable. Fiscal policy is based on the theories of the British economist John Maynard Keynes, whose Keynesian economics theorised that government changes in the levels of taxation and government spending influence aggregate demand and the level of economic activity. Fiscal and monetary policy are the key strategies used by a country's government and central bank to advance its economic objectives. The combination of these policies enables these authorities to target inflation and to increase employment. In modern economies, inflation is conventionally considered \"healthy\" in the range of 2%–3%. Additionally, it is designed to try to keep GDP growth at 2%–3% and the unemployment rate near the natural unemployment rate of 4%–5%.[1] This implies that fiscal policy is used to stabilise the economy over the course of the business cycle.[2]  Changes in the level and composition of taxation and government spending can affect macroeconomic variables, including:  Fiscal policy can be distinguished from monetary policy, in that fiscal policy deals with taxation and government spending and is often administered by a government department; while monetary policy deals with the money supply, interest rates and is often administered by a country's central bank. Both fiscal and monetary policies influence a country's economic performance.  Since the 1970s, it became clear that monetary policy performance has some benefits over fiscal policy due to the fact that it reduces political influence, as it is set by the central bank (to have an expanding economy before the general election, politicians might cut the interest rates). Additionally, fiscal policy can potentially have more supply-side effects on the economy: to reduce inflation, the measures of increasing taxes and lowering spending would not be preferred, so the government might be reluctant to use these. Monetary policy is generally quicker to implement as interest rates can be set every month, while the decision to increase government spending might take time to figure out which area the money should be spent on.[3]  The recession of the 2000s decade shows that monetary policy also has certain limitations. A liquidity trap occurs when interest rate cuts are insufficient as a demand booster as banks do not want to lend and the consumers are reluctant to increase spending due to negative expectations for the economy. Government spending is responsible for creating the demand in the economy and can provide a kick-start to get the economy out of the recession. When a deep recession takes place, it is not sufficient to rely just on monetary policy to restore the economic equilibrium.[3] Each side of these two policies has its differences, therefore, combining aspects of both policies to deal with economic problems has become a solution that is now used by the US. These policies have limited effects; however, fiscal policy seems to have a greater effect over the long-run period, while monetary policy tends to have a short-run success.[4]  In 2000, a survey of 298 members of the American Economic Association (AEA) found that while 84 percent generally agreed with the statement \"Fiscal policy has a significant stimulative impact on a less than fully employed economy\", 71 percent also generally agreed with the statement \"Management of the business cycle should be left to the Federal Reserve; activist fiscal policy should be avoided.\"[5] In 2011, a follow-up survey of 568 AEA members found that the previous consensus about the latter proposition had dissolved and was by then roughly evenly disputed.[6]  Depending on the state of the economy, fiscal policy may reach for different objectives: its focus can be to restrict economic growth by mediating inflation or, in turn, increase economic growth by decreasing taxes, encouraging spending on different projects that act as stimuli to economic growth and enabling borrowing and spending. The three stances of fiscal policy are the following:   However, these definitions can be misleading because, even with no changes in spending or tax laws at all, cyclic fluctuations of the economy cause cyclic fluctuations of tax revenues and of some types of government spending, altering the deficit situation; these are not considered to be policy changes. Therefore, for purposes of the above definitions, \"government spending\" and \"tax revenue\" are normally replaced by \"cyclically adjusted government spending\" and \"cyclically adjusted tax revenue\". Thus, for example, a government budget that is balanced over the course of the business cycle is considered to represent a neutral and effective fiscal policy stance.  Governments spend money on a wide variety of things, from the military and police to services such as education and health care, as well as transfer payments such as welfare benefits. This expenditure can be funded in a number of different ways:  A fiscal deficit is often funded by issuing bonds such as Treasury bills or and gilt-edged securities but can also be funded by issuing equity. Bonds pay interest, either for a fixed period or indefinitely that is funded by taxpayers as a whole. Equity offers returns on investment (interest) that can only be realized in discharging a future tax liability by an individual taxpayer. If available government revenue is insufficient to support the interest payments on bonds, a nation may default on its debts, usually to foreign creditors. Public debt or borrowing refers to the government borrowing from the public. It is impossible for a government to \"default\" on its equity since the total returns available to all investors (taxpayers) are limited at any point by the total current year tax liability of all investors.  A fiscal surplus is often saved for future use, and may be invested in either local currency or any financial instrument that may be traded later once resources are needed and the additional debt is not needed.  The concept of a fiscal straitjacket is a general economic principle that suggests strict constraints on government spending and public sector borrowing, to limit or regulate the budget deficit over a time period. Most US states have balanced budget rules that prevent them from running a deficit. The United States federal government technically has a legal cap on the total amount of money it can borrow, but it is not a meaningful constraint because the cap can be raised as easily as spending can be authorized, and the cap is almost always raised before the debt gets that high.  Governments use fiscal policy to influence the level of aggregate demand in the economy, so that certain economic goals can be achieved:[7]  The Keynesian view of economics suggests that increasing government spending and decreasing the rate of taxes are the best ways to have an influence on aggregate demand, stimulate it, while decreasing spending and increasing taxes after the economic expansion has already taken place. Additionally, Keynesians argue that expansionary fiscal policy should be used in times of recession or low economic activity as an essential tool for building the framework for strong economic growth and working towards full employment. In theory, the resulting deficits would be paid for by an expanded economy during the expansion that would follow; this was the reasoning behind the New Deal.  The IS-LM model is another way of understanding the effects of fiscal expansion. As the government increases spending, there will be a shift in the IS curve up and to the right. In the short run, this increases the real interest rate, which then reduces private investment and increases aggregate demand, placing upward pressure on supply. To meet the short-run increase in aggregate demand, firms increase full-employment output. The increase in short-run price levels reduces the money supply, which shifts the LM curve back, and thus, returning the general equilibrium to the original full employment (FE) level. Therefore, the IS-LM model shows that there will be an overall increase in the price level and real interest rates in the long run due to fiscal expansion.[8]  Governments can use a budget surplus to do two things:   Keynesian theory posits that removing spending from the economy will reduce levels of aggregate demand and contract the economy, thus stabilizing prices.  But economists still debate the effectiveness of fiscal stimulus. The argument mostly centers on crowding out: whether government borrowing leads to higher interest rates that may offset the stimulative impact of spending. When the government runs a budget deficit, funds will need to come from public borrowing (the issue of government bonds), overseas borrowing, or monetizing the debt. When governments fund a deficit with the issuing of government bonds, interest rates can increase across the market, because government borrowing creates higher demand for credit in the financial markets. This decreases aggregate demand for goods and services, either partially or entirely offsetting the direct expansionary impact of the deficit spending, thus diminishing or eliminating the achievement of the objective of a fiscal stimulus. Neoclassical economists generally emphasize crowding out while Keynesians argue that fiscal policy can still be effective, especially in a liquidity trap where, they argue, crowding out is minimal.[9]  In the classical view, expansionary fiscal policy also decreases net exports, which has a mitigating effect on national output and income. When government borrowing increases interest rates it attracts foreign capital from foreign investors. This is because, all other things being equal, the bonds issued from a country executing expansionary fiscal policy now offer a higher rate of return. In other words, companies wanting to finance projects must compete with their government for capital so they offer higher rates of return. To purchase bonds originating from a certain country, foreign investors must obtain that country's currency. Therefore, when foreign capital flows into the country undergoing fiscal expansion, demand for that country's currency increases. The increased demand, in turn, causes the currency to appreciate, reducing the cost of imports and making exports from that country more expensive to foreigners. Consequently, exports decrease and imports increase, reducing demand from net exports.  Some economists oppose the discretionary use of fiscal stimulus because of the inside lag (the time lag involved in implementing it), which is almost inevitably long because of the substantial legislative effort involved. Further, the outside lag between the time of implementation and the time that most of the effects of the stimulus are felt could mean that the stimulus hits an already-recovering economy and overheats the ensuing h rather than stimulating the economy when it needs it.  Some economists are concerned about potential inflationary effects driven by increased demand engendered by a fiscal stimulus. In theory, fiscal stimulus does not cause inflation when it uses resources that would have otherwise been idle. For instance, if a fiscal stimulus employs a worker who otherwise would have been unemployed, there is no inflationary effect; however, if the stimulus employs a worker who otherwise would have had a job, the stimulus is increasing labor demand while labor supply remains fixed, leading to wage inflation and therefore price inflation. "},"meta":{},"created_at":"2025-03-22T14:25:42.277677Z","updated_at":"2025-03-22T14:25:42.277677Z","inner_id":58,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":67,"annotations":[{"id":67,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.320914Z","updated_at":"2025-03-22T14:25:42.320914Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"900ff562-c321-46ed-a723-e30bc5d055c1","import_id":null,"last_action":null,"bulk_created":false,"task":67,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Heterodox  An interest rate is the amount of interest due per period, as a proportion of the amount lent, deposited, or borrowed (called the principal sum). The total interest on an amount lent or borrowed depends on the principal sum, the interest rate, the compounding frequency, and the length of time over which it is lent, deposited, or borrowed.  The annual interest rate is the rate over a period of one year. Other interest rates apply over different periods, such as a month or a day, but they are usually annualized.  The interest rate has been characterized as \"an index of the preference . . . for a dollar of present [income] over a dollar of future income\".[1] The borrower wants, or needs, to have money sooner, and is willing to pay a fee—the interest rate—for that privilege.  Interest rates vary according to:  as well as other factors.  A company borrows capital from a bank to buy assets for its business. In return, the bank charges the company interest. (The lender might also require rights over the new assets as collateral.)  A bank will use the capital deposited by individuals to make loans to their clients. In return, the bank should pay interest to individuals who have deposited their capital. The amount of interest payment depends on the interest rate and the amount of capital they deposited.  Base rate usually refers to the annualized effective interest rate offered on overnight deposits by the central bank or other monetary authority.[citation needed]  The annual percentage rate (APR) may refer either to a nominal APR or an effective APR (EAPR). The difference between the two is that the EAPR accounts for fees and compounding, while the nominal APR does not.  The annual equivalent rate (AER), also called the effective annual rate, is used to help consumers compare products with different compounding frequencies on a common basis, but does not account for fees.  A discount rate[2] is applied to calculate present value.  For an interest-bearing security, coupon rate is the ratio of the annual coupon amount (the coupon paid per year) per unit of par value, whereas current yield is the ratio of the annual coupon divided by its current market price. Yield to maturity is a bond's expected internal rate of return, assuming it will be held to maturity, that is, the discount rate which equates all remaining cash flows to the investor (all remaining coupons and repayment of the par value at maturity) with the current market price.  Based on the banking business, there are deposit interest rate and loan interest rate.  Based on the relationship between supply and demand of market interest rate, there are fixed interest rate and floating interest rate.  Interest rate targets are a vital tool of monetary policy and are taken into account when dealing with variables like investment, inflation, and unemployment. The central banks of countries generally tend to reduce interest rates when they wish to increase investment and consumption in the country's economy. However, a low interest rate as a macro-economic policy can be risky and may lead to the creation of an economic bubble, in which large amounts of investments are poured into the real-estate market and stock market. In developed economies, interest-rate adjustments are thus made to keep inflation within a target range for the health of economic activities or cap the interest rate concurrently with economic growth to safeguard economic momentum.[3][4][5][6][7]  In the past two centuries, interest rates have been variously set either by national governments or central banks. For example, the Federal Reserve federal funds rate in the United States has varied between about 0.25% and 19% from 1954 to 2008, while the Bank of England base rate varied between 0.5% and 15% from 1989 to 2009,[8][9] and Germany experienced rates close to 90% in the 1920s down to about 2% in the 2000s.[10][11] During an attempt to tackle spiraling hyperinflation in 2007, the Central Bank of Zimbabwe increased interest rates for borrowing to 800%.[12]  The interest rates on prime credits in the late 1970s and early 1980s were far higher than had been recorded – higher than previous US peaks since 1800, than British peaks since 1700, or than Dutch peaks since 1600; \"since modern capital markets came into existence, there have never been such high long-term rates\" as in this period.[13]  Before modern capital markets, there have been accounts that savings deposits could achieve an annual return of at least 25% and up to as high as 50%.[14]  The nominal interest rate is the rate of interest with no adjustment for inflation.  For example, suppose someone deposits $100 with a bank for one year, and they receive interest of $10 (before tax), so at the end of the year, their balance is $110 (before tax). In this case, regardless of the rate of inflation, the nominal interest rate is 10% per annum (before tax).  The real interest rate measures the growth in real value of the loan plus interest, taking inflation into account. The repayment of principal plus interest is measured in real terms compared against the buying power of the amount at the time it was borrowed, lent, deposited or invested.  If inflation is 10%, then the $110 in the account at the end of the year has the same purchasing power (that is, buys the same amount) as the $100 had a year ago. The real interest rate is zero in this case.  The real interest rate is given by the Fisher equation:  where p is the inflation rate. For low rates and short periods, the linear approximation applies:  The Fisher equation applies both ex ante and ex post. Ex ante, the rates are projected rates, whereas ex post, the rates are historical.  There is a market for investments, including the money market, bond market, stock market, and currency market as well as retail banking.  Interest rates reflect:  According to the theory of rational expectations, borrowers and lenders form an expectation of inflation in the future. The acceptable nominal interest rate at which they are willing and able to borrow or lend includes the real interest rate they require to receive, or are willing to pay, plus the rate of inflation they expect. Under behavioral expectations, the formation of expectations deviates from rational expectations due to cognitive limitations and information processing costs. Agents may exhibit myopia (limited attention) to certain economic variables, form expectations based on simplified heuristics, or update their beliefs more gradually than under full rationality. These behavioral frictions can affect monetary policy transmission and optimal policy design.[16]  The level of risk in investments is taken into consideration. Riskier investments such as shares and junk bonds are normally expected to deliver higher returns than safer ones like government bonds.  The additional return above the risk-free nominal interest rate which is expected from a risky investment is the risk premium. The risk premium an investor requires on an investment depends on the risk preferences of the investor. Evidence suggests that most lenders are risk-averse.[17]  A maturity risk premium applied to a longer-term investment reflects a higher perceived risk of default.  There are four kinds of risk:  Most economic agents exhibit a liquidity preference, defined as the propensity to hold cash or highly liquid assets over less fungible investments, reflecting both precautionary and transactional motives. Liquidity preference manifests in the yield differential between assets of varying maturities and convertibility costs, where cash provides immediate transaction capability with zero conversion costs. This preference creates a term structure of required returns, exemplified by the higher yields typically demanded for longer-duration assets. For instance, while a 1-year loan offers relatively rapid convertibility to cash, a 10-year loan commands a greater liquidity premium. However, the existence of deep secondary markets can partially mitigate illiquidity costs, as evidenced by US Treasury bonds, which maintain significant liquidity despite longer maturities due to their unique status as a safe asset and the associated financial sector stability benefits.[18][19]  A basic interest rate pricing model for an asset is  where  Assuming perfect information, pe is the same for all participants in the market, and the interest rate model simplifies to  The spread of interest rates is the lending rate minus the deposit rate.[20] This spread covers operating costs for banks providing loans and deposits. A negative spread is where a deposit rate is higher than the lending rate.[21]  Interest rates affect economic activity broadly, which is the reason why they are normally the main instrument of the monetary policies conducted by central banks.[22] Changes in interest rates will affect firms' investment behaviour, either raising or lowering the opportunity cost of investing. Interest rate changes also affect asset prices like stock prices and house prices, which again influence households' consumption decisions through a wealth effect. Additionally, international interest rate differentials affect exchange rates and consequently exports and imports. These various channels are collectively known as the monetary transmission mechanism. Consumption, investment and net exports are all important components of aggregate demand. Consequently, by influencing the general interest rate level, monetary policy can affect overall demand for goods and services in the economy and hence output and employment.[23] Changes in employment will over time affect wage setting, which again affects pricing and consequently ultimately inflation. The relation between employment (or unemployment) and inflation is known as the Phillips curve.[22]  For economies maintaining a fixed exchange rate system, determining the interest rate is also an important instrument of monetary policy as international capital flows are in part determined by interest rate differentials between countries.[24]  The Federal Reserve (often referred to as 'the Fed') implements monetary policy largely by targeting the federal funds rate (FFR). This is the rate that banks charge each other for overnight loans of federal funds, which are the reserves held by banks at the Fed. Until the 2007–2008 financial crisis, the Fed relied on open market operations, i.e. selling and buying securities in the open market to adjust the supply of reserve balances so as to keep the FFR close to the Fed's target.[25] However, since 2008 the actual conduct of monetary policy implementation has changed considerably, the Fed using instead various administered interest rates (i.e.,  interest rates that are set directly by the Fed rather than being determined by the market forces of supply and demand) as the primary tools to steer short-term market interest rates towards the Fed's policy target.[26]  Financial economists such as World Pensions Council (WPC) researchers have argued that durably low interest rates in most G20 countries will have an adverse impact on the funding positions of pension funds as \"without returns that outstrip inflation, pension investors face the real value of their savings declining rather than ratcheting up over the next few years\".[27] Current interest rates in savings accounts often fail to keep up with the pace of inflation.[28]  From 1982 until 2012, most Western economies experienced a period of low inflation combined with relatively high returns on investments across all asset classes including government bonds. This brought a certain sense of complacency[citation needed] amongst some pension actuarial consultants and regulators, making it seem reasonable to use optimistic economic assumptions to calculate the present value of future pension liabilities.  Because interest and inflation are generally given as percentage increases, the formulae above are (linear) approximations.  For instance,  is only approximate. In reality, the relationship is  so  The two approximations, eliminating higher order terms, are:  The formulae in this article are exact if logarithmic units are used for relative changes, or equivalently if logarithms of indices are used in place of rates, and hold even for large relative changes.  A so-called \"zero interest-rate policy\" (ZIRP) is a very low—near-zero—central bank target interest rate. At this zero lower bound the central bank faces difficulties with conventional monetary policy, because it is generally believed that market interest rates cannot realistically be pushed down into negative territory.  In the United States, the policy was used in 2008-2015 (2008 financial crisis) and 2020-2022 (COVID-19 pandemic).[29]  Nominal interest rates are normally positive, but not always. In contrast, real interest rates can be negative, when nominal interest rates are below inflation. When this is done via government policy (for example, via reserve requirements), this is deemed financial repression, and was practiced by countries such as the United States and United Kingdom following World War II (from 1945) until the late 1970s or early 1980s (during and following the Post–World War II economic expansion).[30][31] In the late 1970s, United States Treasury securities with negative real interest rates were deemed certificates of confiscation.[32]  A so-called \"negative interest rate policy\" (NIRP) is a negative (below zero) central bank target interest rate.  Given the alternative of holding cash, and thus earning 0%, rather than lending it out, profit-seeking lenders will not lend below 0%, as that will guarantee a loss, and a bank offering a negative deposit rate will find few takers, as savers will instead hold cash.[33]  Negative interest rates have been proposed in the past, notably in the late 19th century by Silvio Gesell.[34] A negative interest rate can be described (as by Gesell) as a \"tax on holding money\"; he proposed it as the Freigeld (free money) component of his Freiwirtschaft (free economy) system. To prevent people from holding cash (and thus earning 0%), Gesell suggested issuing money for a limited duration, after which it must be exchanged for new bills; attempts to hold money thus result in it expiring and becoming worthless. Along similar lines, John Maynard Keynes approvingly cited the idea of a carrying tax on money,[34] (1936, The General Theory of Employment, Interest and Money) but dismissed it due to administrative difficulties.[35] More recently, a carry tax on currency was proposed by a Federal Reserve employee (Marvin Goodfriend) in 1999, to be implemented via magnetic strips on bills, deducting the carry tax upon deposit, the tax being based on how long the bill had been held.[35]  It has been proposed that a negative interest rate can in principle be levied on existing paper currency via a serial number lottery, such as randomly choosing a number 0 through 9 and declaring that notes whose serial number end in that digit are worthless, yielding an average 10% loss of paper cash holdings to hoarders; a drawn two-digit number could match the last two digits on the note for a 1% loss. This was proposed by an anonymous student of Greg Mankiw,[34] though more as a thought experiment than a genuine proposal.[36]  Both the European Central Bank starting in 2014 and the Bank of Japan starting in early 2016 pursued the policy on top of their earlier and continuing quantitative easing policies. The latter's policy was said at its inception to be trying to \"change Japan's 'deflationary mindset.'\" In 2016 Sweden, Denmark and Switzerland—not directly participants in the Euro currency zone—also had NIRPs in place.[37]  Countries such as Sweden and Denmark have set negative interest on reserves—that is to say, they have charged interest on reserves.[38][39][40][41]  In July 2009, Sweden's central bank, the Riksbank, set its policy repo rate, the interest rate on its one-week deposit facility, at 0.25%, at the same time as setting its overnight deposit rate at −0.25%.[42] The existence of the negative overnight deposit rate was a technical consequence of the fact that overnight deposit rates are generally set at 0.5% below or 0.75% below the policy rate.[42][43] The Riksbank studied the impact of these changes and stated in a commentary report[44] that they led to no disruptions in Swedish financial markets.  During the European debt crisis, government bonds of some countries (Switzerland, Denmark, Germany, Finland, the Netherlands and Austria) have been sold at negative yields. Suggested explanations include desire for safety and protection against the eurozone breaking up (in which case some eurozone countries might redenominate their debt into a stronger currency).[46]  For practical purposes, investors and academics typically view the yields on government or quasi-government bonds guaranteed by a small number of the most creditworthy governments (United Kingdom, United States, Switzerland, EU, Japan) to effectively have negligible default risk. As financial theory would predict, investors and academics typically do not view non-government guaranteed corporate bonds in the same way. Most credit analysts value them at a spread to similar government bonds with similar duration, geographic exposure, and currency exposure. Through 2018 there have only been a few of these corporate bonds that have traded at negative nominal interest rates. The most notable example of this was Nestle, some of whose AAA-rated bonds traded at negative nominal interest rate in 2015. However, some academics and investors believe this may have been influenced by volatility in the currency market during this period. "},"meta":{},"created_at":"2025-03-22T14:25:42.287399Z","updated_at":"2025-03-22T14:25:42.288402Z","inner_id":59,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":68,"annotations":[{"id":68,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.320914Z","updated_at":"2025-03-22T14:25:42.320914Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"a90c9d4a-c714-4a8e-adad-f01eff13f223","import_id":null,"last_action":null,"bulk_created":false,"task":68,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"    The cinema of China is the filmmaking and film industry of mainland China, one of three distinct historical threads of Chinese-language cinema together with the cinema of Hong Kong and the cinema of Taiwan. China is the home of the largest movie and drama production complex and film studios in the world, the Oriental Movie Metropolis[5][6] and Hengdian World Studios. In 2012 the country became the second-largest market in the world by box office receipts behind only the United States. In 2016, the gross box office in China was CN¥45.71 billion (US$6.58 billion). China has also become a major hub of business for Hollywood studios.  In November 2016, China passed a film law banning content deemed harmful to the \"dignity, honor and interests\" of the People's Republic and encouraging the promotion of core socialist values, approved by the National People's Congress Standing Committee.[7]  Motion pictures were introduced to China in 1896. They were introduced through foreign film exhibitors in treaty ports like Shanghai and Hong Kong.[8]: 68   China was one of the earliest countries to be exposed to the medium of film, due to Louis Lumière sending his cameraman to Shanghai a year after inventing cinematography.[9] The first recorded screening of a motion picture in China took place in Shanghai on 11 August 1896 as an \"act\" on a variety bill.[10] The first Chinese film, a recording of the Peking opera, Dingjun Mountain, was made in November 1905 in Beijing.[11] For the next decade the production companies were mainly foreign-owned, and the domestic film industry was centered on Shanghai, a thriving entrepot and the largest city in the Far East.  Chinese-made short melodrama and comedy films began emerging in 1913.[12]: 48  In 1913, the first independent Chinese screenplay, The Difficult Couple, was filmed in Shanghai by Zheng Zhengqiu and Zhang Shichuan.[13]  Zhang Shichuan then set up the first Chinese-owned film production company in 1916. The first full-length feature film was Yan Ruisheng (閻瑞生) released in 1921, which was a docudrama about the killing of a Shanghai courtesan.[9]  Chinese film production developed significantly in the 1920s.[12]: 48  During the 1920s film technicians from the United States trained Chinese technicians in Shanghai, and American influence continued to be felt there for the next two decades.[13] Since film was still in its earliest stages of development, most Chinese silent films at this time were only comic skits or operatic shorts, and training was minimal at a technical aspect due to this being a period of experimental film.[9] Throughout the 1920s and 1930s, filmmaking in China was largely done by film studios and there was comparatively little small scale filmmaking.[12]: 62   Upscale movie theaters in China had contracts which required them to exclusively show Hollywood films, and thus as of the later 1920s, Hollywood films accounted for 90% of screen time in Chinese theaters.[12]: 64   After trial and error, China was able to draw inspiration from its own traditional values and began producing martial arts films, with the first being Burning of Red Lotus Temple (1928). Burning of Red Lotus Temple was so successful at the box office, the Star Motion Pictures (Mingxing) production later filmed 18 sequels, marking the beginning of China's esteemed martial arts films.[9] Many imitators followed, including U. Lien (Youlian) Studio's Red Heroine (1929), which is still extant.[14] It was during this period that some of the more important production companies first came into being, notably Mingxing and the Shaw brothers' Tianyi (\"Unique\"). Mingxing, founded by Zheng Zhengqiu and Zhang Shichuan in 1922, initially focused on comic shorts, including the oldest surviving complete Chinese film, Laborer's Love (1922).[15][16][17] This soon shifted, however, to feature-length films and family dramas including Orphan Rescues Grandfather (1923).[15] Meanwhile, Tianyi shifted their model towards folklore dramas and also pushed into foreign markets; their film White Snake (1926)[a] proved a typical example of their success in the Chinese communities of Southeast Asia.[15] In 1931, the first Chinese sound film Sing-Song Girl Red Peony was made, the product of a cooperation between the Mingxing Film Company's image production and Pathé Frères's sound technology. However, the sound was disc-recorded, which was then played in the theater in-sync with the action on the screen. The first sound-on-film talkie made in China was either Spring on Stage (歌場春色) by Tianyi, or Clear Sky After Storm by Great China Studio and Jinan Studio.[19] Musical films, such as Song at Midnight (1937)[20] and Street Angels (1937),[21] starring Zhou Xuan,[22] became one of the most popular film genres in China.[23]  News films increased in importance following the Japanese air raid on Shanghai in 1932.[12]: 66  The bombing also destroyed significant amounts of the Chinese film industry and resulted in the loss of many early films.[12]: 66   The first truly important Chinese films were produced beginning in the 1930s with the advent of the \"progressive\" or \"left-wing\" movement, like Cheng Bugao's Spring Silkworms (1933),[25] Wu Yonggang's The Goddess (1934),[26] and Sun Yu's The Great Road, also known as The Big Road (1934).[27] These films were noted for their emphasis on class struggle and external threats (i.e. Japanese aggression), as well as on their focus on common people, such as a family of silk farmers in Spring Silkworms and a prostitute in The Goddess.[11] In part due to the success of these kinds of films, this post-1930 era is now often referred to as the first \"golden period\" of Chinese cinema.[11] The Leftist cinematic movement often revolved around the Western-influenced Shanghai, where filmmakers portrayed the struggling lower class of an overpopulated city.[28]  Three production companies dominated the market in the early to mid-1930s: the newly formed Lianhua (\"United China\"),[b] the older and larger Mingxing and Tianyi.[29] Both Mingxing and Lianhua leaned left (Lianhua's management perhaps more so),[11] while Tianyi continued to make less socially conscious fare.  The period also produced the first big Chinese movie stars, such as Hu Die, Ruan Lingyu,[30] Li Lili,[31] Chen Yanyan,[32] Zhou Xuan, Zhao Dan and Jin Yan. Other major films of the period include Love and Duty (1931), Little Toys (1933), New Women (1934),[33] Song of the Fishermen (1934),[34] Plunder of Peach and Plum (1934), Crossroads (1937), and Street Angel (1937).[21] Throughout the 1930s, the Nationalists and the Communists struggled for power and control over the major studios; their influence can be seen in the films the studios produced during this period.  The Japanese invasion of China in 1937, in particular the Battle of Shanghai, ended this golden run in Chinese cinema. All production companies except Xinhua Film Company (\"New China\") closed shop. A large number filmmakers left to join the War of Resistance, with many going to the Nationalist-controlled hinterlands to join the Nationalist film studios Central Motion Picture Studio or China Motion Picture Studio.[12]: 102  A smaller number went to Yan'an or Hong Kong.[12]: 102–103   The Shanghai film industry, though severely curtailed, did not stop however, thus leading to the \"Solitary Island\" period (also known as the \"Sole Island\" or \"Orphan Island\"), with Shanghai's foreign concessions serving as an \"island\" of production in the \"sea\" of Japanese-occupied territory. It was during this period that artists and directors who remained in the city had to walk a fine line between staying true to their leftist and nationalist beliefs and Japanese pressures. Director Bu Wancang's Hua Mu Lan, also known as Mulan Joins the Army (1939),[35] with its story of a young Chinese peasant fighting against a foreign invasion, was a particularly good example of Shanghai's continued film-production in the midst of war.[15][36] This period ended when Japan declared war on the Western allies on 7 December 1941; the solitary island was finally engulfed by the sea of the Japanese occupation. With the Shanghai industry firmly in Japanese control, films like the Greater East Asia Co-Prosperity Sphere-promoting Eternity (1943) were produced.[15]  By the 1930s and 1940s, both the Chinese Nationalist government and the Communist Party used documentary films as a form of propaganda.[37]: 874   During the 1930 and 1940s, both the Chinese Nationalist government and the Japanese occupation authorities sent mobile projectionist units into areas under their control to show propaganda films.[8]: 69   In the Yan'an Soviet during September 1938, the Eighth Route Army established its first film group.[8]: 69  In 1943, the communists released their first campaign film, Nanniwan, which sought to develop relationships between the communist army and local people in the Yan'an area by showcasing the army's production campaign to alleviate material shortages.[12]: 16   Following Japan's unconditional surrender in August 1945, the Soviet Red Army helped the Chinese communists to take over the Japanese colonial film establishment in Manchuria, the Manchukuo Film Association (Man-ei).[12]: 132  Man-ei had state-of-the-art film production equipment and supplies.[12]: 132  The former colonial studio was relocated to Hegang, where it was established as Northeastern Film Studio, the communist party's first full-capacity film studio.[12]: 132  Yuan Muzhi was its director and Chen Bo'er was its party secretary.[12]: 132  Northeastern Film Studio began production in early 1947, focusing on news and documentary films, as well as some fiction, educational film for children, and animation.[12]: 132–133   During the later phase of the Chinese Civil War, filmmakers trained in Yan'an and Northeastern Film Studio documented all the major battles leading to the communists' defeat of the Nationalists.[12]: 134   The film industry continued to develop after 1945. Production in Shanghai once again resumed as a new crop of studios took the place that Lianhua and Mingxing studios had occupied in the previous decade. In 1945, Cai Chusheng returned to Shanghai to revive the Lianhua name as the \"Lianhua Film Society with Shi Dongshan, Meng Junmou, and Zheng Junli.\"[38] This in turn became Kunlun Studios, which would go on to become one of the most important studios of the era (Kunlun Studios merged with seven other studios to form Shanghai film studio in 1949), putting out the classics The Spring River Flows East (1947),[39] Myriad of Lights (1948), Crows and Sparrows (1949),[40] and Wanderings of Three-Hairs the Orphan, also known as San Mao, The Little Vagabond (1949).[41][42] Many of these films showed the disillusionment with the oppressive rule of Chiang Kai-shek's Nationalist Party and the struggling oppression of nation by war. The Spring River Flows East, a three-hour-long two-parter directed by Cai Chusheng and Zheng Junli, was a particularly strong success. Its depiction of the struggles of ordinary Chinese during the Second Sino-Japanese war, replete with biting social and political commentary, struck a chord with audiences of the time.  Meanwhile, companies like the Wenhua Film Company (\"Culture Films\"), moved away from the leftist tradition and explored the evolution and development of other dramatic genres. Wenhua treated postwar problems in universalistic and humanistic ways, avoiding the family narrative and melodramatic formulae. Excellent examples of Wenhua's fare are its first two postwar features, Love Everlasting (Bu liaoqing, 1947)[43] and Fake Bride, Phony Bridegroom (1947).[44] Another memorable Wenhua film is Long Live the Missus (1947),[45] like Love Everlasting with an original screenplay by writer Eileen Chang. Wenhua's romantic drama, Spring in a Small Town (1948),[46] directed by Fei Mu[47] shortly prior to the revolution, is often regarded by Chinese film critics as one of the most important films in the history of Chinese cinema, in 2005, Hong Kong film awards it as the best 100 years of film.[48] Ironically, it was precisely its artistic quality and apparent lack of \"political grounding\" that led to its labeling by the Communists as rightist or reactionary, and the film was quickly forgotten by those on the mainland following the Communist victory in China in 1949.[49] However, with the China Film Archive's re-opening after the Cultural Revolution, a new print was struck from the original negative, allowing Spring of the Small Town to find a new and admiring audience and to influence an entire new generation of filmmakers. Indeed, an acclaimed remake was made in 2002 by Tian Zhuangzhuang. A Chinese Peking opera film, A Wedding in the Dream (1948), by the same director (Fei Mu), was the first Chinese color film.  At the founding of the People's Republic of China (PRC) in 1949, there were fewer than 600 movie theaters in the country.[50]: 102  The government saw motion pictures as an important artform and tool for mass propaganda. The Soviet-led collaborations Victory of the Chinese People (1950) and Liberated China (1951) were among the biggest film events in the PRC's early years.[12]: 17  Victory of the Chinese People depicted re-enactments of four of the communist party's major military victories and was filmed using real ammunition with the participation of the People's Liberation Army.[12]: 15   The private studios in Shanghai, including Kunming, Wenhua, Guotai, and Datong, were at first encouraged to make new films. They made approximately 47 films during the next two years but soon ran into trouble, owing to the furor over the Kunlun-produced drama The Life of Wu Xun (1950), directed by Sun Yu and starring veteran Zhao Dan. In an anonymous article in People's Daily in May 1951, the feature was accused of spreading feudal ideas. After the article was revealed to be penned by Mao Zedong, the film was banned, the Film Steering Committee was formed to \"re-educate\" the film industry, and the private studios were all incorporated into the state-run Shanghai Film Studio.[51][52]  After the establishment of the PRC, China's cultural bureaucracy described American films as screen-opium and began criticizing American film alongside anti-drug campaigns.[8]: 225–226  The Chinese Communist Party (CCP) sought to tighten control over mass media, producing instead movies centering on peasants, soldiers, and workers, such as Bridge (1949) and The White-Haired Girl (1950).[51] One of the production bases in the middle of all the transition was the Changchun Film Studio. American films were banned as part of the Korean War effort.[8]: 225–226   The Communist government solved the problem of a lack of film theaters by building mobile projection units which could tour the remote regions of China, ensuring that even the poorest could have access to films. The vast majority of China's people lived in rural areas, and most people in China had not seen a film until mobile projectionists brought them.[8]: 148  Mobile projection teams during the Mao era typically included three to four workers who physically transported film infrastructure through a large geographic area mostly not covered by any electrical grid.[50]: 102  Yuan Muzhi was important in developing the Communist government's theories and practices of rural film exhibition.[8]: 46  Yuan and Chen Bo'er transformed the post-Second Sino-Japanese War remnants of the Manchurian Motion Picture Association into the Northeast Film Studio and when Yuan became Film Bureau chief in 1949, he applied its model to help institute a film exhibition network around the country.[8]: 46  The Northeast Film Studio also trained the first generation of communist Chinese documentary filmmakers.[12]: 103   In 1950, 1,800 projectionists from around the country traveled to Nanjing for a training program.[8]: 71  These projectionists replicated the training program in their own home provinces to create more projectionists.[8]: 71  Nanjing was later termed a \"Cradle of People's Cinema.\"[8]: 71  The PRC sought to recruit women and ethnic minority projectionists in an effort to more effectively reach marginalized communities.[8]: 72   Until the profusion of mobile projectionist teams in the 1950s, most rural people had not seen a film.[50]: 103  The number of movie-viewers hence increased sharply, partly bolstered by the fact that film tickets were given out to work units and attendance was compulsory,[52] with admissions rising from 47 million in 1949 to 4.15 billion in 1959.[53] By 1965 there were around 20,393 mobile film units.[51] During the course of the Mao era, the majority of films were shown by such units, with only a minority watched in theaters.[50]: 103   Work as a mobile projectionist was physically and technically demanding.[50]: 104  As a result, women projectionists and all-women mobile projection teams were promoted in Chinese media as examples of advancing gender equality under socialism.[50]: 104–105   In the 1950s and the 1960s, the Communist Party built cinemas (among other cultural buildings) in industrial districts on urban peripheries.[8]: 148  These structures were influenced by Soviet architecture and were intended to be vivacious but not \"palatial.\"[8]: 148–149   Rural mobile projectionist teams and urban movie theaters were generally managed through the PRC's cultural bureaucracy.[8]: 47  Trade Unions and PLA propaganda departments also operated film exhibition networks.[8]: 47   In 1950s China, a common view of film was that it served as \"socialist distance horizon education\".[8]: 24  For example, films promoted rural collectivization.[8]: 24  Cinema also sought to develop the proletarian class consciousness of rural workers, encouraging the industrialization and militarization of their labor.[8]: 50  Film projection teams operating in rural China were asked to incorporate lantern slides into their work to introduce national policies and political campaigns.[8]: 82   In the 17 years between the founding of the People's Republic of China and the Cultural Revolution, 603 feature films and 8,342 reels of documentaries and newsreels were produced, sponsored mostly as Communist propaganda by the government.[54] For example, in Guerrilla on the Railroad (铁道游击队), dated 1956, the Chinese Communist Party was depicted as the primary resistance force against the Second Sino-Japanese War.[55] Chinese filmmakers were sent to Moscow to study the Soviet socialist realism style of filmmaking.[53] The Beijing Film Academy was established in 1950 and officially opened in 1956. One important film of this era is This Life of Mine (1950), directed by Shi Hu, which follows an old beggar reflecting on his past life as a policeman working for the various regimes since 1911.[56][57] The first widescreen Chinese film was produced in 1960. Animated films using a variety of folk arts, such as papercuts, shadow plays, puppetry, and traditional paintings, also were very popular for entertaining and educating children. The most famous of these, the classic Havoc in Heaven (two parts, 1961, 4), was made by Wan Laiming of the Wan Brothers and won the Outstanding Film award at the London International Film Festival.  Films such as The White-Haired Girl and Serf were part of a genre of redemptive melodramas, which sought to encourage audiences to \"speak bitterness\".[8]: 183   After the United Kingdom and the PRC established diplomatic relations, cultural exchanges between the two countries gradually resumed, including British moves being made available in China.[58]: 107   The thawing of censorship in 1956–57 (known as the Hundred Flowers Campaign) and in the early 1960s led to more indigenous Chinese films being made, which were less reliant on their Soviet counterparts.[59] During this campaign the sharpest criticisms came from the satirical comedies of Lü Ban. Before the New Director Arrives exposes the hierarchical relationships occurring between the cadres, while his next film, The Unfinished Comedy (1957), was labeled as a \"poisonous weed\" during the Anti-Rightist Movement, and Lü was banned from directing for life.[60][61] Other noteworthy films produced during this period were adaptations of literary classics, such as Sang Hu's The New Year's Sacrifice (1956, adapted from a Lu Xun story) and Shui Hua's The Lin Family Shop (1959, adapted from a Mao Dun story). The most prominent filmmaker of this era was Xie Jin, whose three films in particular, Woman Basketball Player No. 5 (1957), The Red Detachment of Women (1961), and Two Stage Sisters (1964), exemplify China's increased expertise in filmmaking. Films made during this period are polished, exhibiting high production value and elaborate sets.[62] While Beijing and Shanghai remained the main centers of production, between 1957 and 1960 the government built regional studios in Guangzhou, Xi'an, and Chengdu to encourage representation of ethnic minorities in films. Chinese cinema began to directly address the issue of such ethnic minorities during the late 1950s and early 1960s in films like Five Golden Flowers (1959), Third Sister Liu (1960), Serfs (1963), and Ashima (1964).[63][64]  On 9 March 1958, the Ministry of Culture held a meeting to introduce a Great Leap Forward in cinema.[12]: 149–150  During the Great Leap Forward, the film industry rapidly expanded, with documentary films being the genre that experienced the greatest growth.[12]: 150  Trends in documentary film included \"artistic documentaries,\" in which actors and non-actors reenacted events.[12]: 15   Film venues also expanded rapidly, including both urban cinemas and mobile projection units.[12]: 150   As part of the Socialist Education Movement, mobile film projectionist units showed films and slideshows that emphasized class struggle and encouraged audience members to discuss bitter experiences onstage.[8]: 184  New films termed \"emphasis films\" were released to coincide with the campaign, and the film version of The White-Haired Girl was re-released.[8]: 185   In 1965, China launched the Resist America, Aid Vietnam campaign in response to the U.S. bombing of North Vietnam.[65]: 29  To promote campaign themes denouncing U.S. imperialism and promoted Vietnamese resistance, the communist party used film exhibitions and other cultural media.[65]: 29   During the Cultural Revolution, the film industry was severely restricted. Almost all previous films were banned, and only a few new ones were produced, the revolutionary model operas. The most notable of these was a ballet version of the revolutionary opera The Red Detachment of Women, directed by Pan Wenzhan and Fu Jie in 1970.  The release of filmed versions of the revolutionary model operas resulted in a re-organization and expansion of China's film exhibition network.[8]: 73  From 1965 to 1976, the number of film projection units in China quadrupled, total film audiences nearly tripled, and the national film attendance rate doubled.[8]: 133  The Cultural Revolution Group drastically reduced ticket prices which, in its view, would allow film to better serve the needs of workers and of socialism.[8]: 133   In addition to films deemed laudatory, from the middle of 1966 to 1968, the expanding film distribution network screened films characterized as \"poisonous weeds\" to hundreds of millions of audience members for the purpose of criticizing the films.[8]: 232  These criticism screenings were sometimes accompanied by struggle sessions.[8]: 233   Sent-down youth were a major subset of China's rural projectionists during the Cultural Revolution period.[8]: 75   Feature film production came almost to a standstill in the early years from 1967 to 1972. Movie production revived after 1972 under the strict jurisdiction of the Gang of Four until 1976, when they were overthrown. The few films that were produced during this period, such as 1975's Breaking with Old Ideas, were highly regulated in terms of plot and characterization.[66]  In 1972, Chinese officials invited Michelangelo Antonioni to China to film the achievements of the Cultural Revolution.[67]: 13  Antonioni made the documentary Chung Kuo, Cina.[67]: 13   When it was released in 1974, Communist Party leadership in China interpreted the film as reactionary and anti-Chinese.[67]: 13  Viewing art through the principles of the Yan'an Talks, particularly the concept that there is no such thing as art-for-art's-sake, party leadership construed Antonioni's aesthetic choices as politically motivated and banned the film.[67]: 14  Jiang Qing criticized Premier Zhou Enlai's role in Antonioni's invitation to China as not only a failure but also treasonous.[65]: 121  Since its 2004 release in China, the film has been well-regarded by Chinese audiences, especially for its beautiful depictions of a more simple time.[67]: 14   Because China rejection most foreign film importation, comparatively minor cinema like Albanian cinema and North Korean cinema developed mass audiences in China.[8]: 207  Through Albanian films screened during this period, many Chinese audience members were introduced to avant-garde and modernist storytelling techniques and aesthetics.[8]: 206–207   In the years immediately following the Cultural Revolution, the film industry again flourished as a medium of popular entertainment. Production rose steadily, from 19 features in 1977 to 125 in 1986.[68] Domestically produced films played to large audiences, and tickets for foreign film festivals sold quickly. The industry tried to revive crowds by making more innovative and \"exploratory\" films like their counterparts in the West.[citation needed]  Chinese cinema grew significantly in the late 1970s. In 1979, annual box office admissions reached a peak of 29.3 billion tickets sold, equivalent to an average of 30 films per person. Chinese cinema continued to prosper into the early 1980s. In 1980, annual box office admissions stood at 23.4 billion tickets sold, equivalent to an average of 29 films per person.[69] In terms of box office admissions, this period represented the peak ticket sales in the history of the Chinese box office.[70] High ticket sales were driven by low ticket prices, with a cinema ticket typically costing between ¥0.1 ($0.06) and ¥0.3 ($0.19) at the time.[71]  By the early 1980s, there were 162,000 projection units in China, primarily composed of mobile movie teams which showed films outdoors in both rural and urban areas.[50]: 102   A number of films during this period drew box office admissions in the hundreds of millions. China's highest-grossing film in box office admissions was Legend of the White Snake (1980) with an estimated 700 million admissions,[72][73] followed by In-Laws (Full House of Joy) [zh] (1981) and The Undaunted Wudang (1983) with more than 600 million ticket sales each.[74] The highest-grossing foreign film was the Japanese film Kimi yo Fundo no Kawa o Watare (1976), which released in 1978 and sold more than 330 million tickets,[75] followed by the Indian film Caravan (1971) which released in 1979 and sold about 300 million tickets.[76]  In the late 1980s the film industry fell on hard times, faced with the dual problems of competition from other forms of entertainment and concern on the part of the authorities that many of the popular thriller and martial arts films were socially unacceptable.[citation needed] In January 1986 the film industry was transferred from the Ministry of Culture to the newly formed Ministry of Radio, Cinema, and Television to bring it under \"stricter control and management\" and to \"strengthen supervision over production.\"[77]  The end of the Cultural Revolution brought the release of \"scar dramas\" (傷痕剧 shānghén jù), which depicted the emotional traumas left by this period. The best-known of these is probably Xie Jin's Hibiscus Town (1986), although they could be seen as late as the 1990s with Tian Zhuangzhuang's The Blue Kite (1993). In the 1980s, open criticism of certain past Communist Party policies was encouraged by Deng Xiaoping as a way to reveal the excesses of the Cultural Revolution and the earlier Anti-Rightist Campaign, also helping to legitimize Deng's new policies of \"reform and opening up.\" For instance, the Best Picture prize in the inaugural 1981 Golden Rooster Awards was given to two \"scar dramas\", Evening Rain (Wu Yonggang, Wu Yigong, 1980) and Legend of Tianyun Mountain (Xie Jin, 1980).[78]  Many scar dramas were made by members of the Fourth Generation whose own careers or lives had suffered during the events in question, while younger, Fifth Generation directors such as Tian tended to focus on less controversial subjects of the immediate present or the distant past. Official enthusiasm for scar dramas waned by the 1990s when younger filmmakers began to confront negative aspects of the Mao era. The Blue Kite, though sharing a similar subject as the earlier scar dramas, was more realistic in style, and was made only through obfuscating its real script. Shown abroad, it was banned from release in mainland China, while Tian himself was banned from making any films for nearly a decade afterward. After the 1989 Tiananmen Square protests and massacre, few if any scar dramas were released domestically in mainland China.[citation needed]  Beginning in the mid-late 1980s during the New Enlightenment movement in China, the rise of the so-called fifth generation of Chinese filmmakers brought increased popularity of Chinese cinema abroad.[79] Most of the filmmakers who made up the Fifth Generation had graduated from the Beijing Film Academy in 1982 and included Zhang Yimou, Tian Zhuangzhuang, Chen Kaige, Zhang Junzhao, Li Shaohong, Wu Ziniu and others. These graduates constituted the first group of filmmakers to graduate since the Cultural Revolution and they soon jettisoned traditional methods of storytelling and opted for a more free and unorthodox symbolic approach.[80] After the so-called scar literature in fiction had paved the way for frank discussion, Zhang Junzhao's One and Eight (1983) and Chen Kaige's Yellow Earth (1984) in particular were taken to mark the beginnings of the Fifth Generation.[c] Yellow Earth became one of the first Chinese art films to attract international attention.[81]: 42   The most famous of the Fifth Generation directors, Chen Kaige and Zhang Yimou, went on to produce celebrated works such as King of the Children (1987), Ju Dou (1989), Raise the Red Lantern (1991) and Farewell My Concubine (1993), which were not only acclaimed by Chinese cinema-goers but by the Western arthouse audience. Tian Zhuangzhuang's films, though less well known by Western viewers, were well noted by directors such as Martin Scorsese. It was during this period that Chinese cinema began reaping the rewards of international attention, including the 1988 Golden Bear for Red Sorghum, the 1992 Golden Lion for The Story of Qiu Ju, the 1993 Palme d'Or for Farewell My Concubine, and three Best Foreign Language Film nominations from the Academy Awards.[82] All these award-winning films starred actress Gong Li, who became the Fifth Generation's most recognizable star, especially to international audiences.  Diverse in style and subject, the Fifth Generation directors' films ranged from black comedy (Huang Jianxin's The Black Cannon Incident, 1985) to the esoteric (Chen Kaige's Life on a String, 1991), but they share a common rejection of the socialist-realist tradition worked by earlier Chinese filmmakers in the Communist era. Other notable Fifth Generation directors include Wu Ziniu, Hu Mei, Li Shaohong and Zhou Xiaowen. Fifth Generation filmmakers reacted against the ideological purity of Cultural Revolution cinema. By relocating to regional studios, they began to explore the actuality of local culture in a somewhat documentarian fashion. Instead of stories depicting heroic military struggles, the films were built out of the drama of ordinary people's daily lives. They also retained political edge, but aimed at exploring issues rather than recycling approved policy. While Cultural Revolution films used character, the younger directors favored psychological depth along the lines of European cinema. They adopted complex plots, ambiguous symbolism, and evocative imagery.[83] Some of their bolder works with political overtones were banned by Chinese authorities.  These films came with a creative genres of stories, new style of shooting as well, directors utilized extensive color and long shots to present and explore history and structure of national culture. As a result of the new films being so intricate, the films were for more educated audiences than anything. The new style was profitable for some and helped filmmakers to make strides in the business. It allowed directors to get away from reality and show their artistic sense.[84]  The Fourth Generation also returned to prominence. Given their label after the rise of the Fifth Generation, these were directors whose careers were stalled by the Cultural Revolution and who were professionally trained prior to 1966. Wu Tianming, in particular, made outstanding contributions by helping to finance major Fifth Generation directors under the auspices of the Xi'an Film Studio (which he took over in 1983), while continuing to make films like Old Well (1986) and The King of Masks (1996).  The Fifth Generation movement ended in part after the 1989 Tiananmen Square protests and massacre, although its major directors continued to produce notable works. Several of its filmmakers went into self-imposed exile: Wu Tianming moved to the United States (but later returned), Huang Jianxin left for Australia, while many others went into television-related works.  During a period when socialist dramas were beginning to lose viewership, the Chinese government began to involve itself deeper into the world of popular culture and cinema by creating the official genre of the \"main melody\" (主旋律 zhǔxuánlǜ), inspired by Hollywood's strides in musical dramas.[85] In 1987, the Ministry of Radio, Film and Television issued a statement encouraging the making of movies which emphasizes the main melody to \"invigorate national spirit and national pride\".[86] The expression main melody refers to the musical term leitmotif, which translates to the 'theme of our times', which scholars suggest is representative of China's socio-political climate and cultural context of popular cinema.[87] These main melody films, still produced regularly in modern times, try to emulate the commercial mainstream by the use of Hollywood-style music and special effects. A significant feature of these films is the incorporation of a \"red song\", which is a song written as propaganda to support the People's Republic of China.[88] By revolving the film around the motif of a red song, the film is able to gain traction at the box office as songs are generally thought to be more accessible than a film. Theoretically, once the red song dominates the charts, it will stir interest in the film that which it accompanies.[89]  Main melody dramas are often subsidized by the state and have free access to government and military personnel.[90] The Chinese government spends between \"one and two million RMBs\" annually to support the production of films in the main melody genre. August First Film Studio, the film and TV production arm of the People's Liberation Army, is a studio that produces main melody cinema. Main melody films, which often depict past military engagements or are biopics of first-generation CCP leaders, have won several Best Picture prizes at the Golden Rooster Awards.[91] Some of the more famous main melody dramas include the ten-hour epic Decisive Engagement (大决战, 1991), directed by Cai Jiawei, Yang Guangyuan and Wei Lian; The Opium War (1997), directed by Xie Jin; and The Founding of a Republic (2009), directed by Han Sanping and Fifth Generation director Huang Jianxin.[92] The Founding of an Army (2017) was commissioned by the government to celebrate the 90th anniversary of the People's Liberation Army, and is the third instalment in The Founding of a Republic series.[93] The film featured many young Chinese pop singers that are already well-established in the industry, including Li Yifeng, Liu Haoran, and Lay Zhang, so as to further the film's reputation as a main melody drama.  When faced with the complexity of real society, their hands and feet quiver, and they deliriously shoot a bunch of childish fairy tales  The post-1990 era has been labeled the \"return of the amateur filmmaker\" as state censorship policies after the 1989 Tiananmen Square protests produced an edgy underground film movement loosely referred to as the Sixth Generation. Owing to the lack of state funding and backing, these films were shot quickly and cheaply, using materials like 16 mm film and digital video and mostly non-professional actors and actresses, producing a documentary feel, often with long takes, hand-held cameras, and ambient sound; more akin to Italian neorealism and cinéma vérité than the often lush, far more considered productions of the Fifth Generation.[82] Unlike the Fifth Generation, the Sixth Generation brings a more individualistic, anti-romantic life-view and pays far closer attention to contemporary urban life, especially as affected by disorientation, rebellion[95] and dissatisfaction with China's contemporary social marketing economic tensions and comprehensive cultural background.[96] Many were made with an extremely low budget (an example is Jia Zhangke, who shoots on digital video, and formerly on 16 mm; Wang Xiaoshuai's The Days (1993) was made for US$10,000[96]). The title and subjects of many of these films reflect the Sixth Generation's concerns. The Sixth Generation takes an interest in marginalized individuals and the less represented fringes of society. For example, Zhang Yuan's hand-held Beijing Bastards (1993) focuses on youth punk subculture, featuring artists like Cui Jian, Dou Wei and He Yong frowned upon by many state authorities,[97] while Jia Zhangke's debut film Xiao Wu (1997) concerns a provincial pickpocket. While many Fifth Generation filmmakers have become darlings of mainstream Chinese culture, Sixth Generation filmmakers have often experienced harsh treatment by the state's censorship and regulatory system, despite their success at international film festivals and arthouse markets.[98]  As the Sixth Generation gained international exposure, many subsequent movies were joint ventures and projects with international backers, but remained quite resolutely low-key and low budget. Jia's Platform (2000) was funded in part by Takeshi Kitano's production house,[99] while his Still Life was shot on HD video. Still Life was a surprise addition and Golden Lion winner of the 2006 Venice International Film Festival. Still Life, which concerns provincial workers around the Three Gorges region, sharply contrasts with the works of Fifth Generation Chinese directors like Zhang Yimou and Chen Kaige who were at the time producing House of Flying Daggers (2004) and The Promise (2005). It featured no star of international renown and was acted mostly by non-professionals.  Many Sixth Generation films have highlighted the negative attributes of China's entry into the modern capitalist market. Li Yang's Blind Shaft (2003) for example, is an account of two murderous con-men in the unregulated and notoriously dangerous mining industry of northern China.[100] (Li refused the tag of Sixth Generation, although admitted he was not Fifth Generation).[95] While Jia Zhangke's The World (2004) emphasizes the emptiness of globalization in the backdrop of an internationally themed amusement park.[101]  Some of the more prolific Sixth Generation directors to have emerged are Wang Xiaoshuai (The Days, Beijing Bicycle, So Long, My Son), Zhang Yuan (Beijing Bastards, East Palace West Palace), Jia Zhangke (Xiao Wu, Unknown Pleasures, Platform, The World, A Touch of Sin, Mountains May Depart, Ash Is Purest White), He Jianjun (Postman) and Lou Ye (Suzhou River, Summer Palace). One director of their generation who does not share most of the concerns of the Sixth Generation is Lu Chuan (Kekexili: Mountain Patrol, 2004; City of Life and Death, 2010).  In the 2018 Cannes Film Festival, two of China's Sixth generation filmmakers, Jia Zhangke and Zhang Ming – whose grim works transformed Chinese cinema in the 1990s – showed on the French Riviera. While both directors represent Chinese cinema, their profiles are quite different. The 49-year-old Jia set up the Pingyao International Film Festival in 2017 and on the other hand is Zhang, a 56-year-old film school professor who spent years working on government commissions and domestic TV shows after struggling with his own projects. Despite their different profiles, they mark an important cornerstone in Chinese cinema and are both credited with bringing Chinese movies to the international big screen. Chinese director Jia Zhangke's latest film Ash Is Purest White has been selected to compete in the official competition for the Palme d'Or of the 71st Cannes Film Festival, the highest prize awarded at the film festival. It is Jia's fifth movie, a gangster revenge drama that is his most expensive and mainstream film to date. Back in 2013, Jia won Best Screenplay Award for A Touch of Sin, following nominations for Unknown Pleasures in 2002 and 24 City in 2008. In 2014, he was a member of the official jury and the following year his film Mountains May Depart was nominated. According to entertainment website Variety, a record number of Chinese films were submitted this year but only Jia's romantic drama was selected to compete for the Palme d'Or. Meanwhile, Zhang will make his debut at Cannes with The Pluto Moment, a slow-moving relationship drama about a team of filmmakers scouting for locations and musical talent in China's rural hinterland. The film is Zhang's highest profile production so far, as it stars actor Wang Xuebing in the leading role. The film was partly financed by iQiyi, the company behind one of China's most popular online video browsing sharing sites.[102] Diao Yinan is also a notable member of the sixth generation whose works include Black Coal Thin Ice, Wild Goose Lake, Night Train and Uniform which have premiered at festivals such as Cannes and received acclaim abroad.[103]  He Ping is a director of mostly Western-like films set in Chinese locale. His Swordsmen in Double Flag Town (1991) and Sun Valley (1995) explore narratives set in the sparse terrain of West China near the Gobi Desert. His historical drama Red Firecracker, Green Firecracker (1994) won a myriad of prizes home and abroad.  Recent cinema has seen Chinese cinematographers direct some acclaimed films. Other than Zhang Yimou, Lü Yue made Mr. Zhao (1998), a black comedy film well received abroad. Gu Changwei's minimalist epic Peacock (2005), about a quiet, ordinary Chinese family with three very different siblings in the post-Cultural Revolution era, took home the Silver Bear prize for 2005 Berlin International Film Festival. Hou Yong is another cinematographer who made films (Jasmine Women, 2004) and TV series. There are actors who straddle the dual roles of acting and directing. Xu Jinglei, a popular Chinese actress, has made six movies to date. Her second film Letter from an Unknown Woman (2004) landed her the San Sebastián International Film Festival Best Director award. Another popular actress and director is Zhao Wei, whose directorial debut So Young (2013) was a huge box office and critical success.  The most highly regarded Chinese actor-director is undoubtedly Jiang Wen, who has directed several critically acclaimed movies while following on his acting career. His directorial debut, In the Heat of the Sun (1994) was the first PRC film to win Best Picture at the Golden Horse Film Awards held in Taiwan. His other films, like Devils on the Doorstep (2000, Cannes Grand Prix) and Let the Bullets Fly (2010), were similarly well received. By the early 2011, Let the Bullets Fly had become the highest grossing domestic film in China's history.[104][105]  There is a growing number of independent seventh or post-Sixth Generation filmmakers making films with extremely low budgets and using digital equipment. They are the so-called dGeneration (for digital).[106] These films, like those from Sixth Generation filmmakers, are mostly made outside the Chinese film system and are shown mostly on the international film festival circuit. Ying Liang and Jian Yi are two of these generation filmmakers. Ying's Taking Father Home (2005) and The Other Half (2006) are both representative of the generation trends of the feature film. Liu Jiayin made two dGeneration feature films, Oxhide (2004) and Oxhide II (2010), blurring the line between documentary and narrative film. Oxhide, made by Liu when she was a film student, frames herself and her parents in their claustrophobic Beijing apartment in a narrative praised by critics. An Elephant Sitting Still, considered one of the greatest film debuts in Chinese cinema, is also the only film by the late Hu Bo.[107]  Two decades of reform and commercialization have brought dramatic social changes in mainland China, reflected not only in fiction film but in a growing documentary movement. Wu Wenguang's 70-minute Bumming in Beijing: The Last Dreamers (1990) is now seen as one of the first works of this \"New Documentary Movement\" (NDM) in China.[108][109] Bumming, made between 1988 and 1990, contains interviews with five young artists eking out a living in Beijing, subject to state authorized tasks. Shot using a camcorder, the documentary ends with four of the artists moving abroad after the 1989 Tiananmen Square protests and massacre.[110] Dance with the Farm Workers (2001) is another documentary by Wu.[111]  Another internationally acclaimed documentary is Wang Bing's nine-hour tale of deindustrialization Tie Xi Qu: West of the Tracks (2003). Wang's subsequent documentaries, He Fengming (2007), Crude Oil (2008), Man with no name (2009), Three Sisters (2012) and Feng ai (2013), cemented his reputation as a leading documentarist of the movement.[112]  Li Hong, the first woman in the NDM, in Out of Phoenix Bridge (1997) relates the story of four young women, who moving from rural areas to the big cities like millions of other men and women, have come to Beijing to make a living.  The New Documentary Movement in recent times has overlapped with the dGeneration filmmaking, with most documentaries being shot cheaply and independently in the digital format. Xu Xin's Karamay (2010), Zhao Liang's Behemoth, Huang Weikai's Disorder (2009), Zhao Dayong's Ghost Town (2009), Du Haibing's 1428 (2009), Xu Tong's Fortune Teller (2009) and Li Ning's Tape (2010) were all shot in digital format. All had made their impact in the international documentary scene and the use of digital format allows for works of vaster lengths.  Inspired by the success of Disney animation, the self-taught pioneers Wan brothers, Wan Laiming and Wan Guchan, made the first Chinese animated short in the 1920s, thus inaugurating the history of Chinese animation. (Chen Yuanyuan 175)[113] Many live-action films of the Republican era also included animated sequences.[114]  In 1937, the Wan brothers decided to produce 《铁扇公主》 Princess Iron Fan, which was the first Chinese animated feature film and the fourth, after the American feature films Snow White, Gulliver's Travels, and The Adventures of Pinocchio. It was at this time that Chinese animation as an art form had risen to prominence on the world stage. Completed in 1941, the film was released under China United Pictures and aroused a great response in Asia. Japanese animator Shigeru Tezuka once said that he gave up medicine after watching the cartoon and decided to pursue animation.[citation needed]  During this golden era, Chinese animation had developed a variety of styles, including ink animation, shadow play animation, puppet animation, and so on. Some of the most representative works are 《大闹天宫》 Uproar in Heaven, 《哪吒闹海》 Nezha's Rebellion in the Sea and《天书奇谈》 Heavenly Book, which have also won lofty praise and numerous awards in the world.[citation needed]  After Deng Xiaoping's Reform Period and the \"opening up\" of China, the movies《葫芦兄弟》 Calabash Brothers, 《黑猫警长》Black Cat Sheriff, 《阿凡提》Avanti Story and other impressive animated movies were released. However, at this time, China still favored the Japanese's more unique, American and European-influenced animated works over the less-advanced domestic ones.[citation needed]  In the 1990s, digital production methods replaced manual hand-drawing methods; however, even with the use of advanced technology, none of the animated works were considered to be a breakthrough film. Animated films that tried to cater to all age groups, such as Lotus Lantern and Storm Resolution, did not attract much attention. The only animated works that seemed to achieve popularity were the ones for catered for children, such as Pleasant Goat and Big Big Wolf《喜羊羊与灰太狼》.  During this period, the technical level of Chinese domestic animation production has been established comprehensively, and 3D animation films have become the mainstream. However, as more and more foreign films (such as ones from Japan, Europe, and the United States) are being imported into China, Chinese animated works is left in the shadows of these animated foreign films.  It was only with the release of 《西游记之大圣归来》Monkey King: Hero is Back in 2015, a computer-animated film, that Chinese animated works took back the rein. The film was a huge hit and broke the record for Chinese domestic animated movies with CN¥956 million at China's box office. After the success of Journey to the West, several other high-quality animated films were released, such as《大鱼海棠》 Big Fish and Begonia and 《白蛇缘起》 White Snake. Though none of these movies made headway in regards to the box office, they did make filmmakers more and more interested in animated works.  This all changed with the breakthrough animated film, 《哪吒之魔童降世》Ne Zha. Released in 2019, it became the second highest-grossing film of all time in China, the highest-grossing animated non-English film, and the highest-grossing animated film in a single territory. It was with this film that Chinese animated films, as a medium, finally broke the notion in China that domestic animated films are only for children. With Nezha, and a spinoff, Jiang Ziya, Chinese animation has now come to be known as a veritable source of entertainment for all ages.  With China's liberalization in the late 1970s and its opening up to foreign markets, commercial considerations have made its impact in post-1980s filmmaking. Traditionally arthouse movies screened seldom make enough to break even. An example is Fifth Generation director Tian Zhuangzhuang's The Horse Thief (1986), a narrative film with minimal dialog on a Tibetan horse thief. The film, showcasing exotic landscapes, was well received by Chinese and some Western arthouse audiences, but did poorly at the box office.[115] Tian's later The Warrior and the Wolf (2010) was a similar commercial failure.[116] Prior to these, there were examples of successful commercial films in the post-liberalization period. One was the romance film Romance on the Lu Mountain (1980), which was a success with older Chinese. The film broke the Guinness Book of Records as the longest-running film on a first run. Jet Li's cinematic debut Shaolin Temple (1982) was an instant hit at home and abroad (in Japan and the Southeast Asia, for example).[117] Another successful commercial film was Murder in 405  [zh] (405谋杀案, 1980), a murder thriller.[118]  Feng Xiaogang's The Dream Factory (1997) was heralded as a turning point in Chinese movie industry, a hesui pian (Chinese New Year-screened film) which demonstrated the viability of the commercial model in China's socialist market economy. Feng has become one of the most successful commercial director in the post-1997 era. Almost all his films made high returns domestically[119] while he used ethnic Chinese co-stars like Rosamund Kwan, Jacqueline Wu, Rene Liu and Shu Qi to boost his films' appeal.  In the decade following 2010, owing to the influx of Hollywood films (though the number screened each year is curtailed), Chinese domestic cinema faces mounting challenges. The industry is growing and domestic films are starting to achieve the box office impact of major Hollywood blockbusters. However, not all domestic films are successful financially. In January 2010 James Cameron's Avatar was pulled out from non-3D theaters for Hu Mei's biopic Confucius, but this move led to a backlash on Hu's film.[120] Zhang Yang's 2005 Sunflower also made little money, but his earlier, low-budget Spicy Love Soup (1997) grossed ten times its budget of ¥3 million.[121] Likewise, the 2006 Crazy Stone, a sleeper hit, was made for just 3 million HKD\/US$400,000. In 2009–11, Feng's Aftershock (2009) and Jiang Wen's Let the Bullets Fly (2010) became China's highest grossing domestic films, with Aftershock earning ¥670 million (US$105 million)[122] and Let the Bullets Fly ¥674 million (US$110 million).[123] Lost in Thailand (2012) became the first Chinese film to reach ¥1 billion at the Chinese box office and Monster Hunt (2015) became the first to reach CN¥2 billion. As of 2021, 9 of the top 10 highest-grossing films in China are domestic productions. On 8 February 2016, the Chinese box office set a new single-day gross record, with CN¥660 million, beating the previous record of CN¥425 million on 18 July 2015.[124] Also in February 2016, The Mermaid, directed by Stephen Chow, became the highest-grossing film in China, overtaking Monster Hunt.[125] It is also the first film to reach CN¥3 billion.[126]  Under the influence of Hollywood science fiction movies like Prometheus, published on 8 June 2012, such genres especially the space science films have risen rapidly in the Chinese film market in recent years. On 5 February 2019, the film The Wandering Earth directed by Frant Gwo reached $699.8 million worldwide, which became the third highest-grossing film in the history of Chinese cinema.  Since the late 1980s and progressively in the 2000s, Chinese films have enjoyed considerable box office success abroad. Formerly viewed only by cineastes, its global appeal mounted after the international box office and critical success of Ang Lee's period wuxia film Crouching Tiger, Hidden Dragon which won Academy Award for Best Foreign Language Film in 2000. This multi-national production increased its appeal by featuring stars from all parts of the Chinese-speaking world. It provided an introduction to Chinese cinema (and especially the wuxia genre) for many and increased the popularity of many earlier Chinese films. To date Crouching Tiger remains the most commercially successful foreign-language film in U.S. history.  In 2002, Zhang Yimou's Hero was another international box office success.[127] Its cast featured famous actors from mainland China and Hong Kong who were also known to some extent in the West, including Jet Li, Zhang Ziyi, Maggie Cheung and Tony Leung Chiu-Wai. Despite criticisms by some that these two films pander somewhat to Western tastes, Hero was a phenomenal success in most of Asia and topped the U.S. box office for two weeks, making enough in the U.S. alone to cover the production costs.  Other films such as Farewell My Concubine, 2046, Suzhou River, The Road Home and House of Flying Daggers were critically acclaimed around the world. The Hengdian World Studios can be seen as the \"Chinese Hollywood\", with a total area of up to 330 ha. and 13 shooting bases, including a 1:1 copy of the Forbidden City.  The successes of Crouching Tiger, Hidden Dragon and Hero make it difficult to demarcate the boundary between \"Mainland Chinese\" cinema and a more international-based \"Chinese-language cinema\". Crouching Tiger, for example, was directed by a Taiwan-born American director (Ang Lee) who works often in Hollywood. Its pan-Chinese leads include mainland Chinese (Zhang Ziyi), Hong Kong (Chow Yun-Fat), Taiwan (Chang Chen) and Malaysian (Michelle Yeoh) actors and actresses; the film was co-produced by an array of Chinese, American, Hong Kong, and Taiwan film companies. Likewise, Lee's Chinese-language Lust, Caution (2007) drew a crew and cast from mainland China, Hong Kong and Taiwan, and includes an orchestral score by French composer Alexandre Desplat. This merging of people, resources and expertise from the three regions and the broader East Asia and the world, marks the movement of Chinese-language cinema into a domain of large scale international influence. Other examples of films in this mold include The Promise (2005), The Banquet (2006), Fearless (2006), The Warlords (2007), Bodyguards and Assassins (2009) and Red Cliff (2008–09). The ease with which ethnic Chinese actresses and actors straddle the mainland and Hong Kong has significantly increased the number of co-productions in Chinese-language cinema. Many of these films also feature South Korean or Japanese actors to appeal to their East Asian neighbours. Some artistes originating from the mainland, like Hu Jun, Zhang Ziyi, Tang Wei and Zhou Xun, obtained Hong Kong residency under the Quality Migrant Admission Scheme and have acted in many Hong Kong productions.[128]  In 1983, there were 162,000 projection units in China, up from less than 600 at the 1949 founding of the PRC.[8]: 1   In 1998, the Ministry of Culture revived the practice of mobile rural cinema as part of its 2131 Project which aimed to screen one movie per month per village in rural China and upgrade analog equipment to digital projectors.[8]: 246  In 2003, the central government provided more than 400 film projection vans to Tibet and Xinjiang to show films in an effort to oppose what the government viewed as separatism and Westernization.[8]: 249   In 2010, Chinese cinema was the third largest film industry by number of feature films produced annually.[129] In 2013, China's gross box office was ¥21.8 billion (US$3.6 billion), the second-largest film market in the world by box office receipts.[130] In January 2013, Lost in Thailand (2012) became the first Chinese film to reach ¥1 billion at the box office.[131] As of May 2013, 7 of the top 10 highest-grossing films in China were domestic productions.[132] As of 2014, around half of all tickets are sold online, with the largest ticket selling sites being Maoyan.com (82 million), Gewara.com (45 million) and Wepiao.com (28 million).[133]  In 2014, Chinese films earned ¥1.87 billion outside China.[134] By December 2013 there were 17,000 screens in the country.[135] By 6 January 2014, there were 18,195 screens in the country.[130] Greater China has around 251 IMAX theaters.[136] There were 299 cinema chains (252 rural, 47 urban), 5,813 movie theaters and 24,317 screens in the country in 2014.[2]  The country added about 8,035 screens in 2015 (at an average of 22 new screens per day, increasing its total by about 40% to around 31,627 screens, which is about 7,373 shy of the number of screens in the United States.[137][138] Chinese films accounted for 61.48% of ticket sales in 2015 (up from 54% last year) with more than 60% of ticket sales being made online. Average ticket price was down about 2.5% to $5.36 in 2015.[137] It also witnessed 51.08% increase in admissions, with 1.26 billion people buying tickets to the cinema in 2015.[138] Chinese films grossed US$427 million overseas in 2015.[139] During the week of the 2016 Chinese New Year, the country set a new record for the highest box office gross during one week in one territory with US$548 million, overtaking the previous record of US$529.6 million of 26 December 2015 to 1 January 2016 in the United States and Canada.[140] Chinese films grossed CN¥3.83 billion (US$550 million) in foreign markets in 2016.[3]  In 2020, China's market for films surpassed the U.S. market to become the largest such market in the world.[141]: 16   As of April 2015, the largest Chinese film company by worth was Alibaba Pictures (US$8.77 billion). Other large companies include Huayi Brothers Media (US$7.9 billion), Enlight Media (US$5.98 billion) and Bona Film Group (US$542 million).[159] The biggest distributors by market share in 2014 were: China Film Group (32.8%), Huaxia Film (22.89%), Enlight Pictures (7.75%), Bona Film Group (5.99%), Wanda Media (5.2%), Le Vision Pictures (4.1%), Huayi Brothers (2.26%), United Exhibitor Partners (2%), Heng Ye Film Distribution (1.77%) and Beijing Anshi Yingna Entertainment (1.52%).[2] The biggest cinema chains in 2014 by box office gross were: Wanda Cinema Line (US$676.96 million), China Film Stellar (393.35 million), Dadi Theater Circuit (378.17 million), Shanghai United Circuit (355.07 million), Guangzhou Jinyi Zhujiang (335.39 million), China Film South Cinema Circuit (318.71 million), Zhejiang Time Cinema (190.53 million), China Film Group Digital Cinema Line (177.42 million), Hengdian Cinema Line (170.15 million) and Beijing New Film Association (163.09 million).[2]  Huayi Brothers is China's most powerful independent (i.e., non state-owned) entertainment company, Beijing-based Huayi Brothers is a diversified company engaged in film and TV production, distribution, theatrical exhibition, as well as talent management. Notable films include 2004's Kung Fu Hustle; and 2010's Aftershock, which had a 91% rating on Rotten Tomatoes.[160]  Beijing Enlight Media focuses on the action and romance genres. Enlight usually places several films in China's top 20 grossers. Enlight is also a major player in China's TV series production and distribution businesses. Under the leadership of its CEO Wang Changtian, the publicly traded, Beijing-based company has achieved a market capitalization of nearly US$1 billion.[161] "},"meta":{},"created_at":"2025-03-22T14:25:42.288402Z","updated_at":"2025-03-22T14:25:42.288402Z","inner_id":60,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":69,"annotations":[{"id":69,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.320914Z","updated_at":"2025-03-22T14:25:42.320914Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"d6d82326-b92d-42be-a837-200c85e00dbd","import_id":null,"last_action":null,"bulk_created":false,"task":69,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  The film industry in Germany can be traced back to the late 19th century. German cinema made major technical and artistic contributions to early film, broadcasting and television technology. Babelsberg became a household synonym for the early 20th century film industry in Europe, similar to Hollywood later. Early German and German-speaking filmmakers and actors heavily contributed to early Hollywood.[6][7]  Germany witnessed major changes to its identity during the 20th and 21st century. Those changes determined the periodisation of national cinema into a succession of distinct eras and movements.[8]  The history of cinema in Germany can be traced back to the years of the medium's birth. Ottomar Anschütz held the first showing of life sized pictures in motion on 25 November 1894 at the Postfuhramt in Berlin.[9][10] On 1 November 1895, Max Skladanowsky and his brother Emil demonstrated their self-invented film projector, the Bioscop, at the Wintergarten music hall in Berlin. A 15-minute series of eight short films were shown – the first screening of films to a paying audience.[11] This performance pre-dated the first paying public display of the Lumière brothers' Cinematographe in Paris on 28 December of the same year, a performance that Max Skladanowsky attended and at which he was able to ascertain that the Cinematographe was technically superior to his Bioscop. Other German film pioneers included the Berliners Oskar Messter and Max Gliewe, two of several individuals who independently in 1896 first used a Geneva drive (which allows the film to be advanced intermittently one frame at a time) in a projector, and the cinematographer Guido Seeber.  In its earliest days, the cinematograph was perceived as an attraction for upper class audiences, but the novelty of moving pictures did not last long. Soon, trivial short films were being shown as fairground attractions aimed at the working class and lower-middle class. The booths in which these films were shown were known in Germany somewhat disparagingly as Kintopps. Film-makers with an artistic bent attempted to counter this view of cinema with longer films based on literary models, and the first German \"artistic\" films began to be produced from around 1910, an example being the Edgar Allan Poe adaptation The Student of Prague (1913) which was co-directed by Paul Wegener and Stellan Rye, photographed by Guido Seeber and starring actors from Max Reinhardt's company.  Early film theorists in Germany began to write about the significance of Schaulust, or \"visual pleasure\", for the audience, including the Dada movement writer Walter Serner: \"If one looks to where cinema receives its ultimate power, into these strangely flickering eyes that point far back into human history, suddenly it stands there in all its massiveness: visual pleasure.\"[12] Visually striking sets and makeup were key to the style of the expressionist films that were produced shortly after the First World War.  Cinemas themselves began to be established landmarks in the years immediately before World War I. Before this, German filmmakers would tour with their works, travelling from fairground to fairground. The earliest ongoing cinemas were set up in cafes and pubs by owners who saw a way of attracting more customers. The storefront cinema was called a Kientopp, and this is where films were viewed for the most part before the First World War broke out.[13] The first standalone, dedicated cinema in Germany was opened in Mannheim in 1906, and by 1910, there were over 1000 cinemas operating in Germany.[13] Henny Porten and Asta Nielsen (the latter originally from Denmark) were the first major film stars in Germany.[14]  Prior to 1914, however, many foreign films were imported. In the era of the silent film there were no language boundaries and Danish and Italian films were particularly popular in Germany. The public's desire to see more films with particular actors led to the development in Germany, as elsewhere, of the phenomenon of the film star; the actress Henny Porten was one of the earliest German stars. Public desire to see popular film stories being continued encouraged the production of film serials, especially in the genre of mystery films, which is where the director Fritz Lang began his illustrious career.  The outbreak of World War I and the subsequent boycott of, for example, French films left a noticeable gap in the market. By 1916, there already existed some 2000 fixed venues for movie performances and initially film screenings were supplemented or even replaced by variety turns. In 1917 a process of concentration and partial nationalisation of the German film industry began with the founding of Universum Film AG (UFA), which was partly a reaction to the very effective use that the Allied Powers had found for the new medium for the purpose of propaganda. Under the aegis of the military, so-called Vaterland films were produced, which equalled the Allies' films in the matter of propaganda and disparagement of the enemy. Audiences however did not care to swallow the patriotic medicine without the accompanying sugar of the light-entertainment films which, consequently, Ufa also promoted. The German film industry soon became the largest in Europe.  The German film industry, which was protected during the war by the ban on foreign films import, became exposed at the end of the war to the international film industry while having to face an embargo, this time on its own films. Many countries banned the import of German films and audiences themselves were resisting anything that was \"German\".[15] But the ban imposed on German films involved commercial considerations as well – as an American president of one of the film companies was quoted, \"an influx of such films in the United States would throw thousands of our own... out of work, because it would be absolutely impossible for the American producers to compete with the German producers\".[16] At home, the German film industry confronted an unstable economic situation and the devaluation of the currency made it difficult for the smaller production companies to function. Film industry financing was a fragile business and expensive productions occasionally led to bankruptcy. In 1925 UFA itself was forced to go into a disadvantageous partnership called Parufamet with the American studios Paramount and MGM, before being taken over by the nationalist industrialist and newspaper owner Alfred Hugenberg in 1927.[17]  Nevertheless, the German film industry enjoyed an unprecedented development – during the 14 years which comprise the Weimar period, an average of 250 film were being produced each year, a total of 3,500 full-feature films.[15] Apart from UFA, about 230 film companies were active in Berlin alone. This industry was attracting producers and directors from all over Europe. The fact that the films were silent and language was not a factor, enabled even foreign actors, like the Danish film star Asta Nielsen or the American Louise Brooks, to be hired even for leading roles. This period can also be noted for new technological developments in film making and experimentation in set design and lighting, led by UFA. Babelsberg Studio, which was incorporated into UFA, expanded massively and gave the German film industry a highly developed infrastructure. Babelsberg remained the centre of German filmmaking for many years, became the largest film studio in Europe and produced most of the films in this \"golden era\" of German cinema.[17] In essence it was \"the German equivalent to Hollywood\".[18]  Films about an exaggerated version of Japanese culture that included \"geishas, samurai, and Shinto shrines\" were popular in Germany during this era.[19]  Due to the unstable economic condition and in an attempt to deal with modest production budgets, filmmakers were trying to reach the largest audience possible and in that, to maximize their revenues. This led to films being made in a vast array of genres and styles.[15]  One of the main film genres associated with the Weimar Republic cinema is German Expressionism which was inspired by the expressionist movement in art. Expressionist movies relied heavily on symbolism and artistic imagery rather than stark realism to tell their stories. Given the grim mood in post-World War I, it was not surprising that these films focused heavily on crime and horror. The film usually credited with sparking the popularity of expressionism is Robert Wiene's The Cabinet of Dr. Caligari (1920), produced by Erich Pommer. The film tells the story of a demented hypnotist who is using a sleepwalker to perform a series of murders. The film featured a dark and twisted visual style – the set was unrealistic with geometric images painted on the floor and shapes in light and shadow cast on walls, the acting was exaggerated and the costumes bizarre. These stylistic elements became trademarks of this cinematic movement. Other notable works of Expressionism are Friedrich Wilhelm Murnau's Nosferatu (1922), a classic period-piece horror film that remains the first feature-length film adaptation of Bram Stoker's Dracula, Carl Boese and Paul Wegener's The Golem: How He Came Into the World (1920), a Gothic retelling of the Jewish folktale, and Metropolis (1927), a legendary science-fiction epic directed by Fritz Lang. The Expressionist movement began to wane during the mid-1920s, but perhaps the fact that its main creators moved to Hollywood, California, allowed this style to remain influential in world cinema for years to come, particularly in American horror films and film noir and in the works of European directors such as Jean Cocteau and Ingmar Bergman.[21]  Despite its significance, expressionist cinema was not the dominant genre of this era.[21] Many other genres such as period dramas, melodramas, romantic comedies, and films of social and political nature, were much more prevalent and definitely more popular.  The \"master\" of period-dramas was undoubtedly Ernst Lubitsch. His most notable films of this genre were Madame DuBarry (1919) which portrayed the French Revolution through the eyes of the King of France's mistress, and the film Anna Boleyn (1920) on the tragic end of King Henry VIII's second wife. In these films, Lubitsch presented prominent historic personalities who are caught up by their weaknesses and petty urges and thus, ironically, become responsible for huge historical events. Despite modest budgets, his films included extravagant scenes which were meant to appeal to a wide audience and insure a wide international distribution.  As the genre of expressionism began to diminish, the genre of the New Objectivity (die neue Sachlichkeit) began to take its place. It was influenced by new issues which occupied the public in those years, as the rampant inflation caused deterioration in the economic status of the middle class. These films, often called \"street films\" or \"asphalt films\", tried to reflect reality in all its complexity and ugliness. They focused on objects surrounding the characters and cynically symbolized the despair felt by the German people, whose lives were shattered after the war. The most prominent film maker who is associated with this genre is Georg Wilhelm Pabst in his films such as: Joyless Street (1925), Pandora's Box (1929), and The Loves of Jeanne Ney (1927). Pabst is also credited with innovations in film editing, such as reversing the angle of the camera or cutting between two camera angles, which enhanced film continuity and later became standards of the industry.[17]  Pabst is also identified with another genre which branched from the New Objectivity – that of social and political films. These filmmakers dared to confront sensitive and controversial social issues which engaged the public in those days; such as anti-Semitism, prostitution and homosexuality. To a large extent, Weimar cinema was playing a vibrant and important role by leading public debate on those issues.[22] Pabst, in his film Diary of a Lost Girl (1929), tells the story of a young woman who has a child out of wedlock, is thrown out into the street by her family and has to resort to prostitution to survive. As early as 1919, Richard Oswald's film Different from the Others portrayed a man torn between his homosexual tendencies and the moral and social conventions. It is considered to be the first German film to deal with homosexuality and some researchers even believe it to be the first in the world to examine this issue explicitly.[23] That same year, the film Ritual Murder (1919) by Jewish film producer Max Nivelli came to the screen. This film was the first to make the German public aware of the consequences of anti-Semitism and xenophobia. It portrayed a \"pogrom\" which is carried out against the Jewish inhabitants of a village in Tsarist Russia. In the background, a love story also evolves between a young Russian student and the daughter of the leader of the Jewish community, something that was considered a taboo at the time. Later on, in an attempt to reflect the rapidly growing anti-Semitic atmosphere, Oswald confronted the same issue with his film Dreyfus (1930), which portrayed the 1894 political scandal of the \"Dreyfus affair\", which until today remains one of the most striking examples of miscarriage of justice and blatant anti-Semitism.  The polarised politics of the Weimar period were also reflected in some of its films. A series of patriotic films about Prussian history, starring Otto Gebühr as Frederick the Great were produced throughout the 1920s and were popular with the nationalist right-wing, who strongly criticised the \"asphalt\" films' decadence. Another dark chapter of the Weimar period was reflected in Joseph Delmont's film Humanity Unleashed (1920). The film was an adaptation of a novel by the same name, written by Max Glass and published in 1919. The novel described a dark world consumed by disease and war. The filmmakers decided to take the story to a more contemporary context by reflecting the growing fear among the German public of political radicalization. They produced what was to become the first fictional account of the events of January 1919 in Berlin, the so-called \"Spartacist Uprising\". This film is also considered one of the anti-Bolshevik films of that era.[25]  Another important film genre of the Weimar years was the Kammerspiel or \"chamber drama\", which was borrowed from the theater and developed by stage director, who would later become a film producer and director himself, Max Reinhardt. This style was in many ways a reaction against the spectacle of expressionism and thus tended to revolve around ordinary people from the lower-middle-class. Films of this genre were often called \"instinct\" films because they emphasized the impulses and intimate psychology of the characters. The sets were kept to a minimum and there was abundant use of camera movements to add complexity to the rather intimate and simple spaces. Associated with this particular style is also screenwriter Carl Mayer and films such as Murnau's Last Laugh (1924).  Nature films, a genre referred to as Bergfilm, also became popular. Most known in this category are the films by director Arnold Fanck, in which individuals were shown battling against nature in the mountains. Animators and directors of experimental films such as; Lotte Reiniger, Oskar Fischinger and Walter Ruttmann, were also very active in Germany in the 1920s. Ruttman's experimental documentary Berlin: Symphony of a Metropolis (1927) epitomised the energy of 1920s Berlin.  The arrival of sound at the very end of the 1920s, produced a final artistic flourish of German film before the collapse of the Weimar Republic in 1933. As early as 1918, three inventors came up with the Tri-Ergon sound-on-film system and tried to introduce it to the industry between 1922 and 1926. UFA showed an interest, but possibly due to financial difficulties, never made a sound film.[29] But in the late 1920s, sound production and distribution were starting to be adopted by the German film industry and by 1932 Germany had 3,800 cinemas equipped to play sound films. The first filmmakers who experimented with the new technology often shot the film in several versions, using several soundtracks in different languages. The film The Blue Angel (1930), directed by the Austrian Josef von Sternberg and produced by Erich Pommer, was also shot in two versions – German and English, with a different supporting cast in each version. It is considered to be Germany's first \"talkie\" and will always be remembered as the film that made an international superstar of its lead actress Marlene Dietrich. Other notable early sound films, all from 1931, include Jutzi's adaptation to Alfred Döblin's novel Berlin Alexanderplatz, Pabst's Bertolt Brecht adaptation The Threepenny Opera and Lang's M, as well as Hochbaum's Raid in St. Pauli (1932). Brecht was also one of the creators of the explicitly communist film Kuhle Wampe (1932), which was banned soon after its release.  In addition to developments in the industry itself, the Weimar period saw the birth of film criticism as a serious discipline whose practitioners included Rudolf Arnheim in Die Weltbühne and in Film als Kunst (1932), Béla Balázs in Der Sichtbare Mensch (1924), Siegfried Kracauer in the Frankfurter Zeitung, and Lotte H. Eisner in the Filmkurier.    The uncertain economic and political situation in Weimar Germany had already led to a number of film-makers and performers leaving the country, primarily for the United States; Ernst Lubitsch moved to Hollywood as early as 1923, the Hungarian-born Michael Curtiz in 1926. Some 1,500 directors, producers, actors and other film professionals emigrated in the years after the Nazis came to power. Among them were such key figures as the producer Erich Pommer, the studio head of Ufa, stars Marlene Dietrich and Peter Lorre, and director Fritz Lang. Lang's exodus to America is legendary; it is said that Metropolis so greatly impressed Joseph Goebbels that he asked Lang to become the head of his propaganda film unit.  Lang fled to America instead, where he had a long and prosperous career. Many up-and-coming German directors also fled to the U.S., having a major influence on American film as a result. A number of the Universal Horror films of the 1930s were directed by German emigrees, including Karl Freund, Joe May and Robert Siodmak. Directors Edgar Ulmer and Douglas Sirk and the Austrian-born screenwriter (and later director) Billy Wilder also emigrated from Nazi Germany to Hollywood success. Not all those in the film industry threatened by the Nazi regime were able to escape; the actor and director Kurt Gerron, for example, perished in a concentration camp.  Within weeks of the Machtergreifung, Alfred Hugenberg had effectively turned over Ufa to the ends of the Nazis, excluding Jews from employment in the company in March 1933, several months before the foundation in June of the Reichsfilmkammer (Reich Chamber of Film), the body of the Nazi state charged with control of the film industry, which marked the official exclusion of Jews and foreigners from employment in the German film industry. As part of the process of Gleichschaltung all film production in Germany was subordinate to the Reichsfilmkammer, which was directly responsible to Goebbel's Propaganda ministry, and all those employed in the industry had to be members of the Reichsfachschaft Film. \"Non-Aryan\" film professionals and those whose politics or personal life were unacceptable to the Nazis were excluded from the Reichsfachschaft and thus denied employment in the industry. Some 3,000 individuals were affected by this employment ban. In addition, as journalists were also organised as a division of the Propaganda Ministry, Goebbels was able to abolish film criticism in 1936 and replace it with Filmbeobachtung (film observation); journalists could only report on the content of a film, not offer judgement on its artistic or other worth.  With the German film industry now effectively an arm of the totalitarian state, no films could be made that were not ostensibly in accord with the views of the ruling regime. However, despite the existence of anti-semitic propaganda works such as The Eternal Jew (1940)—which was a box-office flop—and the more sophisticated but equally anti-semitic Jud Süß (1940), which achieved commercial success at home and elsewhere in Europe, the majority of German films from the National Socialist period were intended principally as works of entertainment. The import of foreign films was legally restricted after 1936 and the German industry, which was effectively nationalised in 1937, had to make up for the missing foreign films (above all American productions). Entertainment also became increasingly important in the later years of World War II when the cinema provided a distraction from Allied bombing and a string of German defeats. In both 1943 and 1944 cinema admissions in Germany exceeded a billion,[32] and the biggest box office hits of the war years were Die große Liebe (1942) and Wunschkonzert (1941), which both combine elements of the musical, wartime romance and patriotic propaganda, Frauen sind doch bessere Diplomaten (1941), a comic musical which was one of the earliest German films in colour, and Vienna Blood (1942), the adaptation of a Johann Strauß comic operetta. Titanic (1943) was another big-budget epic that arguably inspired other films about the ill-fated ocean liner.[33] The importance of the cinema as a tool of the state, both for its propaganda value and its ability to keep the populace entertained, can be seen in the filming history of Veit Harlan's Kolberg (1945), the most expensive film of the Nazi era, for the shooting of which tens of thousands of soldiers were diverted from their military positions to appear as extras.  Despite the emigration of many film-makers and the political restrictions, the period was not without technical and aesthetic innovations, the introduction of Agfacolor film production being a notable example. Technical and aesthetic achievement could also be turned to the specific ends of the Nazi state, most spectacularly in the work of Leni Riefenstahl. Riefenstahl's Triumph of the Will (1935), documenting the 1934 Nuremberg Rally, and Olympia (1938), documenting the 1936 Summer Olympics, pioneered techniques of camera movement and editing that have influenced many later films. Both films, particularly Triumph of the Will, remain highly controversial, as their aesthetic merit is inseparable from their propagandising of Nazi ideals.  East German cinema initially profited from the fact that much of the country's film infrastructure, notably the former UFA studios, lay in the Soviet occupation zone which enabled film production to get off the ground more quickly than in the Western sectors.[34] The authorities in the Soviet Zone were keen to re-establish the film industry in their sector and an order was issued to re-open cinemas in Berlin in May 1945 within three weeks of German capitulation.[35] The film production company DEFA was founded on 17 May 1946, and took control of the film production facilities in the Soviet Zone which had been confiscated by order of the Soviet Military Administration in Germany in October 1945.[36] A joint-stock company on paper, the majority interest in DEFA was actually held by the Socialist Unity Party of Germany (SED) which became the ruling party of the German Democratic Republic (GDR) after 1949, formally placing DEFA as the state-owned monopoly for film production in East Germany.[37] A sister \"company\", Progress Film, had also been established as a similar monopoly for domestic film distribution, its principal \"competition\" being Sovexportfilm, which handled distribution of Soviet films.[38]  In total, DEFA produced some 900 feature films during its existence as well as around 800 animated films and over 3000 documentaries and short films.[34][39] In 1946 DEFA produced The Murderers are Among Us, which was the first German film released after World War II and created the groundwork for the so-called Trümmerfilme, or rubble films, which were filmed amidst the rubble of structures bombed during World War II.[40] Early on, production of East German film was limited due to strict controls imposed by the authorities which restricted the subject-matter of films to topics that directly contributed to the Communist project of the state. Excluding newsreels and educational films, only 50 films were produced between 1948 and 1953.[citation needed] However, in later years numerous films were produced on a variety of themes. DEFA had particular strengths in children's films, notably fairy tale adaptations such as Drei Haselnüsse für Aschenbrödel (Three Hazelnuts for Cinderella) (1973),[41] but it also attempted other genre works: science-fiction, for example Der schweigende Stern (The Silent Star) (1960),[42] an adaptation of a Stanisław Lem novel, or \"red westerns\" such as The Sons of the Great Mother Bear (1966)[43] in which, in contrast to the typical American western, the heroes tended to be Native Americans. Many of these genre films were co-productions with other Warsaw Pact countries.  Notable non-genre films produced by DEFA include Wolfgang Staudte's adaptation of Heinrich Mann's Der Untertan (1951); Konrad Wolf's Der geteilte Himmel (Divided Heaven) (1964), an adaptation of Christa Wolf's novel; Frank Beyer's adaptation of Jurek Becker's Jacob the Liar (1975), the only East German film to be nominated for an Oscar;[44] The Legend of Paul and Paula (1973), directed by Heiner Carow from Ulrich Plenzdorf's novel; and Solo Sunny (1980), again the work of Konrad Wolf.  However, film-making in the GDR was always constrained and oriented by the political situation in the country at any given time. Ernst Thälmann, the communist leader in the Weimar period, was the subject of several hagiographical films in the 1950s (Ernst Thälmann, 1954), and although East German filmmaking moved away from this overtly Stalinist approach in the 1960s,[45] filmmakers were still subject to the changing political positions, and indeed the whims, of the SED leadership. For example, DEFA's full slate of contemporary films from 1966 were denied distribution, among them Frank Beyer's Traces of Stones (1966) which was pulled from distribution after three days, not because it was antipathetic to communist principles, but because it showed that such principles, which it fostered, were not put into practice at all times in East Germany.[46] The huge box-office hit The Legend of Paul and Paula was initially threatened with a distribution ban because of its satirical elements and supposedly only allowed a release on the say-so of Party General Secretary Erich Honecker.[citation needed]  In the late 1970s, numerous film-makers left the GDR for the West as a result of restrictions on their work, among them director Egon Günther and actors Angelica Domröse, Eva-Maria Hagen, Katharina Thalbach, Hilmar Thate, Manfred Krug and Armin Mueller-Stahl. Many had been signatories of a 1976 petition opposing the expatriation of socially critical singer-songwriter Wolf Biermann and had had their ability to work restricted as a result.[47]  In the final years of the GDR, the availability of television and the programming and films on television broadcasts reaching into the GDR via the uncontrollable airwaves, reduced the influence of DEFA productions, although its continuing role in producing shows for East German television channel remained.[citation needed] Following the Wende, DEFA had ceased production altogether, and its studios and equipment was sold off by the Treuhand in 1992, but its intellectual property rights were handed to the charitable DEFA-Stiftung (DEFA Foundation) which exploits these rights in conjunction with a series of private companies, especially the quickly privatized Progress Film GmbH, which has issued several East German films with English subtitles since the mid-1990s.[36][38]  The occupation and reconstruction of Germany by the Four Powers in the period immediately after the end of World War II brought a major and long-lasting change to the economic conditions under which the industry in Germany had previously operated. The holdings of Ufa were confiscated by the Allies and, as part of the process of decartelisation, licences to produce films were shared between a range of much smaller companies. In addition, the Occupation Statute of 1949, which granted partial independence to the newly created Federal Republic of Germany, specifically forbade the imposition of import quotas to protect German film production from foreign competition, the result of lobbying by the American industry as represented by the MPAA.  Amidst the devastation of the Stunde Null year of 1945 cinema attendance was unsurprisingly down to a fraction of its wartime heights, but already by the end of the decade it had reached levels that exceeded the pre-war period.[32] For the first time in many years, German audiences had free access to cinema from around the world and in this period the films of Charlie Chaplin remained popular, as were melodramas from the United States. Nonetheless, the share of the film market for German films in this period and into the 1950s remained relatively large, taking up some 40 percent of the total market. American films took up around 30 percent of the market despite having around twice as many films in distribution as the German industry in the same time frame.[48]  Many of the German films of the immediate post-war period can be characterised as belonging to the genre of the Trümmerfilm (literally \"rubble film\"). These films show strong affinities with the work of Italian neorealists, not least Roberto Rossellini's neorealist trilogy which included Germany Year Zero (1948), and are concerned primarily with day-to-day life in the devastated Germany and an initial reaction to the events of the Nazi period (the full horror of which was first experienced by many in documentary footage from liberated concentration camps). Such films include Wolfgang Staudte's Die Mörder sind unter uns (The Murderers are among us) (1946), the first film made in post-war Germany (produced in the soviet sector), and Wolfgang Liebeneiner's Liebe 47 (Love 47) (1949), an adaptation of Wolfgang Borchert's play Draußen vor der Tür.  Despite the advent of a regular television service in the Federal Republic in 1952, cinema attendances continued to grow through much of the 1950s, reaching a peak of 817.5 million visits in 1956.[32] The majority of the films of this period set out to do no more than entertain the audience and had few pretensions to artistry or active engagement with social issues. The defining genre of the period was arguably the Heimatfilm (\"homeland film\"), in which morally simplistic tales of love and family were played out in a rural setting, often in the mountains of Bavaria, Austria or Switzerland.  In their day Heimatfilms were of little interest to more scholarly film critics, but in recent years they have been the subject of study in relation to what they say about the culture of West Germany in the years of the Wirtschaftswunder. Other film genres typical of this period were adaptations of operettas, hospital melodramas, comedies and musicals. Many films were remakes of earlier Ufa productions.  Rearmament and the founding of the Bundeswehr in 1955 brought with it a wave of war films which tended to depict the ordinary German soldiers of World War II as brave and apolitical.[49] The Israeli historian Omer Bartov wrote that German films of the 1950s showed the average German soldier as a heroic victim: noble, tough, brave, honourable, and patriotic while fighting hard in a senseless war for a regime that he did not care for.[50]  The 08\/15 film trilogy of 1954–55 concerns a sensitive young German soldier who would rather play the piano than fight, and who fights on the Eastern Front without understanding why; however, no mention is made of the genocidal aspects of Germany's war in East.[49] The last of the 08\/15 films ends with Germany occupied by a gang of American soldiers portrayed as bubble-gum chewing, slack-jawed morons and uncultured louts, totally inferior in every respect to the heroic German soldiers shown in the 08\/15 films.[49] The only exception is the Jewish American officer, who is shown as both hyper-intelligent and very unscrupulous, which Bartov noted seems to imply that the real tragedy of World War II was the Nazis did not get a chance to exterminate all of the Jews, who have now returned with Germany's defeat to once more exploit the German people.[49]  In The Doctor of Stalingrad (1958) dealing with German POWs in the Soviet Union, Germans are portrayed as more civilized, humane and intelligent than the Soviets, who are shown for the most part as Mongol savages who brutalized the German POWs.[51] One of the German POWs successfully seduces the beautiful and tough Red Army Captain Alexandra Kasalniskaya (Eva Bartok) who prefers him to the sadistic camp commandant, which as Bartov comments also is meant to show that even in defeat, German men were more sexually virile and potent than their Russian counterparts.[51] In Hunde, wollt ihr ewig leben? (Dogs, do you want to live forever?) of 1959, which deals with the Battle of Stalingrad, the focus is on celebrating the heroism of the German soldier in that battle, who are shown as valiantly holding out against overwhelming odds with no mention at all of what those soldiers were fighting for, namely National Socialist ideology or the Holocaust.[52] This period also saw a number of films that depicted the military resistance to Hitler. In Des Teufels General (The Devil's General) of 1954, a Luftwaffe general named Harras loosely modeled after Ernst Udet, appears at first to be cynical fool, but turns out to an anti-Nazi who is secretly sabotaging the German war effort by designing faulty planes.[53] Bartov commented that in this film, the German officer corps is shown as a group of fundamentally noble and civilized men who happened to be serving an evil regime made up of a small gang of gangsterish misfits totally unrepresentative of German society, which served to exculpate both the officer corps and by extension Germany society.[54] Bartov wrote that no German film of the 1950s showed the deep commitment felt by many German soldiers to National Socialism, the utter ruthless way the German Army fought the war and the mindless nihilist brutality of the later Wehrmacht.[55] Bartov wrote that German film-makers liked to show the heroic last stand of the 6th Army at Stalingrad, but none has so far showed the 6th Army's massive co-operation with the Einsatzgruppen in murdering Soviet Jews in 1941.[56]  Even though there are countless film adaptations of Edgar Wallace novels worldwide, the crime films produced by the German company Rialto Film between 1959 and 1972 are the best-known of those, to the extent that they form their own subgenre known as Krimis (abbreviation for the German term \"Kriminalfilm\" (or \"Kriminalroman\"). Other Edgar Wallace adaptations in a similar style were made by the Germans Artur Brauner and Kurt Ulrich, and the British producer Harry Alan Towers.  The international significance of the West German film industry of the 1950s could no longer measure up to that of France, Italy, or Japan. German films were only rarely distributed internationally as they were perceived as provincial. International co-productions of the kind which were becoming common in France and Italy tended to be rejected by German producers (Schneider 1990:43). However a few German films and film-makers did achieve international recognition at this time, among them Bernhard Wicki's Oscar-nominated Die Brücke (The Bridge) (1959), and the actresses Hildegard Knef and Romy Schneider.  In the late 1950s, the growth in cinema attendance of the preceding decade first stagnated and then went into freefall throughout the 1960s. By 1969 West German cinema attendance at 172.2 million visits per year was less than a quarter of its 1956 post-war peak.[32] As a consequence of this, numerous German production and distribution companies went out of business in the 1950s and 1960s and cinemas across the Federal Republic closed their doors; the number of screens in West Germany almost halved between the beginning and the end of the decade.[citation needed]  Initially, the crisis was perceived as a problem of overproduction. Consequently, the German film industry cut back on production. 123 German movies were produced in 1955, only 65 in 1965. However, many German film companies followed the 1960s trends of international co-productions with Italy and Spain in such genres as spaghetti westerns and Eurospy films with films shot in those nations or in Yugoslavia that featured German actors in the casts.  The roots of the problem lay deeper in changing economic and social circumstances. Average incomes in the Federal Republic rose sharply and this opened up alternative leisure activities to compete with cinema-going. At this time too, television was developing into a mass medium that could compete with the cinema. In 1953 there were only 1,000,000 sets in West Germany; by 1962 there were 7 million (Connor 1990:49) (Hoffman 1990:69).  The majority of films produced in the Federal Republic in the 1960s were genre works: westerns, especially the series of movies adapted from Karl May's popular genre novels which starred Pierre Brice as the Apache Winnetou and Lex Barker as his white blood brother Old Shatterhand; thrillers and crime films, notably a series of Edgar Wallace movies from Rialto Film in which Klaus Kinski, Heinz Drache, Karin Dor and Joachim Fuchsberger were among the regular players.  The traditional Krimi films expanded into series based on German pulp fiction heroes such as Jerry Cotton played by George Nader and Kommissar X played by Tony Kendall and Brad Harris. West Germany also made several horror films including ones starring Christopher Lee.  The two genres were combined in the return of Doctor Mabuse in a series of several films of the early 1960s.  At the end of the 1960s softcore sex films, both the relatively serious Aufklärungsfilme (sex education films) of Oswalt Kolle and such exploitation films as Schulmädchen-Report (Schoolgirl Report) (1970) and its successors were produced into the 1970s. Such movies were commercially successful and often enjoyed international distribution, but won little acclaim from critics.  In the 1960s more than three-quarters of the regular cinema audience were lost as consequence of the rising popularity of TV sets at home. As a reaction to the artistic and economic stagnation of German cinema, a group of young film-makers issued the Oberhausen Manifesto on 28 February 1962. This call to arms, which included Alexander Kluge, Edgar Reitz, Peter Schamoni and Franz-Josef Spieker among its signatories, provocatively declared \"Der alte Film ist tot. Wir glauben an den neuen\" (\"The old cinema is dead. We believe in the new cinema\"). Other up-and-coming filmmakers allied themselves to this Oberhausen group, among them Rainer Werner Fassbinder, Volker Schlöndorff, Werner Herzog, Jean-Marie Straub, Wim Wenders, Werner Schroeter and Hans-Jürgen Syberberg in their rejection of the existing German film industry and their determination to build a new cinema founded on artistic and social measures rather than commercial success. Most of these directors organized themselves in, or partially co-operated with, the film production and distribution company Filmverlag der Autoren established in 1971, which throughout the 1970s brought forth a number of critically acclaimed films. Rosa von Praunheim, who formed the German lesbian and gay movement with his film It Is Not the Homosexual Who Is Perverse, But the Society in Which He Lives (1971), also plays an important role.[57]  Despite the foundation of the Kuratorium Junger Deutscher Film (Young German Film Committee) in 1965, set up under the auspices of the Federal Ministry of the Interior to support new German films financially, the directors of this New German Cinema were consequently often dependent on money from television. Young filmmakers had the opportunity to test their mettle in such programmes as the stand-alone drama and documentary series Das kleine Fernsehspiel (The Little TV Play) or the television films of the crime series Tatort. However, the broadcasters sought TV premieres for the films which they had supported financially, with theatrical showings only occurring later. As a consequence, such films tended to be unsuccessful at the box office.  This situation changed after 1974 when the Film-Fernseh-Abkommen (Film and Television Accord) was agreed between the Federal Republic's main broadcasters, ARD and ZDF, and the German Federal Film Board (a government body created in 1968 to support film-making in Germany).[58] This accord, which has been repeatedly extended up to the present day, provides for the television companies to make available an annual sum to support the production of films which are suitable for both theatrical distribution and television presentation. (The amount of money provided by the public broadcasters has varied between 4.5 and 12.94 million euros per year. Under the terms of the accord, films produced using these funds can only be screened on television 24 months after their theatrical release. They may appear on video or DVD no sooner than six months after cinema release. Nevertheless, the New German Cinema found it difficult to attract a large domestic or international audience.  The socially critical films of the New German Cinema strove to delineate themselves from what had gone before and the works of auteur film-makers such as Kluge and Fassbinder are examples of this, although Fassbinder in his use of stars from German cinema history also sought a reconciliation between the new cinema and the old. In addition, a distinction is sometimes drawn between the avantgarde \"Young German Cinema\" of the 1960s and the more accessible \"New German Cinema\" of the 1970s. For their influences the new generation of film-makers looked to Italian neorealism, the French Nouvelle Vague and the British New Wave but combined this eclectically with references to the well-established genres of Hollywood cinema. The New German Cinema dealt with contemporary German social problems in a direct way; the Nazi past, the plight of the Gastarbeiter (\"guest workers\"), and modern social developments, were all subjects prominent in New German Cinema films.[59]  Films such as Kluge's Abschied von Gestern (1966), Herzog's Aguirre, the Wrath of God (1972), Fassbinder's Fear Eats the Soul (1974) and The Marriage of Maria Braun (1979), and Wenders' Paris, Texas (1984) found critical approval. Often the work of these auteurs was first recognised abroad rather than in Germany itself. The work of post-war Germany's leading novelists Heinrich Böll and Günter Grass provided source material for the adaptations The Lost Honour of Katharina Blum (1975) (by Schlöndorff and Margarethe von Trotta) and The Tin Drum (1979) (by Schlöndorff alone) respectively, the latter becoming the first German film to win the Academy Award for Best Foreign Language Film. The New German Cinema also allowed for female directors to come to the fore and for the development of a feminist cinema which encompassed the works of directors such as Margarethe von Trotta, Helma Sanders-Brahms, Jutta Brückner, Helke Sander and Cristina Perincioli.  German production companies have been quite commonly involved in expensive French and Italian productions from Spaghetti Westerns to French comic book adaptations.  Having achieved some of its goals, among them the establishment of state funding for the film industry and renewed international recognition for German films, the New German Cinema had begun to show signs of fatigue by the 1980s, even though many of its proponents continued to enjoy individual success.  Among the commercial successes for German films of the 1980s were the Otto film series beginning in 1985 starring comedian Otto Waalkes, Wolfgang Petersen's adaptation of The NeverEnding Story (1984), and the internationally successful Das Boot (1981), which still holds the record for most Academy Award nominations for a German film (six). Other notable film-makers who came to prominence in the 1980s include producer Bernd Eichinger and directors Doris Dörrie, Uli Edel, and Loriot.  Away from the mainstream, the splatter film director Jörg Buttgereit came to prominence in the 1980s. The development of arthouse cinemas (Programmkinos) from the 1970s onwards provided a venue for the works of less mainstream film-makers like Herbert Achternbusch, Hark Bohm, Dominik Graf, Oliver Herbrich, Rosa von Praunheim or Christoph Schlingensief.  From the mid-1980s the spread of videocassette recorders and the arrival of private TV channels such as RTL Television provided new competition for theatrical film distribution. Cinema attendance, having rallied slightly in the late 1970s after an all-time low of 115.1 million visits in 1976, dropped sharply again from the mid-1980s to end at just 101.6 million visits in 1989.[32] However, the availability of a back catalogue of films on video also allowed for a different relationship between the viewer and an individual film, while private TV channels brought new money into the film industry and provided a launch pad from which new talent could later move into film.  Today's biggest German production studios include Babelsberg Studio, Bavaria Film, Constantin Film and UFA. Film releases such as Run Lola Run by Tom Tykwer, Good Bye Lenin! by Wolfgang Becker, Head-On  by Fatih Akin, Perfume by Tom Tykwer and The Lives of Others by Florian Henckel von Donnersmarck, have arguably managed to recapture a provocative and innovative nature. Movies like The Baader Meinhof Complex produced by Bernd Eichinger achieved some popular success.  Notable directors working in German currently include Sönke Wortmann, Caroline Link (winner of an Academy Award), Romuald Karmakar, Dani Levy, Hans-Christian Schmid, Andreas Dresen, Dennis Gansel and Uli Edel as well as comedy directors Michael Herbig and Til Schweiger.  Internationally, German filmmakers such as Roland Emmerich or Wolfgang Petersen or Uwe Boll built successful careers as directors and producers. Hans Zimmer, a film composer, has become one of the world's most acclaimed producers of movie scores. Michael Ballhaus became a renowned cinematographer.  Germany has a long tradition of cooperation with the European-based film industry, which started as early as during the 1960s. Since 1990 the number of international projects financed and co-produced by German filmmakers has expanded.  The new millennium since 2000 has seen a general resurgence of the German film industry, with a higher output and improved returns at the German box office.  The collapse of the GDR had a large effect on the German cinema industry. The viewer count increased with the new population's access to western movies. The movies produced in the United States were the most popular, due to the fact that the market was dominated by them and the production was more advanced than Germany's. Some other genres that were popular consisted of Romantic Comedies, and Social Commentaries. Wolfgang Petersen and Roland Emmerich both established international success.  Internationally, German productions are benefitting from streaming. Their global market share is rising.[61] Domestically, the German movies improved their market share of about 16% in 1996 to around 30% in 2021.,[62] so the movie culture is partly recognized to be underfunded, problem laden and rather inward looking.    The main production incentive provided by governmental authorities is the Deutscher Filmförderfonds (DFFF) (German Federal Film Fund). The DFFF is a grant given by the Staatsministerin für Kultur und Medien (Federal Government Commissioner for Culture and the Media). To receive the grant a producer has to fulfill different requirements including a cultural eligibility test. The fund offers 50 million euros a year to film producers and or co-producers and grants can amount to up to 20% of the approved German production costs. At least 25% the production costs must be spent in Germany, or only 20%, if the production costs are higher than 20 million euros. The DFFF has been established in 2007 and supported projects in all categories and genres.  In 2015, the Deutsche Filmförderungsfond was reduced from 60 million euros to 50 million euros. To compensate, Finance minister Gabriel announced that the difference will be made up from the budget of the Bundesministerium für Wirtschaft und Klimaschutz (Federal Ministry for Economic Affairs and Climate Action).[63][64] For the first time in Germany high-profile tv series and digital filmmaking will be funded at a federal level in the same manner as feature films.[65] Funding is also increasingly flowing to international co-productions.  In 1979, the German states also began to establish funding institutions, often with the intention of supporting their own production locations. Today, film funding by the federal states makes up the largest share of film funding in Germany. A total of more than 200 million euros in grants are distributed annually, with an upward trend.  The history of film funding began in Germany with the founding of the UFA GmbH (1917), which was to produce pro-German propaganda films - equipped with funds from industry and banks. During the period of National Socialism (1933–1945), the state indirectly promoted the financing of film projects by establishing the Filmkreditbank GmbH (FKB) (Film Credit Bank).  After the end of World War II, many feature films were initially supported by federal guarantees. However, film funding in its current form did not develop until the 1950s, when television began to supplant motion pictures. In 1967, a film funding law was passed for the first time. The Berlin-based Filmförderungsanstalt (FFA) (Film Funding Agency) was the first major funding institution to be founded in 1968.  Critics accuse film funding in Germany of being institutionally fragmented, making it virtually impossible to coordinate all measures, which would ultimately benefit the quality of productions. They also say that a blanket distribution of grants stifles the incentive to produce films that recoup their production costs.  Film funding in Germany is provided, among others, by the following institutions:  Lokal:  The Berlin International Film Festival, also called Berlinale, is one of the world's leading film festivals and most reputable media events.[66] It is held in Berlin, Germany.[67] Founded in West Berlin in 1951, the festival has been celebrated annually in February since 1978. With 274,000 tickets sold and 487,000 admissions it is considered the largest publicly attended film festival worldwide.[68][69] Up to 400 films are shown in several sections, representing a comprehensive array of the cinematic world. Around twenty films compete for the awards called the Golden and Silver Bears. Since 2001 the director of the festival has been Dieter Kosslick.[70][71]  The festival, the EFM and other satellite events are attended by around 20,000 professionals from over 130 countries.[72] More than 4200 journalists are responsible for the media exposure in over 110 countries.[73] At high-profile feature film premieres, movie stars and celebrities are present at the red carpet.[74]  The Deutsche Filmakademie was founded in 2003 in Berlin and aims to provide native filmmakers a forum for discussion and a way to promote the reputation of German cinema through publications, presentations, discussions and regular promotion of the subject in the schools.  Since 2005, the winners of the Deutscher Filmpreis, also known as the Lolas are elected by the members of the Deutsche Filmakademie. With a cash prize of three million euros it is the most highly endowed German cultural award.  Several institutions, both government run and private, provide formal education in various aspects of filmmaking. "},"meta":{},"created_at":"2025-03-22T14:25:42.288402Z","updated_at":"2025-03-22T14:25:42.288402Z","inner_id":61,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":70,"annotations":[{"id":70,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.320914Z","updated_at":"2025-03-22T14:25:42.320914Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"55a0de7f-de14-4ee4-ade0-590fcbe6c743","import_id":null,"last_action":null,"bulk_created":false,"task":70,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Financial statements (or financial reports) are formal records of the financial activities and position of a business, person, or other entity.  Relevant financial information is presented in a structured manner and in a form which is easy to understand. They typically include four basic financial statements[1][2] accompanied by a management discussion and analysis:[3]  Notably, a balance sheet represents a snapshot in time, whereas the income statement, the statement of changes in equity, and the cash flow statement each represent activities over an accounting period. By understanding the key functional statements within the balance sheet, business owners and financial professionals can make informed decisions that drive growth and stability.  \"The objective of financial statements is to provide information about the financial position, performance and changes in financial position of an enterprise that is useful to a wide range of users in making economic decisions.\"  Financial statements should be understandable, relevant, reliable and comparable. Reported assets, liabilities, equity, income and expenses are directly related to an organization's financial position.   Financial statements are intended to be understandable by readers who have \"a reasonable knowledge of business and economic activities and accounting and who are willing to study the information diligently.\"[4]  Financial statements may be used by users for different purposes:  Consolidated financial statements are defined as \"Financial statements  of a group in which the assets, liabilities, equity, income, expenses and cash flows of the parent (company) and its subsidiaries are presented as those of a single economic entity\", according to International Accounting Standard 27 \"Consolidated and separate financial statements\", and International Financial Reporting Standard 10 \"Consolidated financial statements\".[6][7]  Different countries have developed their own accounting principles over time, making international comparisons of companies difficult. To ensure uniformity and comparability between financial statements prepared by different companies, a set of guidelines and rules are used. Commonly referred to as Generally Accepted Accounting Principles (GAAP), these set of guidelines provide the basis in the preparation of financial statements, although many companies voluntarily disclose information beyond the scope of such requirements.[8]  Recently there has been a push towards standardizing accounting rules made by the International Accounting Standards Board (IASB). IASB develops International Financial Reporting Standards that have been adopted by Australia, Canada and the European Union (for publicly quoted companies only), are under consideration in South Africa and other countries. The United States Financial Accounting Standards Board has made a commitment to converge the U.S. GAAP and IFRS over time.  Management discussion and analysis or MD&A is an integrated part of a company's annual financial statements. The purpose of the MD&A is to provide a narrative explanation, through the eyes of management, of how an entity has performed in the past, its financial condition, and its future prospects. In so doing, the MD&A attempt to provide investors with complete, fair, and balanced information to help them decide whether to invest or continue to invest in an entity.[9]  The section contains a description of the year gone by and some of the key factors that influenced the business of the company in that year, as well as a fair and unbiased overview of the company's past, present, and future.  MD&A typically describes the corporation's liquidity position, capital resources,[10] results of its operations, underlying causes of material changes in financial statement items (such as asset impairment and restructuring charges), events of unusual or infrequent nature (such as mergers and acquisitions or share buybacks), positive and negative trends, effects of inflation, domestic and international market risks,[11] and significant uncertainties. "},"meta":{},"created_at":"2025-03-22T14:25:42.288402Z","updated_at":"2025-03-22T14:25:42.288402Z","inner_id":62,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":71,"annotations":[{"id":71,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.320914Z","updated_at":"2025-03-22T14:25:42.320914Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"20145cb7-ee07-4d41-98e4-749401a361ba","import_id":null,"last_action":null,"bulk_created":false,"task":71,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  A commodity market is a market that trades in the primary economic sector rather than manufactured products.  The primary sector includes agricultural products, energy products, and metals. Soft commodities may be perishable and harvested, while hard commodities are usually mined, such as gold and oil.[1] Futures contracts are the oldest way of investing in commodities.[citation needed] Commodity markets can include physical trading and derivatives trading using spot prices, forwards, futures, and options on futures.[clarification needed] Farmers have used a simple form of derivative trading in the commodities market for centuries for price risk management.[2]  A financial derivative is a financial instrument whose value is derived from a commodity termed an underlier.[3] Derivatives are either exchange-traded or over-the-counter (OTC). An increasing number of derivatives are traded via clearing houses some with central counterparty clearing, which provide clearing and settlement services on a futures exchange, as well as off-exchange in the OTC market.[4]  Derivatives such as futures contracts, Swaps (1970s–), and Exchange-traded Commodities (ETC) (2003–) have become the primary trading instruments in commodity markets. Futures are traded on regulated commodities exchanges. Over-the-counter (OTC) contracts are \"privately negotiated bilateral contracts entered into between the contracting parties directly\".[5][6]  Exchange-traded funds (ETFs) began to feature commodities in 2003. Gold ETFs are based on \"electronic gold\" that does not entail the ownership of physical bullion, with its added costs of insurance and storage in repositories such as the London bullion market. According to the World Gold Council, ETFs allow investors to be exposed to the gold market without the risk of price volatility associated with gold as a physical commodity.[7][8][notes 1]  Commodity-based money and commodity markets in a crude early form are believed to have originated in Sumer between 4500 BC and 4000 BC. Sumerians first used clay tokens sealed in a clay vessel, then clay writing tablets to represent the amount—for example, the number of goats, to be delivered.[9][10] These promises of time and date of delivery resemble futures contract.  Early civilizations variously used pigs, rare seashells, or other items as commodity money. Since that time traders have sought ways to simplify and standardize trade contracts.[11][12]  Gold and silver markets evolved in classical civilizations. At first, the precious metals were valued for their beauty and intrinsic worth and were associated with royalty.[11] In time, they were used for trading and were exchanged for other goods and commodities, or for payments of labor.[13] Gold, measured out, then became money. Gold's scarcity, its unique density and the way it could be easily melted, shaped, and measured made it a natural trading asset.[14]  Beginning in the late 10th century, commodity markets grew as a mechanism for allocating goods, labor, land and capital across Europe. Between the late 11th and the late 13th century, English urbanization, regional specialization, expanded and improved infrastructure, the increased use of coinage and the proliferation of markets and fairs were evidence of commercialization.[15] The spread of markets is illustrated by the 1466 installation of reliable scales in the villages of Sloten and Osdorp so villagers no longer had to travel to Haarlem or Amsterdam to weigh their locally produced cheese and butter.[15]  The Amsterdam Stock Exchange, often cited as the first stock exchange, originated as a market for the exchange of commodities. Early trading on the Amsterdam Stock Exchange often involved the use of very sophisticated contracts, including short sales, forward contracts, and options. \"Trading took place at the Amsterdam Bourse, an open aired venue, which was created as a commodity exchange in 1530 and rebuilt in 1608. Commodity exchanges themselves were a relatively recent invention, existing in only a handful of cities.\"[16]  In 1864, in the United States, wheat, corn, cattle, and pigs were widely traded using standard instruments on the Chicago Board of Trade (CBOT), the world's oldest futures and options exchange. Other food commodities were added to the Commodity Exchange Act and traded through CBOT in the 1930s and 1940s, expanding the list from grains to include rice, mill feeds, butter, eggs, Irish potatoes and soybeans.[17] Successful commodity markets require broad consensus on product variations to make each commodity acceptable for trading, such as the purity of gold in bullion.[18] Classical civilizations built complex global markets trading gold or silver for spices, cloth, wood and weapons, most of which had standards of quality and timeliness.[19]  Through the 19th century \"the exchanges became effective spokesmen for, and innovators of, improvements in transportation, warehousing, and financing, which paved the way to expanded interstate and international trade.\"[20]  Reputation and clearing became central concerns, and states that could handle them most effectively developed powerful financial centers.[21]  In 1934, the U.S. Bureau of Labor Statistics began the computation of a daily Commodity price index that became available to the public in 1940. By 1952, the Bureau of Labor Statistics issued a Spot Market Price Index that measured the price movements of \"22 sensitive basic commodities whose markets are presumed to be among the first to be influenced by changes in economic conditions. As such, it serves as one early indication of impending changes in business activity.\"[22]  A commodity index fund is a fund whose assets are invested in financial instruments based on or linked to a commodity index. In just about every case the index is in fact a Commodity Futures Index. The first such index was the Dow Jones Commodity Index, which began in 1933.[23]  The first practically investable commodity futures index was the Goldman Sachs Commodity Index, created in 1991,[24] and known as the \"GSCI\". The next was the Dow Jones AIG Commodity Index. It differed from the GSCI primarily in the weights allocated to each commodity. The DJ AIG had mechanisms to periodically limit the weight of any one commodity and to remove commodities whose weights became too small. After AIG's financial problems in 2008 the Index rights were sold to UBS and it is now known as the DJUBS index. Other commodity indices include the Reuters \/ CRB index (which is the old CRB Index as re-structured in 2005) and the Rogers Index.  Cash commodities or \"actuals\" refer to the physical goods—e.g., wheat, corn, soybeans, crude oil, gold, silver—that someone is buying\/selling\/trading as distinguished from derivatives.[2]  In traditional stock market exchanges such as the New York Stock Exchange (NYSE), most trading activity took place in the trading pits in face-to-face interactions between brokers and dealers in open outcry trading.[25] In 1992 the  Financial Information eXchange (FIX) protocol was introduced, allowing international real-time exchange of information regarding market transactions. The U.S. Securities and Exchange Commission ordered U.S. stock markets to convert from the fractional system to a decimal system by April 2001. Metrification, conversion from the imperial system of measurement to the metrical, increased throughout the 20th century.[26] Eventually FIX-compliant interfaces were adopted globally by commodity exchanges using the FIX Protocol.[27] In 2001 the Chicago Board of Trade and the Chicago Mercantile Exchange (later merged into the CME group, the world's largest futures exchange company)[26] launched their FIX-compliant interface.  By 2011, the alternative trading system (ATS) of electronic trading featured computers buying and selling without human dealer intermediation. High-frequency trading (HFT) algorithmic trading, had almost phased out \"dinosaur floor-traders\".[25][notes 2]  The robust growth of emerging market economies (EMEs, such as Brazil, Russia, India, and China), beginning in the 1990s, \"propelled commodity markets into a supercycle\". The size and diversity of commodity markets expanded internationally,[28] and pension funds and sovereign wealth funds started allocating more capital to commodities, in order to diversify into an asset class with less exposure to currency depreciation.[29]  In 2012, as emerging-market economies slowed down, commodity prices peaked and started to decline. From 2005 through 2013, energy and metals' real prices remained well above their long-term averages. In 2012, real food prices were their highest since 1982.[28]  The price of gold bullion fell dramatically on 12 April 2013 and analysts frantically sought explanations. Rumors spread that the European Central Bank (ECB) would force Cyprus to sell its gold reserves in response to its financial crisis. Major banks such as Goldman Sachs began immediately to short gold bullion. Investors scrambled to liquidate their exchange-traded funds (ETFs)[notes 3] and margin call selling accelerated. George Gero, precious metals commodities expert at the Royal Bank of Canada (RBC) Wealth Management section reported that he had not seen selling of gold bullion as panicked as this in his forty years in commodity markets.[30]  The earliest commodity exchange-traded fund (ETFs), such as SPDR Gold Shares NYSE Arca: GLD and iShares Silver Trust NYSE Arca: SLV, actually owned the physical commodities. Similar to these are NYSE Arca: PALL (palladium) and NYSE Arca: PPLT (platinum). However, most Exchange Traded Commodities (ETCs) implement a futures trading strategy. At the time Russian Prime Minister Dmitry Medvedev warned that Russia could sink into recession. He argued that \"We live in a dynamic, fast-developing world. It is so global and so complex that we sometimes cannot keep up with the changes\". Analysts have claimed that Russia's economy is overly dependent on commodities.[31]  A Spot contract is an agreement where delivery and payment either takes place immediately, or with a short lag. Physical trading normally involves a visual inspection and is carried out in physical markets such as a farmers market. Derivatives markets, on the other hand, require the existence of agreed standards so that trades can be made without visual inspection.  US soybean futures do not qualify as \"standard grade\" if they are \"GMO or a mixture of GMO and Non-GMO No. 2 yellow soybeans of Indiana, Ohio and Michigan origin produced in the U.S.A. (Non-screened, stored in silo)\". They are of \"deliverable grade\" if they are \"GMO or a mixture of GMO and Non-GMO No. 2 yellow soybeans of Iowa, Illinois and Wisconsin origin produced in the U.S.A. (Non-screened, stored in silo)\".  Note the distinction between states, and the need to clearly mention their status as GMO (genetically modified organism) which makes them unacceptable to most organic food buyers.  Similar specifications apply for cotton, orange juice, cocoa, sugar, wheat, corn, barley, pork bellies, milk, feed stuffs, fruits, vegetables, other grains, other beans, hay, other livestock, meats, poultry, eggs, or any other commodity which is so traded.  Standardization has also occurred technologically, as the use of the FIX Protocol by commodities exchanges has allowed trade messages to be sent, received and processed in the same format as stocks or equities. This process began in 2001 when the Chicago Mercantile Exchange launched a FIX-compliant interface that was adopted by commodity exchanges around the world.[27]  Derivatives evolved from simple commodity future contracts into a diverse group of financial instruments that apply to every kind of asset, including mortgages, insurance and many more. Futures contracts, Swaps (1970s–), Exchange-traded Commodities (ETC) (2003–), forward contracts, etc. are examples. They can be traded through formal exchanges or through Over-the-counter (OTC). Commodity market derivatives unlike credit default derivatives, for example, are secured by the physical assets or commodities.[3]  A forward contract is an agreement between two parties to exchange at a fixed future date a given quantity of a commodity for a specific price defined when the contract is finalized. The fixed price is also called forward price. Such forward contracts began as a way of reducing pricing risk in food and agricultural product markets. By agreeing in advance on a price for a future delivery, farmers were able protect their output against a possible fall of market prices and in contrast buyers were able to protect themselves against a possible rise of market prices.  Forward contracts, for example, were used for rice in seventeenth century Japan.  Futures contracts are standardized forward contracts that are transacted through an exchange. In futures contracts the buyer and the seller stipulate product, grade, quantity and location and leaving price as the only variable.[32]  Agricultural futures contracts are the oldest, in use in the United States for more than 170 years.[33] Modern futures agreements, began in Chicago in the 1840s, with the appearance of grain elevators.[34] Chicago, centrally located, emerged as the hub between Midwestern farmers and east coast consumer population centers.  In a call option counterparties enter into a financial contract option where the buyer purchases the right but not the obligation to buy an agreed quantity of a particular commodity or financial instrument (the underlying) from the seller of the option at a certain time (the expiration date) for a certain price (the strike price). The seller (or \"writer\") is obligated to sell the commodity or financial instrument should the buyer so decide. The buyer pays a fee (called a premium) for this right.[35]  A swap is a derivative in which counterparties exchange the cash flows of one party's financial instrument for those of the other party's financial instrument. They were introduced in the 1970s.[36][37]  Exchange-traded commodity is a term used for commodity ETFs (which are funds) or commodity exchange-traded notes (which are notes). These track the performance of an underlying commodity index including total return indices based on a single commodity. They are similar to ETFs and traded and settled exactly like stock funds. ETCs have market maker support with guaranteed liquidity, enabling investors to easily invest in commodities.  They were introduced in 2003.  At first, only professional institutional investors had access, but online exchanges opened some ETC markets to almost anyone. ETCs were introduced partly in response to the tight supply of commodities in 2000, combined with record low inventories and increasing demand from emerging markets such as China and India.[38]  Prior to the introduction of ETCs, by the 1990s ETFs pioneered by Barclays Global Investors (BGI) revolutionized the mutual funds industry.[38] By the end of December 2009 BGI assets hit an all-time high of $1 trillion.[39]  Gold was the first commodity to be securitised through an ETF in the early 1990s, but it was not available for trade until 2003.[38] The idea of a Gold ETF was first officially conceptualised by Benchmark Asset Management Company Private Ltd in India, when they filed a proposal with the Securities and Exchange Board of India in May 2002.[40] The first gold exchange-traded fund was Gold Bullion Securities launched on the ASX in 2003, and the first silver exchange-traded fund was iShares Silver Trust launched on the NYSE in 2006. As of November 2010 a commodity ETF, namely SPDR Gold Shares, was the second-largest ETF by market capitalization.[41]  Generally, commodity ETFs are index funds tracking non-security indices. Because they do not invest in securities, commodity ETFs are not regulated as investment companies under the Investment Company Act of 1940 in the United States, although their public offering is subject to SEC review and they need an SEC no-action letter under the Securities Exchange Act of 1934. They may, however, be subject to regulation by the Commodity Futures Trading Commission.[42][43]  The earliest commodity ETFs, such as SPDR Gold Shares NYSE Arca: GLD and iShares Silver Trust NYSE Arca: SLV, actually owned the physical commodity (e.g., gold and silver bars). Similar to these are NYSE Arca: PALL (palladium) and NYSE Arca: PPLT (platinum). However, most ETCs implement a futures trading strategy, which may produce quite different results from owning the commodity.  Commodity ETFs trade provide exposure to an increasing range of commodities and commodity indices, including energy, metals, softs and agriculture. Many commodity funds, such as oil roll so-called front-month futures contracts from month to month. This provides exposure to the commodity, but subjects the investor to risks involved in different prices along the term structure, such as a high cost to roll.[7][8]  ETCs in China and India gained in importance due to those countries' emergence as commodities consumers and producers. China accounted for more than 60% of exchange-traded commodities in 2009, up from 40% the previous year. The global volume of ETCs increased by a 20% in 2010, and 50% since 2008, to around 2.5 billion million contracts.{{[44]}}  Over-the-counter (OTC) commodities derivatives trading originally involved two parties, without an exchange.  Exchange trading offers greater transparency and regulatory protections. In an OTC trade, the price is not generally made public. OTC commodities derivatives are higher risk but may also lead to higher profits.[45]  Between 2007 and 2010, global physical exports of commodities fell by 2%, while the outstanding value of OTC commodities derivatives declined by two-thirds as investors reduced risk following a five-fold increase in the previous three years.  Money under management more than doubled between 2008 and 2010 to nearly $380 billion. Inflows into the sector totaled over $60 billion in 2010, the second-highest year on record, down from $72 billion the previous year. The bulk of funds went into precious metals and energy products. The growth in prices of many commodities in 2010 contributed to the increase in the value of commodities funds under management.[46]  A commodities exchange is an exchange where various commodities and derivatives are traded. Most commodity markets across the world trade in agricultural products and other raw materials (like wheat, barley, sugar, maize, cotton, cocoa, coffee, milk products, pork bellies, oil, metals, etc.) and contracts based on them. These contracts can include spot prices, forwards, futures and options on futures. Other sophisticated products may include interest rates, environmental instruments, swaps, or freight contracts.[2]  Source: International Trade Centre[48]  Energy commodities include crude oil particularly West Texas Intermediate (WTI) crude oil and Brent crude oil, natural gas, heating oil, ethanol and purified terephthalic acid. Hedging is a common practice for these commodities.  For many years, West Texas Intermediate (WTI) crude oil, a light, sweet crude oil, was the world's most-traded commodity. WTI is a grade used as a benchmark in oil pricing. It is the underlying commodity of Chicago Mercantile Exchange's oil futures contracts. WTI is often referenced in news reports on oil prices, alongside Brent Crude. WTI is lighter and sweeter than Brent and considerably lighter and sweeter than Dubai or Oman.[49]  From April through October 2012, Brent futures contracts exceeded those for WTI, the longest streak since at least 1995.[50]  Crude oil can be light or heavy. Oil was the first form of energy to be widely traded. Some commodity market speculation is directly related to the stability of certain states, e.g., Iraq, Bahrain, Iran, Venezuela and many others. Most commodities markets are not so tied to the politics of volatile regions.  Oil and gasoline are traded in units of 1,000 barrels (42,000 US gallons). WTI crude oil is traded through NYMEX under trading symbol CL and through Intercontinental Exchange (ICE) under trading symbol WBS. Brent crude oil is traded in through Intercontinental Exchange under trading symbol BRN and on the CME under trading symbol BZ. Gulf Coast Gasoline is traded through NYMEX  with the trading symbol of LR. Gasoline (reformulated gasoline blendstock for oxygen blending or RBOB) is traded through NYMEX via trading symbol RB. Propane is traded through NYMEX, a subsidiary of Intercontinental Exchange since early 2013, via trading symbol PN.  Natural gas is traded through NYMEX in units of 10,000 million BTU with the trading symbol of NG. Heating oil is traded through NYMEX under trading symbol HO.  Purified terephthalic acid (PTA) is traded through ZCE in units of 5 tons with the trading symbol of TA. Ethanol is traded at CBOT in units of 29,000 U.S. gal under trading symbols AC (Open Auction) and ZE (Electronic).  Precious metals currently traded on the commodity market include gold, platinum, palladium and silver which are sold by the troy ounce. One of the main exchanges for these precious metals is COMEX.  According to the World Gold Council, investments in gold are the primary driver of industry growth. Gold prices are highly volatile, driven by large flows of speculative money.[51]  Industrial metals are sold by the metric ton through the London Metal Exchange and New York Mercantile Exchange. The London Metal Exchange trades include copper, aluminium, lead, tin, aluminium alloy, nickel, cobalt and molybdenum. In 2007, steel began trading on the London Metal Exchange.  Iron ore has been the latest addition to industrial metal derivatives. Deutsche Bank first began offering iron ore swaps in 2008, other banks quickly followed. Since then the size of the market has more than doubled each year between 2008 and 2012.[52]  Agricultural commodities include grains, food and fiber as well as livestock and meat, various regulatory bodies define agricultural products.[53]  In 1900, corn acreage was double that of wheat in the United States. But from the 1930s through the 1970s soybean acreage surpassed corn. Early in the 1970s grain and soybean prices, which had been relatively stable, \"soared to levels that were unimaginable at the time\". There were a number of factors affecting prices including the \"surge in crude oil prices caused by the Arab Oil Embargo in October 1973 (U.S. inflation reached 11% in 1975)\".[54]  On 21 July 2010, United States Congress passed the Dodd–Frank Wall Street Reform and Consumer Protection Act with changes to the definition of agricultural commodity. The operational definition used by Dodd-Frank includes \"[a]ll other commodities that are, or once were, or are derived from, living organisms, including plant, animal and aquatic life, which are generally fungible, within their respective classes, and are used primarily for human food, shelter, animal feed, or natural fiber\". Three other categories were explained and listed.[55]  In February 2013, Cornell Law School included lumber, soybeans, oilseeds, livestock (live cattle and hogs), dairy products. Agricultural commodities can include lumber (timber and forests), grains excluding stored grain (wheat, oats, barley, rye, grain sorghum, cotton, flax, forage, tame hay, native grass), vegetables (potatoes, tomatoes, sweet corn, dry beans, dry peas, freezing and canning peas), fruit (citrus such as oranges, apples, grapes) corn, tobacco, rice, peanuts, sugar beets, sugar cane, sunflowers, raisins, nursery crops, nuts, soybean complex, aquacultural fish farm species such as finfish, mollusk, crustacean, aquatic invertebrate, amphibian, reptile, or plant life cultivated in aquatic plant farms.[56][57]  As of 2012, diamond was not traded as a commodity. Institutional investors were repelled by campaign against \"blood diamonds\", the monopoly structure of the diamond market and the lack of uniform standards for diamond pricing. In 2012 the SEC reviewed a proposal to create the \"first diamond-backed exchange-traded fund\" that would trade online in units of one-carat diamonds with a storage vault and delivery point in Antwerp, home of the Antwerp Diamond Bourse. The exchange fund was backed by a company based in New York City called IndexIQ. IndexIQ had already introduced 14 exchange-traded funds since 2008.[51][58][notes 4]  According to Citigroup analysts, the annual production of polished diamonds is about $18 billion. Like gold, diamonds are easily authenticated and durable. Diamond prices have been more stable than the metals, as the global diamond monopoly De Beers once held almost 90% (by 2013 reduced to 40%) of the new diamond market.[51]  Rubber trades on the Singapore Commodity Exchange in units of 1 kg priced in U.S. cents. Palm oil is traded on the Malaysian Ringgit (RM), Bursa Malaysia in units of 1 kg priced in U.S. cents. Wool is traded on the AUD in units of 1 kg. Polypropylene and Linear Low Density Polyethylene (LL) did trade on the London Metal Exchange in units of 1,000 kg priced in USD but was dropped in 2011.  Fossil fuels and other commodities have been major drivers of inflationary periods, including the 2021-2022 inflation spike exacerbated by the Russian Invasion of Ukraine.[59][60] Gernot Wagner argues that commodites are undesirable energy sources because of inflationary periods that come with commodity prices.[59][61]  In the United States, the principal regulator of commodity and futures markets is the Commodity Futures Trading Commission (CFTC). The National Futures Association (NFA) was formed in 1976 and is the futures industry's self-regulatory organization. The NFA's first regulatory operations began in 1982 and fall under the Commodity Exchange Act of the Commodity Futures Trading Commission Act.[62]  Dodd–Frank was enacted in response to the 2008 financial crisis. It called for \"strong measures to limit speculation in agricultural commodities\" calling upon the CFTC to further limit positions and to regulate over-the-counter trades.[63]  Markets in Financial Instruments Directive (MiFID) is the cornerstone of the European Commission's Financial Services Action Plan that regulate operations of the EU financial service markets. It was reviewed in 2012 by the European Parliament (EP) and the Economic and Financial Affairs Council (ECOFIN).[64] The European Parliament adopted a revised version of Mifid II on 26 October 2012 which include \"provisions for position limits on commodity derivatives\", aimed at \"preventing market abuse\" and supporting \"orderly pricing and settlement conditions\".[65]  The European Securities and Markets Authority (Esma), based in Paris and formed in 2011, is an \"EU-wide financial markets watchdog\". Esma sets position limits on commodity derivatives as described in Mifid II.[65]  The EP voted in favor of stronger regulation of commodity derivative markets in September 2012 to \"end abusive speculation in commodity markets\" that were \"driving global food prices increases and price volatility\". In July 2012, \"food prices globally soared by 10 percent\" (World Bank 2012). Senior British MEP Arlene McCarthy called for \"putting a brake on excessive food speculation and speculating giants profiting from hunger\" ending immoral practices that \"only serve the interests of profiteers\".[66] In March 2012, EP Member Markus Ferber suggested amendments to the European Commission's proposals, intended to strengthen restrictions on high-frequency trading and commodity price manipulation.[67] "},"meta":{},"created_at":"2025-03-22T14:25:42.288402Z","updated_at":"2025-03-22T14:25:42.288402Z","inner_id":63,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":72,"annotations":[{"id":72,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.320914Z","updated_at":"2025-03-22T14:25:42.320914Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"63832e9f-8b43-4311-a4a8-60cb509c80d7","import_id":null,"last_action":null,"bulk_created":false,"task":72,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Heterodox  Economic growth is the increase or improvement in the inflation-adjusted economy in a financial year.[2] The economic growth rate is typically calculated as real Gross domestic product (GDP) growth rate, real GDP per capita growth rate or GNI per capita growth. The \"rate\" of economic growth refers to the geometric annual rate of growth in GDP or GDP per capita between the first and the last year over a period of time. This growth rate represents the trend in the average level of GDP over the period, and ignores any fluctuations in the GDP around this trend. Growth is usually calculated in \"real\" value, which is inflation-adjusted, to eliminate the distorting effect of inflation on the prices of goods produced.[3] Real GDP per capita is the GDP of the entire country divided by the number of people in the country. Measurement of economic growth uses national income accounting.[4]  Economists refer to economic growth caused by more efficient use of inputs (increased productivity of labor, of physical capital, of energy or of materials) as intensive growth. In contrast, economic growth caused only by increases in the amount of inputs available for use (increased population, for example, or new territory) counts as extensive growth.[5] Innovation also generates economic growth. In the U.S. about 60% of consumer spending in 2013 went on goods and services that did not exist in 1869.[6]  In national income accounting, per capita output can be calculated using the following factors: output per unit of labor input (labor productivity), hours worked (intensity), the percentage of the working-age population actually working (participation rate) and the proportion of the working-age population to the total population (demographics). \"The rate of change of GDP\/population is the sum of the rates of change of these four variables plus their cross products.\"[7]  Economists distinguish between long-run economic growth and short-run economic changes in production. Short-run variation in economic growth is termed the business cycle. Generally, according to economists, the ups and downs in the business cycle can be attributed to fluctuations in aggregate demand. In contrast, economic growth is concerned with the long-run trend in production due to structural causes such as technological growth and factor accumulation.  Increases in labor productivity (the ratio of the value of output to labor input) have historically been the most important source of real per capita economic growth.[8][9][10][11][12] In a famous estimate, MIT Professor Robert Solow concluded that technological progress has accounted for 80 percent of the long-term rise in U.S. per capita income, with increased investment in capital explaining only the remaining 20 percent.[13]  Increases in productivity lower the real cost of goods. Over the 20th century, the real price of many goods fell by over 90%.[14]  Economic growth has traditionally been attributed to the accumulation of human and physical capital and the increase in productivity and creation of new goods arising from technological innovation.[15] Further division of labour (specialization) is also fundamental to rising productivity.[16]  Before industrialization technological progress resulted in an increase in the population, which was kept in check by food supply and other resources, which acted to limit per capita income, a condition known as the Malthusian trap.[17][18] The rapid economic growth that occurred during the Industrial Revolution was remarkable because it was in excess of population growth, providing an escape from the Malthusian trap.[19] Countries that industrialized eventually saw their population growth slow down, a phenomenon known as the demographic transition.  Increases in productivity are the major factor responsible for per capita economic growth—this has been especially evident since the mid-19th century. Most of the economic growth in the 20th century was due to increased output per unit of labor, materials, energy, and land (less input per widget). The balance of the growth in output has come from using more inputs. Both of these changes increase output. The increased output included more of the same goods produced previously and new goods and services.[20]  During the Industrial Revolution, mechanization began to replace hand methods in manufacturing, and new processes streamlined production of chemicals, iron, steel, and other products.[21] Machine tools made the economical production of metal parts possible, so that parts could be interchangeable.[22] (See: Interchangeable parts.)  During the Second Industrial Revolution, a major factor of productivity growth was the substitution of inanimate power for human and animal labor. Also there was a great increase in power as steam-powered electricity generation and internal combustion supplanted limited wind and water power.[21] Since that replacement, the great expansion of total power was driven by continuous improvements in energy conversion efficiency.[23] Other major historical sources of productivity were automation, transportation infrastructures (canals, railroads, and highways),[24][25] new materials (steel) and power, which includes steam and internal combustion engines and electricity. Other productivity improvements included mechanized agriculture and scientific agriculture including chemical fertilizers and livestock and poultry management, and the Green Revolution. Interchangeable parts made with machine tools powered by electric motors evolved into mass production, which is universally used today.[22]  Great sources of productivity improvement in the late 19th century were railroads, steam ships, horse-pulled reapers and combine harvesters, and steam-powered factories.[26][27] The invention of processes for making cheap steel were important for many forms of mechanization and transportation. By the late 19th century both prices and weekly work hours fell because less labor, materials, and energy were required to produce and transport goods. However, real wages rose, allowing workers to improve their diet, buy consumer goods and afford better housing.[26]  Mass production of the 1920s created overproduction, which was arguably one of several causes of the Great Depression of the 1930s.[28] Following the Great Depression, economic growth resumed, aided in part by increased demand for existing goods and services, such as automobiles, telephones, radios, electricity and household appliances. New goods and services included television, air conditioning and commercial aviation (after 1950), creating enough new demand to stabilize the work week.[29] The building of highway infrastructures also contributed to post-World War II growth, as did capital investments in manufacturing and chemical industries.[30] The post-World War II economy also benefited from the discovery of vast amounts of oil around the world, particularly in the Middle East. By John W. Kendrick's estimate, three-quarters of increase in U.S. per capita GDP from 1889 to 1957 was due to increased productivity.[12]  Economic growth in the United States slowed down after 1973.[31] In contrast, growth in Asia has been strong since then, starting with Japan and spreading to Four Asian Tigers, China, Southeast Asia, the Indian subcontinent and Asia Pacific.[32] In 1957 South Korea had a lower per capita GDP than Ghana,[33] and by 2008 it was 17 times as high as Ghana's.[34] The Japanese economic growth has slackened considerably since the late 1980s.  Productivity in the United States grew at an increasing rate throughout the 19th century and was most rapid in the early to middle decades of the 20th century.[35][36][37][38][39] U.S. productivity growth spiked towards the end of the century in 1996–2004, due to an acceleration in the rate of technological innovation known as Moore's law.[40][41][42][43] After 2004 U.S. productivity growth returned to the low levels of 1972–96.[40]  Capital in economics ordinarily refers to physical capital, which consists of structures (largest component of physical capital) and equipment used in business (machinery, factory equipment, computers and office equipment, construction equipment, business vehicles, medical equipment, etc.).[4] Up to a point increases in the amount of capital per worker are an important cause of economic output growth. Capital is subject to diminishing returns because of the amount that can be effectively invested and because of the growing burden of depreciation. In the development of economic theory, the distribution of income was considered to be between labor and the owners of land and capital.[44] In recent decades there have been several Asian countries with high rates of economic growth driven by capital investment.[45]  The work week declined considerably over the 19th century.[46][47] By the 1920s the average work week in the U.S. was 49 hours, but the work week was reduced to 40 hours (after which overtime premium was applied) as part of the National Industrial Recovery Act of 1933.  Demographic factors may influence growth by changing the employment to population ratio and the labor force participation rate.[8] Industrialization creates a demographic transition in which birth rates decline and the average age of the population increases.  Women with fewer children and better access to market employment tend to join the labor force in higher percentages. There is a reduced demand for child labor and children spend more years in school. The increase in the percentage of women in the labor force in the U.S. contributed to economic growth, as did the entrance of the baby boomers into the workforce.[8]  It has been observed that GDP growth is influenced by the size of the economy. The relation between GDP growth and GDP across the countries at a particular point of time is convex. Growth increases as GDP reaches its maximum and then begins to decline. There exists some extremum value. This is not exactly middle-income trap. It is observed for both developed and developing economies. Actually, countries having this property belong to conventional growth domain. However, the extremum could be extended by technological and policy innovations and some countries move into innovative growth domain with higher limiting values.[48]  Many theoretical and empirical analyses of economic growth attribute a major role to a country's level of human capital, defined as the skills of the population or the work force. Human capital has been included in both neoclassical and endogenous growth models.[49][50][51]  A country's level of human capital is difficult to measure since it is created at home, at school, and on the job. Economists have attempted to measure human capital using numerous proxies, including the population's level of literacy, its level of numeracy, its level of book production\/capita, its average level of formal schooling, its average test score on international tests, and its cumulative depreciated investment in formal schooling. The most commonly-used measure of human capital is the level (average years) of school attainment in a country, building upon the data development of Robert Barro and Jong-Wha Lee.[52] This measure is widely used because Barro and Lee provide data for numerous countries in five-year intervals for a long period of time.  One problem with the schooling attainment measure is that the amount of human capital acquired in a year of schooling is not the same at all levels of schooling and is not the same in all countries. This measure also presumes that human capital is only developed in formal schooling, contrary to the extensive evidence that families, neighborhoods, peers, and health also contribute to the development of human capital. Despite these potential limitations, Theodore Breton has shown that this measure can represent human capital in log-linear growth models because across countries GDP\/adult has a log-linear relationship to average years of schooling, which is consistent with the log-linear relationship between workers' personal incomes and years of schooling in the Mincer model.[53]  Eric Hanushek and Dennis Kimko introduced measures of students' mathematics and science skills from international assessments into growth analysis.[54] They found that this measure of human capital was very significantly related to economic growth. Eric Hanushek and Ludger Wößmann have extended this analysis.[55][56] Theodore Breton shows that the correlation between economic growth and students' average test scores in Hanushek and Wößmann's analyses is actually due to the relationship in countries with less than eight years of schooling. He shows that economic growth is not correlated with average scores in more educated countries.[53] Hanushek and Wößmann further investigate whether the relationship of knowledge capital to economic growth is causal. They show that the level of students' cognitive skills can explain the slow growth in Latin America and the rapid growth in East Asia.[57]  Joerg Baten and Jan Luiten van Zanden employ book production per capita as a proxy for sophisticated literacy capabilities and find that \"Countries with high levels of human capital formation in the 18th century initiated or participated in the industrialization process of the 19th century, whereas countries with low levels of human capital formation were unable to do so, among them many of today's Less Developed Countries such as India, Indonesia, and China.\"[58]  Here, health is approached as a functioning from Amartya Sen and Martha Nussbaum's capability approach that an individual has to realise the achievements like economic success. Thus health in a broader sense is not the absence of illness, but the opportunity for people to biologically develop to their full potential their entire lives [59] It is established that human capital is an important asset for economic growth, however, it can only be so if that population is healthy and well-nourished. One of the most important aspects of health is the mortality rate and how the rise or decline can affect the labour supply predominant in a developing economy.[60] Mortality decline triggers greater investments in individual human capital and an increase in economic growth. Matteo Cervellati and Uwe Sunde[61] and Rodrigo.R Soares[62] consider frameworks in which mortality decline has an influence on parents to have fewer children and to provide quality education for those children, as a result instituting an economic-demographic transition.  The relationship between health and economic growth is further nuanced by distinguishing the influence of specific diseases on GDP per capita from that of aggregate measures of health, such as life expectancy[63] Thus, investing in health is warranted both from the growth and equity perspectives, given the important role played by health in the economy. Protecting health assets from the impact of systemic transitional costs on economic reforms, pandemics, economic crises and natural disasters is also crucial. Protection from the shocks produced by illness and death, are usually taken care of within a country’s social insurance system. In areas such as Sub-Saharan Africa, where the prevalence of HIV and AIDS, has a comparative negative impact on economical development. It will be interesting to see how research in the areas of health in near future uncover how the world will be performing living with the SARS-CoV-2, especially looking at the economic impacts it already has in a space of two years. Ultimately, when people live longer on average, human capital expenditures are more likely to pay off, and all of these mechanisms center around the complementarity of longevity, health, and education, for which there is ample empirical evidence.[63][59][61][62][60]  \"As institutions influence behavior and incentives in real life, they forge the success or failure of nations.\"[64] In economics and economic history, the transition from earlier economic systems to capitalism was facilitated by the adoption of government policies which fostered  commerce and gave individuals more personal and economic freedom. These included new laws favorable to the establishment of business, including contract law, laws providing for the protection of private property, and the abolishment of anti-usury laws.[65][66]  Much of the literature on economic growth refers to the success story of the British state after the Glorious Revolution of 1688, in which high fiscal capacity combined with constraints on the power of the king generated some respect for the rule of law.[67][68][69][64] However, others have questioned that this institutional formula is not so easily replicable elsewhere as a change in the Constitution—and the type of institutions created by that change—does not necessarily create a change in political power if the economic powers of that society are not aligned with the new set of rule of law institutions.[70] In England, a dramatic increase in the state's fiscal capacity followed the creation of constraints on the crown, but elsewhere in Europe increases in state capacity happened before major rule of law reforms.[71]  There are many different ways through which states achieved state (fiscal) capacity and this different capacity accelerated or hindered their economic development. Thanks to the underlying homogeneity of its land and people, England was able to achieve a unified legal and fiscal system since the Middle Ages that enabled it to substantially increase the taxes it raised after 1689.[71] On the other hand, the French experience of state building faced much stronger resistance from local feudal powers keeping it legally and fiscally fragmented until the French Revolution despite significant increases in state capacity during the seventeenth century.[72][73] Furthermore, Prussia and the Habsburg empire—much more heterogeneous states than England—were able to increase state capacity during the eighteenth century without constraining the powers of the executive.[71] Nevertheless, it is unlikely that a country will generate institutions that respect property rights and the rule of law without having had first intermediate fiscal and political institutions that create incentives for elites to support them. Many of these intermediate level institutions relied on informal private-order arrangements that combined with public-order institutions associated with states, to lay the foundations of modern rule of law states.[71]  In many poor and developing countries much land and housing are held outside the formal or legal property ownership registration system. In many urban areas the poor \"invade\" private or government land to build their houses, so they do not hold title to these properties. Much unregistered property is held in informal form through various property associations and other arrangements. Reasons for extra-legal ownership include excessive bureaucratic red tape in buying property and building; failures to notarize transaction documents; or having documents notarized but failing to have them recorded with the official agency.[74] According to Hernando De Soto, unclear property rights limits economic growth, as people cannot use land as collateral to secure loans, depriving many poor countries of one of their most important potential sources of capital. Unregistered businesses and lack of accepted accounting methods are other factors that limit potential capital.[74] Businesses and individuals participating in unreported business activity and owners of unregistered property face costs such as bribes and pay-offs that offset much of any taxes avoided.[74]  \"Democracy Does Cause Growth\", according to Acemoglu et al. Specifically, they state that \"democracy increases future GDP by encouraging investment, increasing schooling, inducing economic reforms, improving public goods provision, and reducing social unrest\".[75] UNESCO and the United Nations also consider that cultural property protection, high-quality education, cultural diversity and social cohesion in armed conflicts are particularly necessary for qualitative growth.[76]  According to Daron Acemoglu, Simon Johnson and James Robinson, the positive correlation between high income and cold climate is a by-product of history. Europeans adopted very different colonization policies in different colonies, with different associated institutions. In places where these colonizers faced high mortality rates (e.g., due to the presence of tropical diseases), they could not settle permanently, and they were thus more likely to establish extractive institutions, which persisted after independence; in places where they could settle permanently (e.g. those with temperate climates), they established institutions with this objective in mind and modeled them after those in their European homelands. In these 'neo-Europes' better institutions in turn produced better development outcomes. Thus, although other economists focus on the identity or type of legal system of the colonizers to explain institutions, these authors look at the environmental conditions in the colonies to explain institutions. For instance, former colonies have inherited corrupt governments and geopolitical boundaries (set by the colonizers) that are not properly placed regarding the geographical locations of different ethnic groups, creating internal disputes and conflicts that hinder development. In another example, societies that emerged in colonies without solid native populations established better property rights and incentives for long-term investment than those where native populations were large.[77]  In Why Nations Fail, Acemoglu and Robinson said that the English in North America started by trying to repeat the success of the Spanish Conquistadors in extracting wealth (especially gold and silver) from the countries they had conquered. This system repeatedly failed for the English. Their successes rested on giving land and a voice in the government to every male settler to incentivize productive labor. In Virginia it took twelve years and many deaths from starvation before the governor decided to try democracy.[78]  Economic growth, its sustainability and its distribution remain central aspects of government policy. For example, the UK Government recognises that \"Government can play an important role in supporting economic growth by helping to level the playing field through the way it buys public goods, works and services\",[79] and \"Post-Pandemic Economic Growth\" has been featured in a series of inquiries undertaken by the parliamentary Business, Energy and Industrial Strategy Committee, which argues that the UK Government \"has a big job to do in helping businesses survive, stimulating economic growth and encouraging the creation of well-paid meaningful jobs\".[80]  Policymakers and scholars frequently emphasize the importance of entrepreneurship for economic growth. However, surprisingly few research empirically examine and quantify entrepreneurship's impact on growth. This is due to endogeneity—forces that drive economic growth also drive entrepreneurship. In other words, the empirical analysis of the impact of entrepreneurship on growth is difficult because of the joint determination of entrepreneurship and economic growth. A few papers use quasi-experimental designs, and have found that entrepreneurship and the density of small businesses indeed have a causal impact on regional growth.[82][83]  Another major cause of economic growth is the introduction of new products and services and the improvement of existing products. New products create demand, which is necessary to offset the decline in employment that occurs through labor-saving technology (and to a lesser extent employment declines due to savings in energy and materials).[41][84] In the U.S. by 2013 about 60% of consumer spending was for goods and services that did not exist in 1869. Also, the creation of new services has been more important than invention of new goods.[85]  Economic growth in the U.S. and other developed countries went through phases that affected growth through changes in the labor force participation rate and the relative sizes of economic sectors. The transition from an agricultural economy to manufacturing increased the size of the sector with high output per hour (the high-productivity manufacturing sector), while reducing the size of the sector with lower output per hour (the lower productivity agricultural sector). Eventually high productivity growth in manufacturing reduced the sector size, as prices fell and employment shrank relative to other sectors.[86][87] The service and government sectors, where output per hour and productivity growth is low, saw increases in their shares of the economy and employment during the 1990s.[8] The public sector has since contracted, while the service economy expanded in the 2000s.  Adam Smith pioneered modern economic growth and performance theory in his book The Wealth of Nations, first published in 1776. For Smith, the main factors of economic growth are division of labour and capital accumulation. However, these are conditioned by what he calls \"the extent of the market\". This is conditioned notably by geographic factors but also institutional ones such as the political-legal environment.[88]  Malthusianism is the idea that population growth is potentially exponential while the growth of the food supply or other resources is linear, which eventually reduces living standards to the point of triggering a population die off. The Malthusian theory also proposes that over most of human history technological progress caused larger population growth but had no impact on income per capita in the long run. According to the theory, while technologically advanced economies over this epoch were characterized by higher population density, their level of income per capita was not different from those among technologically regressed society.  The conceptual foundations of the Malthusian theory were formed by Thomas Malthus,[89] and a modern representation of these approach is provided by Ashraf and Galor.[90] In line with the predictions of the Malthusian theory, a cross-country analysis finds a significant positive effect of the technological level on population density and an insignificant effect on income per capita significantly over the years 1–1500.[90]  In classical (Ricardian) economics, the theory of production and the theory of growth are based on the theory of sustainability and law of variable proportions, whereby increasing either of the factors of production (labor or capital), while holding the other constant and assuming no technological change, will increase output, but at a diminishing rate that eventually will approach zero. These concepts have their origins in Thomas Malthus’s theorizing about agriculture. Malthus's examples included the number of seeds harvested relative to the number of seeds planted (capital) on a plot of land and the size of the harvest from a plot of land versus the number of workers employed.[91] (See also Diminishing returns)  Criticisms of classical growth theory are that technology, an important factor in economic growth, is held constant and that economies of scale are ignored.[92]  One popular theory in the 1940s was the big push model, which suggested that countries needed to jump from one stage of development to another through a virtuous cycle, in which large investments in infrastructure and education coupled with private investments would move the economy to a more productive stage, breaking free from economic paradigms appropriate to a lower productivity stage.[93] The idea was revived and formulated rigorously, in the late 1980s by Kevin Murphy, Andrei Shleifer and Robert Vishny.[94]  Robert Solow and Trevor Swan developed what eventually became the main model used in growth economics in the 1950s.[95][96] This model assumes that there are diminishing returns to capital and labor. Capital accumulates through investment, but its level or stock continually decreases due to depreciation. Due to the diminishing returns to capital, with increases in capital\/worker and absent technological progress, economic output\/worker eventually reaches a point where capital per worker and economic output\/worker remain constant because annual investment in capital equals annual depreciation. This condition is called the 'steady state'.  In the Solow–Swan model if productivity increases through technological progress, then output\/worker increases even when the economy is in the steady state. If productivity increases at a constant rate, output\/worker also increases at a related steady-state rate. As a consequence, growth in the model can occur either by increasing the share of GDP invested or through technological progress. But at whatever share of GDP invested, capital\/worker eventually converges on the steady state, leaving the growth rate of output\/worker determined only by the rate of technological progress. As a consequence, with world technology available to all and progressing at a constant rate, all countries have the same steady state rate of growth. Each country has a different level of GDP\/worker determined by the share of GDP it invests, but all countries have the same rate of economic growth. Implicitly in this model rich countries are those that have invested a high share of GDP for a long time. Poor countries can become rich by increasing the share of GDP they invest. One important prediction of the model, mostly borne out by the data, is that of conditional convergence; the idea that poor countries will grow faster and catch up with rich countries as long as they have similar investment (and saving) rates and access to the same technology.  The Solow–Swan model is considered an \"exogenous\" growth model because it does not explain why countries invest different shares of GDP in capital nor why technology improves over time. Instead, the rate of investment and the rate of technological progress are exogenous. The value of the model is that it predicts the pattern of economic growth once these two rates are specified. Its failure to explain the determinants of these rates is one of its limitations.  Although the rate of investment in the model is exogenous, under certain conditions the model implicitly predicts convergence in the rates of investment across countries. In a global economy with a global financial capital market, financial capital flows to the countries with the highest return on investment. In the Solow-Swan model countries with less capital\/worker (poor countries) have a higher return on investment due to the diminishing returns to capital. As a consequence, capital\/worker and output\/worker in a global financial capital market should converge to the same level in all countries.[97] Since historically financial capital has not flowed to the countries with less capital\/worker, the basic Solow–Swan model has a conceptual flaw. Beginning in the 1990s, this flaw has been addressed by adding additional variables to the model that can explain why some countries are less productive than others and, therefore, do not attract flows of global financial capital even though they have less (physical) capital\/worker.  In practice, convergence was rarely achieved. In 1957, Solow applied his model to data from the U.S. gross national product to estimate contributions. This showed that the increase in capital and labor stock only accounted for about half of the output, while the population increase adjustments to capital explained eighth. This remaining unaccounted growth output is known as the Solow Residual. Here the A of (t) \"technical progress\" was the reason for increased output. Nevertheless, the model still had flaws. It gave no room for policy to influence the growth rate. Few attempts were also made by the RAND Corporation the non-profit think tank and frequently visiting economist Kenneth Arrow to work out the kinks in the model. They suggested that new knowledge was indivisible and that it is endogenous with a certain fixed cost. Arrow's further explained that new knowledge obtained by firms comes from practice and built a model that \"knowledge\" accumulated through experience.[98]  Unsatisfied with the assumption of exogenous technological progress in the Solow–Swan model, economists worked to \"endogenize\" (i.e., explain it \"from within\" the models) productivity growth in the 1980s. The resulting endogenous growth theory, most notably advanced by Robert Lucas, Jr. and his student Paul Romer, includes a mathematical explanation of technological advancement.[15][99] This model was notable for its incorporation of human capital, which is interpreted from changes to investment patterns in education, training, and healthcare by private sector firms or governments. Notwithstanding the implications this component has for policy, the endogenous perspective on human capital investment emphasizes the possibility for broad-based effects which can be realized by other firms in the economy. Accordingly, human capital is theorized to deliver increasing rates of return unlike physical capital. Research done in this area has focused on what increases human capital (e.g. education) or technological change (e.g. innovation).[100] The quantity theory of endogenous productivity growth was proposed by Russian economist Vladimir Pokrovskii. It explains growth as a consequence of the dynamics of three factors, including the technological characteristics of production equipment. Without any arbitrary parameters, historical rates of economic growth can be predicted with considerable precision.[101][102][103]  On Memorial Day weekend in 1988, a conference in Buffalo brought together influential thinkers to evaluate the conflicting theories of growth. Romer, Krugman, Barro, and Becker were in attendance along with many other high profiled economists of the time. Amongst many papers that day the one that stood out was Romer's \"Micro Foundations for Aggregate Technological Change.\" The Micro Foundation claimed that endogenous technological change had the concept of Intellectual Property imbedded and that knowledge is an input and output of production. Romer argued that outcomes to the national growth rates were significantly affected by public policy, trade activity, and intellectual property. He stressed that cumulative capital and specialization were key, and that not only population growth can increase capital of knowledge, it was human capital that is specifically trained in harvesting new ideas.[104]  While intellectual property may be important, Baker (2016) cites multiple sources claiming that \"stronger patent protection seems to be associated with slower growth\". That's particularly true for patents in the ethical health care industry. In effect taxpayers pay twice for new drugs and diagnostic procedures: First in tax subsidies and second for the high prices of diagnostic procedures treatments. If the results of research paid by taxpayers were placed in the public domain, Baker claims that people everywhere would be healthier, because better diagnoses and treatment would be more affordable the world over.[105]  One branch of endogenous growth theory was developed on the foundations of the Schumpeterian theory, named after the 20th-century Austrian economist Joseph Schumpeter.[106] The approach explains growth as a consequence of innovation and a process of creative destruction that captures the dual nature of technological progress: in terms of creation, entrepreneurs introduce new products or processes in the hope that they will enjoy temporary monopoly-like profits as they capture markets. In doing so, they make old technologies or products obsolete. This can be seen as an annulment of previous technologies, which makes them obsolete, and \"destroys the rents generated by previous innovations\".[107]: 855 [108] A major model that illustrates Schumpeterian growth is the Aghion–Howitt model [ru].[109][107]  Unified growth theory was developed by Oded Galor and his co-authors to address the inability of endogenous growth theory to explain key empirical regularities in the growth processes of individual economies and the world economy as a whole.[110][111] Unlike endogenous growth theory that focuses entirely on the modern growth regime and is therefore unable to explain the roots of inequality across nations, unified growth theory captures in a single framework the fundamental phases of the process of development in the course of human history: (i) the Malthusian epoch that was prevalent over most of human history, (ii) the escape from the Malthusian trap, (iii) the emergence of human capital as a central element in the growth process, (iv) the onset of the fertility decline, (v) the origins of the modern era of sustained economic growth, and (vi) the roots of divergence in income per capita across nations in the past two centuries. The theory suggests that during most of human existence, technological progress was offset by population growth, and living standards were near subsistence across time and space. However, the reinforcing interaction between the rate of technological progress and the size and composition of the population has gradually increased the pace of technological progress, enhancing the importance of education in the ability of individuals to adapt to the changing technological environment. The rise in the allocation of resources towards education triggered a fertility decline enabling economies to allocate a larger share of the fruits of technological progress to a steady increase in income per capita, rather than towards the growth of population, paving the way for the emergence of sustained economic growth. The theory further suggests that variations in biogeographical characteristics, as well as cultural and institutional characteristics, have generated a differential pace of transition from stagnation to growth across countries and consequently divergence in their income per capita over the past two centuries.[110][111]  The prevailing views about the role of inequality in the growth process has radically shifted in the past century.[112]  The classical perspective, as expressed by Adam Smith, and others, suggests that inequality fosters the growth process.[113][114] Specifically, since the aggregate saving increases with inequality due to higher property to save among the wealthy, the classical viewpoint suggests that inequality stimulates capital accumulation and therefore economic growth.[115]  The Neoclassical perspective that is based on representative agent approach denies the role of inequality in the growth process. It suggests that while the growth process may affect inequality, income distribution has no impact on the growth process.  The modern perspective which has emerged in the late 1980s suggests, in contrast, that income distribution has a significant impact on the growth process. The modern perspective, originated by Galor and Zeira,[116][117] highlights the important role of heterogeneity in the determination of aggregate economic activity, and economic growth. In particular, Galor and Zeira argue that since credit markets are imperfect, inequality has an enduring impact on human capital formation, the level of income per capita, and the growth process.[118] In contrast to the classical paradigm, which underlined the positive implications of inequality for capital formation and economic growth, Galor and Zeira argue that inequality has an adverse effect on human capital formation and the development process, in all but the very poor economies.  Later theoretical developments have reinforced the view that inequality has an adverse effect on the growth process. Specifically, Alesina and Rodrik and Persson and Tabellini advance a political economy mechanism and argue that inequality has a negative impact on economic development since it creates a pressure for distortionary redistributive policies that have an adverse effect on investment and economic growth.[119][120]  In accordance with the credit market imperfection approach, a study by Roberto Perotti showed that inequality is associated with lower level of human capital formation (education, experience, apprenticeship) and higher level of fertility, while lower level of human capital is associated with lower growth and lower levels of economic growth. In contrast, his examination of the political economy channel found no support for the political economy mechanism.[121] Consequently, the political economy perspective on the relationship between inequality and growth have been revised and later studies have established that inequality may provide an incentive for the elite to block redistributive policies and institutional changes. In particular, inequality in the distribution of land ownership provides the landed elite with an incentive to limit the mobility of rural workers by depriving them from education and by blocking the development of the industrial sector.[122]  A unified theory of inequality and growth that captures that changing role of inequality in the growth process offers a reconciliation between the conflicting predictions of classical viewpoint that maintained that inequality is beneficial for growth and the modern viewpoint that suggests that in the presence of credit market imperfections, inequality predominantly results in underinvestment in human capital and lower economic growth. This unified theory of inequality and growth, developed by Oded Galor and Omer Moav,[123] suggests that the effect of inequality on the growth process has been reversed as human capital has replaced physical capital as the main engine of economic growth. In the initial phases of industrialization, when physical capital accumulation was the dominating source of economic growth, inequality boosted the development process by directing resources toward individuals with higher propensity to save. However, in later phases, as human capital become the main engine of economic growth, more equal distribution of income, in the presence of credit constraints, stimulated investment in human capital and economic growth.  In 2013, French economist Thomas Piketty postulated that in periods when the average annual rate on return on investment in capital (r) exceeds the average annual growth in economic output (g), the rate of inequality will increase.[124] According to Piketty, this is the case because wealth that is already held or inherited, which is expected to grow at the rate r, will grow at a rate faster than wealth accumulated through labor, which is more closely tied to g. An advocate of reducing inequality levels, Piketty suggests levying a global wealth tax in order to reduce the divergence in wealth caused by inequality.  The reduced form empirical relationship between inequality and growth was studied by Alberto Alesina and Dani Rodrik, and Torsten Persson and Guido Tabellini.[119][120] They find that inequality is negatively associated with economic growth in a cross-country analysis.  Robert Barro reexamined the reduced form relationship between inequality on economic growth in a panel of countries.[125] He argues that there is \"little overall relation between income inequality and rates of growth and investment\". However, his empirical strategy limits its applicability to the understanding of the relationship between inequality and growth for several reasons. First, his regression analysis control for education, fertility, investment, and it therefore excludes, by construction, the important effect of inequality on growth via education, fertility, and investment. His findings simply imply that inequality has no direct effect on growth beyond the important indirect effects through the main channels proposed in the literature. Second, his study analyzes the effect of inequality on the average growth rate in the following 10 years. However, existing theories suggest that the effect of inequality will be observed much later, as is the case in human capital formation, for instance. Third, the empirical analysis does not account for biases that are generated by reverse causality and omitted variables.  Recent papers based on superior data, find negative relationship between inequality and growth. Andrew Berg and Jonathan Ostry of the International Monetary Fund, find that \"lower net inequality is robustly correlated with faster and more durable growth, controlling for the level of redistribution\".[126] Likewise, Dierk Herzer and Sebastian Vollmer find that increased income inequality reduces economic growth.[127]  The Galor and Zeira's model predicts that the effect of rising inequality on GDP per capita is negative in relatively rich countries but positive in poor countries.[116][117] These testable predictions have been examined and confirmed empirically in recent studies.[128][129] In particular, Brückner and Lederman test the prediction of the model by in the panel of countries during the period 1970–2010, by considering the impact of the interaction between the level of income inequality and the initial level of GDP per capita. In line with the predictions of the model, they find that at the 25th percentile of initial income in the world sample, a 1 percentage point increase in the Gini coefficient increases income per capita by 2.3%, whereas at the 75th percentile of initial income a 1 percentage point increase in the Gini coefficient decreases income per capita by -5.3%. Moreover, the proposed human capital mechanism that mediates the effect of inequality on growth in the Galor-Zeira model is also confirmed. Increases in income inequality increase human capital in poor countries but reduce it in high and middle-income countries.  This recent support for the predictions of the Galor-Zeira model is in line with earlier findings. Roberto Perotti showed that in accordance with the credit market imperfection approach, developed by Galor and Zeira, inequality is associated with lower level of human capital formation (education, experience, apprenticeship) and higher level of fertility, while lower level of human capital is associated with lower levels of economic growth.[121] Princeton economist Roland Benabou's finds that the growth process of Korea and the Philippines \"are broadly consistent with the credit-constrained human-capital accumulation hypothesis\".[130] In addition, Andrew Berg and Jonathan Ostry[126] suggest that inequality seems to affect growth through human capital accumulation and fertility channels.  In contrast, Perotti argues that the political economy mechanism is not supported empirically. Inequality is associated with lower redistribution, and lower redistribution (under-investment in education and infrastructure) is associated with lower economic growth.[121]  Living standards vary widely from country to country, and furthermore, the change in living standards over time varies widely from country to country. Below is a table which shows GDP per person and annualized per person GDP growth for a selection of countries over a period of about 100 years. The GDP per person data are adjusted for inflation, hence they are \"real\".  Seemingly small differences in yearly GDP growth lead to large changes in GDP when compounded over time. For instance, in the above table, GDP per person in the United Kingdom in the year 1870 was $4,808. At the same time in the United States, GDP per person was $4,007, lower than the UK by about 20%. However, in 2008 the positions were reversed: GDP per person was $36,130 in the United Kingdom and $46,970 in the United States, i.e. GDP per person in the US was 30% more than it was in the UK. As the above table shows, this means that GDP per person grew, on average, by 1.80% per year in the US and by 1.47% in the UK. Thus, a difference in GDP growth by only a few tenths of a percent per year results in large differences in outcomes when the growth is persistent over a generation. This and other observations have led some economists to view GDP growth as the most important part of the field of macroeconomics:  ...if we can learn about government policy options that have even small effects on long-term growth rates, we can contribute much more to improvements in standards of living than has been provided by the entire history of macroeconomic analysis of countercyclical policy and fine-tuning. Economic growth [is] the part of macroeconomics that really matters.[132] Over long periods of time, even small rates of growth, such as a 2% annual increase, have large effects. For example, the United Kingdom experienced a 1.97% average annual increase in its inflation-adjusted GDP between 1830 and 2008.[133] In 1830, the GDP was 41,373 million pounds. It grew to 1,330,088 million pounds by 2008. A growth rate that averaged 1.97% over 178 years resulted in a 32-fold increase in GDP by 2008.  The large impact of a relatively small growth rate over a long period of time is due to the power of exponential growth. The rule of 72, a mathematical result, states that if something grows at the rate of x% per year, then its level will double every 72\/x years. For example, a growth rate of 2.5% per annum leads to a doubling of the GDP within 28.8 years, whilst a growth rate of 8% per year leads to a doubling of GDP within nine years. Thus, a small difference in economic growth rates between countries can result in very different standards of living for their populations if this small difference continues for many years.  One theory that relates economic growth with quality of life is the \"Threshold Hypothesis\", which states that economic growth up to a point brings with it an increase in quality of life. But at that point – called the threshold point – further economic growth can bring with it a deterioration in quality of life.[134] This results in an upside-down-U-shaped curve, where the vertex of the curve represents the level of growth that should be targeted. Happiness has been shown to increase with GDP per capita, at least up to a level of $15,000 per person.[135]  Economic growth has the indirect potential to alleviate poverty, as a result of a simultaneous increase in employment opportunities and increased labor productivity.[136] A study by researchers at the Overseas Development Institute (ODI) of 24 countries that experienced growth found that in 18 cases, poverty was alleviated.[136]  In some instances, quality of life factors such as healthcare outcomes and educational attainment, as well as social and political liberties, do not improve as economic growth occurs.[137][dubious – discuss]  Productivity increases do not always lead to increased wages, as can be seen in the United States, where the gap between productivity and wages has been rising since the 1980s.[136]  While acknowledging the central role economic growth can potentially play in human development, poverty reduction and the achievement of the Millennium Development Goals, it is becoming widely understood amongst the development community that special efforts must be made to ensure poorer sections of society are able to participate in economic growth.[138][139][140] The effect of economic growth on poverty reduction – the growth elasticity of poverty – can depend on the existing level of inequality.[141][142] For instance, with low inequality a country with a growth rate of 2% per head and 40% of its population living in poverty, can halve poverty in ten years, but a country with high inequality would take nearly 60 years to achieve the same reduction.[143][144] In the words of the Secretary-General of the United Nations Ban Ki-moon: \"While economic growth is necessary, it is not sufficient for progress on reducing poverty.\"[138]  Critics such as the Club of Rome argue that a narrow view of economic growth, combined with globalization, is creating a scenario where we could see a systemic collapse of our planet's natural resources.[145][146]  Concerns about negative environmental effects of growth have prompted some people to advocate lower levels of growth, or the abandoning of growth altogether. In academia, concepts like uneconomic growth, steady-state economy, eco-taxes, green investments, basic income guarantees, along with more radical approaches associated with degrowth, commoning, eco-socialism and eco-anarchism have been developed in order to achieve this and to overcome possible growth imperatives.[147][148][149][150] In politics, green parties embrace the Global Greens Charter, recognising that \"... the dogma of economic growth at any cost and the excessive and wasteful use of natural resources without considering Earth's carrying capacity, are causing extreme deterioration in the environment and a massive extinction of species.\"[151]: 2   The 2019 Global Assessment Report on Biodiversity and Ecosystem Services published by the United Nations' Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services warned that given the substantial loss of biodiversity, society should not focus solely on economic growth.[152][153] Anthropologist Eduardo S. Brondizio, one of the co-chairs of the report, said \"We need to change our narratives. Both our individual narratives that associate wasteful consumption with quality of life and with status, and the narratives of the economic systems that still consider that environmental degradation and social inequality are inevitable outcomes of economic growth. Economic growth is a means and not an end. We need to look for the quality of life of the planet.\"[154]  Those more optimistic about the environmental impacts of growth believe that, though localized environmental effects may occur, large-scale ecological effects are minor. The argument, as posited by commentator Julian Lincoln Simon, stated in 1981 that if these global-scale ecological effects exist, human ingenuity will find ways to adapt to them.[155] Conversely Partha Dasgupta, in a 2021 report on the economics of biodiversity commissioned by the British Treasury, argued that biodiversity is collapsing faster than at any time in human history as a result of the demands of contemporary human civilization, which \"far exceed nature's capacity to supply us with the goods and services we all rely on. We would require 1.6 Earths to maintain the world's current living standards.\" He says that major transformative changes will be needed \"akin to, or even greater than, those of the Marshall Plan,\" including abandoning GDP as a measure of economic success and societal progress.[156] Philip Cafaro, professor of philosophy at the School of Global Environmental Sustainability at Colorado State University, wrote in 2022 that a scientific consensus has emerged which demonstrates that humanity is on the precipice of unleashing a major extinction event, and that \"the cause of global biodiversity loss is clear: other species are being displaced by a rapidly growing human economy.\"[157]  In 2019, a warning on climate change signed by 11,000 scientists from over 150 nations said economic growth is the driving force behind the \"excessive extraction of materials and overexploitation of ecosystems\" and that this \"must be quickly curtailed to maintain long-term sustainability of the biosphere.\" They add that \"our goals need to shift from GDP growth and the pursuit of affluence toward sustaining ecosystems and improving human well-being by prioritizing basic needs and reducing inequality.\"[158][159] A 2021 paper authored by top scientists in Frontiers in Conservation Science posited that given the environmental crises including biodiversity loss and climate change, and possible \"ghastly future\" facing humanity, there must be \"fundamental changes to global capitalism,\" including the \"abolition of perpetual economic growth.\"[160][161][162]  Up to the present, there is a close correlation between economic growth and the rate of carbon dioxide emissions across nations, although there is also a considerable divergence in carbon intensity (carbon emissions per GDP).[163] Up to the present, there is also a direct relation between global economic wealth and the rate of global emissions.[164] The Stern Review notes that the prediction that, \"Under business as usual, global emissions will be sufficient to propel greenhouse gas concentrations to over 550 ppm CO2 by 2050 and over 650–700 ppm by the end of this century is robust to a wide range of changes in model assumptions.\" The scientific consensus is that planetary ecosystem functioning without incurring dangerous risks requires stabilization at 450–550 ppm.[165]  As a consequence, growth-oriented environmental economists propose government intervention into switching sources of energy production, favouring wind, solar, hydroelectric, and nuclear. This would largely confine use of fossil fuels to either domestic cooking needs (such as for kerosene burners) or where carbon capture and storage technology can be cost-effective and reliable.[166] The Stern Review, published by the United Kingdom Government in 2006, concluded that an investment of 1% of GDP (later changed to 2%) would be sufficient to avoid the worst effects of climate change, and that failure to do so could risk climate-related costs equal to 20% of GDP. Because carbon capture and storage are as yet widely unproven, and its long term effectiveness (such as in containing carbon dioxide 'leaks') unknown, and because of current costs of alternative fuels, these policy responses largely rest on faith of technological change.  British conservative politician and journalist Nigel Lawson has deemed carbon emission trading an 'inefficient system of rationing'. Instead, he favours carbon taxes to make full use of the efficiency of the market. However, in order to avoid the migration of energy-intensive industries, the whole world should impose such a tax, not just Britain, Lawson pointed out. There is no point in taking the lead if nobody follows suit.[167]  Many earlier predictions of resource depletion, such as Thomas Malthus' 1798 predictions about approaching famines in Europe, The Population Bomb,[168][169] and the Simon–Ehrlich wager (1980)[170] have not materialized. Diminished production of most resources has not occurred so far, one reason being that advancements in technology and science have allowed some previously unavailable resources to be produced.[170] In some cases, substitution of more abundant materials, such as plastics for cast metals, lowered growth of usage for some metals. In the case of the limited resource of land, famine was relieved firstly by the revolution in transportation caused by railroads and steam ships, and later by the Green Revolution and chemical fertilizers, especially the Haber process for ammonia synthesis.[171][172]  Resource quality is composed of a variety of factors including ore grades, location, altitude above or below sea level, proximity to railroads, highways, water supply and climate. These factors affect the capital and operating cost of extracting resources. In the case of minerals, lower grades of mineral resources are being extracted, requiring higher inputs of capital and energy for both extraction and processing. Copper ore grades have declined significantly over the last century.[173][174] Another example is natural gas from shale and other low permeability rock, whose extraction requires much higher inputs of energy, capital, and materials than conventional gas in previous decades. Offshore oil and gas have exponentially increased cost as water depth increases.  Some physical scientists like Sanyam Mittal regard continuous economic growth as unsustainable.[175][176] Several factors may constrain economic growth – for example: finite, peaked, or depleted resources.  In 1972, The Limits to Growth study modeled limitations to infinite growth; originally ridiculed,[168][169][177] some of the predicted trends have materialized, raising concerns of an impending collapse or decline due to resource constraints.[178][179][180]  Malthusians such as William R. Catton, Jr. are skeptical of technological advances that improve resource availability. Such advances and increases in efficiency, they suggest, merely accelerate the drawing down of finite resources. Catton claims that increasing rates of resource extraction are \"...stealing ravenously from the future\".[181]  Energy economic theories hold that rates of energy consumption and energy efficiency are linked causally to economic growth.[182] The Garrett Relation holds that there has been a fixed relationship between current rates of global energy consumption and the historical accumulation of world GDP, independent of the year considered. It follows that economic growth, as represented by GDP growth, requires higher rates of energy consumption growth. Seemingly paradoxically, these are sustained through increases in energy efficiency.[183] Increases in energy efficiency were a portion of the increase in Total factor productivity.[12] Some of the most technologically important innovations in history involved increases in energy efficiency. These include the great improvements in efficiency of conversion of heat to work, the reuse of heat, the reduction in friction and the transmission of power, especially through electrification.[184][185] There is a strong correlation between per capita electricity consumption and economic development.[186][187]  *Top country subdivisions by GDP *Top country subdivisions by GDP per capita *Top country metropolitan by GDP "},"meta":{},"created_at":"2025-03-22T14:25:42.289399Z","updated_at":"2025-03-22T14:25:42.289399Z","inner_id":64,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":73,"annotations":[{"id":73,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.320914Z","updated_at":"2025-03-22T14:25:42.320914Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"44f592fd-9267-4203-ae83-c242503f7638","import_id":null,"last_action":null,"bulk_created":false,"task":73,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A leveraged buyout (LBO) is the acquisition of a company using a significant proportion of borrowed money (leverage) to fund the acquisition with the remainder of the purchase price funded with private equity. The assets of the acquired company are often used as collateral for the financing, along with any equity contributed by the acquiror.[1]  While corporate acquisitions often employ leverage to finance the purchase of the target, the term \"leveraged buyout\" is typically only employed when the acquiror is a financial sponsor (a private equity investment firm).   The use of debt, which normally has a lower cost of capital than equity, serves to reduce the overall cost of financing for the acquisition and enhance returns for the private equity investor. The equity investor can increase their projected returns by employing more leverage, creating incentives to maximize the proportion of debt relative to equity (i.e., debt-to-equity ratio).  While the lenders have an incentive to limit the amount of leverage they will provide, in certain cases the acquired company may be \"overleveraged\", meaning that the amount of leverage assumed by the target company was too high for the cash flows generated by the company to service the debt.[2]  As a result, the increased use of leverage increases the risk of default should the company perform poorly after the buyout.[3]  Since the early 2000s, the debt-to-equity ratio in leveraged buyouts has declined significantly, resulting in increased focus on operational improvements and follow-on M&A activity to generate attractive returns.[4]  LBOs can come in various forms.  The first time a company is acquired through a leveraged buyout it can be referred to as a \"primary buyout\" although this term is not commonly used, whereas the term \"secondary buyout\" is very commonly used to refer to the leveraged buyout of a company already owned by a private equity sponsor (please note the distinction between \"secondary buyouts\" and \"private equity secondaries\"). The terms \"management buyout\" (MBO) and \"management buy-in\" (MBI) involve transactions in which the current or former management of the company operate as the financial sponsor or as an active party in the transaction alongside the sponsor.[5]  Finally the leveraged buyout of a public company is often referred to as a \"take-private\" or a \"public-to-private\" (PtP).  A leveraged buyout is characterized by the extensive use of debt financing to acquire a company. This financing structure enables private equity firms and financial sponsors to control businesses while investing a relatively small portion of their own equity. The acquired company’s assets and future cash flows serve as collateral for the debt, making lenders more willing to provide financing.[1]  While different firms pursue different strategies, there are some characteristics that hold true across many types of leveraged buyouts:  Debt volumes of up to 100% of a purchase price have been provided to companies with very stable and secured cash flows, such as real estate portfolios with rental income secured by long-term rental agreements. Typically, debt of 40–60% of the purchase price may be offered. Debt ratios vary significantly among regions and target industries.  Debt for an acquisition comes in two types: senior and junior. Senior debt is secured with the target company's assets and has lower interest rates. Junior debt has no security interests and higher interest rates. In big purchases, debt and equity can come from more than one party. Banks can also syndicate debt, meaning they sell pieces of the debt to other banks. Seller notes (or vendor loans) can also happen when the seller uses part of the sale to give the purchaser a loan. In LBOs, the only collateral is the company's assets and cash flows. The financial sponsor can treat their investment as common equity, preferred equity, or other securities. Preferred equity pays dividends and has priority over common equity.  In addition to the amount of debt that can be used to fund leveraged buyouts, it is also important to understand the types of companies that private equity firms look for when considering leveraged buyouts.  Another key benefit to the equity investor in a leveraged buyout is the tax deductibility of interest payments on the acquisition financing which can offset the company's earnings and reduce the corporate income tax.  Of course, the interest income on the interest payments are taxed at ordinary income rates rather than capital gains rates so while the allocation of taxes is shifted from the borrower to the lender, the total income tax generated from the company's earnings is often higher than it would be if less leverage were used.[10][7]  A \"management buyout\" (MBO) is a form of buyout in which the incumbent management team acquires a sizeable portion of the shares of the company. Similar to an MBO is an MBI (Management Buy In) in which an external management team acquires the shares.[11][12][13]  Management buyouts are usually an indication of a high degree of conviction by management in the future prospects of the business relative to the existing ownership.  Often, management is able to secure the company outside of an auction process allowing the management team to acquire the company on favorable terms. In many cases the management may still require additional equity from a financial sponsor, which may also be actively involved in securing the financing for the acquisition. Financial sponsors often find MBOs to be attractive situations as they have the opportunity to align itself with an insider who may have unique perspectives on the company and potential areas of operational improvement.[11][12][13]  A secondary buyout is the leveraged buyout of a company already owned by a private equity sponsor. A secondary buyout will often provide a realization event for the selling private equity owner(s) and its limited partner investors. Historically, given that private equity firms were extolling their unique ability to source attractive investments and drive value through a leveraged buyout, secondary buyouts were perceived as less attractive than the \"primary buyout\" of a private company or a \"take-private\" of a public company and were disdained by investors.  Over time, it has been difficult to distinguish a differential investment returns based on the prior owner of the company and secondary buyouts have become a common part of the private equity ecosystem typically representing 25% to 35% of all leverage buyouts.[8][14][9]  For sellers, secondary buyouts have led to faster realizations than an initial public offering which often takes months to prepare and requires years after the IPO to realize the remaining public shares.  Similarly the sale to another private equity sponsor may be less complex than the sale to a strategic corporate acquiror which could face regulatory scrutiny or challenges financing the purchase.[15][8][14]  Secondary buyouts differ from secondaries which typically involve the acquisition of portfolios of private equity assets including limited partnership stakes and direct investments in corporate securities.  More recently GP-led Secondaries, in which a private equity sponsor creates a \"continuation fund\" to acquire a company from one of its own funds has brought together elements of secondary buyouts and management buyouts.[8][9]  The first leveraged buyout may have been the purchase by McLean Industries, Inc. of Pan-Atlantic Steamship Company in January 1955 and Waterman Steamship Corporation in May 1955.[16] Under the terms of that transaction, McLean borrowed $42 million and raised an additional $7 million through an issue of preferred stock. When the deal closed, $20 million of Waterman cash and assets were used to retire $20 million of the loan debt.[17]  Lewis Cullman's acquisition of Orkin Exterminating Company in 1964 is among the first significant leveraged buyout transactions.[18][19][20][21] Similar to the approach employed in the McLean transaction, the use of publicly traded holding companies as investment vehicles to acquire portfolios of investments in corporate assets was a relatively new trend in the 1960s, popularized by the likes of Warren Buffett (Berkshire Hathaway) and Victor Posner (DWG Corporation), and later adopted by Nelson Peltz (Triarc), Saul Steinberg (Reliance Insurance) and Gerry Schwartz (Onex Corporation). These investment vehicles would utilize a number of the same tactics and target the same type of companies as more traditional leveraged buyouts and in many ways could be considered a forerunner of the later private-equity firms. In fact, it is Posner who is often credited with coining the term \"leveraged buyout\" or \"LBO.\"[22]  The leveraged buyout boom of the 1980s was conceived in the 1960s by a number of corporate financiers, most notably Jerome Kohlberg, Jr. and later his protégé Henry Kravis. Working for Bear Stearns at the time, Kohlberg and Kravis, along with Kravis' cousin George Roberts, began a series of what they described as \"bootstrap\" investments. Many of the target companies lacked a viable or attractive exit for their founders, as they were too small to be taken public and the founders were reluctant to sell out to competitors: thus, a sale to an outside buyer might prove attractive.  In the following years, the three Bear Stearns bankers would complete a series of buyouts including Stern Metals (1965), Incom (a division of Rockwood International, 1971), Cobblers Industries (1971), and Boren Clay (1973) as well as Thompson Wire, Eagle Motors and Barrows through their investment in Stern Metals.[23] By 1976, tensions had built up between Bear Stearns and Kohlberg, Kravis and Roberts leading to their departure and the formation of Kohlberg Kravis Roberts in that year.  In January 1982, former U.S. Secretary of the Treasury William E. Simon and a group of investors acquired Gibson Greetings, a producer of greeting cards, for $80 million, of which only $1 million was rumored to have been contributed by the investors. By mid-1983, just sixteen months after the original deal, Gibson completed a $290 million IPO and Simon made approximately $66 million.[24] The success of the Gibson Greetings investment attracted the attention of the wider media to the nascent boom in leveraged buyouts.[25] Between 1980 and 1990, there were 180 leveraged buyouts involving firms with an aggregate book value of $39.2 billion.[26]  In the summer of 1984 the LBO was a target for virulent criticism by Paul Volcker, then chairman of the Federal Reserve, by John S.R. Shad, chairman of the U.S. Securities and Exchange Commission, and other senior financiers. The gist of all the denunciations was that top-heavy reversed pyramids of debt were being created and that they would soon crash, destroying assets and jobs.[27]  During the 1980s, constituencies within acquired companies and the media ascribed the \"corporate raid\" label to many private equity investments, particularly those that featured a hostile takeover of the company, perceived asset stripping, major layoffs or other significant corporate restructuring activities. Among the most notable investors to be labeled corporate raiders in the 1980s included Carl Icahn, Victor Posner, Nelson Peltz, Robert M. Bass, T. Boone Pickens, Harold Clark Simmons, Kirk Kerkorian, Sir James Goldsmith, Saul Steinberg and Asher Edelman. Carl Icahn developed a reputation as a ruthless corporate raider after his hostile takeover of TWA in 1985.[28][29] Many of the corporate raiders were onetime clients of Michael Milken, whose investment banking firm, Drexel Burnham Lambert helped raise blind pools of capital with which corporate raiders could make a legitimate attempt to take over a company and provided high-yield debt financing of the buyouts.[30]  One of the final major buyouts of the 1980s proved to be its most ambitious and marked both a high-water mark and a sign of the beginning of the end of the boom that had begun nearly a decade earlier. In 1989, KKR closed in on a $31.1 billion takeover of RJR Nabisco. It was, at that time and for over 17 years following, the largest leveraged buyout in history. The event was chronicled in the book (and later the movie) Barbarians at the Gate: The Fall of RJR Nabisco.[31] KKR would eventually prevail in acquiring RJR Nabisco at $109 per share, marking a dramatic increase from the original announcement that Shearson Lehman Hutton would take RJR Nabisco private at $75 per share. A fierce series of negotiations and horse-trading ensued which pitted KKR against Shearson Lehman Hutton and later Forstmann Little & Co. Many of the major banking players of the day, including Morgan Stanley, Goldman Sachs, Salomon Brothers, and Merrill Lynch were actively involved in advising and financing the parties. After Shearson Lehman's original bid, KKR quickly introduced a tender offer to obtain RJR Nabisco for $90 per share – a price that enabled it to proceed without the approval of RJR Nabisco's management. RJR's management team, working with Shearson Lehman and Salomon Brothers, submitted a bid of $112, a figure they felt certain would enable them to outflank any response by Kravis's team. KKR's final bid of $109, while a lower dollar figure, was ultimately accepted by the board of directors of RJR Nabisco.[32] At $31.1 billion of transaction value, RJR Nabisco was the largest leveraged buyout in history until the 2007 buyout of TXU Energy by KKR and Texas Pacific Group.[33] In 2006 and 2007, a number of leveraged buyout transactions were completed that for the first time surpassed the RJR Nabisco leveraged buyout in terms of nominal purchase price. However, adjusted for inflation, none of the leveraged buyouts of the 2006–2007 period surpassed RJR Nabisco.[citation needed]  By the end of the 1980s the excesses of the buyout market were beginning to show, with the bankruptcy of several large buyouts including Robert Campeau's 1988 buyout of Federated Department Stores, the 1986 buyout of the Revco drug stores, Walter Industries, FEB Trucking and Eaton Leonard. Additionally, the RJR Nabisco deal was showing signs of strain, leading to a recapitalization in 1990 that involved the contribution of $1.7 billion of new equity from KKR.[34]  Drexel Burnham Lambert was the investment bank most responsible for the boom in private equity during the 1980s due to its leadership in the issuance of high-yield debt.[35] Drexel reached an agreement with the government in which it pleaded nolo contendere (no contest) to six felonies – three counts of stock parking and three counts of stock manipulation.[36] It also agreed to pay a fine of $650 million – at the time, the largest fine ever levied under securities laws. Milken left the firm after his own indictment in March 1989.[37] On February 13, 1990, after being advised by United States Secretary of the Treasury Nicholas F. Brady, the U.S. Securities and Exchange Commission (SEC), the New York Stock Exchange, and the Federal Reserve, Drexel Burnham Lambert officially filed for Chapter 11 bankruptcy protection.[37]  The combination of decreasing interest rates, loosening lending standards, and regulatory changes for publicly traded companies (specifically the Sarbanes–Oxley Act) would set the stage for the largest boom the private equity industry had seen. Marked by the buyout of Dex Media in 2002, large multibillion-dollar U.S. buyouts could once again obtain significant high yield debt financing from various banks and larger transactions could be completed. By 2004 and 2005, major buyouts were once again becoming common, including the acquisitions of Toys \"R\" Us,[38] The Hertz Corporation,[39][40] Metro-Goldwyn-Mayer[41] and SunGard[42] in 2005.  As 2005 ended and 2006 began, new \"largest buyout\" records were set and surpassed several times with nine of the top ten buyouts at the end of 2007 having been announced in an 18-month window from the beginning of 2006 through the middle of 2007. In 2006, private-equity firms bought 654 U.S. companies for $375 billion, representing 18 times the level of transactions closed in 2003.[43] Additionally, U.S.-based private-equity firms raised $215.4 billion in investor commitments to 322 funds, surpassing the previous record set in 2000 by 22% and 33% higher than the 2005 fundraising total.[44] The following year, despite the onset of turmoil in the credit markets in the summer, saw yet another record year of fundraising with $302 billion of investor commitments to 415 funds.[45] Among the mega-buyouts completed during the 2006 to 2007 boom were: EQ Office, HCA,[46] Alliance Boots[47] and TXU.[48]  In July 2007, turmoil that had been affecting the mortgage markets spilled over into the leveraged finance and high-yield debt markets.[49][50] The markets had been highly robust during the first six months of 2007, with highly issuer friendly developments including PIK and PIK Toggle (interest is \"Payable In Kind\") and covenant light debt widely available to finance large leveraged buyouts. July and August saw a notable slowdown in issuance levels in the high yield and leveraged loan markets with only few issuers accessing the market. Uncertain market conditions led to a significant widening of yield spreads, which coupled with the typical summer slowdown led many companies and investment banks to put their plans to issue debt on hold until the autumn. However, the expected rebound in the market after Labor Day 2007 did not materialize and the lack of market confidence prevented deals from pricing. By the end of September, the full extent of the credit situation became obvious as major lenders including Citigroup and UBS AG announced major writedowns due to credit losses. The leveraged finance markets came to a near standstill.[51] As 2007 ended and 2008 began, it was clear that lending standards had tightened and the era of \"mega-buyouts\" had come to an end. Nevertheless, private equity continues to be a large and active asset class and the private-equity firms, with hundreds of billions of dollars of committed capital from investors are looking to deploy capital in new and different transactions.  As with all companies, a proportion of companies acquired in leveraged buyouts will experience financial challenges and given the higher debt-to-equity ratio of LBO targets, these financial challenges can result in default.  Especially in the leveraged buyouts of the 1980s in which debt-to-equity ratios often exceeded 9 to 1, defaults occurred at notable levels.  Robert Campeau's 1988 buyout of Federated Department Stores and the 1986 buyout of the Revco drug stores were well documented failures that resulted in bankruptcy. The failure of the Federated buyout was a result of excessive debt financing, comprising about 97% of the total consideration, which led to large interest payments that exceeded the company's operating cash flow.  Many LBOs of the boom period 2005–2007 were also financed with too high a debt burden, however default rates were significantly below the expectations of market observers given the proximate onset of the 2008 global financial crisis.[52][53]  The inability to repay debt in an LBO can be caused by initial overpricing of the target firm and\/or its assets. Over-optimistic forecasts of the revenues of the target company may also lead to financial distress after acquisition. Some courts have found that in certain situations, LBO debt constitutes a fraudulent transfer under U.S. insolvency law if it is determined to be the cause of the acquired firm's failure.[54]  The outcome of litigation attacking a leveraged buyout as a fraudulent transfer will generally turn on the financial condition of the target at the time of the transaction – that is, whether the risk of failure was substantial and known at the time of the LBO, or whether subsequent unforeseeable events led to the failure. The analysis historically depended on \"dueling\" expert witnesses and was notoriously subjective and it was rare that such findings were sustained.[55]  In addition, the Bankruptcy Code includes a so-called \"safe harbor\" provision, preventing bankruptcy trustees from recovering settlement payments to the bought-out shareholders.[56] In 2009, the U.S. Court of Appeals for the Sixth Circuit held that such settlement payments could not be avoided, irrespective of whether they occurred in an LBO of a public or private company.[57]  To the extent that public shareholders are protected, insiders and secured lenders become the primary targets of fraudulent transfer actions.  In certain cases, instead of declaring insolvency, the company negotiates a debt restructuring with its lenders. The financial restructuring might entail that the equity owners inject some more money in the company and the lenders waive parts of their claims. In other situations, the lenders inject new money and assume the equity of the company, with the present equity owners losing their shares and investment. The operations of the company are not affected by the financial restructuring. Nonetheless, the financial restructuring requires significant management attention and may lead to customers losing faith in the company.[52][53] "},"meta":{},"created_at":"2025-03-22T14:25:42.289399Z","updated_at":"2025-03-22T14:25:42.289399Z","inner_id":65,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":74,"annotations":[{"id":74,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.320914Z","updated_at":"2025-03-22T14:25:42.320914Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"6a0eedca-db15-4fb5-874e-3544d1e61b8d","import_id":null,"last_action":null,"bulk_created":false,"task":74,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"An audit is an \"independent examination of financial information of any entity, whether profit oriented or not, irrespective of its size or legal form when such an examination is conducted with a view to express an opinion thereon.\"[1] In a narrower sense, an audit refers specifically to the verification of financial statements, primarily assessing their authenticity, legality, and fairness.[2] In a broader sense, an audit encompasses not only financial auditing but also operational, compliance, management, environmental, and other specialized areas. Its purpose extends beyond verifying financial data to ensuring the efficiency, compliance, and risk management of a business's overall operations.[2]  Audits provide third-party assurance to various stakeholders that the audited subject matter is free from material misstatement. By offering this assurance, audits help stakeholders evaluate and enhance the effectiveness of risk management, internal controls, and governance over the subject matter.[3]  In recent years auditing has expanded to encompass many areas of public and corporate life. Professor Michael Power refers to this extension of auditing practices as the \"Audit Society\".[4]  The word \"audit\" derives from the Latin word audire which means \"to hear\".[5]  Auditing has been a safeguard measure since ancient times.[6] During medieval times, when manual bookkeeping was prevalent, auditors in Britain used to hear the accounts read out for them and checked that the organization's personnel were not negligent or fraudulent.[7] In 1951, Moyer identified that the most important duty of the auditor was to detect fraud.[8] Chatfield documented that early United States auditing was viewed mainly as verification of bookkeeping detail.[9]  The Central Auditing Commission of the Communist Party of the Soviet Union (Russian: Центральная ревизионная комиссия КПСС) operated from 1921 to 1990.  An information technology audit, or information systems audit, is an examination of the management controls within an Information technology (IT) infrastructure. The evaluation of obtained evidence determines if the information systems are safeguarding assets, maintaining data integrity, and operating effectively to achieve the organization's goals or objectives. These reviews may be performed in conjunction with a financial statement audit, internal audit, or other form of attestation engagement.  Due to strong incentives (including taxation, misselling and other forms of fraud) to misstate financial information, auditing has become a legal requirement for many entities who have the power to exploit financial information for personal gain. Traditionally, audits were mainly associated with gaining information about financial systems and the financial records of a company or a business. Financial audits also assess whether a business or corporation adheres to legal duties as well as other applicable statutory customs and regulations.[10][11]  Financial audits are performed to ascertain the validity and reliability of information, as well as to provide an assessment of a system's internal control. As a result, a third party can express an opinion of the person \/ organization \/ system (etc.) in question. The opinion given on financial statements will depend on the audit evidence obtained.  A statutory audit is a legally required review of the accuracy of a company's or government's financial statements and records. The purpose of a statutory audit is to determine whether an organization provides a fair and accurate representation of its financial position by examining information such as bank balances, bookkeeping records, and financial transactions.  Due to constraints, an audit seeks to provide only reasonable assurance that the statements are free from material error. Hence, statistical sampling is often adopted in audits. In the case of financial audits, a set of financial statements are said to be true and fair when they are free of material misstatements – a concept influenced by both quantitative (numerical) and qualitative factors. But recently, the argument that auditing should go beyond just true and fair is gaining momentum.[12] And the US Public Company Accounting Oversight Board has come out with a concept release on the same.[13]  Cost accounting is a process for verifying the cost of manufacturing or producing of any article, on the basis of accounts measuring the use of material, labor or other items of cost. In simple words, the term, cost audit means a systematic and accurate verification of the cost accounts and records, and checking for adherence to the cost accounting objectives. According to the Institute of Cost and Management Accountants, cost audit is \"an examination of cost accounting records and verification of facts to ascertain that the cost of the product has been arrived at, in accordance with principles of cost accounting.\"[citation needed]  In most nations, an audit must adhere to generally accepted standards established by governing bodies. These standards assure third parties or external users that they can rely upon the auditor's opinion on the fairness of financial statements or other subjects on which the auditor expresses an opinion. The audit must therefore be precise and accurate, containing no additional misstatements or errors.[citation needed]  In the US, audits of publicly traded companies are governed by rules laid down by the Public Company Accounting Oversight Board (PCAOB), which was established by Section 404 of the Sarbanes–Oxley Act of 2002.  Such an audit is called an integrated audit, where auditors, in addition to an opinion on the financial statements, must also express an opinion on the effectiveness of a company's internal control over financial reporting, in accordance with PCAOB Auditing Standard No. 5.[14]  There are also new types of integrated auditing becoming available that use unified compliance material (see the unified compliance section in Regulatory compliance).  Due to the increasing number of regulations and need for operational transparency, organizations are adopting risk-based audits that can cover multiple regulations and standards from a single audit event.[citation needed]  This is a very new but necessary approach in some sectors to ensure that all the necessary governance requirements can be met without duplicating effort from both audit and audit hosting resources.[citation needed]  The purpose of an assessment is to measure something or calculate a value for it. An auditor's objective is to determine whether financial statements are presented fairly, in all material respects, and are free of material misstatement. Although the process of producing an assessment may involve an audit by an independent professional, its purpose is to provide a measurement rather than to express an opinion about the fairness of statements or quality of performance.[15]  Auditors of financial statements & non-financial information (including compliance audit) can be classified into various categories:  The most commonly used external audit standards are the US GAAS of the American Institute of Certified Public Accountants and the International Standards on Auditing (ISA) developed by the International Auditing and Assurance Standard.  Performance audit refers to an independent examination of a program, function, operation or the management systems and procedures of a governmental or non-profit entity to assess whether the entity is achieving economy, efficiency and effectiveness in the employment of available resources. Safety, security, information systems performance, and environmental concerns are increasingly the subject of audits.[19] There are now audit professionals who specialize in security audits and information systems audits. With nonprofit organizations and government agencies, there has been an increasing need for performance audits, examining their success in satisfying mission objectives[citation needed].  Quality audits are performed to verify conformance to standards through review of objective evidence.  A system of quality audits may verify the effectiveness of a quality management system. This is part of certifications such as ISO 9001. Quality audits are essential to verify the existence of objective evidence showing conformance to required processes, to assess how successfully processes have been implemented, and to judge the effectiveness of achieving any defined target levels. Quality audits are also necessary to provide evidence concerning reduction and elimination of problem areas, and they are a hands-on management tool for achieving continual improvement in an organization.  To benefit the organization, quality auditing should not only report non-conformance and corrective actions but also highlight areas of good practice and provide evidence of conformance. In this way, other departments may share information and amend their working practices as a result, also enhancing continual improvement.  A project audit provides an opportunity to uncover issues, concerns and challenges encountered during the project lifecycle.[20] Conducted midway through the project, an audit affords the project manager, project sponsor and project team an interim view of what has gone well, as well as what needs to be improved to successfully complete the project. If done at the close of a project, the audit can be used to develop success criteria for future projects by providing a forensic review. This review identifies which elements of the project were successfully managed and which ones presented challenges. As a result, the review will help the organization identify what it needs to do to avoid repeating the same mistakes on future projects  Projects can undergo 2 types of Project audits:[19]  Other forms of Project audits:  Formal: Applies when the project is in trouble, sponsor agrees that the audit is needed, sensitivities are high, and need to be able prove conclusions via sustainable evidence.  Informal: Apply when a new project manager is provided, there is no indication the projects in trouble and there is a need to report whether the project is as opposed to where its supposed to Informal audits can apply the same criteria as formal audit but there is no need for such a in depth report or formal report.[21]  An energy audit is an inspection, survey and analysis of energy flows for energy conservation in a building, process or system to reduce the amount of energy input into the system without negatively affecting the output(s).  An operations audit is an examination of the operations of the client's business. In this audit, the auditor thoroughly examines the efficiency, effectiveness and economy of the operations with which the management of the entity (client) is achieving its objective. The operational audit goes beyond the internal controls issues since management does not achieve its objectives merely by compliance of satisfactory system of internal controls. Operational audits cover any matters which may be commercially unsound. The objective of operational audit is to examine Three E's, namely:[citation needed] Effectiveness – doing the right things with least wastage of resources. Efficiency – performing work in least possible time. Economy – balance between benefits and costs to run the operations[citation needed]  A control self-assessment is a commonly used tool for completing an operations audit.[22]  Also refer to forensic accountancy, forensic accountant or forensic accounting. It refers to an investigative audit in which accountants with specialized on both accounting and investigation seek to uncover frauds, missing money and negligence.[citation needed] "},"meta":{},"created_at":"2025-03-22T14:25:42.289399Z","updated_at":"2025-03-22T14:25:42.289399Z","inner_id":66,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":75,"annotations":[{"id":75,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.321899Z","updated_at":"2025-03-22T14:25:42.321899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"bd5fac74-30e4-4d54-b4f6-dbfbeb7be186","import_id":null,"last_action":null,"bulk_created":false,"task":75,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  In finance, an option is a contract which conveys to its owner, the holder, the right, but not the obligation, to buy or sell a specific quantity of an underlying asset or instrument at a specified strike price on or before a specified date, depending on the style of the option.   Options are typically acquired by purchase, as a form of compensation, or as part of a complex financial transaction. Thus, they are also a form of asset (or contingent liability) and have a valuation that may depend on a complex relationship between underlying asset price, time until expiration, market volatility, the risk-free rate of interest, and the strike price of the option.   Options may be traded between private parties in over-the-counter (OTC) transactions, or they may be exchange-traded in live, public markets in the form of standardized contracts.  An option is a contract that allows the holder the right to buy or sell an underlying asset or financial instrument at a specified strike price on or before a specified date, depending on the form of the option. Selling or exercising an option before expiry typically requires a buyer to pick the contract up at the agreed upon price. The strike price may be set by reference to the spot price (market price) of the underlying security or commodity on the day an option is issued, or it may be fixed at a discount or at a premium. The issuer has the corresponding obligation to fulfill the transaction (to sell or buy) if the holder \"exercises\" the option. An option that conveys to the holder the right to buy at a specified price is referred to as a call, while one that conveys the right to sell at a specified price is known as a put.  The issuer may grant an option to a buyer as part of another transaction (such as a share issue or as part of an employee incentive scheme), or the buyer may pay a premium to the issuer for the option. A call option would normally be exercised only when the strike price is below the market value of the underlying asset, while a put option would normally be exercised only when the strike price is above the market value. When an option is exercised, the cost to the option holder is the strike price of the asset acquired plus the premium, if any, paid to the issuer. If the option's expiration date passes without the option being exercised, the option expires, and the holder forfeits the premium paid to the issuer. In any case, the premium is income to the issuer, and normally a capital loss to the option holder.  An option holder may on-sell the option to a third party in a secondary market, in either an over-the-counter transaction or on an options exchange, depending on the option. The market price of an American-style option normally closely follows that of the underlying stock being the difference between the market price of the stock and the strike price of the option. The actual market price of the option may vary depending on a number of factors, such as a significant option holder needing to sell the option due to the expiration date approaching and not having the financial resources to exercise the option, or a buyer in the market trying to amass a large option holding. The ownership of an option does not generally entitle the holder to any rights associated with the underlying asset, such as voting rights or any income from the underlying asset, such as a dividend.  Contracts similar to options have been used since ancient times.[1] The first reputed option buyer was the ancient Greek mathematician and philosopher Thales of Miletus. On a certain occasion, it was predicted that the season's olive harvest would be larger than usual, and during the off-season, he acquired the right to use a number of olive presses the following spring. When spring came and the olive harvest was larger than expected, he exercised his options and then rented the presses out at a much higher price than he paid for his 'option'.[2][3]  The 1688 book Confusion of Confusions describes the trading of \"opsies\" on the Amsterdam stock exchange (now Euronext), explaining that \"there will be only limited risks to you, while the gain may surpass all your imaginings and hopes.\"[4]  In London, puts and \"refusals\" (calls) first became well-known trading instruments in the 1690s during the reign of William and Mary.[5] Privileges were options sold over the counter in nineteenth-century America, with both puts and calls on shares offered by specialized dealers. Their exercise price was fixed at a rounded-off market price on the day or week that the option was bought, and the expiry date was generally three months after purchase. They were not traded in secondary markets.  In the real estate market, call options have long been used to assemble large parcels of land from separate owners; e.g., a developer pays for the right to buy several adjacent plots, but is not obligated to buy these plots and might not unless they can buy all the plots in the entire parcel. Additionally, purchase of real property, like houses, requires a buyer paying the seller into an escrow account an earnest payment, which offers the buyer the right to buy the property at the set terms, including the purchase price.[citation needed]  In the motion picture industry, film or theatrical producers often buy an option giving the right – but not the obligation – to dramatize a specific book or script.  Lines of credit give the potential borrower the right – but not the obligation – to borrow within a specified time period.  Many choices, or embedded options, have traditionally been included in bond contracts. For example, many bonds are convertible into common stock at the buyer's option, or may be called (bought back) at specified prices at the issuer's option. Mortgage borrowers have long had the option to repay the loan early, which corresponds to a callable bond option.  Options contracts have been known for decades. The Chicago Board Options Exchange was established in 1973, which set up a regime using standardized forms and terms and trade through a guaranteed clearing house. Trading activity and academic interest have increased since then.  Today, many options are created in a standardized form and traded through clearing houses on regulated options exchanges. In contrast, other over-the-counter options are written as bilateral, customized contracts between a single buyer and seller, one or both of which may be a dealer or market-maker. Options are part of a larger class of financial instruments known as derivative products, or simply, derivatives.[6][7]  A financial option is a contract between two counterparties with the terms of the option specified in a term sheet. Option contracts may be quite complicated; however, at minimum, they usually contain the following specifications:[8]  Exchange-traded options (also called \"listed options\") are a class of exchange-traded derivatives. Exchange-traded options have standardized contracts and are settled through a clearing house with fulfillment guaranteed by the Options Clearing Corporation (OCC). Since the contracts are standardized, accurate pricing models are often available. Exchange-traded options include:[9][10]  Over-the-counter options (OTC options, also called \"dealer options\") are traded between two private parties and are not listed on an exchange. The terms of an OTC option are unrestricted and may be individually tailored to meet any business need. In general, the option writer is a well-capitalized institution (to prevent credit risk). Option types commonly traded over the counter include:  By avoiding an exchange, users of OTC options can narrowly tailor the terms of the option contract to suit individual business requirements. In addition, OTC option transactions generally do not need to be advertised to the market and face little or no regulatory requirements. However, OTC counterparties must establish credit lines with each other and conform to each other's clearing and settlement procedures.  With few exceptions,[11] there are no secondary markets for employee stock options. These must either be exercised by the original grantee or allowed to expire.  The most common way to trade options is via standardized options contracts listed by various futures and options exchanges.[12] Listings and prices are tracked and can be looked up by ticker symbol. By publishing continuous, live markets for option prices, an exchange enables independent parties to engage in price discovery and execute transactions. As an intermediary to both sides of the transaction, the benefits the exchange provides to the transaction include:  These trades are described from the point of view of a speculator. If they are combined with other positions, they can also be used in hedging. An option contract in US markets usually represents 100 shares of the underlying security.[13][14]  A trader who expects a stock's price to increase can buy a call option to purchase the stock at a fixed price (strike price) at a later date, rather than purchase the stock outright. The cash outlay on the option is the premium. The trader would have no obligation to buy the stock, but only has the right to do so on or before the expiration date. The risk of loss would be limited to the premium paid, unlike the possible loss had the stock been bought outright.  The holder of an American-style call option can sell the option holding at any time until the expiration date and would consider doing so when the stock's spot price is above the exercise price, especially if the holder expects the price of the option to drop. By selling the option early in that situation, the trader can realise an immediate profit. Alternatively, the trader can exercise the option – for example, if there is no secondary market for the options – and then sell the stock, realising a profit. A trader would make a profit if the spot price of the shares rises by more than the premium. For example, if the exercise price is 100 and the premium paid is 10, then if the spot price of 100 rises to only 110, the transaction is break-even; an increase in the stock price above 110 produces a profit.  If the stock price at expiration is lower than the exercise price, the holder of the option at that time will let the call contract expire and lose only the premium (or the price paid on transfer).  A trader who expects a stock's price to decrease can buy a put option to sell the stock at a fixed price (strike price) at a later date. The trader is not obligated to sell the stock, but has the right to do so on or before the expiration date. If the stock price at expiration is below the exercise price by more than the premium paid, the trader makes a profit. If the stock price at expiration is above the exercise price, the trader lets the put contract expire and loses only the premium paid. In the transaction, the premium also plays a role as it enhances the break-even point. For example, if the exercise price is 100 and the premium paid is 10, then a spot price between 90 and 100 is not profitable. The trader makes a profit only if the spot price is below 90.  The trader exercising a put option on a stock does not need to own the underlying asset, because most stocks can be shorted.  A trader who expects a stock's price to decrease can sell the stock short or instead sell, or \"write\", a call. The trader selling a call has an obligation to sell the stock to the call buyer at a fixed price (\"strike price\"). If the seller does not own the stock when the option is exercised, they are obligated to purchase the stock in the market at the prevailing market price. If the stock price decreases, the seller of the call (call writer) makes a profit in the amount of the premium. If the stock price increases over the strike price by more than the amount of the premium, the seller loses money, with the potential loss being unlimited.  A trader who expects a stock's price to increase can buy the stock or instead sell, or \"write\", a put. The trader selling a put has an obligation to buy the stock from the put buyer at a fixed price (\"strike price\"). If the stock price at expiration is above the strike price, the seller of the put (put writer) makes a profit in the amount of the premium. If the stock price at expiration is below the strike price by more than the amount of the premium, the trader loses money, with the potential loss being up to the strike price minus the premium. A benchmark index for the performance of a cash-secured short put option position is the CBOE S&P 500 PutWrite Index (ticker PUT).  Combining any of the four basic kinds of option trades (possibly with different exercise prices and maturities) and the two basic kinds of stock trades (long and short) allows a variety of options strategies. Simple strategies usually combine only a few trades, while more complicated strategies can combine several.  Strategies are often used to engineer a particular risk profile to movements in the underlying security. For example, buying a butterfly spread (long one X1 call, short two X2 calls, and long one X3 call) allows a trader to profit if the stock price on the expiration date is near the middle exercise price, X2, and does not expose the trader to a large loss.  A condor is a strategy similar to a butterfly spread, but with different strikes for the short options – offering a larger likelihood of profit but with a lower net credit compared to the butterfly spread.  Selling a straddle (selling both a put and a call at the same exercise price) would give a trader a greater profit than a butterfly if the final stock price is near the exercise price, but might result in a large loss.  Similar to the straddle is the strangle which is also constructed by a call and a put, but whose strikes are different, reducing the net debit of the trade, but also reducing the risk of loss in the trade.  One well-known strategy is the covered call, in which a trader buys a stock (or holds a previously purchased stock position), and sells a call. (This can be contrasted with a naked call. See also naked put.) If the stock price rises above the exercise price, the call will be exercised and the trader will get a fixed profit. If the stock price falls, the call will not be exercised, and any loss incurred to the trader will be partially offset by the premium received from selling the call. Overall, the payoffs match the payoffs from selling a put. This relationship is known as put–call parity and offers insights for financial theory. A benchmark index for the performance of a buy-write strategy is the CBOE S&P 500 BuyWrite Index (ticker symbol BXM).  Another very common strategy is the protective put, in which a trader buys a stock (or holds a previously-purchased long stock position), and buys a put. This strategy acts as an insurance when investing long on the underlying stock, hedging the investor's potential losses, but also shrinking an otherwise larger profit, if just purchasing the stock without the put. The maximum profit of a protective put is theoretically unlimited as the strategy involves being long on the underlying stock. The maximum loss is limited to the purchase price of the underlying stock less the strike price of the put option and the premium paid. A protective put is also known as a married put.  Options can be classified in a few ways.  Another important class of options, particularly in the U.S., are employee stock options, which a company awards to their employees as a form of incentive compensation. Other types of options exist in many financial contracts. For example real estate options are often used to assemble large parcels of land, and prepayment options are usually included in mortgage loans. However, many of the valuation and risk management principles apply across all financial options.  Options are classified into a number of styles, the most common of which are:  These are often described as vanilla options. Other styles include:  Because the values of option contracts depend on a number of different variables in addition to the value of the underlying asset, they are complex to value. There are many pricing models in use, although all essentially incorporate the concepts of rational pricing (i.e. risk neutrality), moneyness, option time value, and put–call parity.  The valuation itself combines a model of the behavior (\"process\") of the underlying price with a mathematical method which returns the premium as a function of the assumed behavior. The models range from the (prototypical) Black–Scholes model for equities,[16][unreliable source?][17] to the Heath–Jarrow–Morton framework for interest rates, to the Heston model where volatility itself is considered stochastic. See Asset pricing for a listing of the various models here.  In its most basic terms, the value of an option is commonly decomposed into two parts:  As above, the value of the option is estimated using a variety of quantitative techniques, all based on the principle of risk-neutral pricing and using stochastic calculus in their solution. The most basic model is the Black–Scholes model. More sophisticated models are used to model the volatility smile. These models are implemented using a variety of numerical techniques.[18] In general, standard option valuation models depend on the following factors:  More advanced models can require additional factors, such as an estimate of how volatility changes over time and for various underlying price levels, or the dynamics of stochastic interest rates.  The following are some principal valuation techniques used in practice to evaluate option contracts.  Following early work by Louis Bachelier and later work by Robert C. Merton, Fischer Black and Myron Scholes made a major breakthrough by deriving a differential equation that must be satisfied by the price of any derivative dependent on a non-dividend-paying stock. By employing the technique of constructing a risk-neutral portfolio that replicates the returns of holding an option, Black and Scholes produced a closed-form solution for a European option's theoretical price.[19] At the same time, the model generates hedge parameters necessary for effective risk management of option holdings.  While the ideas behind the Black–Scholes model were ground-breaking and eventually led to Scholes and Merton receiving the Swedish Central Bank's associated Prize for Achievement in Economics (a.k.a., the Nobel Prize in Economics),[20] the application of the model in actual options trading is clumsy because of the assumptions of continuous trading, constant volatility, and a constant interest rate. Nevertheless, the Black–Scholes model is still one of the most important methods and foundations for the existing financial market in which the result is within the reasonable range.[21]  Since the market crash of 1987, it has been observed that market implied volatility for options of lower strike prices is typically higher than for higher strike prices, suggesting that volatility varies both for time and for the price level of the underlying security –  a so-called volatility smile; and with a time dimension, a volatility surface.  The main approach here is to treat volatility as stochastic, with the resultant stochastic volatility models and the Heston model as a prototype;[22] see #Risk-neutral_measure for a discussion of the logic. Other models include the CEV and SABR volatility models.  One principal advantage of the Heston model, however, is that it can be solved in closed form, while other stochastic volatility models require complex numerical methods.[22]  An alternate, though related, approach is to apply a local volatility model, where volatility is treated as a deterministic function of both the current asset level      S  t     {\\displaystyle S_{t}}   and of time     t   {\\displaystyle t}  .  As such, a local volatility model is a generalisation of the Black–Scholes model, where the volatility is a constant. The concept was developed when Bruno Dupire[23]  and Emanuel Derman and Iraj Kani[24] noted that there is a unique diffusion process consistent with the risk neutral densities derived from the market prices of European options. See #Development for discussion.  For the valuation of bond options, swaptions (i.e. options on swaps), and interest rate cap and floors (effectively options on the interest rate) various short-rate models have been developed (applicable, in fact, to interest rate derivatives generally). The best known of these are Black-Derman-Toy and Hull–White.[25] These models describe the future evolution of interest rates by describing the future evolution of the short rate. The other major framework for interest rate modelling is the Heath–Jarrow–Morton framework (HJM). The distinction is that HJM gives an analytical description of the entire yield curve, rather than just the short rate. (The HJM framework incorporates the Brace–Gatarek–Musiela model and market models. And some of the short rate models can be straightforwardly expressed in the HJM framework.) For some purposes, e.g., valuation of mortgage-backed securities, this can be a big simplification; regardless, the framework is often preferred for models of higher dimension. Note that for the simpler options here, i.e. those mentioned initially, the Black model can instead be employed, with certain assumptions.  Once a valuation model has been chosen, there are a number of different techniques used to implement the models.  In some cases, one can take the mathematical model and using analytical methods, develop closed form solutions such as the Black–Scholes model and the Black model. The resulting solutions are readily computable, as are their \"Greeks\". Although the Roll–Geske–Whaley model applies to an American call with one dividend, for other cases of American options, closed form solutions are not available; approximations here include Barone-Adesi and Whaley, Bjerksund and Stensland and others.  Closely following the derivation of Black and Scholes, John Cox, Stephen Ross and Mark Rubinstein developed the original version of the binomial options pricing model.[26][27] It models the dynamics of the option's theoretical value for discrete time intervals over the option's life. The model starts with a binomial tree of discrete future possible underlying stock prices. By constructing a riskless portfolio of an option and stock (as in the Black–Scholes model) a simple formula can be used to find the option price at each node in the tree. This value can approximate the theoretical value produced by Black–Scholes, to the desired degree of precision. However, the binomial model is considered more accurate than Black–Scholes because it is more flexible; e.g., discrete future dividend payments can be modeled correctly at the proper forward time steps, and American options can be modeled as well as European ones. Binomial models are widely used by professional option traders. The trinomial tree is a similar model, allowing for an up, down or stable path; although considered more accurate, particularly when fewer time-steps are modelled, it is less commonly used as its implementation is more complex.  For a more general discussion, as well as for application to commodities, interest rates and hybrid instruments, see Lattice model (finance).  For many classes of options, traditional valuation techniques are intractable because of the complexity of the instrument. In these cases, a Monte Carlo approach may often be useful. Rather than attempt to solve the differential equations of motion that describe the option's value in relation to the underlying security's price, a Monte Carlo model uses simulation to generate random price paths of the underlying asset, each of which results in a payoff for the option. The average of these payoffs can be discounted to yield an expectation value for the option.[28] Note though, that despite its flexibility, using simulation for American styled options is somewhat more complex than for lattice based models.  The equations used to model the option are often expressed as partial differential equations (see for example Black–Scholes equation). Once expressed in this form, a finite difference model can be derived, and the valuation obtained. A number of implementations of finite difference methods exist for option valuation, including: explicit finite difference, implicit finite difference and the Crank–Nicolson method. A trinomial tree option pricing model can be shown to be a simplified application of the explicit finite difference method. Although the finite difference approach is mathematically sophisticated, it is particularly useful where changes are assumed over time in model inputs – for example dividend yield, risk-free rate, or volatility, or some combination of these – that are not tractable in closed form.  Other numerical implementations which have been used to value options include finite element methods.  A call option (also known as a CO) expiring in 99 days on 100 shares of XYZ stock is struck at $50, with XYZ currently trading at $48. With future realized volatility over the life of the option estimated at 25%, the theoretical value of the option is $1.89. The hedge parameters     Δ   {\\displaystyle \\Delta }  ,     Γ   {\\displaystyle \\Gamma }  ,     κ   {\\displaystyle \\kappa }  ,     θ   {\\displaystyle \\theta }   are (0.439, 0.0631, 9.6, and −0.022), respectively. Assume that on the following day, XYZ stock rises to $48.5 and volatility falls to 23.5%. We can calculate the estimated value of the call option by applying the hedge parameters to the new model inputs as:  Under this scenario, the value of the option increases by $0.0614 to $1.9514, realizing a profit of $6.14. Note that for a delta neutral portfolio, whereby the trader had also sold 44 shares of XYZ stock as a hedge, the net loss under the same scenario would be ($15.86).  As with all securities, trading options entails the risk of the option's value changing over time. However, unlike traditional securities, the return from holding an option varies non-linearly with the value of the underlying and other factors. Therefore, the risks associated with holding options are more complicated to understand and predict.  In general, the change in the value of an option can be derived from Itô's lemma as:  where the Greeks     Δ   {\\displaystyle \\Delta }  ,     Γ   {\\displaystyle \\Gamma }  ,     κ   {\\displaystyle \\kappa }   and     θ   {\\displaystyle \\theta }   are the standard hedge parameters calculated from an option valuation model, such as Black–Scholes, and     d S   {\\displaystyle dS}  ,     d σ   {\\displaystyle d\\sigma }   and     d t   {\\displaystyle dt}   are unit changes in the underlying's price, the underlying's volatility and time, respectively.  Thus, at any point in time, one can estimate the risk inherent in holding an option by calculating its hedge parameters and then estimating the expected change in the model inputs,     d S   {\\displaystyle dS}  ,     d σ   {\\displaystyle d\\sigma }   and     d t   {\\displaystyle dt}  , provided the changes in these values are small. This technique can be used effectively to understand and manage the risks associated with standard options. For instance, by offsetting a holding in an option with the quantity     − Δ   {\\displaystyle -\\Delta }   of shares in the underlying, a trader can form a delta neutral portfolio that is hedged from loss for small changes in the underlying's price. The corresponding price sensitivity formula for this portfolio     Π   {\\displaystyle \\Pi }   is:  A special situation called pin risk can arise when the underlying closes at or very close to the option's strike value on the last day the option is traded prior to expiration. The option writer (seller) may not know with certainty whether or not the option will actually be exercised or be allowed to expire. Therefore, the option writer may end up with a large, unwanted residual position in the underlying when the markets open on the next trading day after expiration, regardless of his or her best efforts to avoid such a residual.  A further, often ignored, risk in derivatives such as options is counterparty risk. In an option contract this risk is that the seller will not sell or buy the underlying asset as agreed. The risk can be minimized by using a financially strong intermediary able to make good on the trade, but in a major panic or crash the number of defaults can overwhelm even the strongest intermediaries.  To limit risk, brokers use access control systems to restrict traders from executing certain options strategies that would not be suitable for them. Brokers generally offer about four or five approval levels, with the lowest level offering the lowest risk and the highest level offering the highest risk. The actual numbers of levels, and the specific options strategies permitted at each level, vary between brokers. Brokers may also have their own specific vetting criteria, but they are usually based on factors such as the trader's annual salary and net worth, trading experience, and investment goals (capital preservation, income, growth, or speculation). For example, a trader with a low salary and net worth, little trading experience, and only concerned about preserving capital generally would not be permitted to execute high-risk strategies like naked calls and naked puts. Traders can update their information when requesting permission to upgrade to a higher approval level.[29]  The Chicago Board Options Exchange (CBOE) is an options exchange located in Chicago, Illinois. Founded in 1973, the CBOE is the first options exchange in the United States. The CBOE offers options trading on various underlying securities including market indexes, exchange-traded funds (ETFs), stocks, and volatility indexes. Its flagship product is options on the S&P 500 Index (SPX), one of the most actively traded options globally. In addition to its floor-based open outcry trading, the CBOE also operates an all-electronic trading platform. The CBOE is regulated by the U.S. Securities and Exchange Commission (SEC).[30]  Founded in 1790, The NASDAQ OMX PHLX, also known as the Philadelphia Stock Exchange is an options and futures exchange located in Philadelphia, Pennsylvania. It is the oldest stock exchange in the United States. The NASDAQ OMX PHLX allows trading of options on equities, indexes, ETFs, and foreign currencies. It is one of the few exchanges designated for trading currency options in the U.S. In 2008, NASDAQ acquired the Philadelphia Stock Exchange and renamed it NASDAQ OMX PHLX. It operates as a subsidiary of NASDAQ, Inc.[31]  International Securities Exchange (ISE) is an electronic options exchange located in New York City. Launched in 2000, ISE was the first all-electronic U.S. options exchange. ISE provides options trading on U.S. equities, indexes, and ETFs. Its trading platform provides a maximum price improvement auction to allow market makers to compete for orders. ISE is regulated by the SEC and is owned by Nasdaq, Inc.[32]  Eurex Exchange is a derivatives exchange located in Frankfurt, Germany. It offers trading in futures and options on interest rates, equities, indexes, and fixed-income products. Formed in 1998 from the merger of Deutsche Terminbörse (DTB) and Swiss Options and Financial Futures Exchange (SOFFEX), Eurex Exchange operates electronic and open outcry trading platforms. Eurex Exchange is owned by Eurex Frankfurt AG.[33]  Founded in 1878, the Tokyo Stock Exchange (TSE) is a stock exchange located in Tokyo, Japan. In addition to equities, the TSE also provides trading in stock index futures and options. Trading is conducted electronically as well as through auction bidding by securities companies. The TSE is regulated by the Financial Services Agency of Japan. It is owned by the Japan Exchange Group.[34] "},"meta":{},"created_at":"2025-03-22T14:25:42.289399Z","updated_at":"2025-03-22T14:25:42.289399Z","inner_id":67,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":76,"annotations":[{"id":76,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.321899Z","updated_at":"2025-03-22T14:25:42.321899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"25f38c4f-e356-442f-8e12-6cdcb0b8ea4d","import_id":null,"last_action":null,"bulk_created":false,"task":76,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A hedge is an investment position intended to offset potential losses or gains that may be incurred by a companion investment. A hedge can be constructed from many types of financial instruments, including stocks, exchange-traded funds, insurance, forward contracts, swaps, options, gambles,[1] many types of over-the-counter and derivative products, and futures contracts.  Public futures markets were established in the 19th century[2] to allow transparent, standardized, and efficient hedging of agricultural commodity prices; they have since expanded to include futures contracts for hedging the values of energy, precious metals, foreign currency, and interest rate fluctuations.  Hedging is the practice of taking a position in one market to offset and balance against the risk adopted by assuming a position in a contrary or opposing market or investment. The word hedge is from Old English hecg, originally any fence, living or artificial. The first known use of the word as a verb meaning 'dodge, evade' dates from the 1590s; that of 'insure oneself against loss,' as in a bet, is from the 1670s.[3]  Optimal hedging and optimal investments are intimately connected. It can be shown that one person's optimal investment is another's optimal hedge (and vice versa). This follows from a geometric structure formed by probabilistic representations of market views and risk scenarios. In practice, the hedge-investment duality is related to the widely used notion of risk recycling.  A typical hedger might be a commercial farmer. The market values of wheat and other crops fluctuate constantly as supply and demand for them vary, with occasional large moves in either direction. Based on current prices and forecast levels at harvest time, the farmer might decide that planting wheat is a good idea one season, but the price of wheat might change over time. Once the farmer plants wheat, he is committed to it for an entire growing season. If the actual price of wheat rises greatly between planting and harvest, the farmer stands to make a lot of unexpected money, but if the actual price drops by harvest time, he is going to lose the invested money.[4]  Due to the uncertainty of future supply and demand fluctuations, and the price risk imposed on the farmer, the farmer in this example may use different financial transactions to reduce, or hedge, their risk. One such transaction is the use of forward contracts. Forward contracts are mutual agreements to deliver a certain amount of a commodity at a certain date for a specified price and each contract is unique to the buyer and seller. For this example, the farmer can sell a number of forward contracts equivalent to the amount of wheat he expects to harvest and essentially lock in the current price of wheat. Once the forward contracts expire, the farmer will harvest the wheat and deliver it to the buyer at the price agreed to in the forward contract. Therefore, the farmer has reduced his risks to fluctuations in the market of wheat because he has already guaranteed a certain number of bushels for a certain price. However, there are still many risks associated with this type of hedge. For example, if the farmer has a low yield year and he harvests less than the amount specified in the forward contracts, he must purchase the bushels elsewhere in order to fill the contract. This becomes even more of a problem when the lower yields affect the entire wheat industry and the price of wheat increases due to supply and demand pressures. Also, while the farmer hedged all of the risks of a price decrease away by locking in the price with a forward contract, he also gives up the right to the benefits of a price increase. Another risk associated with the forward contract is the risk of default or renegotiation. The forward contract locks in a certain amount and price at a certain future date. Because of that, there is always the possibility that the buyer will not pay the amount required at the end of the contract or that the buyer will try to renegotiate the contract before it expires.[5]  Futures contracts are another way our farmer can hedge his risk without a few of the risks that forward contracts have. Futures contracts are similar to forward contracts except they are more standardized (i.e. each contract is the same quantity and date for everyone).[6] These contracts trade on exchanges and are guaranteed through clearing houses. Clearing houses ensure that every contract is honored and they take the opposite side of every contract. Futures contracts typically are more liquid than forward contracts and move with the market. Because of this, the farmer can minimize the risk he faces in the future through the selling of futures contracts. Futures contracts also differ from forward contracts in that delivery never happens. The exchanges and clearing houses allow the buyer or seller to leave the contract early and cash out. So tying back into the farmer selling his wheat at a future date, he will sell short futures contracts for the amount that he predicts to harvest to protect against a price decrease. The current (spot) price of wheat and the price of the futures contracts for wheat converge as time gets closer to the delivery date, so in order to make money on the hedge, the farmer must close out his position earlier than then. On the chance that prices decrease in the future, the farmer will make a profit on his short position in the futures market which offsets any decrease in revenues from the spot market for wheat. On the other hand, if prices increase, the farmer will generate a loss on the futures market which is offset by an increase in revenues on the spot market for wheat. Instead of agreeing to sell his wheat to one person on a set date, the farmer will just buy and sell futures on an exchange and then sell his wheat wherever he wants once he harvests it.[5]  A common hedging technique used in the financial industry is the long\/short equity technique.  A stock trader believes that the stock price of Company A will rise over the next month, due to the company's new and efficient method of producing widgets. They want to buy Company A shares to profit from their expected price increase, as they believe that shares are currently underpriced. But Company A is part of a highly volatile widget industry. So there is a risk of a future event that affects stock prices across the whole industry, including the stock of Company A along with all other companies.  Since the trader is interested in the specific company, rather than the entire industry, they want to hedge out the industry-related risk by short selling an equal value of shares from Company A's direct, yet weaker competitor, Company B.  The first day the trader's portfolio is:  The trader has sold short the same value of shares (the value, number of shares × price, is $1000 in both cases).  If the trader was able to short sell an asset whose price had a mathematically defined relation with Company A's stock price (for example a put option on Company A shares), the trade might be essentially riskless. In this case, the risk would be limited to the put option's premium.  On the second day, a favorable news story about the widgets industry is published and the value of all widgets stock goes up. Company A, however, because it is a stronger company, increases by 10%, while Company B increases by just 5%:  The trader might regret the hedge on day two, since it reduced the profits on the Company A position. But on the third day, an unfavorable news story is published about the health effects of widgets, and all widgets stocks crash: 50% is wiped off the value of the widgets industry in the course of a few hours. Nevertheless, since Company A is the better company, it suffers less than Company B:  Value of long position (Company A):  Value of short position (Company B):  Without the hedge, the trader would have lost $450. But the hedge – the short sale of Company B – nets a profit of $25 during a dramatic market collapse.  The introduction of stock market index futures has provided a second means of hedging risk on a single stock by selling short the market, as opposed to another single or selection of stocks. Futures are generally highly fungible and cover a wide variety of potential investments, which makes them easier to use than trying to find another stock which somehow represents the opposite of a selected investment. Futures hedging is widely used as part of the traditional long\/short play.  Employee stock options (ESOs) are securities issued by the company mainly to its own executives and employees. These securities are more volatile than stocks. An efficient way to lower the ESO risk is to sell exchange traded calls and, to a lesser degree,[clarification needed] to buy puts. Companies discourage hedging the ESOs but there is no prohibition against it.  Airlines use futures contracts and derivatives to hedge their exposure to the price of jet fuel. They know that they must purchase jet fuel for as long as they want to stay in business, and fuel prices are notoriously volatile. By using crude oil futures contracts to hedge their fuel requirements (and engaging in similar but more complex derivatives transactions), Southwest Airlines was able to save a large amount of money when buying fuel as compared to rival airlines when fuel prices in the U.S. rose dramatically after the 2003 Iraq war and Hurricane Katrina.  As an emotion regulation strategy, people can bet against a desired outcome. A New England Patriots fan, for example, could bet their opponents to win to reduce the negative emotions felt if the team loses a game. Some scientific wagers, such as Hawking's 1974 \"insurance policy\" bet, fall into this category.  People typically do not bet against desired outcomes that are important to their identity, due to negative signal about their identity that making such a gamble entails. Betting against your team or political candidate, for example, may signal to you that you are not as committed to them as you thought you were.[1]  Equity in a portfolio can be hedged by taking an opposite position in futures. To protect your stock picking against systematic market risk, futures are shorted when equity is purchased, or long futures when stock is shorted.  One way to hedge is the market neutral approach. In this approach, an equivalent dollar amount in the stock trade is taken in futures – for example, by buying 10,000 GBP worth of Vodafone and shorting 10,000 worth of FTSE futures (the index in which Vodafone trades).  Another way to hedge is the beta neutral. Beta is the historical correlation between a stock and an index. If the beta of a Vodafone stock is 2, then for a 10,000 GBP long position in Vodafone an investor would hedge with a 20,000 GBP equivalent short position in the FTSE futures.  Futures contracts and forward contracts are means of hedging against the risk of adverse market movements. These originally developed out of commodity markets in the 19th century, but over the last fifty years a large global market developed in products to hedge financial market risk.  Investors who primarily trade in futures may hedge their futures against synthetic futures. A synthetic in this case is a synthetic future comprising a call and a put position. Long synthetic futures means long call and short put at the same expiry price. To hedge against a long futures trade a short position in synthetics can be established, and vice versa.  Stack hedging is a strategy which involves buying various futures contracts that are concentrated in nearby delivery months to increase the liquidity position. It is generally used by investors to ensure the surety of their earnings for a longer period of time.  A contract for difference (CFD) is a two-way hedge or swap contract that allows the seller and purchaser to fix the price of a volatile commodity. Consider a deal between an electricity producer and an electricity retailer, both of whom trade through an electricity market pool. If the producer and the retailer agree to a strike price of $50 per MWh, for 1 MWh in a trading period, and if the actual pool price is $70, then the producer gets $70 from the pool but has to rebate $20 (the \"difference\" between the strike price and the pool price) to the retailer.  Conversely, the retailer pays the difference to the producer if the pool price is lower than the agreed upon contractual strike price. In effect, the pool volatility is nullified and the parties pay and receive $50 per MWh. However, the party who pays the difference is \"out of the money\" because without the hedge they would have received the benefit of the pool price.  Hedging can be used in many different ways including foreign exchange trading. The stock example above is a \"classic\" sort of hedge, known in the industry as a pairs trade due to the trading on a pair of related securities. As investors became more sophisticated, along with the mathematical tools used to calculate values (known as models), the types of hedges have increased greatly.  Examples of hedging include:[7]  A hedging strategy usually refers to the general risk management policy of a financially and physically trading firm how to minimize their risks. As the term hedging indicates, this risk mitigation is usually done by using financial instruments, but a hedging strategy as used by commodity traders like large energy companies, is usually referring to a business model (including both financial and physical deals).  In order to show the difference between these strategies, consider the fictional company BlackIsGreen Ltd trading coal by buying this commodity at the wholesale market and selling it to households mostly in winter.  Back-to-back (B2B) is a strategy where any open position is immediately closed, e.g. by buying the respective commodity on the spot market. This technique is often applied in the commodity market when the customers’ price is directly calculable from visible forward energy prices at the point of customer sign-up.[8]  If BlackIsGreen decides to have a B2B-strategy, they would buy the exact amount of coal at the very moment when the household customer comes into their shop and signs the contract. This strategy minimizes many commodity risks, but has the drawback that it has a large volume and liquidity risk, as BlackIsGreen does not know whether it can find enough coal on the wholesale market to fulfill the need of the households.  Tracker hedging is a pre-purchase approach, where the open position is decreased the closer the maturity date comes.  If BlackIsGreen knows that most of the consumers demand coal in winter to heat their house, a strategy driven by a tracker would now mean that BlackIsGreen buys e.g. half of the expected coal volume in summer, another quarter in autumn and the remaining volume in winter. The closer the winter comes, the better are the weather forecasts and therefore the estimate, how much coal will be demanded by the households in the coming winter.  Retail customers’ price will be influenced by long-term wholesale price trends. A certain hedging corridor around the pre-defined tracker-curve is allowed and fraction of the open positions decreases as the maturity date comes closer.  Delta-hedging mitigates the financial risk of an option by hedging against price changes in its underlying. It is so called as Delta is the first derivative of the option's value with respect to the underlying instrument's price. This is performed in practice by buying a derivative with an inverse price movement. It is also a type of market neutral strategy.  Only if BlackIsGreen chooses to perform delta-hedging as strategy, actual financial instruments come into play for hedging (in the usual, stricter meaning).  Risk reversal means simultaneously buying a call option and selling a put option. This has the effect of simulating being long on a stock or commodity position.  Many hedges do not involve exotic financial instruments or derivatives such as the married put. A natural hedge is an investment that reduces the undesired risk by matching cash flows (i.e. revenues and expenses). For example, an exporter to the United States faces a risk of changes in the value of the U.S. dollar and chooses to open a production facility in that market to match its expected sales revenue to its cost structure.  Another example is a company that opens a subsidiary in another country and borrows in the foreign currency to finance its operations, even though the foreign interest rate may be more expensive than in its home country: by matching the debt payments to expected revenues in the foreign currency, the parent company has reduced its foreign currency exposure. Similarly, an oil producer may expect to receive its revenues in U.S. dollars, but faces costs in a different currency; it would be applying a natural hedge if it agreed to, for example, pay bonuses to employees in U.S. dollars.  One common means of hedging against risk is the purchase of insurance to protect against financial loss due to accidental property damage or loss, personal injury, or loss of life.  There are various types of financial risk that can be protected against with a hedge. Those include: "},"meta":{},"created_at":"2025-03-22T14:25:42.289399Z","updated_at":"2025-03-22T14:25:42.289399Z","inner_id":68,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":77,"annotations":[{"id":77,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.321899Z","updated_at":"2025-03-22T14:25:42.321899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"75296330-1db9-4f49-a674-3156b2b449e4","import_id":null,"last_action":null,"bulk_created":false,"task":77,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Economic sanctions or embargoes are commercial and financial penalties applied by states or institutions against states, groups, or individuals.[1][2] Economic sanctions are a form of coercion that attempts to get an actor to change its behavior through disruption in economic exchange. Sanctions can be intended to compel (an attempt to change an actor's behavior) or deterrence (an attempt to stop an actor from certain actions).[3][4][5]  Sanctions can target an entire country or they can be more narrowly targeted at individuals or groups; this latter form of sanctions are sometimes called \"smart sanctions\".[6] Prominent forms of economic sanctions include trade barriers, asset freezes, travel bans, arms embargoes, and restrictions on financial transactions.  The efficacy of sanctions in achieving intended goals is a subject of debate.[1][2][3][4][6][7] Scholars have also considered the policy externalities of sanctions.[7][8] The humanitarian consequences of country-wide sanctions have been a subject of controversy.[9] As a consequence, since the mid-1990s, United Nations Security Council (UNSC) sanctions have tended to target individuals and entities, in contrast to the country-wide sanctions of earlier decades.[10]  One of the most comprehensive attempts at an embargo occurred during the Napoleonic Wars of 1803–1815. Aiming to cripple the United Kingdom economically, Emperor Napoleon I of France in 1806 promulgated the Continental System—which forbade European nations from trading with the UK. In practice the French Empire could not completely enforce the embargo, which proved as harmful (if not more so) to the continental nations involved as to the British.[11] By the time of the Hague Conventions of 1899 and 1907, diplomats and legal scholars regularly discussed using coordinated economic pressure to enforce international law. This idea was also included in reform proposals by Latin American and Chinese international lawyers in the years leading up to World War I.[12]  Sanctions in the form of blockades were prominent during World War I.[13] Debates about implementing sanctions through international organizations, such as the League of Nations, became prominent after the end of World War I.[14] Leaders saw sanctions as a viable alternative to war.[15]  The League Covenant permitted the use of sanctions in five cases:[16]  The Abyssinia Crisis in 1935 resulted in League sanctions against Mussolini's Italy under Article 16 of the Covenant. Oil supplies, however, were not stopped, nor the Suez Canal closed to Italy, and the conquest proceeded. The sanctions were lifted in 1936 and Italy left the League in 1937.[17][18][19][20]  In the lead-up to the Japanese attack on Pearl Harbor in 1941, the United States imposed severe trade restrictions on Japan to discourage further Japanese conquests in East Asia.[15]  After World War II, the League was replaced by the more expansive United Nations (UN) in 1945. Throughout the Cold War, the use of sanctions increased gradually.[15] After the end of the Cold War, there was a major increase in economic sanctions.[9]  According to the Global Sanctions Data Base, there have been 1,325 sanctions in the period 1950–2022.[15]  Economic sanctions are used as a tool of foreign policy by many governments. Economic sanctions are usually imposed by a larger country upon a smaller country for one of two reasons: either the latter is a perceived threat to the security of the former nation or that country treats its citizens unfairly. They can be used as a coercive measure for achieving particular policy goals related to trade or for humanitarian violations. Economic sanctions are used as an alternative weapon instead of going to war to achieve desired outcomes.  The Global Sanctions Data Base categorizes nine objectives of sanctions: \"changing policy, destabilizing regimes, resolving territorial conflicts, fighting terrorism, preventing war, ending war, restoring and promoting human rights, restoring and promoting democracy, and other objectives.\"[15]  According to a study by Neuenkirch and Neumeier, UN economic sanctions had a statistically significant impact on targeted states by reducing their GDP growth by an average of 2.3–3.5% per year—and more than 5% per year in the case of comprehensive UN embargoes—with the negative effects typically persisting for a period of ten years. By contrast, unilateral US sanctions had a considerably smaller impact on GDP growth, restricting it by 0.5–0.9% per year, with an average duration of seven years.[21]  Oryoie, A. R. demonstrates that economic sanctions result in welfare losses across all income groups in Iran, with wealthier groups suffering greater losses compared to poorer groups.[22]  Imposing sanctions on an opponent also affects the economy of the imposing country to a degree. If import restrictions are promulgated, consumers in the imposing country may have restricted choices of goods. If export restrictions are imposed or if sanctions prohibit companies in the imposing country from trading with the target country, the imposing country may lose markets and investment opportunities to competing countries.[23]  Hufbauer, Schott, and Elliot (2008) argue that regime change is the most frequent foreign-policy objective of economic sanctions, accounting for just over 39 percent of cases of their imposition.[24] Hufbauer et al. found that 34 percent of the cases studied were successful.[25] However, when Robert A. Pape examined their study, he found that only 5 of their reported 40 successes were actually effective,[26] reducing the success rate to 4%. In either case, the difficulty and unexpected nuances of measuring the actual success of sanctions in relation to their goals are both increasingly apparent and still under debate. In other words, it is difficult to determine why a regime or country changes (i.e., whether it was the sanction or inherent instability) and doubly so to measure the full political effect of a given action.[27]  Offering an explanation as to why sanctions are still imposed even when they may be marginally effective, British diplomat Jeremy Greenstock suggests sanctions are popular not because they are known to be effective, but because \"there is nothing else [to do] between words and military action if you want to bring pressure upon a government\".[28] Critics of sanctions like Belgian jurist Marc Bossuyt argue that in nondemocratic regimes, the extent to which this affects political outcomes is contested, because by definition such regimes do not respond as strongly to the popular will.[29]  A strong connection has been found between the effectiveness of sanctions and the size of veto players in a government. Veto players represent individual or collective actors whose agreement is required for a change of the status quo, for example, parties in a coalition, or the legislature's check on presidential powers. When sanctions are imposed on a country, it can try to mitigate them by adjusting its economic policy. The size of the veto players determines how many constraints the government will face when trying to change status quo policies, and the larger the size of the veto players, the more difficult it is to find support for new policies, thus making the sanctions more effective.[30]  Francesco Giumelli writes that the \"set of sanctions ... that many observers would be likely to consider the most persuasive (and effective)\", namely, UN sanctions against \"central bank assets and sovereign wealth funds\", are \"of all the types of measures applied ... the one least frequently used\".[10] Giumelli also distinguishes between sanctions against international terrorists, in which \"the nature of the request is not as important as the constraining aspect\", and sanctions imposed in connection with \"post-conflict scenarios\", which should \"include flexible demands and the potential for adaptation if the situation changes\".[10]  Economic sanctions can be used for achieving domestic and international purposes.[31]  Foreign aid suspensions are typically considered as a type of economic sanctions. Previously mentioned work by Hufbauer, Schott, Elliot, and Oegg is a prominent example.[32] Claas Mertens finds that \"suspending aid is more effective than adopting economic sanctions because (1) aid suspensions are economically beneficial for the adopting state, while sanctions are costly, (2) aid suspensions directly affect the targeted government's budget, (3) market forces undermine sanctions but not aid suspensions, and (4) aid suspensions are less likely to spark adverse behavioral reactions. [...] The findings suggest that economic sanctions are less effective than previously thought and that large donor states have a higher chance of achieving political goals through economic coercion.\"[33]  Sanctions have been criticized on humanitarian grounds, as they negatively impact a nation's economy and can also cause collateral damage on ordinary citizens. Peksen implies that sanctions can degenerate human rights in the target country.[34] Some policy analysts believe that imposing trade restrictions only serves to hurt ordinary people as opposed to government elites,[35][36][37][38] and others have likened the practice to siege warfare.[39][40] The United Nations Security Council (UNSC) has generally refrained from imposing comprehensive sanctions since the mid-1990s, in part due to the controversy over the efficacy and civilian harms attributed to the sanctions against Iraq.[10]  Sanctions can have unintended consequences.[41]  One of the most popular suggestions to combat the humanitarian issues that arise from sanctions is the concept of \"smart sanctions\", and a lot of research has been done on this concept also known as targeted sanctions.[42] The term \"smart sanctions\" refers to measures like asset freezes, travel bans, and arms embargoes that aim to target responsible parties like political leaders and elites with the goal of avoiding causing widespread collateral damage to innocent civilians and neighboring nations.[42]  Though there has been enthusiasm about the concept, as of 2016, the Targeted Sanctions Consortium (TSC) found that targeted sanctions only result in policy goals being met 22% of the time.[43]  Smart Sanctions have also not been totally successful in avoiding civilian harm or unintended consequences.[42] For example, arms embargoes can impact the self-defense efforts of those under attack, aviation bans can affect a nation's transportation sector and the jobs of civilians associated with them, and financial sanctions targeting individuals raise due process issues.[42] One example of smart sanctions in practice can be seen with sanctions imposed by the United States on the Russian Federation following the latter's 2014 annexation of Crimea, which were intended to exert pressure on Russia's financial sector.[44] The sanctions resulted in American credit card companies Visa and MasterCard suspending all transactions of sanctioned Russian banks, effectively canceling the credit cards of ordinary Russian consumers.[44]  There is an importance, especially with relation to financial loss, for companies to be aware of embargoes that apply to their intended export or import destinations.[45] Properly preparing products for trade, sometimes referred to as an embargo check, is a difficult and timely process for both importers and exporters.[46]  There are many steps that must be taken to ensure that a business entity does not accrue unwanted fines, taxes, or other punitive measures.[47] Common examples of embargo checks include referencing embargo lists,[48][49][50] cancelling transactions, and ensuring the validity of a trade entity.[51]  This process can become very complicated, especially for countries with changing embargoes. Before better tools became available, many companies relied on spreadsheets and manual processes to keep track of compliance issues. Today, there are software based solutions that automatically handle sanctions and other complications with trade.[52][53][54]  The United States Embargo of 1807 involved a series of laws passed by the US Congress (1806–1808) during the second term of President Thomas Jefferson.[55] Britain and France were engaged in the War of the Fourth Coalition; the US wanted to remain neutral and to trade with both sides, but both countries objected to American trade with the other.[56] American policy aimed to use the new laws to avoid war and to force both France and Britain to respect American rights.[57] The embargo failed to achieve its aims, and Jefferson repealed the legislation in March 1809.  The United States embargo against Cuba began on March 14, 1958, during the overthrow of dictator Fulgencio Batista by Fidel Castro during the Cuban Revolution. At first, the embargo applied only to arms sales; however, it later expanded to include other imports, eventually extending to almost all trade on February 7, 1962.[58] Referred to by Cuba as \"el bloqueo\" (the blockade),[59] the US embargo on Cuba remains as of 2022[update] one of the longest-standing embargoes in modern history.[60] Few of the United States' allies embraced the embargo, and many have argued it has been ineffective in changing Cuban government behavior.[61] While taking some steps to allow limited economic exchanges with Cuba, American President Barack Obama nevertheless reaffirmed the policy in 2011, stating that without the granting of improved human rights and freedoms by Cuba's current government, the embargo remains \"in the national interest of the United States\".[62]  Russia has been known to utilize economic sanctions to achieve its political goals. Russia's focus has been primarily on implementing sanctions against the pro-Western governments of former Soviet Union states. The Kremlin's aim is particularly on states that aspire to join the European Union and NATO, such as Ukraine, Moldova, and Georgia.[63] Russia has enacted a law, the Dima Yakovlev Law, that defines sanctions against US citizens involved in \"violations of the human rights and freedoms of Russian citizens\". It lists US citizens who are banned from entering Russia.[64]  Viktor Yushchenko, the third president of Ukraine who was elected in 2003, lobbied during his term to gain admission to NATO and the EU.[65] Soon after Yushchenko entered office, Russia demanded Kyiv pay the same rate that it charged Western European states. This quadrupled Ukraine's energy bill overnight.[65] Russia subsequently cut off the supply of natural gas in 2006, causing significant harm to the Ukrainian and Russian economies.[66] As the Ukrainian economy began to struggle, Yushchenko's approval ratings dropped significantly; reaching the single digits by the 2010 election; Viktor Yanukovych, who was more supportive of Moscow won the election in 2010 to become the fourth president of Ukraine. After his election, gas prices were reduced substantially.[65]  The Rose Revolution in Georgia brought Mikheil Saakashvili to power as the third president of the country. Saakashvili wanted to bring Georgia into NATO and the EU and was a strong  supporter of the US-led war in Iraq and Afghanistan.[67] Russia would soon implement a number of different sanctions on Georgia, including natural gas price raises through Gazprom and wider trade sanctions that impacted the Georgian economy, particularly Georgian exports of wine, citrus fruits, and mineral water. In 2006, Russia banned all imports from Georgia which was able to deal a significant blow to the Georgian economy.[67] Russia also expelled nearly 2,300 Georgians who worked within its borders.[67]  The United Nations issues sanctions by consent of the United Nations Security Council (UNSC) and\/or General Assembly in response to major international events, receiving authority to do so under Article 41 of Chapter VII of the United Nations Charter.[68] The nature of these sanctions may vary, and include financial, trade, or weaponry restrictions. Motivations can also vary, ranging from humanitarian and environmental concerns[69] to efforts to halt nuclear proliferation. Over two dozen sanctions measures have been implemented by the United Nations since its founding in 1945.[68]  Most UNSC sanctions since the mid-1990s have targeted individuals and entities rather than entire governments, a change from the comprehensive trade sanctions of earlier decades. For example, the UNSC maintains lists of individuals indicted for crimes or linked to international terrorism, which raises novel legal questions regarding due process. According to a dataset covering the years 1991 to 2013, 95% of UNSC sanction regimes included \"sectoral bans\" on aviation and\/or the import (or export) of arms or raw materials, 75% included \"individual\/group\" sanctions such as asset freezes or restrictions on travel, and just 10% targeted national finances or included measures against central banks, sovereign wealth funds, or foreign investment. The most frequently used UNSC sanction documented in the dataset is an embargo against imported weapons, which applied in 87% of all cases and was directed against non-state actors more often than against governments. Targeted sanctions regimes may contain hundreds of names, a handful, or none at all.[10]  The UN implemented sanctions against Somalia beginning in April 1992, after the overthrow of the Siad Barre regime in 1991 during the Somali Civil War. UNSC Resolution 751 forbade members to sell, finance, or transfer any military equipment to Somalia.[70]  The UNSC passed Resolution 1718 in 2006 in response to a nuclear test that the Democratic People's Republic of Korea (DPRK) conducted in violation of the Treaty on Non-Proliferation of Nuclear Weapons. The resolution banned the sale of military and luxury goods and froze government assets.[71] Since then, the UN has passed multiple resolutions subsequently expanding sanctions on North Korea. Resolution 2270 from 2016 placed restrictions on transport personnel and vehicles employed by North Korea while also restricting the sale of natural resources and fuel for aircraft.[72]  The efficacy of such sanctions has been questioned in light of continued nuclear tests by North Korea in the decade following the 2006 resolution. Professor William Brown of Georgetown University argued that \"sanctions don't have much of an impact on an economy that has been essentially bankrupt for a generation\".[73]  On February 26, 2011, the UNSC issued an arms embargo against the Libya through Security Council Resolution 1970 in response to humanitarian abuses occurring in the First Libyan Civil War.[74] The embargo was later extended to mid-2018. Under the embargo, Libya has suffered severe inflation because of increased dependence on the private sector to import goods.[75] The sanctions caused large cuts to health and education, which caused social conditions to decrease. Even though the sanctions were in response to human rights, their effects were limited.[76]  In 2013 the UN decreed an arms embargo against the CAR. The arms embargo was established in the context of an intercommunity conflict between the Séléka rebels, with a Muslim majority, and the predominantly Christian militias. to fight back. Raised UN Security Council lifts arms embargo on CAR on August 1, 2024.[77]  In effort to punish South Africa for its policies of apartheid, the United Nations General Assembly adopted a voluntary international oil-embargo against South Africa on November 20, 1987; that embargo had the support of 130 countries.[78] South Africa, in response, expanded its Sasol production of synthetic crude.[79]  All United Nations sanctions on South Africa ended over the Negotiations to end Apartheid, Resolution 919 and the 1994 South African elections, in which Nelson Mandela was elected as the first post-Apartheid president.  When asked in 1993 if economic sanctions had helped end apartheid, Mandela replied \"Oh, there is no doubt.\"[80]  The United States, Britain, the Republic of China and the Netherlands imposed sanctions against Japan in 1940–1941 in response to its expansionism. Deprived of access to vital oil, iron-ore and steel supplies, Japan started planning for military action to seize the resource-rich Dutch East Indies, which required a preemptive attack on Pearl Harbor, triggering the American entry into the Pacific War.[81]  In 1973–1974, OAPEC instigated the 1973 oil crisis through its oil embargo against the United States and other industrialized nations that supported Israel in the Yom Kippur War. The results included a sharp rise in oil prices and in OPEC revenues, an emergency period of energy rationing, a global economic recession, large-scale conservation efforts, and long-lasting shifts toward natural gas, ethanol, nuclear and other alternative energy sources.[82][83] Israel continued to receive Western support, however.  In 2010, the European Union made the decision to sanction Iran due to their involvement in their nuclear program.[84] Theresa Papademetriou states the exact restrictions the EU posed on Iran, \"prohibition on the provision of insurance, increased restrictions on and notifications needed for transfers of funds to and from Iran, restrictions on the supply of or traffic in technology and equipment to be used in certain oil and gas fields and prohibition of investment in such fields, expansion of the list of goods and technology whose supply to Iran is either subject to prior authorization or is completely banned and new visa restrictions.” [84] Also in 2010, the UN Council imposed sanctions on Iran due to their involvement in their nuclear program.[85] These sanctions banned Iran from carrying out tests on their nuclear weapons and imposed an embargo on the transfer of weapons into the country.[85] These sanctions resulted in drastic macroeconomic downturns for the Iranian economy including volatility in GDP, increase in unemployment, and increase in inflation.[86]  List of sanctioned countries (the below is not an exhaustive list):[87]   This article incorporates public domain material from European Union: Renewed Sanctions Against Iran. Library of Congress. Retrieved 2023-12-14. "},"meta":{},"created_at":"2025-03-22T14:25:42.289399Z","updated_at":"2025-03-22T14:25:42.289399Z","inner_id":69,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":78,"annotations":[{"id":78,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.321899Z","updated_at":"2025-03-22T14:25:42.321899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"18660f01-fd6d-45e6-9a2e-b6db084a4e0d","import_id":null,"last_action":null,"bulk_created":false,"task":78,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  A cryptocurrency (colloquially crypto) is a digital currency designed to work through a computer network that is not reliant on any central authority, such as a government or bank, to uphold or maintain it.[2]  Individual coin ownership records are stored in a digital ledger or blockchain, which is a computerized database that uses a consensus mechanism to secure transaction records, control the creation of additional coins, and verify the transfer of coin ownership.[3][4][5] The two most common consensus mechanisms are proof of work and proof of stake.[6] Despite the name, which has come to describe many of the fungible blockchain tokens that have been created, cryptocurrencies are not considered to be currencies in the traditional sense, and varying legal treatments have been applied to them in various jurisdicitons, including classification as commodities, securities, and currencies. Cryptocurrencies are generally viewed as a distinct asset class in practice.[7][8][9]  The first cryptocurrency was bitcoin, which was first released as open-source software in 2009. As of June 2023, there were more than 25,000 other cryptocurrencies in the marketplace, of which more than 40 had a market capitalization exceeding $1 billion.[10]  In 1983, American cryptographer David Chaum conceived of a type of cryptographic electronic money called ecash.[11][12] Later, in 1995, he implemented it through Digicash,[13] an early form of cryptographic electronic payments. Digicash required user software in order to withdraw notes from a bank and designate specific encrypted keys before they could be sent to a recipient. This allowed the digital currency to be untraceable by a third party.  In 1996, the National Security Agency published a paper entitled How to Make a Mint: The Cryptography of Anonymous Electronic Cash, describing a cryptocurrency system. The paper was first published in an MIT mailing list (October 1996) and later (April 1997) in The American Law Review.[14]  In 1998, Wei Dai described \"b-money,\" an anonymous, distributed electronic cash system.[15] Shortly thereafter, Nick Szabo described bit gold.[16] Like bitcoin and other cryptocurrencies that would follow it, bit gold (not to be confused with the later gold-based exchange BitGold) was described as an electronic currency system that required users to complete a proof of work function with solutions being cryptographically put together and published.  In January 2009, bitcoin was created by pseudonymous developer Satoshi Nakamoto. It used SHA-256, a cryptographic hash function, in its proof-of-work scheme.[17][18] In April 2011, Namecoin was created as an attempt at forming a decentralized DNS. In October 2011, Litecoin was released, which used scrypt as its hash function instead of SHA-256. Peercoin, created in August 2012, used a hybrid of proof-of-work and proof-of-stake.[19]  Cryptocurrency has undergone several periods of growth and retraction, including several bubbles and market crashes, such as in 2011, 2013–2014\/15, 2017–2018, and 2021–2023.[20][21]  On 6 August 2014, the UK announced its Treasury had commissioned a study of cryptocurrencies and what role, if any, they could play in the UK economy. The study was also to report on whether regulation should be considered.[22] Its final report was published in 2018,[23] and it issued a consultation on cryptoassets and stablecoins in January 2021.[24]  In June 2021, El Salvador became the first country to accept bitcoin as legal tender, after the Legislative Assembly had voted 62–22 to pass a bill submitted by President Nayib Bukele classifying the cryptocurrency as such.[25]  In August 2021, Cuba followed with Resolution 215 to recognize and regulate cryptocurrencies such as bitcoin.[26]  In September 2021, the government of China, the single largest market for cryptocurrency, declared all cryptocurrency transactions illegal. This completed a crackdown on cryptocurrency that had previously banned the operation of intermediaries and miners within China.[27]  On 15 September 2022, the world's second largest cryptocurrency at that time, Ethereum, transitioned its consensus mechanism from proof-of-work (PoW) to proof-of-stake (PoS) in an upgrade process known as \"the Merge\".  According to the Ethereum Founder, the upgrade would cut both Ethereum's energy use and carbon-dioxide emissions by 99.9%.[28]  On 11 November 2022, FTX Trading Ltd., a cryptocurrency exchange, which also operated a crypto hedge fund, and had been valued at $18 billion,[29] filed for bankruptcy.[30] The financial impact of the collapse extended beyond the immediate FTX customer base, as reported,[31] while, at a Reuters conference, financial industry executives said that \"regulators must step in to protect crypto investors.\"[32] Technology analyst Avivah Litan commented on the cryptocurrency ecosystem that \"everything...needs to improve dramatically in terms of user experience, controls, safety, customer service.\"[33]  According to Jan Lansky, a cryptocurrency is a system that meets six conditions:[34]  In March 2018, the word cryptocurrency was added to the Merriam-Webster Dictionary.[35]  After the early innovation of bitcoin in 2008 and the early network effect gained by bitcoin, tokens, cryptocurrencies, and other digital assets that were not bitcoin became collectively known during the 2010s as alternative cryptocurrencies,[36][37][38] or \"altcoins\".[39] Sometimes the term \"alt coins\" was used,[40][41] or disparagingly, \"shitcoins\".[42] Paul Vigna of The Wall Street Journal described altcoins in 2020 as \"alternative versions of Bitcoin\"[43] given its role as the model protocol for cryptocurrency designers. A Polytechnic University of Catalonia thesis in 2021 used a broader description, including not only alternative versions of bitcoin but every cryptocurrency other than bitcoin. As of early 2020, there were more than 5,000 cryptocurrencies.   Altcoins often have underlying differences when compared to bitcoin. For example, Litecoin aims to process a block every 2.5 minutes, rather than bitcoin's 10 minutes which allows Litecoin to confirm transactions faster than bitcoin.[19] Another example is Ethereum, which has smart contract functionality that allows decentralized applications to be run on its blockchain.[44] Ethereum was the most used blockchain in 2020, according to Bloomberg News.[45] In 2016, it had the largest \"following\" of any altcoin, according to the New York Times.[46]  Significant market price rallies across multiple altcoin markets are often referred to as an \"altseason\".[47][48]  Stablecoins are cryptocurrencies designed to maintain a stable level of purchasing power.[49] Notably, these designs are not foolproof, as a number of stablecoins have crashed or lost their peg. For example, on 11 May 2022, Terra's stablecoin UST fell from $1 to 26 cents.[50][51] The subsequent failure of Terraform Labs resulted in the loss of nearly $40B invested in the Terra and Luna coins.[52] In September 2022, South Korean prosecutors requested the issuance of an Interpol Red Notice against the company's founder, Do Kwon.[53] In Hong Kong, the expected regulatory framework for stablecoins in 2023\/24 is being shaped and includes a few considerations.[54]  Memecoins are a category of cryptocurrencies that originated from Internet memes or jokes. The most notable example is Dogecoin, a memecoin featuring the Shiba Inu dog from the Doge meme.[55] Memecoins are known for extreme volatility; for example, the record-high value for a Dogecoin was 73 cents, but that had plunged to 13 cents by mid-2024.[55] Scams are prolific among memecoins.[55]  Physical cryptocurrency coins have been made as promotional items and some have become collectibles.[56] Some of these have a private key embedded in them to access crypto worth a few dollars. There have also been attempts to issue bitcoin \"bank notes\".[57]  The term \"physical bitcoin\" is used in the finance industry when investment funds that hold crypto purchased from crypto exchanges put their crypto holdings in a specialised bank called a \"custodian\".[58]  These physical representations of cryptocurrency do not hold any value by themselves; these are only utilized for collectable purposes.  Cryptocurrency is produced by an entire cryptocurrency system collectively, at a rate that is defined when the system is created and that is publicly stated. In centralized banking and economic systems such as the US Federal Reserve System, corporate boards or governments control the supply of currency.[citation needed] In the case of cryptocurrency, companies or governments cannot produce new units and have not so far provided backing for other firms, banks, or corporate entities that hold asset value measured in it. The underlying technical system upon which cryptocurrencies are based was created by Satoshi Nakamoto.[59]  Within a proof-of-work system such as bitcoin, the safety, integrity, and balance of ledgers are maintained by a community of mutually distrustful parties referred to as miners. Miners use their computers to help validate and timestamp transactions, adding them to the ledger in accordance with a particular timestamping scheme.[17] In a proof-of-stake blockchain, transactions are validated by holders of the associated cryptocurrency, sometimes grouped together in stake pools.  Most cryptocurrencies are designed to gradually decrease the production of that currency, placing a cap on the total amount of that currency that will ever be in circulation.[60] Compared with ordinary currencies held by financial institutions or kept as cash on hand, cryptocurrencies can be more difficult for seizure by law enforcement.[3]  The validity of each cryptocurrency's coins is provided by a blockchain. A blockchain is a continuously growing list of records, called blocks, which are linked and secured using cryptography.[59][61] Each block typically contains a hash pointer as a link to a previous block,[61] a timestamp, and transaction data.[62] By design, blockchains are inherently resistant to modification of the data. A blockchain is \"an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way\".[63] For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without the alteration of all subsequent blocks, which requires collusion of the network majority.  Blockchains are secure by design and are an example of a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been achieved with a blockchain.[64]  A node is a computer that connects to a cryptocurrency network. The node supports the cryptocurrency's network through either relaying transactions, validation, or hosting a copy of the blockchain. In terms of relaying transactions, each network computer (node) has a copy of the blockchain of the cryptocurrency it supports. When a transaction is made, the node creating the transaction broadcasts details of the transaction using encryption to other nodes throughout the node network so that the transaction (and every other transaction) is known.  Node owners are either volunteers, those hosted by the organization or body responsible for developing the cryptocurrency blockchain network technology, or those who are enticed to host a node to receive rewards from hosting the node network.[65]  Cryptocurrencies use various timestamping schemes to \"prove\" the validity of transactions added to the blockchain ledger without the need for a trusted third party.  The first timestamping scheme invented was the proof-of-work scheme. The most widely used proof-of-work schemes are based on SHA-256 and scrypt.[19]  Some other hashing algorithms that are used for proof-of-work include CryptoNote, Blake, SHA-3, and X11.  Another method is called the proof-of-stake scheme. Proof-of-stake is a method of securing a cryptocurrency network and achieving distributed consensus through requesting users to show ownership of a certain amount of currency. It is different from proof-of-work systems that run difficult hashing algorithms to validate electronic transactions. The scheme is largely dependent on the coin, and there is currently no standard form of it. Some cryptocurrencies use a combined proof-of-work and proof-of-stake scheme.[19]  On a blockchain, mining is the validation of transactions. For this effort, successful miners obtain new cryptocurrency as a reward. The reward decreases transaction fees by creating a complementary incentive to contribute to the processing power of the network. The rate of generating hashes, which validate any transaction, has been increased by the use of specialized hardware such as FPGAs and ASICs running complex hashing algorithms like SHA-256 and scrypt.[66] This arms race for cheaper-yet-efficient machines has existed since bitcoin was introduced in 2009.[66] Mining is measured by hash rate, typically in TH\/s.[67] A 2023 IMF working paper found that crypto mining could generate 450 million tons of CO2 emissions by 2027, accounting for 0.7 percent of global emissions, or 1.2 percent of the world total[68]  With more people entering the world of virtual currency, generating hashes for validation has become more complex over time, forcing miners to invest increasingly large sums of money to improve computing performance. Consequently, the reward for finding a hash has diminished and often does not justify the investment in equipment and cooling facilities (to mitigate the heat the equipment produces) and the electricity required to run them.[69] Popular regions for mining include those with inexpensive electricity, a cold climate, and jurisdictions with clear and conducive regulations. By July 2019, bitcoin's electricity consumption was estimated to be approximately 7 gigawatts, around 0.2% of the global total, or equivalent to the energy consumed nationally by Switzerland.[70]  Some miners pool resources, sharing their processing power over a network to split the reward equally, according to the amount of work they contributed to the probability of finding a block. A \"share\" is awarded to members of the mining pool who present a valid partial proof-of-work.  As of February 2018[update], the Chinese government has halted trading of virtual currency, banned initial coin offerings, and shut down mining. Many Chinese miners have since relocated to Canada[71] and Texas.[72] One company is operating data centers for mining operations at Canadian oil and gas field sites due to low gas prices.[73] In June 2018, Hydro Quebec proposed to the provincial government to allocate 500 megawatts of power to crypto companies for mining.[74] According to a February 2018 report from Fortune, Iceland has become a haven for cryptocurrency miners in part because of its cheap electricity.[75]  In March 2018, the city of Plattsburgh, New York put an 18-month moratorium on all cryptocurrency mining in an effort to preserve natural resources and the \"character and direction\" of the city.[76] In 2021, Kazakhstan became the second-biggest crypto-currency mining country, producing 18.1% of the global exahash rate. The country built a compound containing 50,000 computers near Ekibastuz.[77]  An increase in cryptocurrency mining increased the demand for graphics cards (GPU) in 2017.[78] The computing power of GPUs makes them well-suited to generating hashes. Popular favorites of cryptocurrency miners, such as Nvidia's GTX 1060 and GTX 1070 graphics cards, as well as AMD's RX 570 and RX 580 GPUs, doubled or tripled in price – or were out of stock.[79] A GTX 1070 Ti, which was released at a price of $450, sold for as much as $1,100. Another popular card, the GTX 1060 (6 GB model), was released at an MSRP of $250 and sold for almost $500. RX 570 and RX 580 cards from AMD were out of stock for almost a year. Miners regularly buy up the entire stock of new GPUs as soon as they are available.[80]  Nvidia has asked retailers to do what they can when it comes to selling GPUs to gamers instead of miners. Boris Böhles, PR manager for Nvidia in the German region, said: \"Gamers come first for Nvidia.\"[81]  Numerous companies developed dedicated crypto-mining accelerator chips, capable of price-performance far higher than that of CPU or GPU mining. At one point, Intel marketed its own brand of crypto accelerator chip, named Blockscale.[82]  A cryptocurrency wallet is a means of storing the public and private \"keys\" (address) or seed, which can be used to receive or spend the cryptocurrency.[83] With the private key, it is possible to write in the public ledger, effectively spending the associated cryptocurrency. With the public key, it is possible for others to send currency to the wallet.  There exist multiple methods of storing keys or seed in a wallet. These methods range from using paper wallets (which are public, private, or seed keys written on paper), to using hardware wallets (which are hardware to store your wallet information), to a digital wallet (which is a computer with software hosting your wallet information), to hosting your wallet using an exchange where cryptocurrency is traded, or by storing your wallet information on a digital medium such as plaintext.[84]  Bitcoin is pseudonymous, rather than anonymous; the cryptocurrency in a wallet is not tied to a person but rather to one or more specific keys (or \"addresses\").[85] Thereby, bitcoin owners are not immediately identifiable, but all transactions are publicly available in the blockchain.[86] Still, cryptocurrency exchanges are often required by law to collect the personal information of their users.[87]  Some cryptocurrencies, such as Monero, Zerocoin, Zerocash, and CryptoNote, implement additional measures to increase privacy, such as by using zero-knowledge proofs.[88][89]  A recent 2020 study presented different attacks on privacy in cryptocurrencies. The attacks demonstrated how the anonymity techniques are not sufficient safeguards. In order to improve privacy, researchers suggested several different ideas, including new cryptographic schemes and mechanisms for hiding the IP address of the source.[90]  Cryptocurrencies are used primarily outside banking and governmental institutions and are exchanged over the Internet.  Proof-of-work cryptocurrencies, such as bitcoin, offer block rewards incentives for miners. There has been an implicit belief that whether miners are paid by block rewards or transaction fees does not affect the security of the blockchain, but a study suggests that this may not be the case under certain circumstances.[91]  The rewards paid to miners increase the supply of the cryptocurrency. By making sure that verifying transactions is a costly business, the integrity of the network can be preserved as long as benevolent nodes control a majority of computing power. The verification algorithm requires a lot of processing power, and thus electricity, in order to make verification costly enough to accurately validate the public blockchain. Not only do miners have to factor in the costs associated with expensive equipment necessary to stand a chance of solving a hash problem, they must further consider the significant amount of electrical power in search of the solution. Generally, the block rewards outweigh electricity and equipment costs, but this may not always be the case.[92]  The current value, not the long-term value, of the cryptocurrency supports the reward scheme to incentivize miners to engage in costly mining activities.[93] In 2018, bitcoin's design caused a 1.4% welfare loss compared to an efficient cash system, while a cash system with 2% money growth has a minor 0.003% welfare cost. The main source for this inefficiency is the large mining cost, which is estimated to be US$360 million per year. This translates into users being willing to accept a cash system with an inflation rate of 230% before being better off using bitcoin as a means of payment. However, the efficiency of the bitcoin system can be significantly improved by optimizing the rate of coin creation and minimizing transaction fees. Another potential improvement is to eliminate inefficient mining activities by changing the consensus protocol altogether.[94]  Transaction fees (sometimes also referred to as miner fees or gas fees) for cryptocurrency depend mainly on the supply of network capacity at the time, versus the demand from the currency holder for a faster transaction.[95] The ability for the holder to be allowed to set the fee manually often depends on the wallet software used, and central exchanges for cryptocurrency (CEX) usually do not allow the customer to set a custom transaction fee for the transaction.[citation needed] Their wallet software, such as Coinbase Wallet, however, might support adjusting the fee.[96]  Select cryptocurrency exchanges have offered to let the user choose between different presets of transaction fee values during the currency conversion. One of those exchanges, namely LiteBit, previously headquartered in the Netherlands, was forced to cease all operations on August 13th, 2023, \"due to market changes and regulatory pressure\".[97]  The \"recommended fee\" suggested by the network will often depend on the time of day (due to depending on network load).  For Ethereum, transaction fees differ by computational complexity, bandwidth use, and storage needs, while bitcoin transaction fees differ by transaction size and whether the transaction uses SegWit. In February 2023, the median transaction fee for Ether corresponded to $2.2845,[98] while for bitcoin it corresponded to $0.659.[99]  Some cryptocurrencies have no transaction fees, the most well-known example being Nano (XNO), and instead rely on client-side proof-of-work as the transaction prioritization and anti-spam mechanism.[100][101][102]  Cryptocurrency exchanges allow customers to trade cryptocurrencies[103] for other assets, such as conventional fiat money, or to trade between different digital currencies.  Crypto marketplaces do not guarantee that an investor is completing a purchase or trade at the optimal price. As a result, as of 2020, it was possible to arbitrage to find the difference in price across several markets.[104]  Atomic swaps are a mechanism where one cryptocurrency can be exchanged directly for another cryptocurrency without the need for a trusted third party, such as an exchange.[105]  Jordan Kelley, founder of Robocoin, launched the first bitcoin ATM in the United States on 20 February 2014. The kiosk installed in Austin, Texas, is similar to bank ATMs but has scanners to read government-issued identification such as a driver's license or a passport to confirm users' identities.[106]  An initial coin offering (ICO) is a controversial means of raising funds for a new cryptocurrency venture. An ICO may be used by startups with the intention of avoiding regulation. However, securities regulators in many jurisdictions, including in the U.S. and Canada, have indicated that if a coin or token is an \"investment contract\" (e.g., under the Howey test, i.e., an investment of money with a reasonable expectation of profit based significantly on the entrepreneurial or managerial efforts of others), it is a security and is subject to securities regulation. In an ICO campaign, a percentage of the cryptocurrency (usually in the form of \"tokens\") is sold to early backers of the project in exchange for legal tender or other cryptocurrencies, often bitcoin or Ether.[107][108][109]  According to PricewaterhouseCoopers, four of the 10 biggest proposed initial coin offerings have used Switzerland as a base, where they are frequently registered as non-profit foundations. The Swiss regulatory agency FINMA stated that it would take a \"balanced approach\" to ICO projects and would allow \"legitimate innovators to navigate the regulatory landscape and so launch their projects in a way consistent with national laws protecting investors and the integrity of the financial system.\" In response to numerous requests by industry representatives, a legislative ICO working group began to issue legal guidelines in 2018, which are intended to remove uncertainty from cryptocurrency offerings and to establish sustainable business practices.[110]  The market capitalization of a cryptocurrency is calculated by multiplying the price by the number of coins in circulation. The total cryptocurrency market cap has historically been dominated by bitcoin accounting for at least 50% of the market cap value where altcoins have increased and decreased in market cap value in relation to bitcoin. Bitcoin's value is largely determined by speculation among other technological limiting factors known as blockchain rewards coded into the architecture technology of bitcoin itself. The cryptocurrency market cap follows a trend known as the \"halving\", which is when the block rewards received from bitcoin are halved due to technological mandated limited factors instilled into bitcoin which in turn limits the supply of bitcoin. As the date reaches near of a halving (twice thus far historically) the cryptocurrency market cap increases, followed by a downtrend.[111]  By June 2021, cryptocurrency had begun to be offered by some wealth managers in the US for 401(k)s.[112][113][114]  Cryptocurrency prices are much more volatile than established financial assets such as stocks. For example, over one week in May 2022, bitcoin lost 20% of its value and Ethereum lost 26%, while Solana and Cardano lost 41% and 35% respectively. The falls were attributed to warnings about inflation. By comparison, in the same week, the Nasdaq tech stock index fell 7.6 per cent and the FTSE 100 was 3.6 per cent down.[115]  In the longer term, of the 10 leading cryptocurrencies identified by the total value of coins in circulation in January 2018, only four (bitcoin, Ethereum, Cardano and Ripple (XRP)) were still in that position in early 2022.[116] The total value of all cryptocurrencies was  $2 trillion at the end of 2021, but had halved nine months later.[117][118] The Wall Street Journal has commented that the crypto sector has become \"intertwined\" with the rest of the capital markets and \"sensitive to the same forces that drive tech stocks and other risk assets,\" such as inflation forecasts.[119]  There are also centralized databases, outside of blockchains, that store crypto market data. Compared to the blockchain, databases perform fast as there is no verification process. Four of the most popular cryptocurrency market databases are CoinMarketCap, CoinGecko, BraveNewCoin, and Cryptocompare.[120]  According to Alan Feuer of The New York Times, libertarians and anarcho-capitalists were attracted to the philosophical idea behind bitcoin. Early bitcoin supporter Roger Ver said: \"At first, almost everyone who got involved did so for philosophical reasons. We saw bitcoin as a great idea, as a way to separate money from the state.\"[121] Economist Paul Krugman argues that cryptocurrencies like bitcoin are \"something of a cult\" based in \"paranoid fantasies\" of government power.[122]  David Golumbia says that the ideas influencing bitcoin advocates emerge from right-wing extremist movements such as the Liberty Lobby and the John Birch Society and their anti-Central Bank rhetoric, or, more recently, Ron Paul and Tea Party-style libertarianism.[123] Steve Bannon, who owns a \"good stake\" in bitcoin, sees cryptocurrency as a form of disruptive populism, taking control back from central authorities.[124]  Bitcoin's founder, Satoshi Nakamoto, supported the idea that cryptocurrencies go well with libertarianism. \"It's very attractive to the libertarian viewpoint if we can explain it properly,\" Nakamoto said in 2008.[125]  According to the European Central Bank, the decentralization of money offered by bitcoin has its theoretical roots in the Austrian school of economics, especially with Friedrich von Hayek in his book Denationalisation of Money: The Argument Refined,[126] in which Hayek advocates a complete free market in the production, distribution and management of money to end the monopoly of central banks.[127][128]  The rise in the popularity of cryptocurrencies and their adoption by financial institutions has led some governments to assess whether regulation is needed to protect users. The Financial Action Task Force (FATF) has defined cryptocurrency-related services as \"virtual asset service providers\" (VASPs) and recommended that they be regulated with the same money laundering (AML) and know your customer (KYC) requirements as financial institutions.[129]  In May 2020, the Joint Working Group on interVASP Messaging Standards published \"IVMS 101\", a universal common language for communication of required originator and beneficiary information between VASPs. The FATF and financial regulators were informed as the data model was developed.[130]  In June 2020, FATF updated its guidance to include the \"Travel Rule\" for cryptocurrencies, a measure which mandates that VASPs obtain, hold, and exchange information about the originators and beneficiaries of virtual asset transfers.[131] Subsequent standardized protocol specifications recommended using JSON for relaying data between VASPs and identity services. As of December 2020, the IVMS 101 data model has yet to be finalized and ratified by the three global standard setting bodies that created it.[132]  The European Commission published a digital finance strategy in September 2020. This included a draft regulation on Markets in Crypto-Assets (MiCA), which aimed to provide a comprehensive regulatory framework for digital assets in the EU.[133][134]  On 10 June 2021, the Basel Committee on Banking Supervision proposed that banks that held cryptocurrency assets must set aside capital to cover all potential losses. For instance, if a bank were to hold bitcoin worth $2 billion, it would be required to set aside enough capital to cover the entire $2 billion. This is a more extreme standard than banks are usually held to when it comes to other assets. However, this is a proposal and not a regulation.  The IMF is seeking a coordinated, consistent and comprehensive approach to supervising cryptocurrencies. Tobias Adrian, the IMF's financial counsellor and head of its monetary and capital markets department said in a January 2022 interview that \"Agreeing global regulations is never quick. But if we start now, we can achieve the goal of maintaining financial stability while also enjoying the benefits which the underlying technological innovations bring,\"[135]  In May 2024, 15 years after the advent of the first blockchain, bitcoin, the US Congress advanced a bill to the full House of Representatives to provide regulatory clarity for digital assets. The Financial Innovation and Technology for the 21st Century Act, which defines responsibilities between various US agencies, notably between the Commodity Futures Trading Commission (CFTC) for decentralized blockchains and the Securities and Exchange Commission (SEC) for blockchains that are functional but not decentralized.  Stablecoins are excluded from both CFTC and SEC regulation in this bill, \"except for fraud and certain activities by registered firms.\"[136]  In September 2017, China banned ICOs to cause abnormal return from cryptocurrency decreasing during announcement window. The liquidity changes by banning ICOs in China was temporarily negative while the liquidity effect became positive after news.[137]  On 18 May 2021, China banned financial institutions and payment companies from being able to provide cryptocurrency transaction related services.[138] This led to a sharp fall in the price of the biggest proof of work cryptocurrencies. For instance, bitcoin fell 31%, Ethereum fell 44%, Binance Coin fell 32% and Dogecoin fell 30%.[139] Proof of work mining was the next focus, with regulators in popular mining regions citing the use of electricity generated from highly polluting sources such as coal to create bitcoin and Ethereum.[140]  In September 2021, the Chinese government declared all cryptocurrency transactions of any kind illegal, completing its crackdown on cryptocurrency.[27]  In April 2024, TVNZ's 1News reported that the Cook Islands government was proposing legislation that would allow \"recovery agents\" to use various means including hacking to investigate or find cryptocurrency that may have been used for illegal means or is the \"proceeds of crime.\" The Tainted Cryptocurrency Recovery Bill was drafted by two lawyers hired by US-based debt collection company Drumcliffe. The proposed legislation was criticised by Cook Islands Crown Law's deputy solicitor general David Greig, who described it as \"flawed\" and said that some provisions were \"clearly unconstitutional\". The Cook Islands Financial Services Development Authority described Drumcliffe's involvement as a conflict of interest.[141]  Similar criticism was echoed by Auckland University of Technology cryptocurrency specialist and senior lecturer Jeff Nijsse and University of Otago political scientist Professor Robert Patman, who described it as government overreach and described it as inconsistent with international law. Since the Cook Islands is an associated state that is part of the Realm of New Zealand, Patman said that the law would have \"implications for New Zealand's governance arrangements.\" A spokesperson for New Zealand Foreign Minister Winston Peters confirmed that New Zealand officials were discussing the legislation with their Cook Islands counterparts.  Cook Islands Prime Minister Mark Brown defended the legislation as part of the territory's fight against international cybercrime.[141]  On 9 June 2021, El Salvador announced that it will adopt bitcoin as legal tender, becoming the first country to do so.[142]  The EU defines crypto assets as \"a digital representation of a value or of a right that is able to be transferred and stored electronically using distributed ledger technology or similar technology.\"[143] The EU regulation Markets in Crypto-Assets (MiCA) covering asset-referenced tokens (ARTs) and electronic money tokens (EMTs) (also known as stablecoins) came into force on 30 June 2024. As of 17 January 2025, the European Securities and Markets Authority (ESMA) issued guidance to crypto-asset service providers (CASPs) allowing them to maintain crypto-asset services for non-compliant ARTs and EMTs until the end of March 2025.[144][145]  The rest of MiCA came into force as of 30 December 2024, covering crypto-assets other than ART and EMT and CASPs. MiCA excludes crypto-assets if they qualify as financial instruments according to ESMA guidelines published on 17 December 2024 as well as crypto-assets that are unique and not fungible with other crypto-assets.[146][147]  At present, India neither prohibits nor allows investment in the cryptocurrency market. In 2020, the Supreme Court of India had lifted the ban on cryptocurrency, which was imposed by the Reserve Bank of India.[148][149][150][151] Since then, an investment in cryptocurrency is considered legitimate, though there is still ambiguity about the issues regarding the extent and payment of tax on the income accrued thereupon and also its regulatory regime. But it is being contemplated that the Indian Parliament will soon pass a specific law to either ban or regulate the cryptocurrency market in India.[152] Expressing his public policy opinion on the Indian cryptocurrency market to a well-known online publication, a leading public policy lawyer and Vice President of SAARCLAW (South Asian Association for Regional Co-operation in Law) Hemant Batra has said that the \"cryptocurrency market has now become very big with involvement of billions of dollars in the market hence, it is now unattainable and irreconcilable for the government to completely ban all sorts of cryptocurrency and its trading and investment\".[153] He mooted regulating the cryptocurrency market rather than completely banning it. He favoured following IMF and FATF guidelines in this regard.  South Africa, which has seen a large number of scams related to cryptocurrency, is said to be putting a regulatory timeline in place that will produce a regulatory framework.[154] The largest scam occurred in April 2021, where the two founders of an African-based cryptocurrency exchange called Africrypt, Raees Cajee and Ameer Cajee, disappeared with $3.8 billion worth of bitcoin.[155] Additionally, Mirror Trading International disappeared with $170 million worth of cryptocurrency in January 2021.[155]  In March 2021, South Korea implemented new legislation to strengthen their oversight of digital assets. This legislation requires all digital asset managers, providers and exchanges to be registered with the Korea Financial Intelligence Unit in order to operate in South Korea.[156] Registering with this unit requires that all exchanges are certified by the Information Security Management System and that they ensure all customers have real name bank accounts. It also requires that the CEO and board members of the exchanges have not been convicted of any crimes and that the exchange holds sufficient levels of deposit insurance to cover losses arising from hacks.[156]  Switzerland was one of the first countries to implement the FATF's Travel Rule. FINMA, the Swiss regulator, issued its own guidance to VASPs in 2019. The guidance followed the FATF's Recommendation 16, however with stricter requirements. According to FINMA's[157] requirements, VASPs need to verify the identity of the beneficiary of the transfer.  On 30 April 2021, the Central Bank of the Republic of Turkey banned the use of cryptocurrencies and cryptoassets for making purchases on the grounds that the use of cryptocurrencies for such payments poses significant transaction risks.[158]  In the United Kingdom, as of 10 January 2021, all cryptocurrency firms, such as exchanges, advisors and professionals that have either a presence, market product or provide services within the UK market must register with the Financial Conduct Authority. Additionally, on 27 June 2021, the financial watchdog demanded that Binance, the world's largest cryptocurrency exchange,[159] cease all regulated activities in the UK.[160]  The incoming Labour government confirmed in November 2024 that it will proceed with the regulation of cryptoassets and new UK requirements are expected to come into force in 2026.[161]  In 2021, 17 states in the US passed laws and resolutions concerning cryptocurrency regulation.[162] This led the Securities and Exchange Commission to start considering what steps to take. On 8 July 2021, Senator Elizabeth Warren, part of the Senate Banking Committee, wrote to the chairman of the SEC and demanded answers on cryptocurrency regulation due to the increase in cryptocurrency exchange use and the danger this posed to consumers. On 5 August 2021, the chairman, Gary Gensler, responded to Warren's letter and called for legislation focused on \"crypto trading, lending and DeFi platforms,\" because of how vulnerable investors could be when they traded on crypto trading platforms without a broker. He also argued that many tokens in the crypto market may be unregistered securities without required disclosures or market oversight. Additionally, Gensler did not hold back in his criticism of stablecoins. These tokens, which are pegged to the value of fiat currencies, may allow individuals to bypass important public policy goals related to traditional banking and financial systems, such as anti-money laundering, tax compliance, and sanctions.[163]  On 19 October 2021, the first bitcoin-linked exchange-traded fund (ETF) from ProShares started trading on the NYSE under the ticker \"BITO.\" ProShares CEO Michael L. Sapir said the ETF would expose bitcoin to a wider range of investors without the hassle of setting up accounts with cryptocurrency providers. Ian Balina, the CEO of Token Metrics, stated that SEC approval of the ETF was a significant endorsement for the crypto industry because many regulators globally were not in favor of crypto, and retail investors were hesitant to accept crypto. This event would eventually open more opportunities for new capital and new people in this space.[164]  The Department of the Treasury, on 20 May 2021, announced that it would require any transfer worth $10,000 or more to be reported to the Internal Revenue Service since cryptocurrency already posed a problem where illegal activity like tax evasion was facilitated broadly. This release from the IRS was a part of efforts to promote better compliance and consider more severe penalties for tax evaders.[165]  On 17 February 2022, the Department of Justice named Eun Young Choi as the first director of a National Cryptocurrency Enforcement Team to help identify and deal with misuse of cryptocurrencies and other digital assets.[166]  The Biden administration faced a dilemma as it tried to develop regulations for the cryptocurrency industry. On one hand, officials were hesitant to restrict a growing industry. On the other hand, they were committed to preventing illegal cryptocurrency transactions. To reconcile these conflicting goals, on 9 March 2022, Biden issued an executive order.[167] Followed this, on 16 September 2022, the Comprehensive Framework for Responsible Development of Digital Assets document was released[168] to support development of cryptocurrencies and restrict their illegal use. The executive order included all digital assets, but cryptocurrencies posed both the greatest security risks and potential economic benefits. Though this might not address all of the challenges in crypto industry, it was a significant milestone in the US cryptocurrency regulation history.[169]  In February 2023, the SEC ruled that cryptocurrency exchange Kraken's estimated $42 billion in staked assets globally operated as an illegal securities seller. The company agreed to a $30 million settlement with the SEC and to cease selling its staking service in the US. The case would impact other major crypto exchanges operating staking programs.[170]  On 23 March 2023, the SEC issued an alert to investors stating that firms offering crypto asset securities might not be complying with US laws. The SEC argued that unregistered offerings of crypto asset securities might not include important information.[171]  On 23 January 2025, President Donald Trump signed Executive Order 14178, Strengthening American Leadership in Digital Financial Technology[172] revoking Executive Order 14067 of 9 March 2022, Ensuring Responsible Development of Digital Assets and the Department of the Treasury's Framework for International Engagement on Digital Assets of 7 July 2022. In addition the order prohibits the establishment, issuance or promotion of Central bank digital currency and establishes a group tasked with proposing a federal regulatory framework for digital assets within 180 days.[173]  The legal status of cryptocurrencies varies substantially from country to country and is still undefined or changing in many of them. At least one study has shown that broad generalizations about the use of bitcoin in illicit finance are significantly overstated and that blockchain analysis is an effective crime fighting and intelligence gathering tool.[174] While some countries have explicitly allowed their use and trade,[175] others have banned or restricted it. According to the Library of Congress in 2021, an \"absolute ban\" on trading or using cryptocurrencies applies in 9 countries: Algeria, Bangladesh, Bolivia, China, Egypt, Iraq, Morocco, Nepal, and the United Arab Emirates. An \"implicit ban\" applies in another 39 countries or regions, which include: Bahrain, Benin, Burkina Faso, Burundi, Cameroon, Chad, Cote d’Ivoire, the Dominican Republic, Ecuador, Gabon, Georgia, Guyana, Indonesia, Iran, Jordan, Kazakhstan, Kuwait, Lebanon, Lesotho, Macau, Maldives, Mali, Moldova, Namibia, Niger, Nigeria, Oman, Pakistan, Palau, Republic of Congo, Saudi Arabia, Senegal, Tajikistan, Tanzania, Togo, Turkey, Turkmenistan, Qatar and Vietnam.[176] In the United States and Canada, state and provincial securities regulators, coordinated through the North American Securities Administrators Association, are investigating \"Bitcoin scams\" and ICOs in 40 jurisdictions.[177]  Various government agencies, departments, and courts have classified bitcoin differently. China Central Bank banned the handling of bitcoins by financial institutions in China in early 2014.  In Russia, though owning cryptocurrency is legal, its residents are only allowed to purchase goods from other residents using the Russian ruble while nonresidents are allowed to use foreign currency.[178] Regulations and bans that apply to bitcoin probably extend to similar cryptocurrency systems.[179]  In August 2018, the Bank of Thailand announced its plans to create its own cryptocurrency, the Central Bank Digital Currency (CBDC).[180]  Cryptocurrency advertisements have been banned on the following platforms:  On 25 March 2014, the United States Internal Revenue Service (IRS) ruled that bitcoin will be treated as property for tax purposes. Therefore, virtual currencies are considered commodities subject to capital gains tax.[188]  As the popularity and demand for online currencies has increased since the inception of bitcoin in 2009,[189] so have concerns that such an unregulated person to person global economy that cryptocurrencies offer may become a threat to society. Concerns abound that altcoins may become tools for anonymous web criminals.[190]  Cryptocurrency networks display a lack of regulation that has been criticized as enabling criminals who seek to evade taxes and launder money. Money laundering issues are also present in regular bank transfers, however with bank-to-bank wire transfers for instance, the account holder must at least provide a proven identity.  Transactions that occur through the use and exchange of these altcoins are independent from formal banking systems, and therefore can make tax evasion simpler for individuals. Since charting taxable income is based upon what a recipient reports to the revenue service, it becomes extremely difficult to account for transactions made using existing cryptocurrencies, a mode of exchange that is complex and difficult to track.[190]  Systems of anonymity that most cryptocurrencies offer can also serve as a simpler means to launder money. Rather than laundering money through an intricate net of financial actors and offshore bank accounts, laundering money through altcoins can be achieved through anonymous transactions.[190]  Cryptocurrency makes legal enforcement against extremist groups more complicated, which consequently strengthens them.[191] White supremacist Richard Spencer went as far as to declare bitcoin the \"currency of the alt-right\".[192]  In February 2014, the world's largest bitcoin exchange, Mt. Gox, declared bankruptcy. Likely due to theft, the company claimed that it had lost nearly 750,000 bitcoins belonging to their clients. This added up to approximately 7% of all bitcoins in existence, worth a total of $473 million. Mt. Gox blamed hackers, who had exploited the transaction malleability problems in the network. The price of a bitcoin fell from a high of about $1,160 in December to under $400 in February.[193]  On 21 November 2017, Tether announced that it had been hacked, losing $31 million in USDT from its core treasury wallet.[194]  On 7 December 2017, Slovenian cryptocurrency exchange Nicehash reported that hackers had stolen over $70 million using a hijacked company computer.[195]  On 19 December 2017, Yapian, the owner of South Korean exchange Youbit, filed for bankruptcy after suffering two hacks that year.[196][197] Customers were still granted access to 75% of their assets.  In May 2018, Bitcoin Gold had its transactions hijacked and abused by unknown hackers.[198] Exchanges lost an estimated $18m and bitcoin Gold was delisted from Bittrex after it refused to pay its share of the damages.  On 13 September 2018, Homero Josh Garza was sentenced to 21 months of imprisonment, followed by three years of supervised release.[199] Garza had founded the cryptocurrency startups GAW Miners and ZenMiner in 2014, acknowledged in a plea agreement that the companies were part of a pyramid scheme, and pleaded guilty to wire fraud in 2015. The SEC separately brought a civil enforcement action in the US against Garza, who was eventually ordered to pay a judgment of $9.1 million plus $700,000 in interest. The SEC's complaint stated that Garza, through his companies, had fraudulently sold \"investment contracts representing shares in the profits they claimed would be generated\" from mining.[200]  In January 2018, Japanese exchange Coincheck reported that hackers had stolen cryptocurrency worth $530 million.[201]  In June 2018, South Korean exchange Coinrail was hacked, losing over $37 million in crypto.[202] The hack worsened a cryptocurrency selloff by an additional $42 billion.[203]  On 9 July 2018, the exchange Bancor, whose code and fundraising had been subjects of controversy, had $23.5 million in crypto stolen.[204]  A 2020 EU report found that users had lost crypto-assets worth hundreds of millions of US dollars in security breaches at exchanges and storage providers. Between 2011 and 2019, reported breaches ranged from four to twelve a year. In 2019, more than a billion dollars worth of cryptoassets was reported stolen. Stolen assets \"typically find their way to illegal markets and are used to fund further criminal activity\".[205]  According to a 2020 report produced by the United States Attorney General's Cyber-Digital Task Force, three categories make up the majority of illicit cryptocurrency uses: \"(1) financial transactions associated with the commission of crimes; (2) money laundering and the shielding of legitimate activity from tax, reporting, or other legal requirements; or (3) crimes, such as theft, directly implicating the cryptocurrency marketplace itself.\" The report concluded that \"for cryptocurrency to realize its truly transformative potential, it is imperative that these risks be addressed\" and that \"the government has legal and regulatory tools available at its disposal to confront the threats posed by cryptocurrency's illicit uses\".[206][207]  According to the UK 2020 national risk assessment—a comprehensive assessment of money laundering and terrorist financing risk in the UK—the risk of using cryptoassets such as bitcoin for money laundering and terrorism financing is assessed as \"medium\" (from \"low\" in the previous 2017 report).[208] Legal scholars suggested that the money laundering opportunities may be more perceived than real.[209] Blockchain analysis company Chainalysis concluded that illicit activities like cybercrime, money laundering and terrorism financing made up only 0.15% of all crypto transactions conducted in 2021, representing a total of $14 billion.[210][211][212]  In December 2021, Monkey Kingdom, a NFT project based in Hong Kong, lost US$1.3 million worth of cryptocurrencies via a phishing link used by the hacker.[213]  On November 2, 2023, Sam Bankman-Fried was pronounced guilty on seven counts of fraud related to FTX.[214] Federal criminal court sentencing experts speculated on the potential amount of prison time likely to be meted out.[215][216][217] On March 28, 2024, the court sentenced Bankman-Fried to 25 years in prison.[218]  According to blockchain data company Chainalysis, criminals laundered US$8,600,000,000 worth of cryptocurrency in 2021, up by 30% from the previous year.[219] The data suggests that rather than managing numerous illicit havens, cybercriminals make use of a small group of purpose built centralized exchanges for sending and receiving illicit cryptocurrency. In 2021, those exchanges received 47% of funds sent by crime linked addresses.[220] Almost $2.2bn worth of cryptocurrencies was embezzled from DeFi protocols in 2021, which represents 72% of all cryptocurrency theft in 2021.  According to Bloomberg and the New York Times, Federation Tower, a two skyscraper complex in the heart of Moscow City, is home to many cryptocurrency businesses under suspicion of facilitating extensive money laundering, including accepting illicit cryptocurrency funds obtained through scams, darknet markets, and ransomware.[221] Notable businesses include Garantex,[222] Eggchange, Cashbank, Buy-Bitcoin, Tetchange, Bitzlato, and Suex, which was sanctioned by the U.S. in 2021. Bitzlato founder and owner Anatoly Legkodymov was arrested following money-laundering charges by the United States Department of Justice.[223]  Dark money has also been flowing into Russia through a dark web marketplace called Hydra, which is powered by cryptocurrency, and enjoyed more than $1 billion in sales in 2020, according to Chainalysis.[224] The platform demands that sellers liquidate cryptocurrency only through certain regional exchanges, which has made it difficult for investigators to trace the money.  Almost 74% of ransomware revenue in 2021 — over $400 million worth of cryptocurrency — went to software strains likely affiliated with Russia, where oversight is notoriously limited.[221] However, Russians are also leaders in the benign adoption of cryptocurrencies, as the ruble is unreliable, and President Putin favours the idea of \"overcoming the excessive domination of the limited number of reserve currencies.\"[225]  In 2022, RenBridge - an unregulated alternative to exchanges for transferring value between blockchains - was found to be responsible for the laundering of at least $540 million since 2020. It is especially popular with people attempting to launder money from theft. This includes a cyberattack on Japanese crypto exchange Liquid that has been linked to North Korea.[226]  Properties of cryptocurrencies gave them popularity in applications such as a safe haven in banking crises and means of payment, which also led to the cryptocurrency use in controversial settings in the form of online black markets, such as Silk Road.[190] The original Silk Road was shut down in October 2013 and there have been two more versions in use since then. In the year following the initial shutdown of Silk Road, the number of prominent dark markets increased from four to twelve, while the amount of drug listings increased from 18,000 to 32,000.[190]  Darknet markets present challenges in regard to legality. Cryptocurrency used in dark markets are not clearly or legally classified in almost all parts of the world. In the US, bitcoins are regarded as \"virtual assets\".[citation needed] This type of ambiguous classification puts pressure on law enforcement agencies around the world to adapt to the shifting drug trade of dark markets.[227][unreliable source?]  Various studies have found that crypto-trading is rife with wash trading. Wash trading is a process, illegal in some jurisdictions, involving buyers and sellers being the same person or group, and may be used to manipulate the price of a cryptocurrency or inflate volume artificially. Exchanges with higher volumes can demand higher premiums from token issuers.[228] A study from 2019 concluded that up to 80% of trades on unregulated cryptocurrency exchanges could be wash trades.[228] A 2019 report by Bitwise Asset Management claimed that 95% of all bitcoin trading volume reported on major website CoinMarketCap had been artificially generated, and of 81 exchanges studied, only 10 provided legitimate volume figures.[229]  In 2022, cryptocurrencies attracted attention when Western nations imposed severe economic sanctions on Russia in the aftermath of its invasion of Ukraine in February. However, American sources warned in March that some crypto-transactions could potentially be used to evade economic sanctions against Russia and Belarus.[230]  In April 2022, the computer programmer Virgil Griffith received a five-year prison sentence in the US for attending a Pyongyang cryptocurrency conference, where he gave a presentation on blockchains which might be used for sanctions evasion.[231]  The Bank for International Settlements summarized several criticisms of cryptocurrencies in Chapter V of their 2018 annual report. The criticisms include the lack of stability in their price, the high energy consumption, high and variable transactions costs, the poor security and fraud at cryptocurrency exchanges, vulnerability to debasement (from forking), and the influence of miners.[232][233][234]  Cryptocurrencies have been compared to Ponzi schemes, pyramid schemes[235] and economic bubbles,[236] such as housing market bubbles.[237] Howard Marks of Oaktree Capital Management stated in 2017 that digital currencies were \"nothing but an unfounded fad (or perhaps even a pyramid scheme), based on a willingness to ascribe value to something that has little or none beyond what people will pay for it\", and compared them to the tulip mania (1637), South Sea Bubble (1720), and dot-com bubble (1999), which all experienced profound price booms and busts.[238]  Regulators in several countries have warned against cryptocurrency and some have taken measures to dissuade users.[239] However, research in 2021 by the UK's financial regulator suggests such warnings either went unheard, or were ignored. Fewer than one in 10 potential cryptocurrency buyers were aware of consumer warnings on the FCA website, and 12% of crypto users were not aware that their holdings were not protected by statutory compensation.[240][241] Of 1,000 respondents between the ages of eighteen and forty, almost 70% wrongly assumed cryptocurrencies were regulated, 75% of younger crypto investors claimed to be driven by competition with friends and family, 58% said that social media enticed them to make high risk investments.[242] The FCA recommends making use of its warning list, which flags unauthorized financial firms.[243]  Many banks do not offer virtual currency services themselves and can refuse to do business with virtual currency companies.[244] In 2014, Gareth Murphy, a senior banking officer, suggested that the widespread adoption of cryptocurrencies may lead to too much money being obfuscated, blinding economists who would use such information to better steer the economy.[245] While traditional financial products have strong consumer protections in place, there is no intermediary with the power to limit consumer losses if bitcoins are lost or stolen. One of the features cryptocurrency lacks in comparison to credit cards, for example, is consumer protection against fraud, such as chargebacks.  The French regulator Autorité des marchés financiers (AMF) lists 16 websites of companies that solicit investment in cryptocurrency without being authorized to do so in France.[246]  An October 2021 paper by the National Bureau of Economic Research found that bitcoin suffers from systemic risk as the top 10,000 addresses control about one-third of all bitcoin in circulation.[247] It is even worse for miners, with 0.01% controlling 50% of the capacity. According to researcher Flipside Crypto, less than 2% of anonymous accounts control 95% of all available bitcoin supply.[248] This is considered risky as a great deal of the market is in the hands of a few entities.  A paper by John Griffin, a finance professor at the University of Texas, and Amin Shams, a graduate student found that in 2017 the price of bitcoin had been substantially inflated using another cryptocurrency, Tether.[249]  Roger Lowenstein, author of \"Bank of America: The Epic Struggle to Create the Federal Reserve,\" says in a New York Times story that FTX will face over $8 billion in claims.[250]  Non-fungible tokens (NFTs) are digital assets that represent art, collectibles, gaming, etc. Like crypto, their data is stored on the blockchain. NFTs are bought and traded using cryptocurrency. The Ethereum blockchain was the first place where NFTs were implemented, but now many other blockchains have created their own versions of NFTs.  According to Vanessa Grellet, renowned panelist in blockchain conferences,[251] there was an increasing interest from traditional stock exchanges in crypto-assets at the end of the 2010s, while crypto-exchanges such as Coinbase were gradually entering the traditional financial markets. This convergence marked a significant trend where conventional financial actors were adopting blockchain technology to enhance operational efficiency, while the crypto world introduced innovations like Security Token Offering (STO), enabling new ways of fundraising. Tokenization, turning assets such as real estate, investment funds, and private equity into blockchain-based tokens, had the potential to make traditionally illiquid assets more accessible to investors. Despite the regulatory risks associated with such developments, major financial institutions, including JPMorgan Chase, were actively working on blockchain initiatives, exemplified by the creation of Quorum, a private blockchain platform.[252]  As the first big Wall Street bank to embrace cryptocurrencies, Morgan Stanley announced on 17 March 2021 that they will be offering access to bitcoin funds for their wealthy clients through three funds which enable bitcoin ownership for investors with an aggressive risk tolerance.[253] BNY Mellon on 11 February 2021 announced that it would begin offering cryptocurrency services to its clients.[254]  On 20 April 2021,[255] Venmo added support to its platform to enable customers to buy, hold and sell cryptocurrencies.[256]  In October 2021, financial services company Mastercard announced it is working with digital asset manager Bakkt on a platform that would allow any bank or merchant on the Mastercard network to offer cryptocurrency services.[257]  Mining for proof-of-work cryptocurrencies requires enormous amounts of electricity and consequently comes with a large carbon footprint due to causing greenhouse gas emissions.[258] Proof-of-work blockchains such as bitcoin, Ethereum, Litecoin, and Monero were estimated to have added between 3 million and 15 million tons of carbon dioxide (CO2) to the atmosphere in the period from 1 January 2016 to 30 June 2017.[259] By November 2018, bitcoin was estimated to have an annual energy consumption of 45.8TWh, generating 22.0 to 22.9 million tons of CO2, rivalling nations like Jordan and Sri Lanka.[260] By the end of 2021, bitcoin was estimated to produce 65.4 million tons of CO2, as much as Greece,[261] and consume between 91 and 177 terawatt-hours annually.[262][263]  Critics have also identified a large electronic waste problem in disposing of mining rigs.[264] Mining hardware is improving at a fast rate, quickly resulting in older generations of hardware.[265]  Bitcoin is the least energy-efficient cryptocurrency, using 707.6 kilowatt-hours of electricity per transaction.[266]  Before June 2021, China was the primary location for bitcoin mining. However, due to concerns over power usage and other factors, China forced out bitcoin operations, at least temporarily. As a result, the United States promptly emerged as the top global leader in the industry. An example of a gross amount of electronic waste associated with bitcoin mining operations in the US is a facility that located in Dalton, Georgia which is consuming nearly the same amount of electricity as the combined power usage of 97,000 households in its vicinity. Another example is that Riot Platforms operates a bitcoin mining facility in Rockdale, Texas, which consumes approximately as much electricity as the nearby 300,000 households. This makes it the most energy-intensive bitcoin mining operation in the United States.[267]  The world's second-largest cryptocurrency, Ethereum, uses 62.56 kilowatt-hours of electricity per transaction.[268] XRP is the world's most energy efficient cryptocurrency, using 0.0079 kilowatt-hours of electricity per transaction.[269]  Although the biggest PoW blockchains consume energy on the scale of medium-sized countries, the annual power demand from proof-of-stake (PoS) blockchains is on a scale equivalent to a housing estate. The Times identified six \"environmentally friendly\" cryptocurrencies: Chia, IOTA, Cardano, Nano, Solarcoin and Bitgreen.[270] Academics and researchers have used various methods for estimating the energy use and energy efficiency of blockchains. A study of the six largest proof-of-stake networks in May 2021 concluded:  In terms of annual consumption (kWh\/yr), the figures were: Polkadot (70,237), Tezos (113,249), Avalanche (489,311), Algorand (512,671), Cardano (598,755) and Solana (1,967,930). This equates to Polkadot consuming 7 times the electricity of an average U.S. home, Cardano 57 homes and Solana 200 times as much. The research concluded that PoS networks consumed 0.001% the electricity of the bitcoin network.[271] University College London researchers reached a similar conclusion.[272]  Variable renewable energy power stations could invest in bitcoin mining to reduce curtailment, hedge electricity price risk, stabilize the grid, increase the profitability of renewable energy power stations and therefore accelerate transition to sustainable energy.[273][274][275][276][277]  There are also purely technical elements to consider. For example, technological advancement in cryptocurrencies such as bitcoin result in high up-front costs to miners in the form of specialized hardware and software.[278] Cryptocurrency transactions are normally irreversible after a number of blocks confirm the transaction. Additionally, cryptocurrency private keys can be permanently lost from local storage due to malware, data loss or the destruction of the physical media. This precludes the cryptocurrency from being spent, resulting in its effective removal from the markets.[279]  In September 2015, the establishment of the peer-reviewed academic journal Ledger (ISSN 2379-5980) was announced. It covers studies of cryptocurrencies and related technologies, and is published by the University of Pittsburgh.[280]  The journal encourages authors to digitally sign a file hash of submitted papers, which will then be timestamped into the bitcoin blockchain. Authors are also asked to include a personal bitcoin address in the first page of their papers.[281][282]  A number of aid agencies have started accepting donations in cryptocurrencies, including UNICEF.[283] Christopher Fabian, principal adviser at UNICEF Innovation, said the children's fund would uphold donor protocols, meaning that people making donations online would have to pass checks before they were allowed to deposit funds.[284][285]  However, in 2021, there was a backlash against donations in bitcoin because of the environmental emissions it caused. Some agencies stopped accepting bitcoin and others turned to \"greener\" cryptocurrencies.[286] The U.S. arm of Greenpeace stopped accepting bitcoin donations after seven years. It said: \"As the amount of energy needed to run bitcoin became clearer, this policy became no longer tenable.\"[287]  In 2022, the Ukrainian government raised over US$10,000,000 worth of aid through cryptocurrency following the 2022 Russian invasion of Ukraine.[288]  Bitcoin has been characterized as a speculative bubble by eight winners of the Nobel Memorial Prize in Economic Sciences: Paul Krugman,[289] Robert J. Shiller,[290] Joseph Stiglitz,[291] Richard Thaler,[292] James Heckman,[293] Thomas Sargent,[293] Angus Deaton,[293] and Oliver Hart;[293] and by central bank officials including Alan Greenspan,[294] Agustín Carstens,[295] Vítor Constâncio,[296] and Nout Wellink.[297]  Investors Warren Buffett and George Soros have respectively characterized it as a \"mirage\"[298] and a \"bubble\";[299] while business executives Jack Ma and JP Morgan Chase CEO Jamie Dimon have called it a \"bubble\"[300] and a \"fraud\",[301] respectively, although Jamie Dimon later said he regretted dubbing bitcoin a fraud.[302] BlackRock CEO Laurence D. Fink called bitcoin an \"index of money laundering\".[303]  In June 2022, business magnate Bill Gates said that cryptocurrencies are \"100% based on greater fool theory\".[304]  Legal scholars criticize the lack of regulation, which hinders conflict resolution when crypto assets are at the center of a legal dispute, for example a divorce or an inheritance. In Switzerland, jurists generally deny that cryptocurrencies are objects that fall under property law, as cryptocurrencies do not belong to any class of legally defined objects (Typenzwang, the legal numerus clausus). Therefore, it is debated whether anybody could even be sued for embezzlement of cryptocurrency if he\/she had access to someone's wallet. However, in the law of obligations and contract law, any kind of object would be legally valid, but the object would have to be tied to an identified counterparty. However, as the more popular cryptocurrencies can be freely and quickly exchanged into legal tender, they are financial assets and have to be taxed and accounted for as such.[305][306]  In 2018, an increase in crypto-related suicides was noticed after the cryptocurrency market crashed in August. The situation was particularly critical in Korea as crypto traders were on \"suicide watch\". A cryptocurrency forum on Reddit even started providing suicide prevention support to affected investors.[307][308] The May 2022 collapse of the Luna currency operated by Terra also led to reports of suicidal investors in crypto-related subreddits.[309] "},"meta":{},"created_at":"2025-03-22T14:25:42.289399Z","updated_at":"2025-03-22T14:25:42.289399Z","inner_id":70,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":79,"annotations":[{"id":79,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.321899Z","updated_at":"2025-03-22T14:25:42.321899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"df77c64c-c897-47ab-9bd0-e5f91e4ae4e1","import_id":null,"last_action":null,"bulk_created":false,"task":79,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Stocks (also capital stock, or sometimes interchangeably, shares) consist of all the shares[a] by which ownership of a corporation or company is divided.[1] A single share of the stock means fractional ownership of the corporation in proportion to the total number of shares. This typically entitles the shareholder (stockholder) to that fraction of the company's earnings, proceeds from liquidation of assets (after discharge of all senior claims such as secured and unsecured debt),[3] or voting power, often dividing these up in proportion to the number of like shares each stockholder owns. Not all stock is necessarily equal, as certain classes of stock may be issued, for example, without voting rights, with enhanced voting rights, or with a certain priority to receive profits or liquidation proceeds before or after other classes of shareholders.  Stock can be bought and sold privately or on stock exchanges. Transactions of the former are closely overseen by governments and regulatory bodies to prevent fraud, protect investors, and benefit the larger economy. As new shares are issued by a company, the ownership and rights of existing shareholders are diluted in return for cash to sustain or grow the business. Companies can also buy back stock, which often lets investors recoup the initial investment plus capital gains from subsequent rises in stock price. Stock options issued by many companies as part of employee compensation do not represent ownership, but represent the right to buy ownership at a future time at a specified price. This would represent a windfall to the employees if the option were exercised when the market price is higher than the promised price, since if they immediately sold the stock they would keep the difference (minus taxes).  Stock bought and sold in private markets fall within the private equity realm of finance.  A person who owns a percentage of the stock has the ownership of the corporation proportional to their share. The shares form a stock; the stock of a corporation is partitioned into shares, the total of which are stated at the time of business formation. Additional shares may subsequently be authorized by the existing shareholders and issued by the company. In some jurisdictions, each share of stock has a certain declared par value, which is a nominal accounting value used to represent the equity on the balance sheet of the corporation. In other jurisdictions, however, shares of stock may be issued without associated par value.  Shares represent a fraction of ownership in a business. A business may declare different types (or classes) of shares, each having distinctive ownership rules, privileges, or share values. Ownership of shares may be documented by issuance of a stock certificate. A stock certificate is a legal document that specifies the number of shares owned by the shareholder, and other specifics of the shares, such as the par value, if any, or the class of the shares.  In the United Kingdom, Republic of Ireland, South Africa, and Australia, stock can also refer, less commonly, to all kinds of marketable securities.[4]  Stock typically takes the form of shares of either common stock or preferred stock. As a unit of ownership, common stock typically carries voting rights that can be exercised in corporate decisions. Preferred stock differs from common stock in that it typically does not carry voting rights but is legally entitled to receive a certain level of dividend payments before any dividends can be issued to other shareholders.[5][6][page needed] Convertible preferred stock is preferred stock that includes the ability of the holder to convert the preferred shares into a fixed number of common shares, usually any time after a predetermined date. Shares of such stock are called \"convertible preferred shares\" (or \"convertible preference shares\" in the UK).  New equity issue may have specific legal clauses attached that differentiate them from previous issues of the issuer. Some shares of common stock may be issued without the typical voting rights, for instance, or some shares may have special rights unique to them and issued only to certain parties. Often, new issues that have not been registered with a securities governing body may be restricted from resale for certain periods of time.  Preferred stock may be hybrid by having the qualities of bonds of fixed returns and common stock voting rights. They also have preference in the payment of dividends over common stock and also have been given preference at the time of liquidation over common stock. They have other features of accumulation in dividend. In addition, preferred stock usually comes with a letter designation at the end of the security; for example, Berkshire-Hathaway Class \"B\" shares sell under stock ticker BRK.B, whereas Class \"A\" shares of ORION DHC, Inc will sell under ticker OODHA until the company drops the \"A\" creating ticker OODH for its \"Common\" shares only designation. This extra letter does not mean that any exclusive rights exist for the shareholders but it does let investors know that the shares are considered for such, however, these rights or privileges may change based on the decisions made by the underlying company.  \"Rule 144 Stock\" is an American term given to shares of stock subject to SEC Rule 144: Selling Restricted and Control Securities.[7] Under Rule 144, restricted and controlled securities are acquired in unregistered form. Investors either purchase or take ownership of these securities through private sales (or other means such as via ESOPs or in exchange for seed money) from the issuing company (as in the case with Restricted Securities) or from an affiliate of the issuer (as in the case with Control Securities). Investors wishing to sell these securities are subject to different rules than those selling traditional common or preferred stock. These individuals will only be allowed to liquidate their securities after meeting the specific conditions set forth by SEC Rule 144. Rule 144 allows public re-sale of restricted securities if a number of different conditions are met.  A stock derivative is any financial instrument for which the underlying asset is the price of an equity. Futures and options are the main types of derivatives on stocks. The underlying security may be a stock index or an individual firm's stock, e.g. single-stock futures.  Stock futures are contracts where the buyer is long, i.e., takes on the obligation to buy on the contract maturity date, and the seller is short, i.e., takes on the obligation to sell. Stock index futures are generally delivered by cash settlement.  A stock option is a class of option. Specifically, a call option is the right (not obligation) to buy stock in the future at a fixed price and a put option is the right (not obligation) to sell stock in the future at a fixed price. Thus, the value of a stock option changes in reaction to the underlying stock of which it is a derivative. The most popular method of valuing stock options is the Black–Scholes model.[8] Apart from call options granted to employees, most stock options are transferable.  During the Roman Republic, the state contracted (leased) out many of its services to private companies. These government contractors were called publicani, or societas publicanorum as individual companies.[9] These companies were similar to modern corporations, or joint-stock companies more specifically, in a couple of aspects. They issued shares called partes (for large cooperatives) and particulae which were small shares that acted like today's over-the-counter shares.[10] Polybius mentions that \"almost every citizen\" participated in the government leases.[11][12] There is also evidence that the price of stocks fluctuated. The Roman orator Cicero speaks of partes illo tempore carissimae, which means \"shares that had a very high price at that time\".[13] This implies a fluctuation of price and stock market behavior in Rome.  Around 1250 in France at Toulouse, 100 shares of the Société des Moulins du Bazacle, or Bazacle Milling Company were traded at a value that depended on the profitability of the mills the society owned.[14]  In 1288, the Bishop of Västerås acquired a 12.5% interest in Great Copper Mountain (Stora Kopparberget in Swedish) which contained the Falun Mine. The Swedish mining and forestry products company Stora has documented a stock transfer, in 1288 in exchange for an estate.[15]  The earliest recognized joint-stock company in modern times was the English (later British) East India Company. It was granted an English Royal Charter by Elizabeth I on 31 December 1600, with the intention of favouring trade privileges in India. The Royal Charter effectively gave the newly created Honourable East India Company (HEIC) a 15-year monopoly on all trade in the East Indies.[16]  Soon afterwards, in 1602,[17] the Dutch East India Company issued the first shares that were made tradeable on the Amsterdam Stock Exchange. Between 1602 and 1796 it traded 2.5 million tons of cargo with Asia on 4,785 ships and sent a million Europeans to work in Asia.  A shareholder (or stockholder) is an individual or company (including a corporation) that legally owns one or more shares of stock in a joint stock company. Both private and public traded companies have shareholders.  Shareholders are granted special privileges depending on the class of stock, including the right to vote on matters such as elections to the board of directors, the right to share in distributions of the company's income, the right to purchase new shares issued by the company, and the right to a company's assets during a liquidation of the company. However, shareholder's rights to a company's assets are subordinate to the rights of the company's creditors.  Shareholders are one type of stakeholders, who may include anyone who has a direct or indirect equity interest in the business entity or someone with a non-equity interest in a non-profit organization. Thus it might be common to call volunteer contributors to an association stakeholders, even though they are not shareholders.  Although directors and officers of a company are bound by fiduciary duties to act in the best interest of the shareholders, the shareholders themselves normally do not have such duties towards each other.  However, in a few unusual cases, some courts have been willing to imply such a duty between shareholders. For example, in California, United States, majority shareholders of closely held corporations have a duty not to destroy the value of the shares held by minority shareholders.[18][19]  The largest shareholders (in terms of percentages of companies owned) are often mutual funds, and, especially, passively managed exchange-traded funds.  The owners of a private company may want additional capital to invest in new projects within the company. They may also simply wish to reduce their holding, freeing up capital for their own private use. They can achieve these goals by selling shares in the company to the general public, through a sale on a stock exchange. This process is called an initial public offering, or IPO.  By selling shares they can sell part or all of the company to many part-owners. The purchase of one share entitles the owner of that share to literally share in the ownership of the company, a fraction of the decision-making power, and potentially a fraction of the profits, which the company may issue as dividends. The owner may also inherit debt and even litigation.  In the common case of a publicly traded corporation, where there may be thousands of shareholders, it is impractical to have all of them making the daily decisions required to run a company. Thus, the shareholders will use their shares as votes in the election of members of the board of directors of the company.  In a typical case, each share constitutes one vote. Corporations may, however, issue different classes of shares, which may have different voting rights. Owning the majority of the shares allows other shareholders to be out-voted – effective control rests with the majority shareholder (or shareholders acting in concert). In this way the original owners of the company often still have control of the company.  Although ownership of 50% of shares does result in 50% ownership of a company, it does not give the shareholder the right to use a company's building, equipment, materials, or other property. This is because the company is considered a legal person, thus it owns all its assets itself. This is important in areas such as insurance, which must be in the name of the company and not the main shareholder.  In most countries, boards of directors and company managers have a fiduciary responsibility to run the company in the interests of its stockholders. Nonetheless, as Martin Whitman writes:  Even though the board of directors runs the company, the shareholder has some impact on the company's policy, as the shareholders elect the board of directors. Each shareholder typically has a percentage of votes equal to the percentage of shares he or she owns. So as long as the shareholders agree that the management (agent) are performing poorly they can select a new board of directors which can then hire a new management team. In practice, however, genuinely contested board elections are rare. Board candidates are usually nominated by insiders or by the board of the directors themselves, and a considerable amount of stock is held or voted by insiders.  Owning shares does not mean responsibility for liabilities. If a company goes broke and has to default on loans, the shareholders are not liable in any way. However, all money obtained by converting assets into cash will be used to repay loans and other debts first, so that shareholders cannot receive any money unless and until creditors have been paid (often the shareholders end up with nothing).[21]  Financing a company through the sale of stock in a company is known as equity financing. Alternatively, debt financing (for example issuing bonds) can be done to avoid giving up shares of ownership of the company. Unofficial financing known as trade financing usually provides the major part of a company's working capital (day-to-day operational needs).  In general, the shares of a company may be transferred from shareholders to other parties by sale or other mechanisms, unless prohibited. Most jurisdictions have established laws and regulations governing such transfers, particularly if the issuer is a publicly traded entity.  The desire of stockholders to trade their shares has led to the establishment of stock exchanges, organizations which provide marketplaces for trading shares and other derivatives and financial products. Today, stock traders are usually represented by a stockbroker who buys and sells shares of a wide range of companies on such exchanges. A company may list its shares on an exchange by meeting and maintaining the listing requirements of a particular stock exchange.  Many large non-U.S companies choose to list on a U.S. exchange as well as an exchange in their home country in order to broaden their investor base. These companies must maintain a block of shares at a bank in the US, typically a certain percentage of their capital. On this basis, the holding bank establishes American depositary shares and issues an American depositary receipt (ADR) for each share a trader acquires. Likewise, many large U.S. companies list their shares at foreign exchanges to raise capital abroad.  Small companies that do not qualify and cannot meet the listing requirements of the major exchanges may be traded over-the-counter (OTC) by an off-exchange mechanism in which trading occurs directly between parties. The major OTC markets in the United States are the electronic quotation systems OTC Bulletin Board (OTCBB) and OTC Markets Group (formerly known as Pink OTC Markets Inc.)[22] where individual retail investors are also represented by a brokerage firm and the quotation service's requirements for a company to be listed are minimal. Shares of companies in bankruptcy proceedings are usually listed by these quotation services after the stock is delisted from an exchange.  There are various methods of buying and financing stocks, the most common being through a stockbroker. Brokerage firms, whether they are a full-service or discount broker, arrange the transfer of stock from a seller to a buyer. Most trades are actually done through brokers listed with a stock exchange.  There are many different brokerage firms from which to choose, such as full service brokers or discount brokers. The full service brokers usually charge more per trade, but give investment advice or more personal service; the discount brokers offer little or no investment advice but charge less for trades. Another type of broker would be a bank or credit union that may have a deal set up with either a full-service or discount broker.  There are other ways of buying stock besides through a broker. One way is directly from the company itself. If at least one share is owned, most companies will allow the purchase of shares directly from the company through their investor relations departments. However, the initial share of stock in the company will have to be obtained through a regular stock broker. Another way to buy stock in companies is through Direct Public Offerings which are usually sold by the company itself. A direct public offering is an initial public offering in which the stock is purchased directly from the company, usually without the aid of brokers.  When it comes to financing a purchase of stocks there are two ways: purchasing stock with money that is currently in the buyer's ownership, or by buying stock on margin. Buying stock on margin means buying stock with money borrowed against the value of stocks in the same account. These stocks, or collateral, guarantee that the buyer can repay the loan; otherwise, the stockbroker has the right to sell the stock (collateral) to repay the borrowed money. He can sell if the share price drops below the margin requirement, at least 50% of the value of the stocks in the account. Buying on margin works the same way as borrowing money to buy a car or a house, using a car or house as collateral. Moreover, borrowing is not free; the broker usually charges 8–10% interest.  Selling stock is procedurally similar to buying stock. Generally, the investor wants to buy low and sell high, if not in that order (short selling); although a number of reasons may induce an investor to sell at a loss, e.g., to avoid further loss.  As with buying a stock, there is a transaction fee for the broker's efforts in arranging the transfer of stock from a seller to a buyer. This fee can be high or low depending on which type of brokerage, full service or discount, handles the transaction.  After the transaction has been made, the seller is then entitled to all of the money. An important part of selling is keeping track of the earnings. Importantly, on selling the stock, in jurisdictions that have them, capital gains taxes will have to be paid on the additional proceeds, if any, that are in excess of the cost basis.  Short selling consists of an investor immediately selling borrowed shares and then buying them back when their price has gone down (called \"covering\").[23] Essentially, such an investor bets[23] that the price of the shares will drop so that they can be bought back at the lower price and thus returned to the lender at a profit.  The risks of short selling are usually higher than those of buying stock, as the loss can theoretically be unlimited since the stock's value can go up indefinitely in theory.[23]  The price of a stock fluctuates fundamentally due to the theory of supply and demand. Like all commodities in the market, the price of a stock is sensitive to demand. However, there are many factors that influence the demand for a particular stock. The fields of fundamental analysis and technical analysis attempt to understand market conditions that lead to price changes, or even predict future price levels. A recent study shows that customer satisfaction, as measured by the American Customer Satisfaction Index (ACSI), is significantly correlated to the market value of a stock.[24] Stock price may be influenced by analysts' business forecast for the company and outlooks for the company's general market segment. Stocks can also fluctuate greatly due to pump and dump scams (also see List of S&P 600 companies).  At any given moment, an equity's price is strictly a result of supply and demand. The supply, commonly referred to as the float, is the number of shares offered for sale at any one moment. The demand is the number of shares investors wish to buy at exactly that same time. The price of the stock moves in order to achieve and maintain equilibrium. The product of this instantaneous price and the float at any one time is the market capitalization of the entity offering the equity at that point in time.  When prospective buyers outnumber sellers, the price rises. Eventually, sellers attracted to the high selling price enter the market and\/or buyers leave, achieving equilibrium between buyers and sellers. When sellers outnumber buyers, the price falls. Eventually buyers enter and\/or sellers leave, again achieving equilibrium.  Thus, the value of a share of a company at any given moment is determined by all investors voting with their money. If more investors want a stock and are willing to pay more, the price will go up. If more investors are selling a stock and there are not enough buyers, the price will go down.[b]  That does not explain how people decide the maximum price at which they are willing to buy or the minimum at which they are willing to sell. In professional investment circles the efficient market hypothesis (EMH) continues to be popular, although this theory is controversial in academic and professional circles. Briefly, EMH says that investing is overall (weighted by the standard deviation) rational; that the price of a stock at any given moment represents a rational evaluation of the known information that might bear on the future value of the company; and that share prices of equities are priced efficiently, which is to say that they represent accurately the expected value of the stock, as best it can be known at a given moment. In other words, prices are the result of discounting expected future cash flows.  The EMH model, if true, has at least two interesting consequences. First, because financial risk is presumed to require at least a small premium on expected value, the return on equity can be expected to be slightly greater than that available from non-equity investments: if not, the same rational calculations would lead equity investors to shift to these safer non-equity investments that could be expected to give the same or better return at lower risk. Second, because the price of a share at every given moment is an \"efficient\" reflection of expected value, then—relative to the curve of expected return—prices will tend to follow a random walk, determined by the emergence of information (randomly) over time. Professional equity investors therefore immerse themselves in the flow of fundamental information, seeking to gain an advantage over their competitors (mainly other professional investors) by more intelligently interpreting the emerging flow of information (news).  The EMH model does not seem to give a complete description of the process of equity price determination. For example, stock markets are more volatile than EMH would imply. In recent years it has come to be accepted that the share markets are not perfectly efficient, perhaps especially in emerging markets or other markets that are not dominated by well-informed professional investors.  Another theory of share price determination comes from the field of behavioral finance. According to behavioral finance, humans often make irrational decisions—particularly, related to the buying and selling of securities—based upon fears and misperceptions of outcomes. The irrational trading of securities can often create securities prices which vary from rational, fundamental price valuations. For instance, during the technology bubble of the late 1990s (which was followed by the dot-com bust of 2000–2002), technology companies were often bid beyond any rational fundamental value because of what is commonly known as the \"greater fool theory\". The \"greater fool theory\" holds that, because the predominant method of realizing returns in equity is from the sale to another investor, one should select securities that they believe that someone else will value at a higher level at some point in the future, without regard to the basis for that other party's willingness to pay a higher price.Thus, even a rational investor may bank on others' irrationality.  When companies raise capital by offering stock on more than one exchange, the potential exists for discrepancies in the valuation of shares on different exchanges. A keen investor with access to information about such discrepancies may invest in expectation of their eventual convergence, known as arbitrage trading. Electronic trading has resulted in extensive price transparency (efficient-market hypothesis) and these discrepancies, if they exist, are short-lived and quickly equilibrated.[26]   "},"meta":{},"created_at":"2025-03-22T14:25:42.290416Z","updated_at":"2025-03-22T14:25:42.290416Z","inner_id":71,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":80,"annotations":[{"id":80,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.321899Z","updated_at":"2025-03-22T14:25:42.321899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"ebeec95c-efd9-4ab6-be5d-4e837d005170","import_id":null,"last_action":null,"bulk_created":false,"task":80,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Forensic accounting, forensic accountancy or financial forensics is the specialty practice area of accounting that investigates whether firms engage in financial reporting misconduct,[1] or financial misconduct within the workplace by employees, officers or directors of the organization.[2] Forensic accountants apply a range of skills and methods to determine whether there has been financial misconduct by the firm or its employees.[3]  Forensic accounting was not formally defined until the 1940s. Originally Frank Wilson is credited with the birth of forensic accounting in the 1930s. When Wilson was working as a CPA for the US Internal Revenue Service, he was assigned to investigate the transactions of the infamous gangster Al Capone. Capone was known for his involvement in illegal activities, including violent crimes. However it was Capone's federal income tax fraud that was discovered by forensic accountants. Wilson's diligent analysis of the financial records of Al Capone indicted him for federal income tax evasion. Capone owed the government $215,080.48 from illegal gambling profits and was guilty of tax evasion for which he was sentenced to 10 years in federal prison. This case established the significance of forensic accounting.[4]  Forensic accountants are necessary for a variety of reasons. They can be useful for criminal investigations, litigation support, insurance claims, and corporate investigations.[5]  Financial forensic engagements may fall into several categories. For example:  Forensic accountants, investigative accountants or expert accountants may be involved in recovering proceeds of serious crime, and provide evidence to confiscation proceedings concerning actual or assumed proceeds of crime or money laundering. In the United Kingdom, relevant legislation is contained in the Proceeds of Crime Act 2002. Forensic accountants typically hold the following qualifications; Certified Forensic Accounting Professional [Certified Forensic Auditors] (CFA - England & Wales) granted by the Forensic Auditors Certification Board of England and Wales (FACB), Certified Fraud Examiners (CFE - US \/ International), Certificate Course on Forensic Accounting and Fraud Detection (FAFD) by Institute of Chartered Accountants of India (ICAI), Certified Public Accountants (CPA - US) with AICPA's [Certified in Financial Forensics est. 2008] (CFF) Credentials, Chartered Accountants (CA - Canada), Certified Management Accountants (CMA - Canada), Chartered Professional Accountants (CPA - Canada), Chartered Certified Accountants (CCA - UK), or Certified Forensic Investigation Professionals (CFIP). In India there is a separate breed of forensic accountants called Certified Forensic Accounting Professionals.[7]  The Certified Forensic Accountant (CRFAC) program from the American Board of Forensic Accounting assesses Certified Public Accountants (CPAs) knowledge and competence in professional forensic accounting services in a multitude of areas.[8] Forensic accountants may be involved in both litigation support (providing assistance on a given case, primarily related to the calculation or estimation of economic damages and related issues) and investigative accounting (looking into illegal activities). The American Board of Forensic Accounting was established in 1993.[8]  Large accounting firms often have a forensic accounting department.[9] All of the larger accounting firms, as well as many medium-sized and boutique firms and various police and government agencies have specialist forensic accounting departments. Within these groups, there may be further sub-specializations: some forensic accountants may, for example, specialize in insurance claims, personal injury claims, fraud, anti-money-laundering, construction,[10] or royalty audits.[11] Forensic accounting used in large companies is sometimes called financial forensics.  The role of the forensic accountants differ from what auditors do.[12] Forensic accountants are involved with investigating and analyzing the factual information brought about by the crime, whereas auditors handle the gross financial statements.[12] Auditors detect financial deficiencies that need to be corrected, and they give suggestions to investors, based on their professional opinion, on the reliance of financial statements.[12] Forensic accountants examine evidence of criminal offences and through this evidence, make efforts to improve the processes adopted by those affected.[12] Though audits and forensic accounting investigations have their differences, they share a couple similarities; both require knowledge of the practices and processes possessed by the business and the general accounting principles concerned with the particular situation, and they both require the ability to interpret financial documents and be objective and impartial.[12]  Forensic accountants combine knowledge of the law with their accounting skills.  They can assess companies, and help companies resolve issues.  This can help companies prevent corruption, fraud, embezzlement, etc. As with all accounting professionals, forensic accountants performing an audit of a company should remain neutral.  Large companies mainly use forensic accountants when performing audits; however, there are other uses for forensic accountants in companies.[13] Forensic accountants often assist in professional negligence claims where they are assessing and commenting on the work of other professionals. Forensic accountants are also engaged in marital and family law, analyzing lifestyle for spousal support purposes, determining income available for child support, and equitable distribution of marital assets.  Forensic accounting and fraud investigation methodologies[14] are different than internal auditing.[15] Thus forensic accounting services[16] and practice should be handled by forensic accounting experts, not by internal auditing experts. Forensic accountants may appear on the crime scene a little later than fraud auditors; their major contribution is in translating complex financial transactions and numerical data into terms that ordinary laypersons can understand, because if the fraud comes to trial, the jury will be made up of ordinary laypersons. On the other hand, internal auditors investigate using checklists and techniques that may not surface the types of evidence that the jury or regulatory bodies look for in proving fraud. Forensic investigation fieldwork may carry legal risks and consultant malpractice risks if internal auditing checklists are used, rather than the specialized skills of forensic accounting.  The fraud cycle describes the process which is taken by those in order to conduct a fraud.[5] It begins with planning the actions of the fraud, which is then followed by the actual commitment of the act, ending with the conversion of the assets to cash.[5] The main goal of Forensic accountants is to determine whether financial crime has been committed, and if so, to what extent. They are often used as expert witnesses to assist the judge or jury in forming the verdict.[17] It is important that forensic accountants possess skills such as microeconomics, cost-center accounting systems, coming up with conclusions with little data, report writing, research skills and interview skills.[17]  This process can employ one or more of the following techniques: review of Public records; background investigations; interviews of knowledgeable parties; analysis of Real evidence to identify possible Forgery and\/or document alterations; Surveillance and inspection of business premises; analysis of individual Financial transactions or statements; review of Business records to identify fictitious vendors, employees, and\/or business activities, or interrogation of suspects, questioning of witnesses or victims.[18]  Forensic accountants are also increasingly playing more proactive risk reduction roles by designing and performing extended procedures as part of the statutory audit, acting as advisers to audit committees, fraud deterrence engagements, and assisting in investment analyst research.[19]  Forensic accounting combines the work of an auditor and a public or private investigator. Unlike auditors whose goal is focused on finding and preventing errors, the role of a forensic accountant is to detect instances of fraud, as well as identify the suspected perpetrator of the fraud.[2] Some of the most common types of fraud schemes include overstating revenues, understating liabilities, inventory manipulation, asset misappropriation, and bribery\/corruption. To discover these, forensic accountants apply a variety of techniques.[20]  Forensic accounting methods can be classified into quantitative and qualitative. The qualitative approach studies the personal characteristics of the individuals behind financial fraud schemes. A popular theory of fraud revolves around the fraud triangle, which classifies the three elements of fraud as perceived opportunity, perceived need (pressures), and rationalization.[21] This theoretical construct was first articulated by behavioral scientist Donald Cressey.[22]  More recently, forensic accountants have gone beyond incentive effects and focused on behavioral characteristics, a branch of accounting known as accounting, behavior and organizations, or organizational behavior. Certain predictive factors, like being labeled as “narcissistic” or committing adultery, are common traits among fraud perpetrators.[1] These characteristics are often not conclusive enough on their own to identify the culprit, but can help forensic accountants to narrow down a suspect list, sometimes based on behavioral or demographic factors.[23]  The quantitative approach focuses on financial data information and searches for abnormalities or patterns predictive of misconduct.[1] Today, forensic accountants work closely with data analytics to dig through complex financial records. Data collection is an important aspect of forensic accounting because proper analysis requires data that is sufficient and reliable.[24] Once a forensic accountant has access to the relevant data, analytic techniques are applied. Predictive modeling can detect potentially fraudulent activities, entity resolution algorithms and social network analytics can identify hidden relationships, and text mining allows forensic accountants to parse through large amounts of unstructured data quickly.[25] Another common quantitative forensic accounting method is the application of Benford's law. Benford's law predicts patterns in an observed set of accounting data, and the more the data deviates from the pattern, the more likely that the data has been manipulated and falsified.[26]  Forensic accountants utilize an understanding of economic theories, business information, financial reporting systems, accounting and auditing standards and procedures, data management & electronic discovery, data analysis techniques for fraud detection, evidence gathering and investigative techniques, and litigation processes and procedures to perform their work.[27]  When detecting fraud in public organizations accountants will look in areas such as billing, corruption, cash and non-cash asset misappropriation, refunds and issues in the payroll department. To detect fraud, companies may undergo management reviews, audits (both internally and externally) and inspection of documents.[28] Forensic accountants will often try to prevent fraud before it happens but searching for errors and in-precise operations as well as poorly documented transactions.[28]  The process begins with the forensic accountant gathering as much information as possible from clients, suppliers, stakeholders and anyone else involved in the company. Next, they will analyze financial statements in order to try and find errors or mistakes in the reporting of those financial statements as well as they will analyze any background information provided. The next step involves interviewing employees in order to try and find where the fraud may be occurring. Investigators will look at company values, performance reviews, management styles and the overall structure of the company. After this is complete the forensic accountant will try to draw conclusions from their findings.[29] "},"meta":{},"created_at":"2025-03-22T14:25:42.290416Z","updated_at":"2025-03-22T14:25:42.290416Z","inner_id":72,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":81,"annotations":[{"id":81,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.321899Z","updated_at":"2025-03-22T14:25:42.321899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"acf87d79-5dd8-464b-94dd-3e74aed17774","import_id":null,"last_action":null,"bulk_created":false,"task":81,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Asset allocation  is the implementation of an investment strategy that attempts to balance risk versus reward by adjusting the percentage of each asset in an investment portfolio according to the investor's risk tolerance, goals and investment time frame.[1] The focus is on the characteristics of the overall portfolio. Such a strategy contrasts with an approach that focuses on individual assets.  Many financial experts argue that asset allocation is an important factor in determining returns for an investment portfolio.[1] Asset allocation is based on the principle that different assets perform differently in different market and economic conditions.  A fundamental justification for asset allocation is the notion that different asset classes offer returns that are not perfectly correlated, hence diversification reduces the overall risk in terms of the variability of returns for a given level of expected return. Asset diversification has been described as \"the only free lunch you will find in the investment game\".[2] Academic research has painstakingly explained the importance and benefits of asset allocation and the problems of active management (see academic studies section below).  Although the risk is reduced as long as correlations are not perfect, it is typically forecast (wholly or in part) based on statistical relationships (like correlation and variance) that existed over some past period. Expectations for return are often derived in the same way. Studies of these forecasting methods constitute an important direction of academic research.  When such backward-looking approaches are used to forecast future returns or risks using the traditional mean-variance optimization approach to the asset allocation of modern portfolio theory (MPT), the strategy is, in fact, predicting future risks and returns based on history. As there is no guarantee that past relationships will continue in the future, this is one of the \"weak links\" in traditional asset allocation strategies as derived from MPT. Other, more subtle weaknesses include seemingly minor errors in forecasting leading to recommended allocations that are grossly skewed from investment mandates and\/or impractical—often even violating an investment manager's \"common sense\" understanding of a tenable portfolio-allocation strategy.  An asset class is a group of economic resources sharing similar characteristics, such as riskiness and return. There are many types of assets that may or may not be included in an asset allocation strategy.  The \"traditional\" asset classes are stocks, bonds, and cash:  Allocation among these three provides a starting point. Usually included are hybrid instruments such as convertible bonds and preferred stocks, counting as a mixture of bonds and stocks.  Other alternative assets that may be considered include:  There are several types of asset allocation strategies based on investment goals, risk tolerance, time frames and diversification. The most common forms of asset allocation are: strategic, dynamic, tactical, and core-satellite.  The primary goal of strategic asset allocation is to create an asset mix that seeks to provide the optimal balance between expected risk and return for a long-term investment horizon.[3] Generally speaking, strategic asset allocation strategies are agnostic to economic environments, i.e., they do not change their allocation postures relative to changing market or economic conditions.  Dynamic asset allocation is similar to strategic asset allocation in that portfolios are built by allocating to an asset mix that seeks to provide the optimal balance between expected risk and return for a long-term investment horizon.[3] Like strategic allocation strategies, dynamic strategies largely retain exposure to their original asset classes; however, unlike strategic strategies, dynamic asset allocation portfolios will adjust their postures over time relative to changes in the economic environment.  Tactical asset allocation is a strategy in which an investor takes a more active approach that tries to position a portfolio into those assets, sectors, or individual stocks that show the most potential for perceived gains.[4][5] While an original asset mix is formulated much like strategic and dynamic portfolio, tactical strategies are often traded more actively and are free to move entirely in and out of their core asset classes.  Core-satellite allocation strategies generally contain a 'core' strategic element making up the most significant portion of the portfolio, while applying a dynamic or tactical 'satellite' strategy that makes up a smaller part of the portfolio. In this way, core-satellite allocation strategies are a hybrid of the strategic and dynamic\/tactical allocation strategies mentioned above.[6]  Industry sectors may be classified according to an industry classification taxonomy (such as the Industry Classification Benchmark).[7][8] The top-level sectors may be grouped as below:  Per the Tactical asset allocation strategy above, an investor may allocate more to cyclical sectors when the economy is showing gains, and more to defensive when it is not.  In 1986, Gary P. Brinson, L. Randolph Hood, and SEI's Gilbert L. Beebower (BHB) published a study about asset allocation of 91 large pension funds measured from 1974 to 1983.[10] They replaced the pension funds' stock, bond, and cash selections with corresponding market indexes. The indexed quarterly return was found to be higher than the pension plan's actual quarterly return. The two quarterly return series' linear correlation was measured at 96.7%, with shared variance of 93.6%. A 1991 follow-up study by Brinson, Singer, and Beebower measured variance of 91.5%.[11] The conclusion of the study was that replacing active choices with simple asset classes worked just as well as, if not even better than, professional pension managers. Also, a small number of asset classes was sufficient for financial planning. Financial advisors often pointed to this study to support the idea that asset allocation is more important than all other concerns, which the BHB study is incorrectly thought to have lumped together as \"market timing\" but was actually policy selection.[12] One problem with the Brinson study was that the cost factor in the two return series was not clearly discussed. However, in response to a letter to the editor, Hood noted that the returns series were gross of management fees.[13]  In 1997, William Jahnke initiated a debate on this topic, attacking the BHB study in a paper titled \"The Asset Allocation Hoax\".[14] The Jahnke discussion appeared in the Journal of Financial Planning as an opinion piece, not a peer reviewed article. Jahnke's main criticism, still undisputed, was that BHB's use of quarterly data dampens the impact of compounding slight portfolio disparities over time, relative to the benchmark. One could compound 2% and 2.15% quarterly over 20 years and see the sizable difference in cumulative return. However, the difference is still 15 basis points (hundredths of a percent) per quarter; the difference is one of perception, not fact.  In 2000, Ibbotson and Kaplan used five asset classes in their study \"Does Asset Allocation Policy Explain 40, 90, or 100 Percent of Performance?\".[15] The asset classes included were large-cap US stock, small-cap US stock, non-US stock, US bonds, and cash. Ibbotson and Kaplan examined the 10-year return of 94 US balanced mutual funds versus the corresponding indexed returns. This time, after properly adjusting for the cost of running index funds, the actual returns again failed to beat index returns. The linear correlation between monthly index return series and the actual monthly actual return series was measured at 90.2%, with shared variance of 81.4%. Ibbotson concluded 1) that asset allocation explained 40% of the variation of returns across funds, and 2) that it explained virtually 100% of the level of fund returns. Gary Brinson has expressed his general agreement with the Ibbotson-Kaplan conclusions.  In both studies, it is misleading to make statements such as \"asset allocation explains 93.6% of investment return\".[16] Even \"asset allocation explains 93.6% of quarterly performance variance\" leaves much to be desired, because the shared variance could be from pension funds' operating structure.[15] Hood, however, rejects this interpretation on the grounds that pension plans, in particular, cannot cross-share risks and that they are explicitly singular entities, rendering shared variance irrelevant.[13] The statistics were most helpful when used to demonstrate the similarity of the index return series and the actual return series.  A 2000 paper by Meir Statman found that using the same parameters that explained BHB's 93.6% variance result, a hypothetical financial advisor with perfect foresight in tactical asset allocation performed 8.1% better per year, yet the strategic asset allocation still explained 89.4% of the variance.[12] Thus, explaining variance does not explain performance. Statman says that strategic asset allocation is movement along the efficient frontier, whereas tactical asset allocation involves movement of the efficient frontier. A more common sense explanation of the Brinson, Hood, and Beebower study is that asset allocation explains more than 90% of the volatility of returns of an overall portfolio, but will not explain the ending results of your portfolio over long periods of time. Hood notes in his review of the material over 20 years, however, that explaining performance over time is possible with the BHB approach but was not the focus of the original paper.[17]  Bekkers, Doeswijk and Lam (2009) investigate the diversification benefits for a portfolio by distinguishing ten different investment categories simultaneously in a mean-variance analysis as well as a market portfolio approach. The results suggest that real estate, commodities, and high yield add the most value to the traditional asset mix of stocks, bonds, and cash. A study with such broad coverage of asset classes has not been conducted before, not in the context of determining capital market expectations and performing a mean-variance analysis, neither in assessing the global market portfolio.[18]  Doeswijk, Lam and Swinkels (2014) argue that the portfolio of the average investor contains important information for strategic asset allocation purposes. This portfolio shows the relative value of all assets according to the market crowd, which one could interpret as a benchmark or the optimal portfolio for the average investor. The authors determine the market values of equities, private equity, real estate, high yield bonds, emerging debt, non-government bonds, government bonds, inflation linked bonds, commodities, and hedge funds. For this range of assets, they estimate the invested global market portfolio for the period 1990 to 2012. For the main asset categories equities, real estate, non-government bonds, and government bonds they extend the period to 1959 until 2012.[19]  Doeswijk, Lam and Swinkels (2019) show that the global market portfolio realizes a compounded real return of 4.45% per year with a standard deviation of 11.2% from 1960 until 2017. In the inflationary period from 1960 to 1979, the compounded real return of the global market portfolio is 3.24% per year, while this is 6.01% per year in the disinflationary period from 1980 to 2017. The average return during recessions was -1.96% per year, versus 7.72% per year during expansions. The reward for the average investor over the period 1960 to 2017 is a compounded return of 3.39% points above the risk-less rate earned by savers.[20]  Historically, since the 20th century, US equities have outperformed equities of other countries because of the competitive advantage US has due to its large GDP.  McGuigan described an examination of funds that were in the top quartile of performance during 1983 to 1993.[21] During the second measurement period of 1993 to 2003, only 28.57% of the funds remained in the top quartile. 33.33% of the funds dropped to the second quartile. The rest of the funds dropped to the third or fourth quartile.  In fact, low cost was a more reliable indicator of performance. Bogle noted that an examination of five-year performance data of large-cap blend funds revealed that the lowest cost quartile funds had the best performance, and the highest cost quartile funds had the worst performance.[22]  In asset allocation planning, the decision on the amount of stocks versus bonds in one's portfolio is a very important decision. Simply buying stocks without regard of a possible bear market can result in panic selling later. One's true risk tolerance can be hard to gauge until having experienced a real bear market with money invested in the market. Finding the proper balance is key.  The tables show why asset allocation is important. It determines an investor's future return, as well as the bear market burden that he or she will have to carry successfully to realize the returns.  There are various reasons why asset allocation fails to work. "},"meta":{},"created_at":"2025-03-22T14:25:42.290416Z","updated_at":"2025-03-22T14:25:42.290416Z","inner_id":73,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":82,"annotations":[{"id":82,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.321899Z","updated_at":"2025-03-22T14:25:42.321899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"b8fd6454-a6e5-43fc-9642-6045733441e1","import_id":null,"last_action":null,"bulk_created":false,"task":82,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Heterodox  A financial crisis is any of a broad variety of situations in which some financial assets suddenly lose a large part of their nominal value. In the 19th and early 20th centuries, many financial crises were associated with banking panics, and many recessions coincided with these panics. Other situations that are often called financial crises include stock market crashes and the bursting of other financial bubbles, currency crises, and sovereign defaults.[1][2] Financial crises directly result in a loss of paper wealth but do not necessarily result in significant changes in the real economy (for example, the crisis resulting from the famous tulip mania bubble in the 17th century).  Many economists have offered theories about how financial crises develop and how they could be prevented. There is little consensus and financial crises continue to occur from time to time. It is apparent however that a consistent feature of both economic (and other applied finance disciplines) is the obvious inability to predict and avert financial crises.[3] This realization raises the question as to what is known and also capable of being known (i.e. the epistemology) within economics and applied finance. It has been argued that the assumptions of unique, well-defined causal chains being present in economic thinking, models and data, could, in part, explain why financial crises are often inherent and unavoidable.[4]  When a bank suffers a sudden rush of withdrawals by depositors, this is called a bank run. Since banks lend out most of the cash they receive in deposits (see fractional-reserve banking), it is difficult for them to quickly pay back all deposits if these are suddenly demanded, so a run renders the bank insolvent, causing customers to lose their deposits, to the extent that they are not covered by deposit insurance. An event in which bank runs are widespread is called a systemic banking crisis or banking panic.[5]  Examples of bank runs include the run on the Bank of the United States in 1931 and the run on Northern Rock in 2007.[6] Banking crises generally occur after periods of risky lending and resulting loan defaults.  A currency crisis, also called a devaluation crisis,[7] is normally considered as part of a financial crisis. Kaminsky et al. (1998), for instance, define currency crises as occurring when a weighted average of monthly percentage depreciations in the exchange rate and monthly percentage declines in exchange reserves exceeds its mean by more than three standard deviations. Frankel and Rose (1996) define a currency crisis as a nominal depreciation of a currency of at least 25% but it is also defined as at least a 10% increase in the rate of depreciation. In general, a currency crisis can be defined as a situation when the participants in an exchange market come to recognize that a pegged exchange rate is about to fail, causing speculation against the peg that hastens the failure and forces a devaluation.[7]  A speculative bubble (also called a financial bubble or an economic bubble) exists in the event of large, sustained overpricing of some class of assets.[8] One factor that frequently contributes to a bubble is the presence of buyers who purchase an asset based solely on the expectation that they can later resell it at a higher price, rather than calculating the income it will generate in the future. If there is a bubble, there is also a risk of a crash in asset prices: market participants will go on buying only as long as they expect others to buy, and when many decide to sell the price will fall. However, it is difficult to predict whether an asset's price actually equals its fundamental value, so it is hard to detect bubbles reliably. Some economists insist that bubbles never or almost never occur.[9]  Well-known examples of bubbles (or purported bubbles) and crashes in stock prices and other asset prices include the 17th century Dutch tulip mania, the 18th century South Sea Bubble, the Wall Street crash of 1929, the Japanese property bubble of the 1980s, and the crash of the United States housing bubble during 2006–2008.[10][11] The 2000s sparked a real estate bubble where housing prices were increasing significantly as an asset good.[12]  When a country that maintains a fixed exchange rate is suddenly forced to devalue its currency due to accruing an unsustainable current account deficit, this is called a currency crisis or balance of payments crisis. When a country fails to pay back its sovereign debt, this is called a sovereign default. While devaluation and default could both be voluntary decisions of the government, they are often perceived to be the involuntary results of a change in investor sentiment that leads to a sudden stop in capital inflows or a sudden increase in capital flight.  Several currencies that formed part of the European Exchange Rate Mechanism suffered crises in 1992–93 and were forced to devalue or withdraw from the mechanism. Another round of currency crises took place in Asia in 1997–98. Many Latin American countries defaulted on their debt in the early 1980s. The 1998 Russian financial crisis resulted in a devaluation of the ruble and default on Russian government bonds.  Negative GDP growth lasting two or more quarters is called a recession. An especially prolonged or severe recession may be called a depression, while a long period of slow but not necessarily negative growth is sometimes called economic stagnation.  Some economists argue that many recessions have been caused in large part by financial crises. One important example is the Great Depression, which was preceded in many countries by bank runs and stock market crashes. The subprime mortgage crisis and the bursting of other real estate bubbles around the world also led to recession in the U.S. and a number of other countries in late 2008 and 2009. Some economists argue that financial crises are caused by recessions instead of the other way around, and that even where a financial crisis is the initial shock that sets off a recession, other factors may be more important in prolonging the recession. In particular, Milton Friedman and Anna Schwartz argued that the initial economic decline associated with the crash of 1929 and the bank panics of the 1930s would not have turned into a prolonged depression if it had not been reinforced by monetary policy mistakes on the part of the Federal Reserve,[13] a position supported by Ben Bernanke.[14]  It is often observed that successful investment requires each investor in a financial market to guess what other investors will do. Reflexivity refers to the circular relationships often evident in social systems between cause and effect – and relates to the property of self-referencing in financial markets.[15] George Soros has been a proponent of the reflexivity paradigm surrounding financial crises.[16] Similarly, John Maynard Keynes compared financial markets to a beauty contest game in which each participant tries to predict which model other participants will consider most beautiful.[17]  Furthermore, in many cases, investors have incentives to coordinate their choices. For example, someone who thinks other investors want to heavily buy Japanese yen may expect the yen to rise in value, and therefore has an incentive to buy yen, too. Likewise, a depositor in IndyMac Bank who expects other depositors to withdraw their funds may expect the bank to fail, and therefore has an incentive to withdraw, too. Economists call an incentive to mimic the strategies of others strategic complementarity.[18]  It has been argued that if people or firms have a sufficiently strong incentive to do the same thing they expect others to do, then self-fulfilling prophecies may occur.[19] For example, if investors expect the value of the yen to rise, this may cause its value to rise; if depositors expect a bank to fail this may cause it to fail.[20] Therefore, financial crises are sometimes viewed as a vicious circle in which investors shun some institution or asset because they expect others to do so.[21] Reflexivity poses a challenge to the epistemic norms typically assumed within financial economics and all of empirical finance.[4] The possibility of financial crises being beyond the predictive reach of causality is discussed further within Epistemology of finance.  Leverage, which means borrowing to finance investments, is frequently cited as a contributor to financial crises. When a financial institution (or an individual) only invests its own money, it can, in the very worst case, lose its own money. But when it borrows in order to invest more, it can potentially earn more from its investment, but it can also lose more than all it has. Therefore, leverage magnifies the potential returns from investment, but also creates a risk of bankruptcy. Since bankruptcy means that a firm fails to honor all its promised payments to other firms, it may spread financial troubles from one firm to another (see 'Contagion' below).  For example, borrowing to finance investment in the stock market (\"margin buying\") became increasingly common prior to the Wall Street crash of 1929.  Another factor believed to contribute to financial crises is asset-liability mismatch, a situation in which the risks associated with an institution's debts and assets are not appropriately aligned. For example, commercial banks offer deposit accounts that can be withdrawn at any time, and they use the proceeds to make long-term loans to businesses and homeowners. The mismatch between the banks' short-term liabilities (its deposits) and its long-term assets (its loans) is seen as one of the reasons bank runs occur (when depositors panic and decide to withdraw their funds more quickly than the bank can get back the proceeds of its loans).[20] Likewise, Bear Stearns failed in 2007–08 because it was unable to renew the short-term debt it used to finance long-term investments in mortgage securities.  In an international context, many emerging market governments are unable to sell bonds denominated in their own currencies, and therefore sell bonds denominated in US dollars instead. This generates a mismatch between the currency denomination of their liabilities (their bonds) and their assets (their local tax revenues), so that they run a risk of sovereign default due to fluctuations in exchange rates.[22]  Many analyses of financial crises emphasize the role of investment mistakes caused by lack of knowledge or the imperfections of human reasoning. Behavioural finance studies errors in economic and quantitative reasoning. Psychologist Torbjorn K A Eliazon has also analyzed failures of economic reasoning in his concept of 'œcopathy'.[23]  Historians, notably Charles P. Kindleberger, have pointed out that crises often follow soon after major financial or technical innovations that present investors with new types of financial opportunities, which he called \"displacements\" of investors' expectations.[24][25] Early examples include the South Sea Bubble and Mississippi Bubble of 1720, which occurred when the notion of investment in shares of company stock was itself new and unfamiliar,[26] and the Crash of 1929, which followed the introduction of new electrical and transportation technologies.[27] More recently, many financial crises followed changes in the investment environment brought about by financial deregulation, and the crash of the dot com bubble in 2001 arguably began with \"irrational exuberance\" about Internet technology.[28]  Unfamiliarity with recent technical and financial innovations may help explain how investors sometimes grossly overestimate asset values. Also, if the first investors in a new class of assets (for example, stock in \"dot com\" companies) profit from rising asset values as other investors learn about the innovation (in our example, as others learn about the potential of the Internet), then still more others may follow their example, driving the price even higher as they rush to buy in hopes of similar profits. If such \"herd behaviour\" causes prices to spiral up far above the true value of the assets, a crash may become inevitable. If for any reason the price briefly falls, so that investors realize that further gains are not assured, then the spiral may go into reverse, with price decreases causing a rush of sales, reinforcing the decrease in prices.  Governments have attempted to eliminate or mitigate financial crises by regulating the financial sector. One major goal of regulation is transparency: making institutions' financial situations publicly known by requiring regular reporting under standardized accounting procedures. Another goal of regulation is making sure institutions have sufficient assets to meet their contractual obligations, through reserve requirements, capital requirements, and other limits on leverage.  Some financial crises have been blamed on insufficient regulation, and have led to changes in regulation in order to avoid a repeat. For example, the former managing director of the International Monetary Fund, Dominique Strauss-Kahn, has blamed the financial crisis of 2007–2008 on 'regulatory failure to guard against excessive risk-taking in the financial system, especially in the US'.[29] Likewise, The New York Times singled out the deregulation of credit default swaps as a cause of the crisis.[30]  However, excessive regulation has also been cited as a possible cause of financial crises. In particular, the Basel II Accord has been criticized for requiring banks to increase their capital when risks rise, which might cause them to decrease lending precisely when capital is scarce, potentially aggravating a financial crisis.[31]  International regulatory convergence has been interpreted in terms of regulatory herding, deepening market herding (discussed above) and so increasing systemic risk.[32][33] From this perspective, maintaining diverse regulatory regimes would be a safeguard.  Fraud has played a role in the collapse of some financial institutions, when companies have attracted depositors with misleading claims about their investment strategies, or have embezzled the resulting income. Examples include Charles Ponzi's scam in early 20th century Boston, the collapse of the MMM investment fund in Russia in 1994, the scams that led to the Albanian Lottery Uprising of 1997, and the collapse of Madoff Investment Securities in 2008.  Many rogue traders that have caused large losses at financial institutions have been accused of acting fraudulently in order to hide their trades. Fraud in mortgage financing has also been cited as one possible cause of the 2008 subprime mortgage crisis; government officials stated on 23 September 2008 that the FBI was looking into possible fraud by mortgage financing companies Fannie Mae and Freddie Mac, Lehman Brothers, and insurer American International Group.[34] Likewise it has been argued that many financial companies failed in the recent crisis[clarification needed] because their managers failed to carry out their fiduciary duties.[35]  Contagion refers to the idea that financial crises may spread from one institution to another, as when a bank run spreads from a few banks to many others, or from one country to another, as when currency crises, sovereign defaults, or stock market crashes spread across countries. When the failure of one particular financial institution threatens the stability of many other institutions, this is called systemic risk.[32]  One widely cited example of contagion was the spread of the Thai crisis in 1997 to other countries like South Korea. However, economists often debate whether observing crises in many countries around the same time is truly caused by contagion from one market to another, or whether it is instead caused by similar underlying problems that would have affected each country individually even in the absence of international linkages.  The nineteenth century Banking School theory of crises suggested that crises were caused by flows of investment capital between areas with different rates of interest. Capital could be borrowed in areas with low interest rates and invested in areas of high interest. Using this method a small profit could be made with little or no capital. However, when interest rates changed and the incentive for the flow was removed or reversed sudden changes in capital flows could occur. The subjects of investment might be starved of cash possibly becoming insolvent and creating a credit crunch and the loaning banks would be left with defaulting investors leading to a banking crisis.[36] As Charles Read has pointed out, the modern equivalent of this process involves the Carry Trade, see Carry (investment).[37]  Some financial crises have little effect outside of the financial sector, like the Wall Street crash of 1987, but other crises are believed to have played a role in decreasing growth in the rest of the economy. There are many theories why a financial crisis could have a recessionary effect on the rest of the economy. These theoretical ideas include the 'financial accelerator', 'flight to quality' and 'flight to liquidity', and the Kiyotaki-Moore model. Some 'third generation' models of currency crises explore how currency crises and banking crises together can cause recessions.[38]  Austrian School economists Ludwig von Mises and Friedrich Hayek discussed the business cycle starting with Mises' Theory of Money and Credit, published in 1912.  Recurrent major depressions in the world economy at the pace of 20 and 50 years have been the subject of studies since Jean Charles Léonard de Sismondi (1773–1842) provided the first theory of crisis in a critique of classical political economy's assumption of equilibrium between supply and demand. Developing an economic crisis theory became the central recurring concept throughout Karl Marx's mature work. Marx's law of the tendency for the rate of profit to fall borrowed many features of the presentation of John Stuart Mill's discussion Of the Tendency of Profits to a Minimum (Principles of Political Economy Book IV Chapter IV). The theory is a corollary of the Tendency towards the Centralization of Profits.  In a capitalist system, successfully-operating businesses return less money to their workers (in the form of wages) than the value of the goods produced by those workers (i.e. the amount of money the products are sold for). This profit first goes towards covering the initial investment in the business. In the long-run, however, when one considers the combined economic activity of all successfully-operating business, it is clear that less money (in the form of wages) is being returned to the mass of the population (the workers) than is available to them to buy all of these goods being produced. Furthermore, the expansion of businesses in the process of competing for markets leads to an abundance of goods and a general fall in their prices, further exacerbating the tendency for the rate of profit to fall.  The viability of this theory depends upon two main factors: firstly, the degree to which profit is taxed by government and returned to the mass of people in the form of welfare, family benefits and health and education spending; and secondly, the proportion of the population who are workers rather than investors\/business owners. Given the extraordinary capital expenditure required to enter modern economic sectors like airline transport, the military industry, or chemical production, these sectors are extremely difficult for new businesses to enter and are being concentrated in fewer and fewer hands.  Empirical and econometric research continues especially in the world systems theory and in the debate about Nikolai Kondratiev and the so-called 50-years Kondratiev waves. Major figures of world systems theory, like Andre Gunder Frank and Immanuel Wallerstein, consistently warned about the crash that the world economy is now facing.[citation needed] World systems scholars and Kondratiev cycle researchers always implied that Washington Consensus oriented economists never understood the dangers and perils, which leading industrial nations will be facing and are now facing at the end of the long economic cycle which began after the oil crisis of 1973.  Hyman Minsky has proposed a post-Keynesian explanation that is most applicable to a closed economy. He theorized that financial fragility is a typical feature of any capitalist economy. High fragility leads to a higher risk of a financial crisis. To facilitate his analysis, Minsky defines three approaches that financing firms may choose, according to their tolerance of risk. They are hedge finance, speculative finance, and Ponzi finance. Ponzi finance leads to the most fragility.  Financial fragility levels move together with the business cycle. After a recession, firms have lost much financing and choose only hedge, the safest. As the economy grows and expected profits rise, firms tend to believe that they can allow themselves to take on speculative financing. In this case, they know that profits will not cover all the interest all the time. Firms, however, believe that profits will rise and the loans will eventually be repaid without much trouble. More loans lead to more investment, and the economy grows further. Then lenders also start believing that they will get back all the money they lend. Therefore, they are ready to lend to firms without full guarantees of success.  Lenders know that such firms will have problems repaying. Still, they believe these firms will refinance from elsewhere as their expected profits rise. This is Ponzi financing. In this way, the economy has taken on much risky credit. Now it is only a question of time before some big firm actually defaults. Lenders understand the actual risks in the economy and stop giving credit so easily. Refinancing becomes impossible for many, and more firms default. If no new money comes into the economy to allow the refinancing process, a real economic crisis begins. During the recession, firms start to hedge again, and the cycle is closed.  The Banking School theory of crises describes a continuous cycle driven by varying interest rates. It is based on the work of Thomas Tooke, Thomas Attwood, Henry Thornton, William Jevons and a number of bankers opposed to the Bank Charter Act 1844.  Starting at a time when short-term interest rates are low, frustration builds up among investors who search for a better yield in countries and locations with higher rates, leading to increased capital flows to countries with higher rates. Internally, short-term rates rise above long-term rates causing failures where borrowing at short term rates has been used to invest long-term where the funds cannot be liquidated quickly (a similar mechanism was implicated in the March 2023 failure of SVB Bank). Internationally, arbitrage and the need to stop capital flows, which caused bullion drains in the gold standard of the nineteenth century and drains of foreign capital later, bring interest rates in the low-rate country up to equal those in the country which is the subject of investment.  The capital flows reverse or cease suddenly causing the subject of investment to be starved of funds and the remaining investors (often those who are least knowledgeable) to be left with devalued assets. Bankruptcies, defaults and bank failures follow as rates are pushed high. After the crisis governments push short-term interest rates low again to diminish the cost of servicing government borrowing which has been used to overcome the crisis. Funds build up again looking for investment opportunities and the cycle restarts from the beginning.[39]  Mathematical approaches to modeling financial crises have emphasized that there is often positive feedback[40] between market participants' decisions (see strategic complementarity).[41] Positive feedback implies that there may be dramatic changes in asset values in response to small changes in economic fundamentals. For example, some models of currency crises (including that of Paul Krugman) imply that a fixed exchange rate may be stable for a long period of time, but will collapse suddenly in an avalanche of currency sales in response to a sufficient deterioration of government finances or underlying economic conditions.[42][43]  According to some theories, positive feedback implies that the economy can have more than one equilibrium. There may be an equilibrium in which market participants invest heavily in asset markets because they expect assets to be valuable. This is the type of argument underlying Diamond and Dybvig's model of bank runs, in which savers withdraw their assets from the bank because they expect others to withdraw too.[20] Likewise, in Obstfeld's model of currency crises, when economic conditions are neither too bad nor too good, there are two possible outcomes: speculators may or may not decide to attack the currency depending on what they expect other speculators to do.[21]  A variety of models have been developed in which asset values may spiral excessively up or down as investors learn from each other. In these models, asset purchases by a few agents encourage others to buy too, not because the true value of the asset increases when many buy (which is called \"strategic complementarity\"), but because investors come to believe the true asset value is high when they observe others buying.  In \"herding\" models, it is assumed that investors are fully rational, but only have partial information about the economy. In these models, when a few investors buy some type of asset, this reveals that they have some positive information about that asset, which increases the rational incentive of others to buy the asset too. Even though this is a fully rational decision, it may sometimes lead to mistakenly high asset values (implying, eventually, a crash) since the first investors may, by chance, have been mistaken.[44][45][46][47] Herding models, based on Complexity Science, indicate that it is the internal structure of the market, not external influences, which is primarily responsible for crashes.[48]  In \"adaptive learning\" or \"adaptive expectations\" models, investors are assumed to be imperfectly rational, basing their reasoning only on recent experience. In such models, if the price of a given asset rises for some period of time, investors may begin to believe that its price always rises, which increases their tendency to buy and thus drives the price up further. Likewise, observing a few price decreases may give rise to a downward price spiral, so in models of this type, large fluctuations in asset prices may occur. Agent-based models of financial markets often assume investors act on the basis of adaptive learning or adaptive expectations.  As the most recent and most damaging financial crisis event, the Global financial crisis, deserves special attention, as its causes, effects, response, and lessons are most applicable to the current financial system.  A noted survey of financial crises is This Time is Different: Eight Centuries of Financial Folly (Reinhart & Rogoff 2009), by economists Carmen Reinhart and Kenneth Rogoff, who are regarded as among the foremost historians of financial crises.[49] In this survey, they trace the history of financial crisis back to sovereign defaults – default on public debt, – which were the form of crisis prior to the 18th century and continue, then and now causing private bank failures; crises since the 18th century feature both public debt default and private debt default. Reinhart and Rogoff also class debasement of currency and hyperinflation as being forms of financial crisis, broadly speaking, because they lead to unilateral reduction (repudiation) of debt.  Reinhart and Rogoff trace inflation (to reduce debt) to Dionysius I's rule in Syracuse and begin their \"eight centuries\" in 1258; debasement of currency also occurred under the Roman Empire and Byzantine Empire. A financial crisis in 33 A.D. caused by a contraction of money supply had been recorded by several Roman historians.[50]  Among the earliest crises Reinhart and Rogoff studied is the 1340 default of England, due to setbacks in its war with France (the Hundred Years' War; see details). Further early sovereign defaults include seven defaults by the Spanish Empire, four under Philip II, three under his successors.  Other global and national financial mania since the 17th century include:  Specific: "},"meta":{},"created_at":"2025-03-22T14:25:42.290416Z","updated_at":"2025-03-22T14:25:42.290416Z","inner_id":74,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":83,"annotations":[{"id":83,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.321899Z","updated_at":"2025-03-22T14:25:42.321899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"11904377-7e05-4a7a-837c-d316bbae5ae1","import_id":null,"last_action":null,"bulk_created":false,"task":83,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"In  finance, an exchange rate is the rate at which one currency will be exchanged for another currency.[1] Currencies are most commonly national currencies, but may be sub-national as in the case of Hong Kong or supra-national as in the case of the euro.[2]  The exchange rate is also regarded as the value of one country's currency in relation to another currency.[3] For example, an interbank exchange rate of 141 Japanese yen  to the United States dollar means that ¥141 will be exchanged for US$1 or that US$1 will be exchanged for ¥141. In this case it is said that the price of a dollar in relation to yen is ¥141, or equivalently that the price of a yen in relation to dollars is $1\/141.  Each country determines the exchange rate regime that will apply to its currency. For example, a currency may be floating, pegged (fixed), or a hybrid.[4] Governments can impose certain limits and controls on exchange rates. Countries can also have a strong or weak currency.[4] There is no agreement in the economic literature on the optimal national exchange rate policy (unlike on the subject of trade where free trade is considered optimal).[5] Rather, national exchange rate regimes reflect political considerations.[5]  In floating exchange rate regimes, exchange rates are  determined in the foreign exchange market,[6] which is open to a wide range of different types of buyers and sellers, and where currency trading is continuous: 24 hours a day except weekends (i.e. trading from 20:15 GMT on Sunday until 22:00 GMT Friday). The spot exchange rate is the current exchange rate, while the forward exchange rate is an exchange rate that is quoted and traded today but for delivery and payment on a specific future date.  In the retail currency exchange market, different buying and selling rates will be quoted by money dealers. Most trades are to or from the local currency. The buying rate is the rate at which money dealers will buy foreign currency, and the selling rate is the rate at which they will sell that currency. The quoted rates will incorporate an allowance for a dealer's margin (or profit) in trading, or else the margin may be recovered in the form of a commission or in some other way. Different rates may also be quoted for cash, a documentary transaction or for electronic transfers. The higher rate on documentary transactions has been justified as compensating for the additional time and cost of clearing the document. On the other hand, cash is available for resale immediately, but incurs security, storage, and transportation costs, and the cost of tying up capital in a stock of banknotes (bills).  Currency for international travel and cross-border payments is predominantly purchased from banks, foreign exchange brokerages and various forms of bureaux de change.[citation needed] These retail outlets source currency from the interbank markets, which are valued by the Bank for International Settlements at US$5.3 trillion per day.[7] The purchase is made at the spot contract rate. Retail customers will be charged, in the form of commission or otherwise, to cover the provider's costs and generate a profit. One form of charge is the use of an exchange rate that is less favourable than the wholesale spot rate.[8] The difference between retail buying and selling prices is referred to as the bid–ask spread.   Retail foreign exchange trading is a small segment of the larger foreign exchange market where individuals speculate on the exchange rate between different currencies. This segment has developed with the advent of dedicated electronic trading platforms and the internet, which allows individuals to access the global currency markets. As of 2016, it was reported that retail foreign exchange trading represented 5.5% of the whole foreign exchange market ($282 billion in daily trading turnover).[9]  There is a market convention that rules the notation used to communicate the fixed and variable currencies in a quotation. For example, in a conversion from EUR to AUD, EUR is the fixed currency, AUD is the variable currency and the exchange rate indicates how many Australian dollars would be paid or received for 1 euro.  In some areas of Europe and in the retail market in the United Kingdom, EUR and GBP are reversed so that GBP is quoted as the fixed currency to the euro. In order to determine which is the fixed currency when neither currency is on the above list (i.e. both are \"other\"), market convention is to use the fixed currency which gives an exchange rate greater than 1.000. This reduces rounding issues and the need to use excessive numbers of decimal places. There are some exceptions to this rule: for example, the Japanese often quote their currency as the base to other currencies.  Quotation using a country's home currency as the price currency is known as direct quotation or price quotation (from that country's perspective) [clarification needed] For example, €0.8989 = US$1.00 in the Eurozone[10] and is used in most countries.  Quotation using a country's home currency as the unit currency[clarification needed] (for example, US$1.11 = €1.00 in the Eurozone) is known as indirect quotation or quantity quotation and is used in British newspapers; it is also common in Australia, New Zealand and the Eurozone.  Using direct quotation, if the home currency is strengthening (that is, appreciating, or becoming more valuable) then the exchange rate number decreases. Conversely, if the foreign currency is strengthening and the home currency is depreciating, the exchange rate number increases.  Market convention from the early 1980s to 2006 was that most currency pairs were quoted to four decimal places for spot transactions and up to six decimal places for forward outrights or swaps. (The fourth decimal place is usually referred to as a \"pip\"). An exception to this was exchange rates with a value of less than 1.000 which were usually quoted to five or six decimal places. Although there is no fixed rule, exchange rates numerically greater than around 20 were usually quoted to three decimal places and exchange rates greater than 80 were quoted to two decimal places. Currencies over 5000 were usually quoted with no decimal places (for example, the former Turkish Lira). e.g. (GBPOMR : 0.765432 -  : 1.4436 - EURJPY : 165.29). In other words, quotes are given with five digits. Where rates are below 1, quotes frequently include five decimal places.[11]  In 2005, Barclays Capital broke with convention by quoting spot exchange rates with five or six decimal places on their electronic dealing platform.[12] The contraction of spreads (the difference between the bid and ask rates) arguably necessitated finer pricing and gave the banks the ability to try to win transactions on multibank trading platforms where all banks may otherwise have been quoting the same price. A number of other banks have since followed this system.[citation needed]  Countries are free to choose which type of exchange rate regime they will apply to their currency. The main types of exchange rate regimes are: free-floating, pegged (fixed), or a hybrid.  In free-floating regimes, exchange rates are allowed to vary against each other according to the market forces of supply and demand. Exchange rates for such currencies are likely to change almost constantly as quoted on financial markets, mainly by banks, around the world.  A movable or adjustable peg system is a system of fixed exchange rates, but with a provision for the revaluation (usually devaluation) of a currency. For example, between 1994 and 2005, the Chinese yuan renminbi (RMB) was pegged to the United States dollar at RMB 8.2768 to $1. China was not the only country to do this; from the end of World War II until 1967, Western European countries all maintained fixed exchange rates with the US dollar based on the Bretton Woods system.[13] But that system had to be abandoned in favor of floating, market-based regimes due to market pressures and speculation, according to President Richard M. Nixon in a speech on August 15, 1971, in what is known as the Nixon Shock.  Still, some governments strive to keep their currency within a narrow range. As a result, currencies become over-valued or under-valued, leading to excessive trade deficits or surpluses.  Research on target zones has mainly concentrated on the benefit of stability of exchange rates for industrial countries, but some studies have argued that volatile bilateral exchange rates between industrial countries are in part responsible for financial crisis in emerging markets. According to this view the ability of emerging market economies to compete is weakened because many of the currencies are tied to the US dollar in various fashions either implicitly or explicitly, so fluctuations such as the appreciation of the US dollar to the yen or deutsche Mark have contributed to destabilizing shocks. Most of these countries are net debtors whose debt is denominated in one of the G3 currencies.[14]  In September 2019 Argentina restricted the ability to buy US dollars. Mauricio Macri in 2015 campaigned on a promise to lift restrictions put in place by the left-wing government including the capital controls which have been used in Argentina to manage economic instability. When inflation rose above 20 percent transactions denominated in dollars became commonplace as Argentines moved away from using the peso. In 2011 the government of Cristina Fernández de Kirchner restricted the purchase of dollars leading to a rise in black market dollar purchases. The controls were rolled back after Macri took office and Argentina issued dollar denominated bonds, but when various factors led to a loss in the value of the peso relative to the dollar leading to the restoration of capital controls to prevent additional depreciation amidst peso selloffs.[15]  A market-based exchange rate will change whenever the values of either of the two component currencies change. A currency becomes more valuable whenever demand for it is greater than the available supply. It will become less valuable whenever demand is less than available supply (this does not mean people no longer want money, it just means they prefer holding their wealth in some other form, possibly another currency).  Increased demand for a currency can be due to either an increased transaction demand for money or an increased speculative demand for money. The transaction demand is highly correlated to a country's level of business activity, gross domestic product (GDP), and employment levels. The more people that are unemployed, the less the public as a whole will spend on goods and services. Central banks typically have little difficulty adjusting the available money supply to accommodate changes in the demand for money due to business transactions.  Speculative demand is much harder for central banks to accommodate, which they influence by adjusting interest rates. A speculator may buy a currency if the return (that is the interest rate) is high enough. In general, the higher a country's interest rates, the greater will be the demand for that currency.  It has been argued[by whom?] that such speculation can undermine real economic growth, in particular since large currency speculators may deliberately create downward pressure on a currency by shorting in order to force that central bank to buy their own currency to keep it stable. (When that happens, the speculator can buy the currency back after it depreciates, close out their position, and thereby make a profit.)[citation needed]  For carrier companies shipping goods from one nation to another, exchange rates can often impact them severely.  Therefore, most carriers have a CAF charge to account for these fluctuations.[16][17]  The real exchange rate (RER) is the purchasing power of a currency relative to another at current exchange rates and prices. It is the ratio of the number of units of a given country's currency necessary to buy a market basket of goods in the other country, after acquiring the other country's currency in the foreign exchange market, to the number of units of the given country's currency that would be necessary to buy that market basket directly in the given country. There are various ways to measure RER.[18]  Thus the real exchange rate is the exchange rate times the relative prices of a market basket of goods in the two countries. For example, the purchasing power of the US dollar relative to that of the euro is the dollar price of a euro (dollars per euro) times the euro price of one unit of the market basket (euros\/goods unit) divided by the dollar price of the market basket (dollars per goods unit), and hence is dimensionless. This is the exchange rate (expressed as dollars per euro) times the relative price of the two currencies in terms of their ability to purchase units of the market basket (euros per goods unit divided by dollars per goods unit). If all goods were freely tradable, and foreign and domestic residents purchased identical baskets of goods, purchasing power parity (PPP) would hold for the exchange rate and GDP deflators (price levels) of the two countries, and the real exchange rate would always equal 1.  The rate of change of the real exchange rate over time for the euro versus the dollar equals the rate of appreciation of the euro (the positive or negative percentage rate of change of the dollars-per-euro exchange rate) plus the inflation rate of the euro minus the inflation rate of the dollar.  The Real Exchange Rate (RER) represents the nominal exchange rate adjusted by the relative price of domestic and foreign goods and services, thus reflecting the competitiveness of a country with respect to the rest of the world.[19]  More in detail, an appreciation of the currency or a high level of domestic inflation reduces the RER, thus reducing the country's competitiveness and lowering the Current Account (CA). On the other hand, a currency depreciation generates an opposite effect, improving the country's CA.[20]  There is evidence that the RER generally reaches a steady level in the long-term, and that this process is faster in small open economies characterized by fixed exchange rates.[20]  Any substantial and persistent RER deviation from its long-run equilibrium level, the so-called RER misalignment, has shown to produce negative impacts on a country's balance of payments.[21]  An overvalued RER means that the current RER is above its equilibrium value, whereas an undervalued RER indicates the contrary.[22]  Specifically, a prolonged RER overvaluation is widely considered as an early sign of an upcoming crisis, due to the fact that the country becomes vulnerable to both speculative attacks and currency crisis, as happened in Thailand during the 1997 Asian financial crisis.[23]  On the other side, a protracted RER undervaluation usually generates pressure on domestic prices, changing the consumers' consumption incentives and, so, misallocating resources between tradable and non-tradable sectors.[21]  Given that RER misalignment and, in particular overvaluation, can undermine the country's export-oriented development strategy, the equilibrium RER measurement is crucial for policymakers.[19]  Unfortunately, this variable cannot be observed. The most common method in order to estimate the equilibrium RER is the universally accepted Purchasing Power Parity (PPP) theory, according to which the RER equilibrium level is assumed to remain constant over time. Nevertheless, the equilibrium RER is not a fixed value as it follows the trend of key economic fundamentals,[19]  such as different monetary and fiscal policies or asymmetrical shocks between the home country and abroad.[20]  Consequently, the PPP doctrine has been largely debated during the years, given that it may signal a natural RER movement towards its new equilibrium as a RER misalignment.  Starting from the 1980s, in order to overcome the limitations of this approach, many researchers tried to find some alternative equilibrium RER measures.[19]  Two of the most popular approaches in the economic literature are the Fundamental Equilibrium Exchange Rate (FEER), developed by Williamson (1994),[24]  and the Behavioural Equilibrium Exchange Rate (BEER), initially estimated by Clark and MacDonald (1998).[25]  The FEER focuses on long-run determinants of the RER, rather than on short-term cyclical and speculative forces.[25]  It represents a RER consistent with macroeconomic balance, characterized by the achievement of internal and external balances at the same time. Internal balance is reached when the level of output is in line with both full employment of all available factors of production, and a low and stable rate of inflation.[25]  On the other hand, external balance holds when actual and future CA balances are compatible with long-term sustainable net capital flows.[26]  Nevertheless, the FEER is viewed as a normative measure of the RER since it is based on some \"ideal\" economic conditions related to internal and external balances. Particularly, since the sustainable CA position is defined as an exogenous value, this approach has been broadly questioned over time. By contrast, the BEER entails an econometric analysis of the RER behaviour, considering significant RER deviations from its PPP equilibrium level as a consequence of changes in key economic fundamentals. According to this method, the BEER is the RER that results when all the economic fundamentals are at their equilibrium values.[20]  Therefore, the total RER misalignment is given by the extent to which economic fundamentals differ from their long-run sustainable levels. In short, the BEER is a more general approach than the FEER, since it is not limited to the long-term perspective, being able to explain RER cyclical movements.[25]  Bilateral exchange rate involves a currency pair, while an effective exchange rate is a weighted average of a basket of foreign currencies, and it can be viewed as an overall measure of the country's external competitiveness. A nominal effective exchange rate (NEER) is weighted with the inverse of the asymptotic trade weights. A real effective exchange rate (REER) adjusts NEER by appropriate foreign price level and deflates by the home country price level.[18] Compared to NEER, a GDP weighted effective exchange rate might be more appropriate considering the global investment phenomenon.  In many countries there is a distinction between the official exchange rate for permitted transactions within the country, and a parallel exchange rate (or black market, grey, unregulated, unofficial, etc. exchange rate) that responds to excess demand for foreign currency at the official exchange rate. The degree by which the parallel exchange rate exceeds the official exchange rate is known as the parallel premium.[27] Unofficial transactions of this nature may be illegal.  Uncovered interest rate parity (UIRP) states that an appreciation or depreciation of one currency against another currency might be neutralized by a change in the interest rate differential. If US interest rates increase while Japanese interest rates remain unchanged then the US dollar should depreciate against the Japanese yen by an amount that prevents arbitrage (in reality the opposite, appreciation, quite frequently happens in the short-term, as explained below). The future exchange rate is reflected into the forward exchange rate stated today. In our example, the forward exchange rate of the dollar is said to be at a discount because it buys fewer Japanese yen in the forward rate than it does in the spot rate. The yen is said to be at a premium.  UIRP showed no proof of working after the 1990s. Contrary to the theory, currencies with high interest rates characteristically appreciated rather than depreciated on the reward of the containment of inflation and a higher-yielding currency.  The balance of payments model holds that foreign exchange rates are at an equilibrium level if they produce a stable Current account (balance of payments)current account balance. A nation with a trade deficit will experience a reduction in its foreign exchange reserves, which ultimately lowers (depreciates) the value of its currency. A cheaper (undervalued) currency renders the nation's goods (exports) more affordable in the global market while making imports more expensive. After an intermediate period, imports will be forced down and exports to rise, thus stabilizing the trade balance and bring the currency towards equilibrium.  Like purchasing power parity, the balance of payments model focuses largely on tradeable goods and services, ignoring the increasing role of global capital flows. In other words, money is not only chasing goods and services, but to a larger extent, financial assets such as stocks and bonds. Their flows go into the capital account item of the balance of payments, thus balancing the deficit in the current account. The increase in capital flows has given rise to the asset market model effectively.  The increasing volume of trading of financial assets (stocks and bonds) has required a rethink of its impact on exchange rates. Economic variables such as economic growth, inflation and productivity are no longer the only drivers of currency movements. The proportion of foreign exchange transactions stemming from cross border-trading of financial assets has dwarfed the extent of currency transactions generated from trading in goods and services.[28]  The asset market approach views currencies as asset prices traded in an efficient financial market. Consequently, currencies are increasingly demonstrating a strong correlation with other markets, particularly equities.  Like the stock exchange, money can be made (or lost) on trading by investors and speculators in the foreign exchange market. Currencies can be traded at spot and foreign exchange options markets. The spot market represents current exchange rates, whereas options are derivatives of exchange rates.  A country may gain an advantage in international trade if it controls the market for its currency to keep its value low, typically by the national central bank engaging in open market operations in the foreign exchange market, or through preventing the exchange of foreign currency for domestic notes. The People's Republic of China has been periodically accused of exchange rate manipulation, notably by Donald Trump during his successful campaign for the US presidency.[29]  Other nations, including Iceland, Japan, Brazil, and so on have had a policy of maintaining a low value of their currencies in the hope of reducing the cost of exports and thus bolstering their economies. A lower exchange rate lowers the price of a country's goods for consumers in other countries, but raises the price of imported goods and services for consumers in the low value currency country.[30]  This practice is known as \"modern mercantilism\", namely lowering the exchange rate below its real and fair price, to increase the competitiveness of trade and exports, and encourage economic growth.[31] Rodrik said that this practice was quite effective in encouraging economic growth in developing countries but at the expense of other deficit countries.[32]  In general, exporters of goods and services will prefer a lower value for their currencies, while importers will prefer a higher value.   Media related to Exchange rate at Wikimedia Commons "},"meta":{},"created_at":"2025-03-22T14:25:42.290416Z","updated_at":"2025-03-22T14:25:42.290416Z","inner_id":75,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":84,"annotations":[{"id":84,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.321899Z","updated_at":"2025-03-22T14:25:42.321899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"e2296b6e-e287-4296-b108-bd557e558723","import_id":null,"last_action":null,"bulk_created":false,"task":84,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Empirical methods  Prescriptive and policy  An economic indicator is a statistic about an economic activity. Economic indicators allow analysis of economic performance and predictions of future performance. One application of economic indicators is the study of business cycles. Economic indicators include various indices, earnings reports, and economic summaries: for example, the unemployment rate, quits rate (quit rate in American English), housing starts, consumer price index (a measure for inflation), inverted yield curve,[1] consumer leverage ratio, industrial production, bankruptcies, gross domestic product, broadband internet penetration, retail sales, price index, and changes in credit conditions.  The leading business cycle dating committee in the United States of America is the private National Bureau of Economic Research. The Bureau of Labor Statistics is the principal fact-finding agency for the U.S. government in the field of labor economics and statistics. Other producers of economic indicators includes the United States Census Bureau and United States Bureau of Economic Analysis.  Economic indicators can be classified into three categories according to their usual timing in relation to the business cycle: leading indicators, lagging indicators, and coincident indicators.  Leading indicators are indicators that usually, but not always, change before the economy as a whole changes.[3] They are therefore useful as short-term predictors of the economy. Leading indicators include the index of consumer expectations, building permits, and credit conditions. The Conference Board publishes a composite Leading Economic Index consisting of ten indicators designed to predict activity in the U. S. economy six to nine months in future.  Components of the Conference Board's Leading Economic Indicators Index:[4]  Economist D.W. Mackenzie suggests that the ratio of private to public employment may also be useful as a leading economic indicator.  Lagging indicators are indicators that usually change after the economy as a whole does. Typically the lag is a few quarters of a year. The unemployment rate is a lagging indicator: employment tends to increase two or three quarters after an upturn in the general economy.[citation needed]. In a performance measuring system, profit earned by a business is a lagging indicator as it reflects a historical performance; similarly, improved customer satisfaction is the result of initiatives taken in the past.[citation needed]  The Index of Lagging Indicators is published monthly by The Conference Board, a non-governmental organization, which determines the value of the index from seven components.  The Index tends to follow changes in the overall economy.  The components on the Conference Board's index are:  Coincident indicators change at approximately the same time as the whole economy, thereby providing information about the current state of the economy.  There are many coincident economic indicators, such as Gross Domestic Product, industrial production, personal income and retail sales. A coincident index may be used to identify, after the fact, the dates of peaks and troughs in the business cycle.[6]  There are four economic statistics comprising the Index of Coincident Economic Indicators:[7]  The Philadelphia Federal Reserve produces state-level coincident indexes based on 4 state-level variables:[8]  There are also three terms that describe an economic indicator's direction relative to the direction of the general economy:  Local governments often need to project future tax revenues.  The city of San Francisco, for example, uses the price of a one-bedroom apartment on Craigslist, weekend subway ridership numbers, parking garage usage, and monthly reports on passenger landings at the city's airport.[10] "},"meta":{},"created_at":"2025-03-22T14:25:42.290416Z","updated_at":"2025-03-22T14:25:42.290416Z","inner_id":76,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":85,"annotations":[{"id":85,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.321899Z","updated_at":"2025-03-22T14:25:42.321899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"cd4c89ff-e54e-4a80-a8ec-29d9b5be8c9e","import_id":null,"last_action":null,"bulk_created":false,"task":85,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Corporate finance is an area of finance that deals with the sources of funding, and the capital structure of businesses, the actions that managers take to increase the value of the firm to the shareholders, and the tools and analysis used to allocate financial resources. The primary goal of corporate finance is to maximize or increase shareholder value.[1]  Correspondingly, corporate finance comprises two main sub-disciplines.[citation needed] Capital budgeting is concerned with the setting of criteria about which value-adding projects should receive investment funding, and whether to finance that investment with equity or debt capital. Working capital management is the management of the company's monetary funds that deal with the short-term operating balance of current assets and current liabilities; the focus here is on managing cash, inventories, and short-term borrowing and lending (such as the terms on credit extended to customers).  The terms corporate finance and corporate financier are also associated with investment banking. The typical role of an investment bank is to evaluate the company's financial needs and raise the appropriate type of capital that best fits those needs. Thus, the terms \"corporate finance\" and \"corporate financier\" may be associated with transactions in which capital is raised in order to create, develop, grow or acquire businesses.[2]  Although it is in principle different from managerial finance which studies the financial management of all firms, rather than corporations alone, the main concepts in the study of corporate finance are applicable to the financial problems of all kinds of firms. Financial management overlaps with the financial function of the accounting profession. However, financial accounting is the reporting of historical financial information, while financial management is concerned with the deployment of capital resources to increase a firm's value to the shareholders.  Corporate finance for the pre-industrial world began to emerge in the Italian city-states and the low countries of Europe from the 15th century.  The Dutch East India Company (also known by the abbreviation \"VOC\" in Dutch) was the first publicly listed company ever to pay regular dividends.[3][4][5]  The VOC was also the first recorded joint-stock company to get a fixed capital stock. Public markets for investment securities developed in the Dutch Republic during the 17th century.[6][7][8]  By the early 1800s, London acted as a center of corporate finance for companies around the world, which innovated new forms of lending and investment; see City of London § Economy.  The twentieth century brought the rise of managerial capitalism and common stock finance, with share capital raised through listings, in preference to other sources of capital.  Modern corporate finance, alongside investment management, developed in the second half of the 20th century, particularly driven by innovations in theory and practice in the United States and Britain.[9][10][11][12][13][14] Here, see the later sections of History of banking in the United States and of History of private equity and venture capital.  The primary goal of financial management  [15]  is to maximize or to continually increase shareholder value (see Fisher separation theorem). [a] Here, the three main questions that corporate finance addresses are: what long-term investments should we make? What methods should we employ to finance the investment? How do we manage our day-to-day financial activities? These three questions bring us to the primary areas of concern in corporate finance: capital budgeting, capital structure, and working capital management.[19][20] This then requires that managers find an appropriate balance between: investments in \"projects\" that increase the firm's long term profitability; and paying excess cash in the form of dividends to shareholders; short term considerations, such as paying back creditor-related debt, will also feature.[15][21]  Choosing between investment projects will thus be based upon several inter-related criteria. [1] (1) Corporate management seeks to maximize the value of the firm by investing in projects which yield a positive net present value when valued using an appropriate discount rate - \"hurdle rate\" - in consideration of risk. (2) These projects must also be financed appropriately. (3) If no growth is possible by the company and excess cash surplus is not needed to the firm, then financial theory suggests that management should return some or all of the excess cash to shareholders (i.e., distribution via dividends).[22]  The first two criteria concern \"capital budgeting\", the planning of value-adding, long-term corporate financial projects relating to investments funded through and affecting the firm's capital structure, and where management must allocate the firm's limited resources between competing opportunities (\"projects\"). [23] Capital budgeting is thus also concerned with the setting of criteria about which projects should receive investment funding to increase the value of the firm, and whether to finance that investment with equity or debt capital.[24] Investments should be made on the basis of value-added to the future of the corporation. Projects that increase a firm's value may include a wide variety of different types of investments, including but not limited to, expansion policies, or mergers and acquisitions.   The third criterion relates to dividend policy. In general, managers of growth companies (i.e. firms that earn high rates of return on invested capital) will use most of the firm's capital resources and surplus cash on investments and projects so the company can continue to expand its business operations into the future. When companies reach maturity levels within their industry (i.e. companies that earn approximately average or lower returns on invested capital), managers of these companies will use surplus cash to payout dividends to shareholders.  Thus, when no growth or expansion is likely, and excess cash surplus exists and is not needed, then management is expected to pay out some or all of those surplus earnings in the form of cash dividends or to repurchase the company's stock through a share buyback program.[25][26]  Achieving the goals of corporate finance requires that any corporate investment be financed appropriately.[27] The sources of financing are, generically, capital self-generated by the firm and capital from external funders, obtained by issuing new debt and equity (and hybrid- or convertible securities). However, as above, since both hurdle rate and cash flows (and hence the riskiness of the firm) will be affected,  the financing mix will impact the valuation of the firm, and a considered decision  [28] is required here.  See Balance sheet, WACC. Finally, there is much theoretical discussion as to other considerations that management might weigh here.  Corporations may rely on borrowed funds (debt capital or credit) as sources of investment to sustain ongoing business operations or to fund future growth.  Debt comes in several forms, such as through bank loans, notes payable, or bonds issued to the public. Bonds require the corporation to make regular interest payments (interest expenses) on the borrowed capital until the debt reaches its maturity date, therein the firm must pay back the obligation in full. (An exception is zero-coupon bonds - or \"zeros\"). Debt payments can also be made in the form of a sinking fund provision, whereby the corporation pays annual installments of the borrowed debt above regular interest charges.  Corporations that issue callable bonds are entitled to pay back the obligation in full whenever the company feels it is in their best interest to pay off the debt payments.  If interest expenses cannot be made by the corporation through cash payments, the firm may also use collateral assets as a form of repaying their debt obligations (or through the process of liquidation). Especially re debt funded corporations, see Bankruptcy and Financial distress.  Corporations can alternatively sell shares of the company to investors to raise capital.  Investors, or shareholders, expect that there will be an upward trend in value of the company (or appreciate in value) over time to make their investment a profitable purchase.  As outlined: Shareholder value is increased when corporations invest equity capital and other funds into projects (or investments) that earn a positive rate of return for the owners.  Investors then prefer to buy shares of stock in companies that will consistently earn a positive rate of return on capital (on equity) in the future, thus increasing the market value of the stock of that corporation.   Shareholder value may also be increased when corporations payout excess cash surplus (funds that are not needed for business) in the form of dividends. Internal financing, often, is constituted of retained earnings, i.e. those remaining after dividends; this provides, per some measures, the cheapest form of funding.  Preferred stock is a specialized form of financing which combines properties of common stock and debt instruments, and may then be considered a hybrid security. Preferreds are senior (i.e. higher ranking) to common stock, but subordinate to bonds in terms of claim (or rights to their share of the assets of the company).[29] Preferred stock usually carries no voting rights,[30] but may carry a dividend and may have priority over common stock in the payment of dividends and upon liquidation. Terms of the preferred stock are stated in a \"Certificate of Designation\". Similar to bonds, preferred stocks are rated by the major credit-rating companies. The rating for preferreds is generally lower, since preferred dividends do not carry the same guarantees as interest payments from bonds and they are junior to all creditors.[31] Preferred stock is then a special class of shares which may have any combination of features not possessed by common stock. The following features are usually associated with preferred stock:[32]  As outlined, the financing \"mix\" will impact the valuation (as well as the cashflows) of the firm, and must therefore be structured appropriately:  there are then two interrelated considerations [28] here:  The above, are the primary objectives in deciding on the firm's capitalization structure. Parallel considerations, also, will factor into management's thinking. The starting point for discussion here is the Modigliani–Miller theorem. This states, through two connected Propositions, that in a \"perfect market\" how a firm is financed is irrelevant to its value: (i) the value of a company is independent of its capital structure; (ii) the cost of equity will be the same for a leveraged firm and an unleveraged firm. \"Modigliani and Miller\", however, is generally viewed as a theoretical result, and in practice, management will here too focus on enhacing firm value and \/ or reducing the cost of funding.  Re value, much of the discussion falls under the umbrella of the Trade-Off Theory in which firms are assumed to trade-off the tax benefits of debt with the bankruptcy costs of debt when choosing how to allocate the company's resources, finding an optimum re firm value.  The capital structure substitution theory hypothesizes that management manipulates the capital structure such that earnings per share (EPS) are maximized.   Re cost of funds, the Pecking Order Theory (Stewart Myers) suggests that firms avoid external financing while they have internal financing available and avoid new equity financing while they can engage in new debt financing at reasonably low interest rates. One of the more recent innovations in this area from a theoretical point of view is the market timing hypothesis. This hypothesis, inspired by the behavioral finance literature, states that firms look for the cheaper type of financing regardless of their current levels of internal resources, debt and equity.    The process of allocating financial resources to major investment- or capital expenditure is known as capital budgeting.   [37] [23] Consistent with the overall goal of increasing firm value, the decisioning here focuses on whether the investment in question is worthy of funding through the firm's capitalization structures (debt, equity or retained earnings as above).  To be considered acceptable, the investment must be value additive re: (i) improved operating profit and cash flows; as combined with (ii) any new funding commitments and capital implications. Re the latter: if the investment is large in the context of the firm as a whole, so the discount rate applied by outside investors to the (private) firm's equity may be adjusted upwards to reflect the new level of risk, [38] thus impacting future financing activities and overall valuation. More sophisticated treatments will thus produce accompanying sensitivity- and risk metrics, and will incorporate any inherent contingencies. The focus of capital budgeting is on major \"projects\" - often investments in other firms, or expansion into new markets or geographies - but may extend also to new plants, new \/ replacement machinery, new products, and research and development programs; day to day operational expenditure is the realm of financial management as below.  DCF valuation formula, where the value of the firm or project is the sum of its forecasted free cash flows discounted to the present using the weighted average cost of capital, i.e. cost of equity and cost of debt, with the former (often) derived using the CAPM. The final part is the terminal value, aggregating all cash flows beyond the explicit forecast period, for an appropriate long-term growth in earnings.   In general,[39] each \"project's\" value will be estimated using a discounted cash flow (DCF) valuation, and the opportunity with the highest value, as measured by the resultant net present value (NPV) will be selected (first applied in a corporate finance setting by Joel Dean in 1951). This requires estimating the size and timing of all of the incremental cash flows resulting from the project. Such future cash flows are then discounted to determine their present value (see Time value of money). These present values are then summed, and this sum net of the initial investment outlay is the NPV. See Financial modeling § Accounting for general discussion, and Valuation using discounted cash flows for the mechanics, with discussion re modifications for corporate finance.  The NPV is greatly affected by the discount rate. Thus, identifying the proper discount rate – often termed, the project \"hurdle rate\"[40] – is critical to choosing appropriate projects and investments for the firm. The hurdle rate is the minimum acceptable return on an investment – i.e., the project appropriate discount rate. The hurdle rate should reflect the riskiness of the investment, typically measured by volatility of cash flows, and must take into account the project-relevant financing mix.[41] Managers use models such as the CAPM or the APT to estimate a discount rate appropriate for a particular project, and use the weighted average cost of capital (WACC) to reflect the financing mix selected. (A common error in choosing a discount rate for a project is to apply a WACC that applies to the entire firm. Such an approach may not be appropriate where the risk of a particular project differs markedly from that of the firm's existing portfolio of assets.)  In conjunction with NPV, there are several other measures used as (secondary) selection criteria in corporate finance; see Capital budgeting § Ranked projects. These are visible from the DCF and include discounted payback period, IRR, Modified IRR, equivalent annuity, capital efficiency, and ROI.   Alternatives (complements) to the standard DCF, model economic profit as opposed to free cash flow; these include residual income valuation, MVA \/ EVA (Joel Stern, Stern Stewart & Co) and APV (Stewart Myers).  With the cost of capital correctly and correspondingly adjusted, these valuations should yield the same result as the DCF. These may, however, be considered more appropriate for projects with negative free cash flow several years out, but which are expected to generate positive cash flow thereafter (and may also be less sensitive to terminal value).  Given the uncertainty inherent in project forecasting and valuation, [42] [43] [44] analysts will wish to assess the sensitivity of project NPV to the various inputs (i.e. assumptions) to the DCF model. In a typical sensitivity analysis the analyst will vary one key factor while holding all other inputs constant, ceteris paribus. The sensitivity of NPV to a change in that factor is then observed, and is calculated as a \"slope\": ΔNPV \/ Δfactor. For example, the analyst will determine NPV at various growth rates in annual revenue as specified (usually at set increments, e.g. -10%, -5%, 0%, 5%...), and then determine the sensitivity using this formula. Often, several variables may be of interest, and their various combinations produce a \"value-surface\"[45] (or even a \"value-space\"), where NPV is then a function of several variables. See also Stress testing.  Using a related technique, analysts also run scenario based forecasts of NPV. Here, a scenario comprises a particular outcome for economy-wide, \"global\" factors (demand for the product, exchange rates, commodity prices, etc.) as well as for company-specific factors (unit costs, etc.). As an example, the analyst may specify various revenue growth scenarios (e.g. -5% for \"Worst Case\", +5% for \"Likely Case\" and +15% for \"Best Case\"), where all key inputs are adjusted so as to be consistent with the growth assumptions, and calculate the NPV for each. Note that for scenario based analysis, the various combinations of inputs must be internally consistent (see discussion at Financial modeling), whereas for the sensitivity approach these need not be so.  An application of this methodology is to determine an \"unbiased\" NPV, where management determines a (subjective) probability for each scenario – the NPV for the project is then the probability-weighted average of the various scenarios; see First Chicago Method. (See also rNPV, where cash flows, as opposed to scenarios, are probability-weighted.)  A further advancement which \"overcomes the limitations of sensitivity and scenario analyses by examining the effects of all possible combinations of variables and their realizations\"[46] is to construct stochastic[47] or probabilistic financial models – as opposed to the traditional static and deterministic models as above.[43] For this purpose, the most common method is to use Monte Carlo simulation to analyze the project's NPV. This method was introduced to finance by David B. Hertz in 1964, although it has only recently become common: today analysts are even able to run simulations in spreadsheet based DCF models, typically using a risk-analysis add-in, such as @Risk or Crystal Ball. Here, the cash flow components that are (heavily) impacted by uncertainty are simulated, mathematically reflecting their \"random characteristics\". In contrast to the scenario approach above, the simulation produces several thousand random but possible outcomes, or trials, \"covering all conceivable real world contingencies in proportion to their likelihood;\"[48] see Monte Carlo Simulation versus \"What If\" Scenarios. The output is then a histogram of project NPV, and the average NPV of the potential investment – as well as its volatility and other sensitivities – is then observed. This histogram provides information not visible from the static DCF: for example, it allows for an estimate of the probability that a project has a net present value greater than zero (or any other value).  Continuing the above example: instead of assigning three discrete values to revenue growth, and to the other relevant variables, the analyst would assign an appropriate probability distribution to each variable (commonly triangular or beta), and, where possible, specify the observed or supposed correlation between the variables. These distributions would then be \"sampled\" repeatedly – incorporating this correlation – so as to generate several thousand random but possible scenarios, with corresponding valuations, which are then used to generate the NPV histogram. The resultant statistics (average NPV and standard deviation of NPV) will be a more accurate mirror of the project's \"randomness\" than the variance observed under the scenario based approach. (These are often used as estimates of the underlying \"spot price\" and volatility for the real option valuation below; see Real options valuation § Valuation inputs.) A more robust Monte Carlo model would include the possible occurrence of risk events - e.g., a credit crunch - that drive variations in one or more of the DCF model inputs.  In many cases, for example R&D projects, a project may open (or close) various paths of action to the company, but this reality will not (typically) be captured in a strict NPV approach.[49] Some analysts account for this uncertainty by adjusting the discount rate (e.g. by increasing the cost of capital) or the cash flows (using certainty equivalents, or applying (subjective) \"haircuts\" to the forecast numbers; see Penalized present value).[50][51] Even when employed, however, these latter methods do not normally properly account for changes in risk over the project's lifecycle and hence fail to appropriately adapt the risk adjustment.[52][53]  Management will therefore (sometimes) employ tools which place an explicit value on these options. So, whereas in a DCF valuation the most likely or average or scenario specific cash flows are discounted, here the \"flexible and staged nature\" of the investment is modelled, and hence \"all\" potential payoffs are considered. See further under Real options valuation. The difference between the two valuations is the \"value of flexibility\" inherent in the project.  The two most common tools are Decision Tree Analysis (DTA)[42] and real options valuation (ROV);[54] they may often be used interchangeably:  Dividend policy is concerned with financial policies regarding the payment of a cash dividend in the present or retaining earnings and then paying an increased dividend at a later stage.   The policy will be set based upon the type of company and what management determines is the best use of those dividend resources for the firm and its shareholders. Practical and theoretical considerations - interacting with the above funding and investment decisioning, and re overall firm value - will inform this thinking. [55] [56]  In general, whether [57] to issue dividends,[55] and what amount, is determined on the basis of the company's unappropriated profit (excess cash) and influenced by the company's long-term earning power.  In all instances, as above, the appropriate dividend policy is in parallel directed by that which maximizes long-term shareholder value.  When cash surplus exists and is not needed by the firm, then management is expected to pay out some or all of those surplus earnings in the form of cash dividends or to repurchase the company's stock through a share buyback program. Thus, if there are no NPV positive opportunities, i.e. projects where returns exceed the hurdle rate, and excess cash surplus is not needed, then management should return (some or all of) the excess cash to shareholders as dividends.   This is the general case, however the \"style\" of the stock may also impact the decision. Shareholders of a \"growth stock\", for example, expect that the company will retain (most of) the excess cash surplus so as to fund future projects internally to help increase the value of the firm.  Shareholders of value- or secondary stocks, on the other hand, would prefer management to pay surplus earnings in the form of cash dividends, especially when a positive return cannot be earned through the reinvestment of undistributed earnings; a share buyback program may be accepted when the value of the stock is greater than the returns to be realized from the reinvestment of undistributed profits.  Management will also choose the form of the dividend distribution, as stated, generally as cash dividends or via a share buyback. Various factors may be taken into consideration: where shareholders must pay tax on dividends, firms may elect to retain earnings or to perform a stock buyback, in both cases increasing the value of shares outstanding. Alternatively, some companies will pay \"dividends\" from stock rather than in cash or via a share buyback as mentioned; see Corporate action.  As for capital structure above, there are several schools of thought on dividends, in particular re their impact on firm value. [55]  A key consideration will be whether there are any tax disadvantages associated with dividends: i.e. dividends attract a higher tax rate as compared, e.g., to capital gains; see dividend tax and Retained earnings  § Tax implications. Here, per the abovementioned Modigliani–Miller theorem:  if there are no such disadvantages - and companies can raise equity finance cheaply, i.e. can issue stock at low cost - then dividend policy is value neutral;  if dividends suffer a tax disadvantage, then increasing dividends should reduce firm value. Regardless, but particularly in the second (more realistic) case, other considerations apply.  The first set relates to investor preferences and behavior (see Clientele effect). Investors are seen to prefer a “bird in the hand” - i.e. cash dividends are certain as compared to income from future capital gains - and in fact, commonly employ some form of dividend valuation model in valuing shares. Relatedly, investors will then prefer a stable or \"smooth\" dividend payout - as far as is reasonable given earnings prospects and sustainability - which will then positively impact share price; see Lintner model. Cash dividends may also allow management to convey (insider) information about corporate performance; and increasing a company's dividend payout may then predict (or lead to) favorable performance of the company's stock in the future; see Dividend signaling hypothesis  The second set relates to management's thinking re capital structure and earnings, overlapping the above. Under a \"Residual dividend policy\" - i.e. as contrasted with a \"smoothed\" payout policy - the firm will use retained profits to finance capital investments if cheaper than the same via equity financing; see again Pecking order theory. Similarly, under the Walter model, dividends are paid only if capital retained will earn a higher return than that available to investors (proxied: ROE > Ke). Management may also want to \"manipulate\" the capital structure - including by paying or not paying dividends - such that earnings per share are maximized; see again,  Capital structure substitution theory.  Managing the corporation's working capital position so as to sustain ongoing business operations is referred to as working capital management.[58][59]  This entails, essentially, managing the relationship between a firm's short-term assets and its short-term liabilities, conscious of various considerations.  Here, as above, the goal of Corporate Finance is the maximization of firm value. In the context of long term, capital budgeting, firm value is enhanced through appropriately selecting and funding NPV positive investments. These investments, in turn, have implications in terms of cash flow and cost of capital. The goal of Working Capital (i.e. short term) management is therefore to ensure that the firm is able to operate, and that it has sufficient cash flow to service long-term debt, and to satisfy both maturing short-term debt and upcoming operational expenses. In so doing, firm value is enhanced when, and if, the return on capital exceeds the cost of capital; See Economic value added (EVA).  Managing short term finance along with long term finance is therefore one task of a modern CFO.  Working capital is the amount of funds that are necessary for an organization to continue its ongoing business operations, until the firm is reimbursed through payments for the goods or services it has delivered to its customers.[60]  Working capital is measured through the difference between resources in cash or readily convertible into cash (Current Assets), and cash requirements (Current Liabilities). As a result, capital resource allocations relating to working capital are always current, i.e. short-term.  In addition to time horizon, working capital management differs from capital budgeting in terms of discounting and profitability considerations; decisions here are also \"reversible\" to a much larger extent. (Considerations as to risk appetite and return targets remain identical, although some constraints – such as those imposed by loan covenants – may be more relevant here).  The (short term) goals of working capital are therefore not approached on the same basis as (long term) profitability, and working capital management applies different criteria in allocating resources: the main considerations are (1) cash flow \/ liquidity and (2) profitability \/ return on capital (of which cash flow is probably the most important).  Guided by the above criteria, management will use a combination of policies and techniques for the management of working capital.[61] These policies, as outlined, aim at managing the current assets (generally cash and cash equivalents, inventories and debtors) and the short term financing, such that cash flows and returns are acceptable.[59]  As discussed, corporate finance comprises the activities, analytical methods, and techniques that deal with the company's long-term investments, finances and capital. Re the latter, when capital must be raised for the corporation or shareholders, the \"corporate finance team\" will engage [63] its investment bank.  The bank will then facilitate the required share listing (IPO or SEO) or bond issuance, as appropriate given the above anaysis. Thereafter the bank will work closely with the corporate re servicing the new securities, and managing its presence in the capital markets more generally (offering advisory, financial advisory, deal advisory, and \/ or transaction advisory [64]  services).  Use of the term \"corporate finance\", correspondingly, varies considerably across the world.  In the United States, \"Corporate Finance\" corresponds to the first usage. A professional here may be referred to as a \"corporate finance analyst\" and will typically be based in the FP&A area, reporting to the CFO.  [63] [65]  See Financial analyst § Financial planning and analysis. In the United Kingdom and Commonwealth countries, [64] on the other hand, \"corporate finance\" and \"corporate financier\" are associated with investment banking.  Financial risk management,[47][66] generally, is focused on measuring and managing market risk, credit risk and operational risk. Within corporates  [66] (i.e. as opposed to banks), the scope extends to preserving (and enhancing) the firm's economic value.[67]  It will then overlap both corporate finance and enterprise risk management: addressing risks to the firm's overall strategic objectives, by focusing on the financial exposures and opportunities arising from business decisions, and their link to the firm’s appetite for risk, as well as their impact on share price. (In large firms, Risk Management typically exists as an  independent function, with the CRO consulted on capital-investment and other strategic decisions.) Re corporate finance, both operational and funding issues are addressed; respectively:  Broadly, corporate governance considers the mechanisms, processes, practices, and relations by which corporations are controlled and operated by their board of directors, managers, shareholders, and other stakeholders. In the context of corporate finance,  [70] a more specific concern will be that executives do not \"serve their own vested interests\" to the detriment of capital providers.  [71] There are several considerations:  In general, here, debt may be seen as \"an internal means of controlling management\", which has to work hard to ensure that repayments are met,  [73] balancing these interests, and also limiting the possibility of overpaying on investments.  Granting Executive stock options, [74] alternatively or in parallel, is seen as a mechanism to align management with stockholder interests.  A more formal treatment is offered under agency theory, [75]  where these problems and approaches can be seen, and hence analysed, as real options; [76] see Principal–agent problem § Options framework for discussion. "},"meta":{},"created_at":"2025-03-22T14:25:42.290416Z","updated_at":"2025-03-22T14:25:42.290416Z","inner_id":77,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":86,"annotations":[{"id":86,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.321899Z","updated_at":"2025-03-22T14:25:42.321899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"81055897-bd94-48d4-bbfb-6ce8165193a4","import_id":null,"last_action":null,"bulk_created":false,"task":86,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Wealth management (WM) or wealth management advisory (WMA) is an investment advisory service that provides financial management and wealth advisory services to a wide array of clients ranging from affluent to high-net-worth (HNW) and ultra-high-net-worth (UHNW) individuals and families.[1]  It is a discipline which incorporates structuring and planning wealth to assist in growing, preserving, and protecting wealth, whilst passing it onto the family in a tax-efficient manner and in accordance with their wishes. Wealth management brings together tax planning, wealth protection, estate planning, succession planning, and family governance.  Private wealth management is sought by high-net-worth investors. Generally, this includes advice on the use of various estate planning vehicles, business-succession or stock-option planning, and the occasional use of hedging derivatives for large blocks of stock.  Traditionally, the wealthiest retail clients of investment firms demanded a greater level of service, product offering and sales personnel than that received by average clients. With an increase in the number of affluent investors in recent years,[2] there has been an increasing demand for sophisticated financial solutions and expertise throughout the world.  The CFA Institute curriculum on private-wealth management indicates that two primary factors  distinguish the issues facing individual investors from those facing institutions:  The term \"wealth management\" occurs at least as early as 1933.[3] It came into more general use in the elite retail (or \"Private Client\") divisions of firms such as Goldman Sachs or Morgan Stanley (before the Dean Witter Reynolds merger of 1997), to distinguish those divisions' services from mass-market offerings, but has since spread throughout the financial-services industry. Family offices that had formerly served just one family opened their doors to other families, and the term Multi-family office was coined. Accounting firms and investment advisory boutiques created multi-family offices as well.   Certain larger firms (UBS, Morgan Stanley and Merrill Lynch) have \"tiered\" their platforms – with separate branch systems and advisor-training programs, distinguishing \"Private Wealth Management\" from \"Wealth Management\", with the latter term denoting the same type of services but with a lower degree of customization and delivered to mass affluent clients. At Morgan Stanley, the \"Private Wealth Management\" retail division focuses on serving clients with greater than $20 million in investment assets while \"Global Wealth Management\" focuses on accounts smaller than $10 million.  In the late 1980s, private banks and brokerage firms began to offer seminars and client events designed to showcase the expertise and capabilities of the sponsoring firm. Within a few years a new  business model emerged – Family Office Exchange in 1990, the Institute for Private Investors in 1991, and CCC Alliance in 1995. These companies aimed to offer an online community as well as a network of peers for ultra high-net-worth individuals and their families. These entities have grown since the 1990s, with total IT spending (for example) by the global wealth management industry predicted to reach $35bn by 2016, including heavy investment in digital channels.[4]  Wealth management can be provided by large corporate entities, independent financial advisers or multi-licensed portfolio managers who design services to focus on high-net-worth clients. Large banks and large brokerage houses create segmentation marketing-strategies to sell both proprietary and non-proprietary products and services to investors designated as potential high-net-worth clients. Independent wealth-managers use their experience in estate planning, risk management, and their affiliations with tax and legal specialists, to manage the diverse holdings of high-net-worth clients. Banks and brokerage firms use advisory talent-pools to aggregate these same services.  The Great Recession of the late 2000s caused investors to address concerns within their portfolios.[5] For this reason wealth managers have been advised that clients have a greater need to understand, access, and communicate with advisers about their situation.[6]   As awareness of wealth management has become more common, some companies have shifted towards a model which asks clients about life goals,[7] working environments, and spending patterns as a way to increase communication.[8] The industry-recognized wealth management was more than an investment advisory discipline.[9]  In 2015,  United Capital rebranded their wealth management services using the term \"financial life management\", which, according to the company, was intended to more clearly define the difference between wealth management companies and more affordable brokerage firms.[10] The same year Merrill Lynch began a program, Merrill Lynch Clear, which asks investors to describe life goals, and includes an educational program for clients' children.[8] For clients looking to leverage their wealth for the sake of achieving philanthropical and charitable goals, social finance investments may be included.[11]  According to Euromoney's annual Private banking and wealth management ranking 2013, which consider (amongst other factors) assets under management, net income and net new assets, global private banking assets under management grew just 10.8%YoY (compared with 16.7% ten years ago).[12]  The largest private banks and wealth managers in the world as of 2018 are as follows:[13] "},"meta":{},"created_at":"2025-03-22T14:25:42.290416Z","updated_at":"2025-03-22T14:25:42.290416Z","inner_id":78,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":87,"annotations":[{"id":87,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.322899Z","updated_at":"2025-03-22T14:25:42.322899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"ccf39936-3fa1-46f6-8b19-1f7dd2f464ca","import_id":null,"last_action":null,"bulk_created":false,"task":87,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Heterodox  Monetary policy is the policy adopted by the monetary authority of a nation to affect monetary and other financial conditions to accomplish broader objectives like high employment and price stability (normally interpreted as a low and stable rate of inflation).[1][2] Further purposes of a monetary policy may be to contribute to economic stability or to maintain predictable exchange rates with other currencies. Today most central banks in developed countries conduct their monetary policy within an inflation targeting framework,[3] whereas the monetary policies of most developing countries' central banks target some kind of a fixed exchange rate system. A third monetary policy strategy, targeting the money supply, was widely followed during the 1980s, but has diminished in popularity since then, though it is still the official strategy in a number of emerging economies.  The tools of monetary policy vary from central bank to central bank, depending on the country's stage of development, institutional structure, tradition and political system. Interest-rate targeting is generally the primary tool, being obtained either directly via administratively changing the central bank's own interest rates or indirectly via open market operations. Interest rates affect general economic activity and consequently employment and inflation via a number of different channels, known collectively as the monetary transmission mechanism, and are also an important determinant of the exchange rate. Other policy tools include communication strategies like forward guidance and in some countries the setting of reserve requirements. Monetary policy is often referred to as being either expansionary (stimulating economic activity and consequently employment and inflation) or contractionary (dampening economic activity, hence decreasing employment and inflation).  Monetary policy affects the economy through financial channels like interest rates, exchange rates and prices of financial assets. This is in contrast to fiscal policy, which relies on changes in taxation and government spending as methods for a government to manage business cycle phenomena such as recessions.[4] In developed countries, monetary policy is generally formed separately from fiscal policy, modern central banks in developed economies being independent of direct government control and directives.[5]  How best to conduct monetary policy is an active and debated research area, drawing on fields like monetary economics as well as other subfields within macroeconomics.  Monetary policy has evolved over the centuries, along with the development of a money economy. Historians, economists, anthropologists and numismatics do not agree on the origins of money. In the West the common point of view is that coins were first used in ancient Lydia in the 8th century BCE, whereas some date the origins to ancient China. The earliest predecessors to monetary policy seem to be those of debasement, where the government would melt coins down and mix them with cheaper metals. The practice was widespread in the late Roman Empire, but reached its perfection in western Europe in the late Middle Ages.[6]  For many centuries there were only two forms of monetary policy: altering coinage or the printing of paper money. Interest rates, while now thought of as part of monetary authority, were not generally coordinated with the other forms of monetary policy during this time. Monetary policy was considered as an executive decision, and was generally implemented by the authority with seigniorage (the power to coin). With the advent of larger trading networks came the ability to define the currency value in terms of gold or silver, and the price of the local currency in terms of foreign currencies. This official price could be enforced by law, even if it varied from the market price.  Paper money originated from promissory notes termed \"jiaozi\" in 7th-century China. Jiaozi did not replace metallic currency, and were used alongside the copper coins. The succeeding Yuan dynasty was the first government to use paper currency as the predominant circulating medium. In the later course of the dynasty, facing massive shortages of specie to fund war and maintain their rule, they began printing paper money without restrictions, resulting in hyperinflation.  With the creation of the Bank of England in 1694,[7] which was granted the authority to print notes backed by gold, the idea of monetary policy as independent of executive action[how?] began to be established.[8] The purpose of monetary policy was to maintain the value of the coinage, print notes which would trade at par to specie, and prevent coins from leaving circulation. During the period 1870–1920, the industrialized nations established central banking systems, with one of the last being the Federal Reserve in 1913.[9] By this time the role of the central bank as the \"lender of last resort\" was established. It was also increasingly understood that interest rates had an effect on the entire economy, in no small part because of appreciation for the marginal revolution in economics, which demonstrated that people would change their decisions based on changes in their opportunity costs.  The establishment of national banks by industrializing nations was associated then with the desire to maintain the currency's relationship to the gold standard, and to trade in a narrow currency band with other gold-backed currencies. To accomplish this end, central banks as part of the gold standard began setting the interest rates that they charged both their own borrowers and other banks which required money for liquidity. The maintenance of a gold standard required almost monthly adjustments of interest rates.  The gold standard is a system by which the price of the national currency is fixed vis-a-vis the value of gold, and is kept constant by the government's promise to buy or sell gold at a fixed price in terms of the base currency. The gold standard might be regarded as a special case of \"fixed exchange rate\" policy, or as a special type of commodity price level targeting. However, the policies required to maintain the gold standard might be harmful to employment and general economic activity and probably exacerbated the Great Depression in the 1930s in many countries, leading eventually to the demise of the gold standards and efforts to create a more adequate monetary framework internationally after World War II.[10] Nowadays the gold standard is no longer used by any country.[11]  In 1944, the Bretton Woods system was established, which created the International Monetary Fund and introduced a fixed exchange rate system linking the currencies of most industrialized nations to the US dollar, which as the only currency in the system would be directly convertible to gold.[12] During the following decades the system secured stable exchange rates internationally, but the system broke down during the 1970s when the dollar increasingly came to be viewed as overvalued. In 1971, the dollar's convertibility into gold was suspended. Attempts to revive the fixed exchange rates failed, and by 1973 the major currencies began to float against each other.[13] In Europe, various attempts were made to establish a regional fixed exchange rate system via the European Monetary System, leading eventually to the Economic and Monetary Union of the European Union and the introduction of the currency euro.  Monetarist economists long contended that the money-supply growth could affect the macroeconomy. These included Milton Friedman who early in his career advocated that government budget deficits during recessions be financed in equal amount by money creation to help to stimulate aggregate demand for production.[14] Later he advocated simply increasing the monetary supply at a low, constant rate, as the best way of maintaining low inflation and stable production growth.[15] During the 1970s inflation rose in many countries caused by the 1970s energy crisis, and several central banks turned to a money supply target in an attempt to reduce inflation. However, when U.S. Federal Reserve Chairman Paul Volcker tried this policy, starting in October 1979, it was found to be impractical, because of the unstable relationship between monetary aggregates and other macroeconomic variables, and similar results prevailed in other countries.[10][16] Even Milton Friedman later acknowledged that direct money supplying was less successful than he had hoped.[17]  In 1990, New Zealand as the first country ever adopted an official inflation target as the basis of its monetary policy. The idea is that the central bank tries to adjust interest rates in order to steer the country's inflation rate towards the official target instead of following indirect objectives like exchange rate stability or money supply growth, the purpose of which is normally also ultimately to obtain low and stable inflation. The strategy was generally considered to work well, and central banks in most developed countries have over the years adapted a similar strategy.[18]  The Global Financial Crisis of 2008 sparked controversy over the use and flexibility of the inflation targeting employed. Many economists argued that the actual inflation targets decided upon were set too low by many monetary regimes. During the crisis, many inflation-anchoring countries reached the lower bound of zero rates, resulting in inflation rates decreasing to almost zero or even deflation.[19]  As of 2023, the central banks of all G7 member countries can be said to follow an inflation target, including the European Central Bank and the Federal Reserve, who have adopted the main elements of inflation targeting without officially calling themselves inflation targeters.[18] In emerging countries fixed exchange rate regimes are still the most common monetary policy.[20]  The instruments available to central banks for conducting monetary policy vary from country to country, depending on the country's stage of development, institutional structure and political system.[1] The main monetary policy instruments available to central banks are interest rate policy, i.e. setting (administered) interest rates directly, open market operations, forward guidance and other communication activities, bank reserve requirements, and re-lending and re-discount (including using the term repurchase market). While capital adequacy is important, it is defined and regulated by the Bank for International Settlements, and central banks in practice generally do not apply stricter rules.  Expansionary policy occurs when a monetary authority uses its instruments to stimulate the economy. An expansionary policy decreases short-term interest rates, affecting broader financial conditions to encourage spending on goods and services, in turn leading to increased employment. By affecting the exchange rate, it may also stimulate net export.[21] Contractionary policy works in the opposite direction: Increasing interest rates will depress borrowing and spending by consumers and businesses, dampening inflationary pressure in the economy together with employment.[21]  For most central banks in advanced economies, their main monetary policy instrument is a short-term interest rate.[22] For monetary policy frameworks operating under an exchange rate anchor, adjusting interest rates are, together with direct intervention in the foreign exchange market (i.e. open market operations), important tools to maintain the desired exchange rate.[23] For central banks targeting inflation directly, adjusting interest rates are crucial for the monetary transmission mechanism which ultimately affects inflation. Changes in the central bank policy rates normally affect the interest rates that banks and other lenders charge on loans to firms and households.  Higher interest rates reduce inflation by reducing aggregate consumption of goods and services by several causal paths.[24] Higher borrowing costs can cause a cash shortage for companies, which then reduce direct spending on goods and services to reduce costs. They also tend to reduce spending on labor, which in turn reduces household income and then household spending on goods and services. Interest rate changes also affect asset prices like stock prices and house prices. Though unless they are selling or taking out new loans their cash flow is unaffected, asset owners feel less wealthy (the wealth effect) and reduce spending.  Rising interest rates also have smaller secondary effects, which decrease supply and tend to increase inflation (or cause it to decrease more slowly than it otherwise would. On the individual side, rising mortgage rates disincentivize wealthy homeowners from downsizing or moving to a new home if they have an existing mortgage that is locked in at a low fixed rate.[25] On the business side, lower investment and spending may result in lower supply of new homes and other goods and services.  Firms experiencing high borrowing costs are also less willing or able to borrow or spend money on investment in new or expanding business. International interest rate differentials also affect exchange rates, and consequently exports and imports. Consumption, investment, and net exports are all important components of aggregate demand. Stimulating or suppressing the overall demand for goods and services in the economy will tend to increase respectively diminish inflation.[26]  The concrete implementation mechanism used to adjust short-term interest rates differs from central bank to central bank.[27] The \"policy rate\" itself, i.e. the main interest rate which the central bank uses to communicate its policy, may be either an administered rate (i.e. set directly by the central bank) or a market interest rate which the central bank influences only indirectly.[22] By setting administered rates that commercial banks and possibly other financial institutions will receive for their deposits in the central bank, respectively pay for loans from the central bank, the central monetary authority can create a band (or \"corridor\") within which market interbank short-term interest rates will typically move. Depending on the specific details, the resulting specific market interest rate may either be created by open market operations by the central bank (a so-called \"corridor system\") or in practice equal the administered rate (a \"floor system\", practiced by the Federal Reserve[28] among others).[22][29]  As an example of how this functions, the Bank of Canada sets a target overnight rate, and a band of plus or minus 0.25%. Qualified banks borrow from each other within this band, but never above or below, because the central bank will always lend to them at the top of the band, and take deposits at the bottom of the band; in principle, the capacity to borrow and lend at the extremes of the band are unlimited.[30]  The target rates are generally short-term rates. The actual rate that borrowers and lenders receive on the market will depend on (perceived) credit risk, maturity and other factors. For example, a central bank might set a target rate for overnight lending of 4.5%, but rates for (equivalent risk) five-year bonds might be 5%, 4.75%, or, in cases of inverted yield curves, even below the short-term rate.  Many central banks have one primary \"headline\" rate that is quoted as the \"central bank rate\". In practice, they will have other tools and rates that are used, but only one that is rigorously targeted and enforced. A typical central bank consequently has several interest rates or monetary policy tools it can use to influence markets.   Through open market operations, a central bank may influence the level of interest rates, the exchange rate and\/or the money supply in an economy. Open market operations can influence interest rates by expanding or contracting the monetary base, which consists of currency in circulation and banks' reserves on deposit at the central bank. Each time a central bank buys securities (such as a government bond or treasury bill), it in effect creates money. The central bank exchanges money for the security, increasing the monetary base while lowering the supply of the specific security. Conversely, selling of securities by the central bank reduces the monetary base.  Open market operations usually take the form of:  Forward guidance is a communication practice whereby the central bank announces its forecasts and future intentions to influence market expectations of future levels of interest rates.[31] As expectations formation are an important ingredient in actual inflation changes, credible communication is important for modern central banks.[32]  Historically, bank reserves have formed only a small fraction of deposits, a system called fractional-reserve banking. Banks would hold only a small percentage of their assets in the form of cash reserves as insurance against bank runs. Over time this process has been regulated and insured by central banks. Such legal reserve requirements were introduced in the 19th century as an attempt to reduce the risk of banks overextending themselves and suffering from bank runs, as this could lead to knock-on effects on other overextended banks.  A number of central banks have since abolished their reserve requirements over the last few decades, beginning with the Reserve Bank of New Zealand in 1985 and continuing with the Federal Reserve in 2020. For the respective banking systems, bank capital requirements provide a check on the growth of the money supply.  The People's Bank of China retains (and uses) more powers over reserves because the yuan that it manages is a non-convertible currency.[citation needed]  Loan activity by banks plays a fundamental role in determining the money supply. The central bank money after aggregate settlement – \"final money\" – can take only one of two forms:  The currency component of the money supply is far smaller than the deposit component. Currency, bank reserves and institutional loan agreements together make up the monetary base, called M1, M2 and M3. The Federal Reserve Bank stopped publishing M3 and counting it as part of the money supply in 2006.[33]  Central banks can directly or indirectly influence the allocation of bank lending in certain sectors of the economy by applying quotas, limits or differentiated interest rates.[34][35] This allows the central bank to control both the quantity of lending and its allocation towards certain strategic sectors of the economy, for example to support the national industrial policy, or to environmental investment such as housing renovation.[36][37][38]  The Bank of Japan used to apply such policy (\"window guidance\") between 1962 and 1991.[39][40] The Banque de France also widely used credit guidance during the post-war period of 1948 until 1973 .[41] China is also applying a form of dual rate policy.[42][43]  The European Central Bank's ongoing TLTROs operations can also be described as a form of credit guidance insofar as the level of interest rate ultimately paid by banks is differentiated according to the volume of lending made by commercial banks at the end of the maintenance period. If commercial banks achieve a certain lending performance threshold, they get a discount interest rate, that is lower than the standard key interest rate. For this reason, some economists have described the TLTROs as a \"dual interest rates\" policy.[44][45]  Civil society organizations and think tanks have proposed the introduction of a \"green TLTRO\" in order to lower the cost of funding and stimulate bank lending targeted at green projects,[46][47][48] echoing the French President Emmanual Macron, who called for introducing \"green interest rates\".[49]  In 2021, the Bank of Japan and People's Bank of China have introduced differentiated interest rates on green dedicated refinancing operations.[50][51]  To influence the money supply, some central banks may require that some or all foreign exchange receipts (generally from exports) be exchanged for the local currency. The rate that is used to purchase local currency may be market-based or arbitrarily set by the bank. This tool is generally used in countries with non-convertible currencies or partially convertible currencies. The recipient of the local currency may be allowed to freely dispose of the funds, required to hold the funds with the central bank for some period of time, or allowed to use the funds subject to certain restrictions. In other cases, the ability to hold or use the foreign exchange may be otherwise limited.  In this method, the money supply is increased by the central bank when it purchases the foreign currency by issuing (selling) the local currency. The central bank may subsequently reduce the money supply by various means, including selling bonds or foreign exchange interventions.  In some countries, central banks may have other tools that work indirectly to limit lending practices and otherwise restrict or regulate capital markets. For example, a central bank may regulate margin lending, whereby individuals or companies may borrow against pledged securities. The margin requirement establishes a minimum ratio of the value of the securities to the amount borrowed.  Central banks often have requirements for the quality of assets that may be held by financial institutions; these requirements may act as a limit on the amount of risk and leverage created by the financial system. These requirements may be direct, such as requiring certain assets to bear certain minimum credit ratings, or indirect, by the central bank lending to counterparties only when the security of a certain quality is pledged as collateral.  Other forms of monetary policy, particularly used when interest rates are at or near 0% and there are concerns about deflation or deflation is occurring, are referred to as unconventional monetary policy. These include credit easing, quantitative easing, forward guidance, and signalling.[52] In credit easing, a central bank purchases private sector assets to improve liquidity and improve access to credit. Signaling can be used to lower market expectations for lower interest rates in the future. For example, during the credit crisis of 2008, the US Federal Reserve indicated rates would be low for an \"extended period\", and the Bank of Canada made a \"conditional commitment\" to keep rates at the lower bound of 25 basis points (0.25%) until the end of the second quarter of 2010.  Further similar monetary policy proposals include the idea of helicopter money whereby central banks would create money without assets as counterpart in their balance sheet. The money created could be distributed directly to the population as a citizen's dividend. Virtues of such money shocks include the decrease of household risk aversion and the increase in demand, boosting both inflation and the output gap. This option has been increasingly discussed since March 2016 after the ECB's president Mario Draghi said he found the concept \"very interesting\".[53] The idea was also promoted by prominent former central bankers Stanley Fischer and Philipp Hildebrand in a paper published by BlackRock,[54] and in France by economists Philippe Martin and Xavier Ragot from the French Council for Economic Analysis, a think tank attached to the Prime minister's office.[55]  Some have envisaged the use of what Milton Friedman once called \"helicopter money\" whereby the central bank would make direct transfers to citizens[56] in order to lift inflation up to the central bank's intended target. Such a policy option could be particularly effective at the zero lower bound.[57]  Central banks typically use a nominal anchor to pin down expectations of private agents about the nominal price level or its path or about what the central bank might do with respect to achieving that path. A nominal anchor is a variable that is thought to bear a stable relationship to the price level or the rate of inflation over some period of time. The adoption of a nominal anchor is intended to stabilize inflation expectations, which may, in turn, help stabilize actual inflation. Nominal variables historically used as anchors include the gold standard, exchange rate targets, money supply targets, and since the 1990s direct official inflation targets.[10][19] In addition, economic researchers have proposed variants or alternatives like price level targeting (some times described as an inflation target with a memory[58]) or nominal income targeting.  Empirically, some researchers suggest that central banks' policies can be described by a simple method called the Taylor rule, according to which central banks adjust their policy interest rate in response to changes in the inflation rate and the output gap. The rule was proposed by John B. Taylor of Stanford University.[59]  Under this policy approach, the official target is to keep inflation, under a particular definition such as the Consumer Price Index, within a desired range. Thus, while other monetary regimes usually also have as their ultimate goal to control inflation, they go about it in an indirect way, whereas inflation targeting employs a more direct approach.  The inflation target is achieved through periodic adjustments to the central bank interest rate target. In addition, clear communication to the public about the central bank's actions and future expectations is an essential part of the strategy, in itself influencing inflation expectations which are considered crucial for actual inflation developments.[60]  Typically the duration that the interest rate target is kept constant will vary between months and years. This interest rate target is usually reviewed on a monthly or quarterly basis by a policy committee.[19] Changes to the interest rate target are made in response to various market indicators in an attempt to forecast economic trends and in so doing keep the market on track towards achieving the defined inflation target.  The inflation targeting approach to monetary policy approach was pioneered in New Zealand. Since 1990, an increasing number of countries have switched to inflation targeting as their monetary policy framework. It is used in, among other countries, Australia, Brazil, Canada, Chile, Colombia, the Czech Republic, Hungary, Japan, New Zealand, Norway, Iceland, India, Philippines, Poland, Sweden, South Africa, Turkey, and the United Kingdom.[61] In 2022, the International Monetary Fund registered that 45 economies used inflation targeting as their monetary policy framework.[20] In addition, the Federal Reserve and the European Central Bank are generally considered to follow a strategy very close to inflation targeting, even though they do not officially label themselves as inflation targeters.[18] Inflation targeting thus has become the world's dominant monetary policy framework.[62] However, critics contend that there are unintended consequences to this approach such as fueling the increase in housing prices and contributing to wealth inequalities by supporting higher equity values.[63]  This policy is based on maintaining a fixed exchange rate with a foreign currency. There are varying degrees of fixed exchange rates, which can be ranked in relation to how rigid the fixed exchange rate is with the anchor nation.  Under a system of fiat fixed rates, the local government or monetary authority declares a fixed exchange rate but does not actively buy or sell currency to maintain the rate. Instead, the rate is enforced by non-convertibility measures (e.g. capital controls, import\/export licenses, etc.). In this case, there is a black market exchange rate where the currency trades at its market\/unofficial rate.  Under a system of fixed convertibility, currency is bought and sold by the central bank or monetary authority on a daily basis to achieve the target exchange rate. This target rate may be a fixed level or a fixed band within which the exchange rate may fluctuate until the monetary authority intervenes to buy or sell as necessary to maintain the exchange rate within the band. (In this case, the fixed exchange rate with a fixed level can be seen as a special case of the fixed exchange rate with bands where the bands are set to zero.)  Under a system of fixed exchange rates maintained by a currency board every unit of local currency must be backed by a unit of foreign currency (correcting for the exchange rate). This ensures that the local monetary base does not inflate without being backed by hard currency and eliminates any worries about a run on the local currency by those wishing to convert the local currency to the hard (anchor) currency.  Under dollarization, foreign currency (usually the US dollar, hence the term \"dollarization\") is used freely as the medium of exchange either exclusively or in parallel with local currency. This outcome can come about because the local population has lost all faith in the local currency, or it may also be a policy of the government (usually to rein in inflation and import credible monetary policy).  Theoretically, using relative purchasing power parity (PPP), the rate of depreciation of the home country's currency must equal the inflation differential:  which implies that  The anchor variable is the rate of depreciation. Therefore, the rate of inflation at home must equal the rate of inflation in the foreign country plus the rate of depreciation of the exchange rate of the home country currency, relative to the other.  With a strict fixed exchange rate or a peg, the rate of depreciation of the exchange rate is set equal to zero. In the case of a crawling peg, the rate of depreciation is set equal to a constant. With a limited flexible band, the rate of depreciation is allowed to fluctuate within a given range.  By fixing the rate of depreciation, PPP theory concludes that the home country's inflation rate must depend on the foreign country's.  Countries may decide to use a fixed exchange rate monetary regime in order to take advantage of price stability and control inflation. In practice, more than half of nations’ monetary regimes use fixed exchange rate anchoring.[19] The great majority of these are emerging economies, Denmark being the only OECD member in 2022 maintaining an exchange rate anchor according to the IMF.[20]  These policies often abdicate monetary policy to the foreign monetary authority or government as monetary policy in the pegging nation must align with monetary policy in the anchor nation to maintain the exchange rate. The degree to which local monetary policy becomes dependent on the anchor nation depends on factors such as capital mobility, openness, credit channels and other economic factors.  In the 1980s, several countries used an approach based on a constant growth in the money supply. This approach was refined to include different classes of money and credit (M0, M1, etc.) The approach was influenced by the theoretical school of thought called monetarism.[64] In the US this approach to monetary policy was discontinued with the selection of Alan Greenspan as Fed Chairman.  Central banks might choose to set a money supply growth target as a nominal anchor to keep prices stable in the long term. The quantity theory is a long run model, which links price levels to money supply and demand. Using this equation, we can rearrange to see the following:  where π is the inflation rate, μ is the money supply growth rate and g is the real output growth rate. This equation suggests that controlling the money supply's growth rate can ultimately lead to price stability in the long run. To use this nominal anchor, a central bank would need to set μ equal to a constant and commit to maintaining this target. While monetary policy typically focuses on a price signal of one form or another, this approach is focused on monetary quantities.  However, targeting the money supply growth rate was not a success in practice because the relationship between inflation, economic activity, and measures of money growth turned out to be unstable.[10] Consequently, the importance of the money supply as a guide for the conduct of monetary policy has diminished over time,[65] and after the 1980s central banks have shifted away from policies that focus on money supply targeting. Today, it is widely considered a weak policy, because it is not stably related to the growth of real output. As a result, a higher output growth rate will result in a too low level of inflation. A low output growth rate will result in inflation that would be higher than the desired level.[19]  Later research suggests this apparent instability in money demand relationship may have stemmed from measurement error in traditional simple-sum monetary aggregates, which problematically treat all monetary assets as perfect substitutes. Divisia monetary aggregates developed by Barnett (1980),[66] which appropriately weight components based on their user costs and liquidity services, demonstrate more stable relationships with economic variables. Studies by Belongia (1996)[67] and Hendrickson (2014)[68] show many findings of unstable money demand can be reversed when using Divisia rather than simple-sum measures, suggesting measurement methods rather than fundamental economic relationships may have been the key issue. Chen and Valcarcel empirically tested for stable money demand function across subsamples with Divisia monetary aggregates and their associated user costs.[69] Monetary policy rules targeting properly measured monetary aggregates may better characterize central bank actions, particularly during recessions and zero lower bound periods.[70]  In 2022, the International Monetary Fund registered that 25 economies, all of them emerging economies, used some monetary aggregate target as their monetary policy framework.[20]  Related to money targeting, nominal income targeting (also called Nominal GDP or NGDP targeting), originally proposed by James Meade (1978) and James Tobin (1980), was advocated by Scott Sumner and reinforced by the market monetarist school of thought.[71]  So far, no central banks have implemented this monetary policy. However, various academic studies indicate that such a monetary policy targeting would better match central bank losses[72] and welfare optimizing monetary policy[73] compared to more standard monetary policy targeting.  Price level targeting is a monetary policy that is similar to inflation targeting except that CPI growth in one year over or under the long-term price level target is offset in subsequent years such that a targeted price-level trend is reached over time, e.g. five years, giving more certainty about future price increases to consumers. Under inflation targeting what happened in the immediate past years is not taken into account or adjusted for in the current and future years.  The different types of policy are also called monetary regimes, in parallel to exchange-rate regimes. A fixed exchange rate is also an exchange-rate regime. The gold standard results in a relatively fixed regime towards the currency of other countries following a gold standard and a floating regime towards those that are not. Targeting inflation, the price level or other monetary aggregates implies floating the exchange rate.  The short-term effects of monetary policy can be influenced by the degree to which announcements of new policy are deemed credible.[74] In particular, when an anti-inflation policy is announced by a central bank, in the absence of credibility in the eyes of the public inflationary expectations will not drop, and the short-run effect of the announcement and a subsequent sustained anti-inflation policy is likely to be a combination of somewhat lower inflation and higher unemployment (see Phillips curve § NAIRU and rational expectations). But if the policy announcement is deemed credible, inflationary expectations will drop commensurately with the announced policy intent, and inflation is likely to come down more quickly and without so much of a cost in terms of unemployment.  Thus there can be an advantage to having the central bank be independent of the political authority, to shield it from the prospect of political pressure to reverse the direction of the policy. But even with a seemingly independent central bank, a central bank whose hands are not tied to the anti-inflation policy might be deemed as not fully credible; in this case, there is an advantage to be had by the central bank being in some way bound to follow through on its policy pronouncements, lending it credibility.  There is very strong consensus among economists that an independent central bank can run a more credible monetary policy, making market expectations more responsive to signals from the central bank.[75]  Optimal monetary policy in international economics is concerned with the question of how monetary policy should be conducted in interdependent open economies. The classical view holds that international macroeconomic interdependence is only relevant if it affects domestic output gaps and inflation, and monetary policy prescriptions can abstract from openness without harm.[76] This view rests on two implicit assumptions: a high responsiveness of import prices to the exchange rate, i.e. producer currency pricing (PCP), and frictionless international financial markets supporting the efficiency of flexible price allocation.[77][78] The violation or distortion of these assumptions found in empirical research is the subject of a substantial part of the international optimal monetary policy literature. The policy trade-offs specific to this international perspective are threefold:[79]  First, research suggests only a weak reflection of exchange rate movements in import prices, lending credibility to the opposed theory of local currency pricing (LCP).[80] The consequence is a departure from the classical view in the form of a trade-off between output gaps and misalignments in international relative prices, shifting monetary policy to CPI inflation control and real exchange rate stabilization.  Second, another specificity of international optimal monetary policy is the issue of strategic interactions and competitive devaluations, which is due to cross-border spillovers in quantities and prices.[81] Therein, the national authorities of different countries face incentives to manipulate the terms of trade to increase national welfare in the absence of international policy coordination. Even though the gains of international policy coordination might be small, such gains may become very relevant if balanced against incentives for international noncooperation.[77]  Third, open economies face policy trade-offs if asset market distortions prevent global efficient allocation. Even though the real exchange rate absorbs shocks in current and expected fundamentals, its adjustment does not necessarily result in a desirable allocation and may even exacerbate the misallocation of consumption and employment at both the domestic and global level. This is because, relative to the case of complete markets, both the Phillips curve and the loss function include a welfare-relevant measure of cross-country imbalances. Consequently, this results in domestic goals, e.g. output gaps or inflation, being traded-off against the stabilization of external variables such as the terms of trade or the demand gap. Hence, the optimal monetary policy in this case consists of redressing demand imbalances and\/or correcting international relative prices at the cost of some inflation.[82][self-published source?]  Corsetti, Dedola and Leduc (2011)[79] summarize the status quo of research on international monetary policy prescriptions: \"Optimal monetary policy thus should target a combination of inward-looking variables such as output gap and inflation, with currency misalignment and cross-country demand misallocation, by leaning against the wind of misaligned exchange rates and international imbalances.\" This is main factor in country money status.  Developing countries may have problems establishing an effective operating monetary policy. The primary difficulty is that few developing countries have deep markets in government debt. The matter is further complicated by the difficulties in forecasting money demand and fiscal pressure to levy the inflation tax by expanding the base rapidly. In general, the central banks in many developing countries have poor records in managing monetary policy. This is often because the monetary authorities in developing countries are mostly not independent of the government, so good monetary policy takes a backseat to the political desires of the government or is used to pursue other non-monetary goals. For this and other reasons, developing countries that want to establish credible monetary policy may institute a currency board or adopt dollarization. This can avoid interference from the government and may lead to the adoption of monetary policy as carried out in the anchor nation. Recent attempts at liberalizing and reform of financial markets (particularly the recapitalization of banks and other financial institutions in Nigeria and elsewhere) are gradually providing the latitude required to implement monetary policy frameworks by the relevant central banks.  Beginning with New Zealand in 1990, central banks began adopting formal, public inflation targets with the goal of making the outcomes, if not the process, of monetary policy more transparent. In other words, a central bank may have an inflation target of 2% for a given year, and if inflation turns out to be 5%, then the central bank will typically have to submit an explanation. The Bank of England exemplifies both these trends. It became independent of government through the Bank of England Act 1998 and adopted an inflation target of 2.5% RPI, revised to 2% of CPI in 2003.[83] The success of inflation targeting in the United Kingdom has been attributed to the Bank of England's focus on transparency.[84] The Bank of England has been a leader in producing innovative ways of communicating information to the public, especially through its Inflation Report, which have been emulated by many other central banks.[85]  The European Central Bank adopted, in 1998, a definition of price stability within the Eurozone as inflation of under 2% HICP. In 2003, this was revised to inflation below, but close to, 2% over the medium term. Since then, the target of 2% has become common for other major central banks, including the Federal Reserve (since January 2012) and Bank of Japan (since January 2013).[86]  Since 2017-2018, a growing number of central banks have started to consider the effects of climate change on their operational frameworks for monetary policy and supervisory policies.[87][88][89] In the continuation to a famous speech by former Bank of England governor Mark Carney in September 2015,[90] central bank have justified this work by the fact that climate change will likely generate more volatility in certain markets, some inflationary pressure either due to climate shocks and extreme weather events[91] and linked with an overly slow and disordered transition, and generate climate-related financial risks on the financial sector.[92][93][94] As a result, even though central banks are not climate policy makers, from the perspective of their financial stability mandate, they may have to adjust their policies in order to anticipate and mitigate these risks.  This work was spearheaded by the foundation of the Network for Greening the Financial System (NGFS) in 2017 by the Bank of England, Banque de France and the Dutch central bank.[95] The NGFS is composed of more than a hundred central banks and financial supervisors.  Proposals for climate-related ajustements to central bank policies range from green macro-prudential rules,[96] green quantitative easing, green collateral frameworks rules and green refinancing operations.[97] In 2021, the European Central Bank has announced that it will \"tilt\" its corporate bond purchases (effectively implementing a form of Green QE) and look at ways to incorporate climate factors in its collateral framework. The ECB has however refrained so far from implementing a \"green interest rate\".[98]  There continues to be some debate about whether monetary policy can (or should) smooth business cycles. A central conjecture of Keynesian economics is that the central bank can stimulate aggregate demand in the short run, because a significant number of prices in the economy are fixed in the short run and firms will produce as many goods and services as are demanded (in the long run, however, money is neutral, as in the neoclassical model). However, some economists from the new classical school contend that central banks cannot affect business cycles.[99]  Conventional macroeconomic models assume that all agents in an economy are fully rational. A rational agent has clear preferences, models uncertainty via expected values of variables or functions of variables, and always chooses to perform the action with the optimal expected outcome for itself among all feasible actions – they maximize their utility. Monetary policy analysis and decisions hence traditionally rely on this New Classical approach.[100][101][102]  However, as studied by the field of behavioral economics that takes into account the concept of bounded rationality, people often deviate from the way that these neoclassical theories assume.[103] Humans are generally not able to react in a completely rational manner to the world around them[102] – they do not make decisions in the rational way commonly envisioned in standard macroeconomic models. People have time limitations, cognitive biases, care about issues like fairness and equity and follow rules of thumb (heuristics).[103]  This has implications for the conduct of monetary policy. Monetary policy is the outcome of a complex interaction between monetary institutions, central banker preferences and policy rules, and hence human decision-making plays an important role.[101] It is more and more recognized that the standard rational approach does not provide an optimal foundation for monetary policy actions. These models fail to address important human anomalies and behavioral drivers that explain monetary policy decisions.[104][101][102]  An example of a behavioral bias that characterizes the behavior of central bankers is loss aversion: for every monetary policy choice, losses loom larger than gains, and both are evaluated with respect to the status quo.[101] One result of loss aversion is that when gains and losses are symmetric or nearly so, risk aversion may set in. Loss aversion can be found in multiple contexts in monetary policy. The \"hard fought\" battle against the Great Inflation, for instance, might cause a bias against policies that risk greater inflation.[104] Another common finding in behavioral studies is that individuals regularly offer estimates of their own ability, competence, or judgments that far exceed an objective assessment: they are overconfident. Central bank policymakers may fall victim to overconfidence in managing the macroeconomy in terms of timing, magnitude, and even the qualitative impact of interventions. Overconfidence can result in actions of the central bank that are either \"too little\" or \"too much\". When policymakers believe their actions will have larger effects than objective analysis would indicate, this results in too little intervention. Overconfidence can, for instance, cause problems when relying on interest rates to gauge the stance of monetary policy: low rates might mean that policy is easy, but they could also signal a weak economy.[104]  These are examples of how behavioral phenomena may have a substantial influence on monetary policy. Monetary policy analyses should thus account for the fact that policymakers (or central bankers) are individuals and prone to biases and temptations that can sensibly influence their ultimate choices in the setting of macroeconomic and\/or interest rate targets.[101]  US specific: "},"meta":{},"created_at":"2025-03-22T14:25:42.290416Z","updated_at":"2025-03-22T14:25:42.290416Z","inner_id":79,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":88,"annotations":[{"id":88,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.322899Z","updated_at":"2025-03-22T14:25:42.322899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"f499aaa5-c077-43db-b5ae-f85de293470b","import_id":null,"last_action":null,"bulk_created":false,"task":88,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Heterodox  Keynesian economics (\/ˈkeɪnziən\/ KAYN-zee-ən; sometimes Keynesianism, named after British economist John Maynard Keynes) are the various macroeconomic theories and models of how aggregate demand (total spending in the economy) strongly influences economic output and inflation.[1] In the Keynesian view, aggregate demand does not necessarily equal the productive capacity of the economy. It is influenced by a host of factors that sometimes behave erratically and impact production, employment, and inflation.[2]  Keynesian economists generally argue that aggregate demand is volatile and unstable and that, consequently, a market economy often experiences inefficient macroeconomic outcomes, including recessions when demand is too low and inflation when demand is too high. Further, they argue that these economic fluctuations can be mitigated by economic policy responses coordinated between a government and their central bank. In particular, fiscal policy actions taken by the government and monetary policy actions taken by the central bank, can help stabilize economic output, inflation, and unemployment over the business cycle.[3] Keynesian economists generally advocate a regulated market economy – predominantly private sector, but with an active role for government intervention during recessions and depressions.[4]  Keynesian economics developed during and after the Great Depression from the ideas presented by Keynes in his 1936 book, The General Theory of Employment, Interest and Money.[5] Keynes' approach was a stark contrast to the aggregate supply-focused classical economics that preceded his book. Interpreting Keynes's work is a contentious topic, and several schools of economic thought claim his legacy.  Keynesian economics, as part of the neoclassical synthesis, served as the standard macroeconomic model in the developed nations during the later part of the Great Depression, World War II, and the post-war economic expansion (1945–1973). It was developed in part to attempt to explain the Great Depression and to help economists understand future crises. It lost some influence following the oil shock and resulting stagflation of the 1970s.[6] Keynesian economics was later redeveloped as New Keynesian economics, becoming part of the contemporary new neoclassical synthesis, that forms current-day mainstream macroeconomics.[7] The advent of the financial crisis of 2007–2008 sparked renewed interest in Keynesian policies by governments around the world.[8]  Macroeconomics is the study of the factors applying to an economy as a whole. Important macroeconomic variables include the overall price level, the interest rate, the level of employment, and income (or equivalently output) measured in real terms.  The classical tradition of partial equilibrium theory had been to split the economy into separate markets, each of whose equilibrium conditions could be stated as a single equation determining a single variable. The theoretical apparatus of supply and demand curves developed by Fleeming Jenkin and Alfred Marshall provided a unified mathematical basis for this approach, which the Lausanne School generalized to general equilibrium theory.  For macroeconomics, relevant partial theories included the Quantity theory of money determining the price level and the classical theory of the interest rate. In regards to employment, the condition referred to by Keynes as the \"first postulate of classical economics\" stated that the wage is equal to the marginal product, which is a direct application of the marginalist principles developed during the nineteenth century (see The General Theory). Keynes sought to supplant all three aspects of the classical theory.  Although Keynes's work was crystallized and given impetus by the advent of the Great Depression, it was part of a long-running debate within economics over the existence and nature of general gluts. A number of the policies Keynes advocated to address the Great Depression (notably government deficit spending at times of low private investment or consumption), and many of the theoretical ideas he proposed (effective demand, the multiplier, the paradox of thrift), had been advanced by authors in the 19th and early 20th centuries. (E.g. J. M. Robertson raised the paradox of thrift in 1892.[9][10]) Keynes's unique contribution was to provide a general theory of these, which proved acceptable to the economic establishment.  An intellectual precursor of Keynesian economics was underconsumption theories associated with John Law, Thomas Malthus, the Birmingham School of Thomas Attwood,[11] and the American economists William Trufant Foster and Waddill Catchings, who were influential in the 1920s and 1930s. Underconsumptionists were, like Keynes after them, concerned with failure of aggregate demand to attain potential output, calling this \"underconsumption\" (focusing on the demand side), rather than \"overproduction\" (which would focus on the supply side), and advocating economic interventionism. Keynes specifically discussed underconsumption (which he wrote \"under-consumption\") in the General Theory, in Chapter 22, Section IV and Chapter 23, Section VII.  Numerous concepts were developed earlier and independently of Keynes by the Stockholm school during the 1930s; these accomplishments were described in a 1937 article, published in response to the 1936 General Theory, sharing the Swedish discoveries.[12]  In 1923, Keynes published his first contribution to economic theory, A Tract on Monetary Reform, whose point of view is classical but incorporates ideas that later played a part in the General Theory. In particular, looking at the hyperinflation in European economies, he drew attention to the opportunity cost of holding money (identified with inflation rather than interest) and its influence on the velocity of circulation.[13]  In 1930, he published A Treatise on Money, intended as a comprehensive treatment of its subject \"which would confirm his stature as a serious academic scholar, rather than just as the author of stinging polemics\",[14] and marks a large step in the direction of his later views. In it, he attributes unemployment to wage stickiness[15] and treats saving and investment as governed by independent decisions: the former varying positively with the interest rate,[16] the latter negatively.[17] The velocity of circulation is expressed as a function of the rate of interest.[18] He interpreted his treatment of liquidity as implying a purely monetary theory of interest.[19]  Keynes's younger colleagues of the Cambridge Circus and Ralph Hawtrey believed that his arguments implicitly assumed full employment, and this influenced the direction of his subsequent work.[20] During 1933, he wrote essays on various economic topics \"all of which are cast in terms of movement of output as a whole\".[21]  At the time that Keynes wrote the General Theory, it had been a tenet of mainstream economic thought that the economy would automatically revert to a state of general equilibrium: it had been assumed that, because the needs of consumers are always greater than the capacity of the producers to satisfy those needs, everything that is produced would eventually be consumed once the appropriate price was found for it. This perception is reflected in Say's law[22] and in the writing of David Ricardo,[23] which states that individuals produce so that they can either consume what they have manufactured or sell their output so that they can buy someone else's output. This argument rests upon the assumption that if a surplus of goods or services exists, they would naturally drop in price to the point where they would be consumed.  Given the backdrop of high and persistent unemployment during the Great Depression, Keynes argued that there was no guarantee that the goods that individuals produce would be met with adequate effective demand, and periods of high unemployment could be expected, especially when the economy was contracting in size. He saw the economy as unable to maintain itself at full employment automatically, and believed that it was necessary for the government to step in and put purchasing power into the hands of the working population through government spending. Thus, according to Keynesian theory, some individually rational microeconomic-level actions such as not investing savings in the goods and services produced by the economy, if taken collectively by a large proportion of individuals and firms, can lead to outcomes wherein the economy operates below its potential output and growth rate.  Prior to Keynes, a situation in which aggregate demand for goods and services did not meet supply was referred to by classical economists as a general glut, although there was disagreement among them as to whether a general glut was possible. Keynes argued that when a glut occurred, it was the over-reaction of producers and the laying off of workers that led to a fall in demand and perpetuated the problem. Keynesians therefore advocate an active stabilization policy to reduce the amplitude of the business cycle, which they rank among the most serious of economic problems. According to the theory, government spending can be used to increase aggregate demand, thus increasing economic activity, reducing unemployment and deflation.  The Liberal Party fought the 1929 General Election on a promise to \"reduce levels of unemployment to normal within one year by utilising the stagnant labour force in vast schemes of national development\".[24] David Lloyd George launched his campaign in March with a policy document, We can cure unemployment, which tentatively claimed that, \"Public works would lead to a second round of spending as the workers spent their wages.\"[25] Two months later Keynes, then nearing completion of his Treatise on money,[26] and Hubert Henderson collaborated on a political pamphlet seeking to \"provide academically respectable economic arguments\" for Lloyd George's policies.[27] It was titled Can Lloyd George do it? and endorsed the claim that \"greater trade activity would make for greater trade activity ... with a cumulative effect\".[28] This became the mechanism of the \"ratio\" published by Richard Kahn in his 1931 paper \"The relation of home investment to unemployment\",[29] described by Alvin Hansen as \"one of the great landmarks of economic analysis\".[30] The \"ratio\" was soon rechristened the \"multiplier\" at Keynes's suggestion.[31]  The multiplier of Kahn's paper is based on a respending mechanism familiar nowadays from textbooks. Samuelson puts it as follows:  Let's suppose that I hire unemployed resources to build a $1000 woodshed. My carpenters and lumber producers will get an extra $1000 of income... If they all have a marginal propensity to consume of 2\/3, they will now spend $666.67 on new consumption goods. The producers of these goods will now have extra incomes... they in turn will spend $444.44 ... Thus an endless chain of secondary consumption respending is set in motion by my primary investment of $1000.[32] Samuelson's treatment closely follows Joan Robinson's account of 1937[33] and is the main channel by which the multiplier has influenced Keynesian theory. It differs significantly from Kahn's paper and even more from Keynes's book.  The designation of the initial spending as \"investment\" and the employment-creating respending as \"consumption\" echoes Kahn faithfully, though he gives no reason why initial consumption or subsequent investment respending should not have exactly the same effects. Henry Hazlitt, who considered Keynes as much a culprit as Kahn and Samuelson, wrote that ...  ... in connection with the multiplier (and indeed most of the time) what Keynes is referring to as \"investment\" really means any addition to spending for any purpose... The word \"investment\" is being used in a Pickwickian, or Keynesian, sense.[34] Kahn envisaged money as being passed from hand to hand, creating employment at each step, until it came to rest in a cul-de-sac (Hansen's term was \"leakage\"); the only culs-de-sac he acknowledged were imports and hoarding, although he also said that a rise in prices might dilute the multiplier effect. Jens Warming recognised that personal saving had to be considered,[35] treating it as a \"leakage\" (p. 214) while recognising on p. 217 that it might in fact be invested.  The textbook multiplier gives the impression that making society richer is the easiest thing in the world: the government just needs to spend more. In Kahn's paper, it is harder. For him, the initial expenditure must not be a diversion of funds from other uses, but an increase in the total expenditure: something impossible – if understood in real terms – under the classical theory that the level of expenditure is limited by the economy's income\/output. On page 174, Kahn rejects the claim that the effect of public works is at the expense of expenditure elsewhere, admitting that this might arise if the revenue is raised by taxation, but says that other available means have no such consequences. As an example, he suggests that the money may be raised by borrowing from banks, since ...  ... it is always within the power of the banking system to advance to the Government the cost of the roads without in any way affecting the flow of investment along the normal channels. This assumes that banks are free to create resources to answer any demand. But Kahn adds that ...  ... no such hypothesis is really necessary. For it will be demonstrated later on that, pari passu with the building of roads, funds are released from various sources at precisely the rate that is required to pay the cost of the roads. The demonstration relies on \"Mr Meade's relation\" (due to James Meade) asserting that the total amount of money that disappears into culs-de-sac is equal to the original outlay,[36] which in Kahn's words \"should bring relief and consolation to those who are worried about the monetary sources\" (p. 189).  A respending multiplier had been proposed earlier by Hawtrey in a 1928 Treasury memorandum (\"with imports as the only leakage\"), but the idea was discarded in his own subsequent writings.[37] Soon afterwards the Australian economist Lyndhurst Giblin published a multiplier analysis in a 1930 lecture (again with imports as the only leakage).[38] The idea itself was much older. Some Dutch mercantilists had believed in an infinite multiplier for military expenditure (assuming no import \"leakage\"), since ...  ... a war could support itself for an unlimited period if only money remained in the country ... For if money itself is \"consumed\", this simply means that it passes into someone else's possession, and this process may continue indefinitely.[39]  Multiplier doctrines had subsequently been expressed in more theoretical terms by the Dane Julius Wulff (1896), the Australian Alfred de Lissa (late 1890s), the German\/American Nicholas Johannsen (same period), and the Dane Fr. Johannsen (1925\/1927).[40] Kahn himself said that the idea was given to him as a child by his father.[41]  As the 1929 election approached \"Keynes was becoming a strong public advocate of capital development\" as a public measure to alleviate unemployment.[42] Winston Churchill, the Conservative Chancellor, took the opposite view:  It is the orthodox Treasury dogma, steadfastly held ... [that] very little additional employment and no permanent additional employment can, in fact, be created by State borrowing and State expenditure.[43] Keynes pounced on a flaw in the Treasury view. Cross-examining Sir Richard Hopkins, a Second Secretary in the Treasury, before the Macmillan Committee on Finance and Industry in 1930 he referred to the \"first proposition\" that \"schemes of capital development are of no use for reducing unemployment\" and asked whether \"it would be a misunderstanding of the Treasury view to say that they hold to the first proposition\". Hopkins responded that \"The first proposition goes much too far. The first proposition would ascribe to us an absolute and rigid dogma, would it not?\"[44]  Later the same year, speaking in a newly created Committee of Economists, Keynes tried to use Kahn's emerging multiplier theory to argue for public works, \"but Pigou's and Henderson's objections ensured that there was no sign of this in the final product\".[45] In 1933 he gave wider publicity to his support for Kahn's multiplier in a series of articles titled \"The road to prosperity\" in The Times newspaper.[46]  A. C. Pigou was at the time the sole economics professor at Cambridge. He had a continuing interest in the subject of unemployment, having expressed the view in his popular Unemployment (1913) that it was caused by \"maladjustment between wage-rates and demand\"[47] – a view Keynes may have shared prior to the years of the General Theory. Nor were his practical recommendations very different: \"on many occasions in the thirties\" Pigou \"gave public support [...] to State action designed to stimulate employment\".[48] Where the two men differed is in the link between theory and practice. Keynes was seeking to build theoretical foundations to support his recommendations for public works while Pigou showed no disposition to move away from classical doctrine. Referring to him and Dennis Robertson, Keynes asked rhetorically: \"Why do they insist on maintaining theories from which their own practical conclusions cannot possibly follow?\"[49]  Keynes set forward the ideas that became the basis for Keynesian economics in his main work, The General Theory of Employment, Interest and Money (1936). It was written during the Great Depression, when unemployment rose to 25% in the United States and as high as 33% in some countries. It is almost wholly theoretical, enlivened by occasional passages of satire and social commentary. The book had a profound impact on economic thought, and ever since it was published there has been debate over its meaning.  Keynes begins the General Theory with a summary of the classical theory of employment, which he encapsulates in his formulation of Say's Law as the dictum \"Supply creates its own demand\". He also wrote that although his theory was explained in terms of an Anglo-Saxon laissez faire economy, his theory was also more general in the sense that it would be easier to adapt to \"totalitarian states\" than a free market policy would.[50]  Under the classical theory, the wage rate is determined by the marginal productivity of labour, and as many people are employed as are willing to work at that rate. Unemployment may arise through friction or may be \"voluntary\", in the sense that it arises from a refusal to accept employment owing to \"legislation or social practices ... or mere human obstinacy\", but \"...the classical postulates do not admit of the possibility of the third category,\" which Keynes defines as involuntary unemployment.[51]  Keynes raises two objections to the classical theory's assumption that \"wage bargains ... determine the real wage\". The first lies in the fact that \"labour stipulates (within limits) for a money-wage rather than a real wage\". The second is that classical theory assumes that, \"The real wages of labour depend on the wage bargains which labour makes with the entrepreneurs,\" whereas, \"If money wages change, one would have expected the classical school to argue that prices would change in almost the same proportion, leaving the real wage and the level of unemployment practically the same as before.\"[52] Keynes considers his second objection the more fundamental, but most commentators concentrate on his first one: it has been argued that the quantity theory of money protects the classical school from the conclusion Keynes expected from it.[53]  Saving is that part of income not devoted to consumption, and consumption is that part of expenditure not allocated to investment, i.e., to durable goods.[54] Hence saving encompasses hoarding (the accumulation of income as cash) and the purchase of durable goods. The existence of net hoarding, or of a demand to hoard, is not admitted by the simplified liquidity preference model of the General Theory.  Once he rejects the classical theory that unemployment is due to excessive wages, Keynes proposes an alternative based on the relationship between saving and investment. In his view, unemployment arises whenever entrepreneurs' incentive to invest fails to keep pace with society's propensity to save (propensity is one of Keynes's synonyms for \"demand\"). The levels of saving and investment are necessarily equal, and income is therefore held down to a level where the desire to save is no greater than the incentive to invest.  The incentive to invest arises from the interplay between the physical circumstances of production and psychological anticipations of future profitability; but once these things are given the incentive is independent of income and depends solely on the rate of interest r. Keynes designates its value as a function of r as the \"schedule of the marginal efficiency of capital\".[55]  The propensity to save behaves quite differently.[56] Saving is simply that part of income not devoted to consumption, and:  ... the prevailing psychological law seems to be that when aggregate income increases, consumption expenditure will also increase but to a somewhat lesser extent.[57] Keynes adds that \"this psychological law was of the utmost importance in the development of my own thought\".  Keynes viewed the money supply as one of the main determinants of the state of the real economy. The significance he attributed to it is one of the innovative features of his work, and was influential on the politically hostile monetarist school.  Money supply comes into play through the liquidity preference function, which is the demand function that corresponds to money supply. It specifies the amount of money people will seek to hold according to the state of the economy. In Keynes's first (and simplest) account – that of Chapter 13 – liquidity preference is determined solely by the interest rates r—which is seen as the earnings forgone by holding wealth in liquid form:[58] hence liquidity preference can be written L(r ) and in equilibrium must equal the externally fixed money supply M̂.  Money supply, saving and investment combine to determine the level of income as illustrated in the diagram,[59] where the top graph shows money supply (on the vertical axis) against interest rate. M̂ determines the ruling interest rate r̂ through the liquidity preference function. The rate of interest determines the level of investment Î through the schedule of the marginal efficiency of capital, shown as a blue curve in the lower graph. The red curves in the same diagram show what the propensities to save are for different incomes Y ; and the income Ŷ corresponding to the equilibrium state of the economy must be the one for which the implied level of saving at the established interest rate is equal to Î.  In Keynes's more complicated liquidity preference theory (presented in Chapter 15) the demand for money depends on income as well as on the interest rate and the analysis becomes more complicated. Keynes never fully integrated his second liquidity preference doctrine with the rest of his theory, leaving that to John Hicks: see the IS-LM model below.  Keynes rejects the classical explanation of unemployment based on wage rigidity, but it is not clear what effect the wage rate has on unemployment in his system. He treats wages of all workers as proportional to a single rate set by collective bargaining, and chooses his units so that this rate never appears separately in his discussion. It is present implicitly in those quantities he expresses in wage units, while being absent from those he expresses in money terms. It is therefore difficult to see whether, and in what way, his results differ for a different wage rate, nor is it clear what he thought about the matter.  An increase in the money supply, according to Keynes's theory, leads to a drop in the interest rate and an increase in the amount of investment that can be undertaken profitably, bringing with it an increase in total income.  Keynes' name is associated with fiscal, rather than monetary, measures but they receive only passing (and often satirical) reference in the General Theory. He mentions \"increased public works\" as an example of something that brings employment through the multiplier,[60] but this is before he develops the relevant theory, and he does not follow up when he gets to the theory.  Later in the same chapter he tells us that:  Ancient Egypt was doubly fortunate, and doubtless owed to this its fabled wealth, in that it possessed two activities, namely, pyramid-building as well as the search for the precious metals, the fruits of which, since they could not serve the needs of man by being consumed, did not stale with abundance. The Middle Ages built cathedrals and sang dirges. Two pyramids, two masses for the dead, are twice as good as one; but not so two railways from London to York. But again, he does not get back to his implied recommendation to engage in public works, even if not fully justified from their direct benefits, when he constructs the theory. On the contrary he later advises us that ...  ... our final task might be to select those variables which can be deliberately controlled or managed by central authority in the kind of system in which we actually live ...[61] and this appears to look forward to a future publication rather than to a subsequent chapter of the General Theory.  Keynes' view of saving and investment was his most important departure from the classical outlook. It can be illustrated using the \"Keynesian cross\" devised by Paul Samuelson.[62] The horizontal axis denotes total income and the purple curve shows C (Y ), the propensity to consume, whose complement S (Y ) is the propensity to save: the sum of these two functions is equal to total income, which is shown by the broken line at 45°.  The horizontal blue line I (r ) is the schedule of the marginal efficiency of capital whose value is independent of Y. The schedule of the marginal efficiency of capital is dependent on the interest rate, specifically the interest rate cost of a new investment. If the interest rate charged by the financial sector to the productive sector is below the marginal efficiency of capital at that level of technology and capital intensity then investment is positive and grows the lower the interest rate is, given the diminishing return of capital. If the interest rate is above the marginal efficiency of capital then investment is equal to zero. Keynes interprets this as the demand for investment and denotes the sum of demands for consumption and investment as \"aggregate demand\", plotted as a separate curve. Aggregate demand must equal total income, so equilibrium income must be determined by the point where the aggregate demand curve crosses the 45° line.[63] This is the same horizontal position as the intersection of I (r ) with S (Y ).  The equation I (r ) = S (Y ) had been accepted by the classics, who had viewed it as the condition of equilibrium between supply and demand for investment funds and as determining the interest rate (see the classical theory of interest). But insofar as they had had a concept of aggregate demand, they had seen the demand for investment as being given by S (Y ), since for them saving was simply the indirect purchase of capital goods, with the result that aggregate demand was equal to total income as an identity rather than as an equilibrium condition. Keynes takes note of this view in Chapter 2, where he finds it present in the early writings of Alfred Marshall but adds that \"the doctrine is never stated to-day in this crude form\".  The equation I (r ) = S (Y ) is accepted by Keynes for some or all of the following reasons:  Keynes introduces his discussion of the multiplier in Chapter 10 with a reference to Kahn's earlier paper (see below). He designates Kahn's multiplier the \"employment multiplier\" in distinction to his own \"investment multiplier\" and says that the two are only \"a little different\".[64] Kahn's multiplier has consequently been understood by much of the Keynesian literature as playing a major role in Keynes's own theory, an interpretation encouraged by the difficulty of understanding Keynes's presentation. Kahn's multiplier gives the title (\"The multiplier model\") to the account of Keynesian theory in Samuelson's Economics and is almost as prominent in Alvin Hansen's Guide to Keynes and in Joan Robinson's Introduction to the Theory of Employment.  Keynes states that there is ...  ... a confusion between the logical theory of the multiplier, which holds good continuously, without time-lag ... and the consequence of an expansion in the capital goods industries which take gradual effect, subject to a time-lag, and only after an interval ...[65] and implies that he is adopting the former theory.[66] And when the multiplier eventually emerges as a component of Keynes's theory (in Chapter 18) it turns out to be simply a measure of the change of one variable in response to a change in another. The schedule of the marginal efficiency of capital is identified as one of the independent variables of the economic system:[67] \"What [it] tells us, is ... the point to which the output of new investment will be pushed ...\"[68] The multiplier then gives \"the ratio ... between an increment of investment and the corresponding increment of aggregate income\".[69]  G. L. S. Shackle regarded Keynes' move away from Kahn's multiplier as ...  ... a retrograde step ... For when we look upon the Multiplier as an instantaneous functional relation ... we are merely using the word Multiplier to stand for an alternative way of looking at the marginal propensity to consume ...,[70] which G. M. Ambrosi cites as an instance of \"a Keynesian commentator who would have liked Keynes to have written something less 'retrograde'\".[71]  The value Keynes assigns to his multiplier is the reciprocal of the marginal propensity to save: k  = 1 \/ S '(Y ). This is the same as the formula for Kahn's multiplier in a closed economy assuming that all saving (including the purchase of durable goods), and not just hoarding, constitutes leakage. Keynes gave his formula almost the status of a definition (it is put forward in advance of any explanation[72]). His multiplier is indeed the value of \"the ratio ... between an increment of investment and the corresponding increment of aggregate income\" as Keynes derived it from his Chapter 13 model of liquidity preference, which implies that income must bear the entire effect of a change in investment. But under his Chapter 15 model a change in the schedule of the marginal efficiency of capital has an effect shared between the interest rate and income in proportions depending on the partial derivatives of the liquidity preference function. Keynes did not investigate the question of whether his formula for multiplier needed revision.  The liquidity trap is a phenomenon that may impede the effectiveness of monetary policies in reducing unemployment.  Economists generally think the rate of interest will not fall below a certain limit, often seen as zero or a slightly negative number. Keynes suggested that the limit might be appreciably greater than zero but did not attach much practical significance to it. The term \"liquidity trap\" was coined by Dennis Robertson in his comments on the General Theory,[73] but it was John Hicks in \"Mr. Keynes and the Classics\"[74] who recognised the significance of a slightly different concept.  If the economy is in a position such that the liquidity preference curve is almost vertical, as must happen as the lower limit on r is approached, then a change in the money supply M̂ makes almost no difference to the equilibrium rate of interest r̂ or, unless there is compensating steepness in the other curves, to the resulting income Ŷ. As Hicks put it, \"Monetary means will not force down the rate of interest any further.\"  Paul Krugman has worked extensively on the liquidity trap, claiming that it was the problem confronting the Japanese economy around the turn of the millennium.[75] In his later words:  Short-term interest rates were close to zero, long-term rates were at historical lows, yet private investment spending remained insufficient to bring the economy out of deflation. In that environment, monetary policy was just as ineffective as Keynes described. Attempts by the Bank of Japan to increase the money supply simply added to already ample bank reserves and public holdings of cash...[76] Hicks showed how to analyse Keynes' system when liquidity preference is a function of income as well as of the rate of interest. Keynes's admission of income as an influence on the demand for money is a step back in the direction of classical theory, and Hicks takes a further step in the same direction by generalizing the propensity to save to take both Y and r as arguments. Less classically he extends this generalization to the schedule of the marginal efficiency of capital.  The IS-LM model uses two equations to express Keynes' model. The first, now written I (Y, r ) = S (Y,r ), expresses the principle of effective demand. We may construct a graph on (Y, r ) coordinates and draw a line connecting those points satisfying the equation: this is the IS curve. In the same way we can write the equation of equilibrium between liquidity preference and the money supply as L(Y ,r ) = M̂ and draw a second curve – the LM curve – connecting points that satisfy it. The equilibrium values Ŷ of total income and r̂ of interest rate are then given by the point of intersection of the two curves.  If we follow Keynes's initial account under which liquidity preference depends only on the interest rate r, then the LM curve is horizontal.  Joan Robinson commented that:  ... modern teaching has been confused by J. R. Hicks' attempt to reduce the General Theory to a version of static equilibrium with the formula IS–LM. Hicks has now repented and changed his name from J. R. to John, but it will take a long time for the effects of his teaching to wear off. Hicks subsequently relapsed.[77][clarification needed]  Keynes argued that the solution to the Great Depression was to stimulate the country (\"incentive to invest\") through some combination of two approaches:  If the interest rate at which businesses and consumers can borrow decreases, investments that were previously uneconomic become profitable, and large consumer sales normally financed through debt (such as houses, automobiles, and, historically, even appliances like refrigerators) become more affordable. A principal function of central banks in countries that have them is to influence this interest rate through a variety of mechanisms collectively called monetary policy. This is how monetary policy that reduces interest rates is thought to stimulate economic activity, i.e., \"grow the economy\"—and why it is called expansionary monetary policy.  Expansionary fiscal policy consists of increasing net public spending, which the government can effect by a) taxing less, b) spending more, or c) both. Investment and consumption by government raises demand for businesses' products and for employment, reversing the effects of the aforementioned imbalance. If desired spending exceeds revenue, the government finances the difference by borrowing from capital markets by issuing government bonds. This is called deficit spending. Two points are important to note at this point. First, deficits are not required for expansionary fiscal policy, and second, it is only change in net spending that can stimulate or depress the economy. For example, if a government ran a deficit of 10% both last year and this year, this would represent neutral fiscal policy. In fact, if it ran a deficit of 10% last year and 5% this year, this would actually be contractionary. On the other hand, if the government ran a surplus of 10% of GDP last year and 5% this year, that would be expansionary fiscal policy, despite never running a deficit at all.  But – contrary to some critical characterizations of it – Keynesianism does not consist solely of deficit spending, since it recommends adjusting fiscal policies according to cyclical circumstances.[78] An example of a counter-cyclical policy is raising taxes to cool the economy and to prevent inflation when there is abundant demand-side growth, and engaging in deficit spending on labour-intensive infrastructure projects to stimulate employment and stabilize wages during economic downturns.  Keynes's ideas influenced Franklin D. Roosevelt's view that insufficient buying-power caused the Depression. During his presidency, Roosevelt adopted some aspects of Keynesian economics, especially after 1937, when, in the depths of the Depression, the United States suffered from recession yet again following fiscal contraction. But to many the true success of Keynesian policy can be seen at the onset of World War II, which provided a kick to the world economy, removed uncertainty, and forced the rebuilding of destroyed capital. Keynesian ideas became almost official in social-democratic Europe after the war and in the U.S. in the 1960s.  The Keynesian advocacy of deficit spending contrasted with the classical and neoclassical economic analysis of fiscal policy. They admitted that fiscal stimulus could actuate production. But, to these schools, there was no reason to believe that this stimulation would outrun the side-effects that \"crowd out\" private investment: first, it would increase the demand for labour and raise wages, hurting profitability; Second, a government deficit increases the stock of government bonds, reducing their market price and encouraging high interest rates, making it more expensive for business to finance fixed investment. Thus, efforts to stimulate the economy would be self-defeating.  The Keynesian response is that such fiscal policy is appropriate only when unemployment is persistently high, above the non-accelerating inflation rate of unemployment (NAIRU). In that case, crowding out is minimal. Further, private investment can be \"crowded in\": Fiscal stimulus raises the market for business output, raising cash flow and profitability, spurring business optimism. To Keynes, this accelerator effect meant that government and business could be complements rather than substitutes in this situation.  Second, as the stimulus occurs, gross domestic product rises—raising the amount of saving, helping to finance the increase in fixed investment. Finally, government outlays need not always be wasteful: government investment in public goods that is not provided by profit-seekers encourages the private sector's growth. That is, government spending on such things as basic research, public health, education, and infrastructure could help the long-term growth of potential output.  In Keynes's theory, there must be significant slack in the labour market before fiscal expansion is justified.  Keynesian economists believe that adding to profits and incomes during boom cycles through tax cuts, and removing income and profits from the economy through cuts in spending during downturns, tends to exacerbate the negative effects of the business cycle. This effect is especially pronounced when the government controls a large fraction of the economy, as increased tax revenue may aid investment in state enterprises in downturns, and decreased state revenue and investment harm those enterprises.  In the last few years of his life, John Maynard Keynes was much preoccupied with the question of balance in international trade. He was the leader of the British delegation to the United Nations Monetary and Financial Conference in 1944 that established the Bretton Woods system of international currency management. He was the principal author of a proposal – the so-called Keynes Plan – for an International Clearing Union. The two governing principles of the plan were that the problem of settling outstanding balances should be solved by 'creating' additional 'international money', and that debtor and creditor should be treated almost alike as disturbers of equilibrium. In the event, though, the plans were rejected, in part because \"American opinion was naturally reluctant to accept the principle of equality of treatment so novel in debtor-creditor relationships\".[79]  The new system is not founded on free trade (liberalization[80] of foreign trade[81]) but rather on regulating international trade to eliminate trade imbalances. Nations with a surplus would have a powerful incentive to get rid of it, which would automatically clear other nations' deficits.[82] Keynes proposed a global bank that would issue its own currency—the bancor—which was exchangeable with national currencies at fixed rates of exchange and would become the unit of account between nations, which means it would be used to measure a country's trade deficit or trade surplus. Every country would have an overdraft facility in its bancor account at the International Clearing Union. He pointed out that surpluses lead to weak global aggregate demand – countries running surpluses exert a \"negative externality\" on trading partners, and posed far more than those in deficit, a threat to global prosperity. Keynes thought that surplus countries should be taxed to avoid trade imbalances.[83] In \"National Self-Sufficiency\" The Yale Review, Vol. 22, no. 4 (June 1933),[84][85] he already highlighted the problems created by free trade.  His view, supported by many economists and commentators at the time, was that creditor nations may be just as responsible as debtor nations for disequilibrium in exchanges and that both should be under an obligation to bring trade back into a state of balance. Failure for them to do so could have serious consequences. In the words of Geoffrey Crowther, then editor of The Economist, \"If the economic relationships between nations are not, by one means or another, brought fairly close to balance, then there is no set of financial arrangements that can rescue the world from the impoverishing results of chaos.\"[86]  These ideas were informed by events prior to the Great Depression when – in the opinion of Keynes and others – international lending, primarily by the U.S., exceeded the capacity of sound investment and so got diverted into non-productive and speculative uses, which in turn invited default and a sudden stop to the process of lending.[87]  Influenced by Keynes, economic texts in the immediate post-war period put a significant emphasis on balance in trade. For example, the second edition of the popular introductory textbook, An Outline of Money,[88] devoted the last three of its ten chapters to questions of foreign exchange management and in particular the 'problem of balance'. However, in more recent years, since the end of the Bretton Woods system in 1971, with the increasing influence of Monetarist schools of thought in the 1980s, and particularly in the face of large sustained trade imbalances, these concerns – and particularly concerns about the destabilizing effects of large trade surpluses – have largely disappeared from mainstream economics discourse[89] and Keynes' insights have slipped from view.[90] They are receiving some attention again in the wake of the financial crisis of 2007–08.[91]  At the beginning of his career, Keynes was an economist close to Alfred Marshall, deeply convinced of the benefits of free trade. From the crisis of 1929 onwards, noting the commitment of the British authorities to defend the gold parity of the pound sterling and the rigidity of nominal wages, he gradually adhered to protectionist measures.[92]  On 5 November 1929, when heard by the Macmillan Committee to bring the British economy out of the crisis, Keynes indicated that the introduction of tariffs on imports would help to rebalance the trade balance. The committee's report states in a section entitled \"import control and export aid\", that in an economy where there is not full employment, the introduction of tariffs can improve production and employment. Thus the reduction of the trade deficit favours the country's growth.[92]  In January 1930, in the Economic Advisory Council, Keynes proposed the introduction of a system of protection to reduce imports. In the autumn of 1930, he proposed a uniform tariff of 10% on all imports and subsidies of the same rate for all exports.[92] In the Treatise on Money, published in the autumn of 1930, he took up the idea of tariffs or other trade restrictions with the aim of reducing the volume of imports and rebalancing the balance of trade.[92]  On 7 March 1931, in the New Statesman and Nation, he wrote an article entitled Proposal for a Tariff Revenue. He pointed out that the reduction of wages led to a reduction in national demand which constrained markets. Instead, he proposes the idea of an expansionary policy combined with a tariff system to neutralize the effects on the balance of trade. The application of customs tariffs seemed to him \"unavoidable, whoever the Chancellor of the Exchequer might be\". Thus, for Keynes, an economic recovery policy is only fully effective if the trade deficit is eliminated. He proposed a 15% tax on manufactured and semi-manufactured goods and 5% on certain foodstuffs and raw materials, with others needed for exports exempted (wool, cotton).[92]  In 1932, in an article entitled The Pro- and Anti-Tariffs, published in The Listener, he envisaged the protection of farmers and certain sectors such as the automobile and iron and steel industries, considering them indispensable to Britain.[92]  In the post-crisis situation of 1929, Keynes judged the assumptions of the free trade model unrealistic. He criticized, for example, the neoclassical assumption of wage adjustment.[92][93]  As early as 1930, in a note to the Economic Advisory Council, he doubted the intensity of the gain from specialization in the case of manufactured goods. While participating in the MacMillan Committee, he admitted that he no longer \"believed in a very high degree of national specialisation\" and refused to \"abandon any industry which is unable, for the moment, to survive\". He also criticized the static dimension of the theory of comparative advantage, which, in his view, by fixing comparative advantages definitively, led in practice to a waste of national resources.[92][93]  In the Daily Mail of 13 March 1931, he called the assumption of perfect sectoral labour mobility \"nonsense\" since it states that a person made unemployed contributes to a reduction in the wage rate until he finds a job. But for Keynes, this change of job may involve costs (job search, training) and is not always possible. Generally speaking, for Keynes, the assumptions of full employment and automatic return to equilibrium discredit the theory of comparative advantage.[92][93]  In July 1933, he published an article in the New Statesman and Nation entitled National Self-Sufficiency, in which he criticized the argument of the specialization of economies, which is the basis of free trade. He thus proposed the search for a certain degree of self-sufficiency. Instead of the specialization of economies advocated by the Ricardian theory of comparative advantage, he prefers the maintenance of a diversity of activities for nations.[93] In it he refutes the principle of peacemaking trade. His vision of trade became that of a system where foreign capitalists compete for new markets. He defends the idea of producing on national soil when possible and reasonable and expresses sympathy for the advocates of protectionism.[94]  He notes in National Self-Sufficiency:[94][92]  A considerable degree of international specialization is necessary in a rational world in all cases where it is dictated by wide differences of climate, natural resources, native aptitudes, level of culture and density of population. But over an increasingly wide range of industrial products, and perhaps of agricultural products also, I have become doubtful whether the economic loss of national self-sufficiency is great enough to outweigh the other advantages of gradually bringing the product and the consumer within the ambit of the same national, economic, and financial organization. Experience accumulates to prove that most modern processes of mass production can be performed in most countries and climates with almost equal efficiency.  He also writes in National Self-Sufficiency:[92] I sympathize, therefore, with those who would minimize, rather than with those who would maximize, economic entanglement among nations. Ideas, knowledge, science, hospitality, travel—these are the things which should of their nature be international. But let goods be homespun whenever it is reasonably and conveniently possible, and, above all, let finance be primarily national. Later, Keynes had a written correspondence with James Meade centred on the issue of import restrictions. Keynes and Meade discussed the best choice between quota and tariff. In March 1944 Keynes began a discussion with Marcus Fleming after the latter had written an article entitled Quotas versus depreciation. On this occasion, we see that he has definitely taken a protectionist stance after the Great Depression. He considered that quotas could be more effective than currency depreciation in dealing with external imbalances. Thus, for Keynes, currency depreciation was no longer sufficient, and protectionist measures became necessary to avoid trade deficits. To avoid the return of crises due to a self-regulating economic system, it seemed essential to him to regulate trade and stop free trade (deregulation of foreign trade).[92]  He points out that countries that import more than they export weaken their economies. When the trade deficit increases, unemployment rises and GDP slows down. And surplus countries exert a \"negative externality\" on their trading partners. They get richer at the expense of others and destroy the output of their trading partners. John Maynard Keynes believed that the products of surplus countries should be taxed to avoid trade imbalances.[95] Thus he no longer believes in the theory of comparative advantage (on which free trade is based) which states that the trade deficit does not matter, since trade is mutually beneficial. This also explains his desire to replace the liberalization of international trade (Free Trade) with a regulatory system aimed at eliminating trade imbalances in his proposals for the Bretton Woods Agreement.[citation needed]  Keynes's ideas became widely accepted after World War II, and until the early 1970s, Keynesian economics provided the main inspiration for economic policy makers in Western industrialized countries.[6] Governments prepared high quality economic statistics on an ongoing basis and tried to base their policies on the Keynesian theory that had become the norm. In the early era of social liberalism and social democracy, most western capitalist countries enjoyed low, stable unemployment and modest inflation, an era called the Golden Age of Capitalism.  In terms of policy, the twin tools of post-war Keynesian economics were fiscal policy and monetary policy. While these are credited to Keynes, others, such as economic historian David Colander, argue that they are, rather, due to the interpretation of Keynes by Abba Lerner in his theory of functional finance, and should instead be called \"Lernerian\" rather than \"Keynesian\".[96]  Through the 1950s, moderate degrees of government demand leading industrial development, and use of fiscal and monetary counter-cyclical policies continued, and reached a peak in the \"go go\" 1960s, where it seemed to many Keynesians that prosperity was now permanent. In 1971, Republican US President Richard Nixon even proclaimed \"I am now a Keynesian in economics.\"[97]  Beginning in the late 1960s, a new classical macroeconomics movement arose, critical of Keynesian assumptions (see sticky prices), and seemed, especially in the 1970s, to explain certain phenomena better. It was characterized by explicit and rigorous adherence to microfoundations, as well as use of increasingly sophisticated mathematical modelling.  With the oil shock of 1973, and the economic problems of the 1970s, Keynesian economics began to fall out of favour. During this time, many economies experienced high and rising unemployment, coupled with high and rising inflation, contradicting the Phillips curve's prediction. This stagflation meant that the simultaneous application of expansionary (anti-recession) and contractionary (anti-inflation) policies appeared necessary. This dilemma led to the end of the Keynesian near-consensus of the 1960s, and the rise throughout the 1970s of ideas based upon more classical analysis, including monetarism, supply-side economics,[97] and new classical economics.  However, by the late 1980s, certain failures of the new classical models, both theoretical (see Real business cycle theory) and empirical (see the \"Volcker recession\")[98] hastened the emergence of New Keynesian economics, a school that sought to unite the most realistic aspects of Keynesian and neo-classical assumptions and place them on more rigorous theoretical foundation than ever before.  One line of thinking, utilized also as a critique of the notably high unemployment and potentially disappointing GNP growth rates associated with the new classical models by the mid-1980s, was to emphasize low unemployment and maximal economic growth at the cost of somewhat higher inflation (its consequences kept in check by indexing and other methods, and its overall rate kept lower and steadier by such potential policies as Martin Weitzman's share economy).[99]  Multiple schools of economic thought that trace their legacy to Keynes currently exist, the notable ones being neo-Keynesian economics, New Keynesian economics, post-Keynesian economics, and the new neoclassical synthesis. Keynes's biographer Robert Skidelsky writes that the post-Keynesian school has remained closest to the spirit of Keynes's work in following his monetary theory and rejecting the neutrality of money.[100][101] Today these ideas, regardless of provenance, are referred to in academia under the rubric of \"Keynesian economics\", due to Keynes's role in consolidating, elaborating, and popularizing them.  In the postwar era, Keynesian analysis was combined with neoclassical economics to produce what is generally termed the \"neoclassical synthesis\", yielding neo-Keynesian economics, which dominated mainstream macroeconomic thought. Though it was widely held that there was no strong automatic tendency to full employment, many believed that if government policy were used to ensure it, the economy would behave as neoclassical theory predicted. This post-war domination by neo-Keynesian economics was broken during the stagflation of the 1970s.[102] There was a lack of consensus among macroeconomists in the 1980s, and during this period New Keynesian economics was developed, ultimately becoming- along with new classical macroeconomics- a part of the current consensus, known as the new neoclassical synthesis.[103]  Post-Keynesian economists, on the other hand, reject the neoclassical synthesis and, in general, neoclassical economics applied to the macroeconomy. Post-Keynesian economics is a heterodox school that holds that both neo-Keynesian economics and New Keynesian economics are incorrect, and a misinterpretation of Keynes's ideas. The post-Keynesian school encompasses a variety of perspectives, but has been far less influential than the other more mainstream Keynesian schools.[104]  Interpretations of Keynes have emphasized his stress on the international coordination of Keynesian policies, the need for international economic institutions, and the ways in which economic forces could lead to war or could promote peace.[105]  In a 2014 paper, economist Alan Blinder argues that, \"for not very good reasons\", public opinion in the United States has associated Keynesianism with liberalism, and he states that such is incorrect. For example, both Presidents Ronald Reagan (1981–89) and George W. Bush (2001–09) supported policies that were, in fact, Keynesian, even though both men were conservative leaders. And tax cuts can provide highly helpful fiscal stimulus during a recession, just as much as infrastructure spending can. Blinder concludes: \"If you are not teaching your students that 'Keynesianism' is neither conservative nor liberal, you should be.\"[106]  The Keynesian schools of economics are situated alongside a number of other schools that have the same perspectives on what the economic issues are, but differ on what causes them and how best to resolve them. Today, most of these schools of thought have been subsumed into modern macroeconomic theory.  The Stockholm school rose to prominence at about the same time that Keynes published his General Theory and shared a common concern in business cycles and unemployment. The second generation of Swedish economists also advocated government intervention through spending during economic downturns[107] although opinions are divided over whether they conceived the essence of Keynes's theory before he did.[108]  There was debate between monetarists and Keynesians in the 1960s over the role of government in stabilizing the economy. Both monetarists and Keynesians agree that issues such as business cycles, unemployment, and deflation are caused by inadequate demand. However, they had fundamentally different perspectives on the capacity of the economy to find its own equilibrium, and the degree of government intervention that would be appropriate. Keynesians emphasized the use of discretionary fiscal policy and monetary policy, while monetarists argued the primacy of monetary policy, and that it should be rules-based.[109]  The debate was largely resolved in the 1980s. Since then, economists have largely agreed that central banks should bear the primary responsibility for stabilizing the economy, and that monetary policy should largely follow the Taylor rule – which many economists credit with the Great Moderation.[110][111] The financial crisis of 2007–08, however, has convinced many economists and governments of the need for fiscal interventions and highlighted the difficulty in stimulating economies through monetary policy alone during a liquidity trap.[112]  Some Marxist economists criticized Keynesian economics.[113] For example, in his 1946 appraisal[114] Paul Sweezy—while admitting that there was much in the General Theory's analysis of effective demand that Marxists could draw on—described Keynes as a prisoner of his neoclassical upbringing. Sweezy argued that Keynes had never been able to view the capitalist system as a totality. He argued that Keynes regarded the class struggle carelessly, and overlooked the class role of the capitalist state, which he treated as a deus ex machina, and some other points. While Michał Kalecki was generally enthusiastic about the Keynesian Revolution, he predicted that it would not endure, in his article \"Political Aspects of Full Employment\". In the article Kalecki predicted that the full employment delivered by Keynesian policy would eventually lead to a more assertive working class and weakening of the social position of business leaders, causing the elite to use their political power to force the displacement of the Keynesian policy even though profits would be higher than under a laissez faire system: The elites would not care about risking the higher profits in the pursuit of reclaiming prestige in the society and the political power.[115]  James M. Buchanan[116] criticized Keynesian economics on the grounds that governments would in practice be unlikely to implement theoretically optimal policies. The implicit assumption underlying the Keynesian fiscal revolution, according to Buchanan, was that economic policy would be made by wise men, acting without regard to political pressures or opportunities, and guided by disinterested economic technocrats. He argued that this was an unrealistic assumption about political, bureaucratic and electoral behaviour. Buchanan blamed Keynesian economics for what he considered a decline in America's fiscal discipline.[117] Buchanan argued that deficit spending would evolve into a permanent disconnect between spending and revenue, precisely because it brings short-term gains, so, ending up institutionalizing irresponsibility in the federal government, the largest and most central institution in our society.[118]  Martin Feldstein argues that the legacy of Keynesian economics–the misdiagnosis of unemployment, the fear of saving, and the unjustified government intervention–affected the fundamental ideas of policy makers.[119] Milton Friedman thought that Keynes's political bequest was harmful for two reasons. First, he thought whatever the economic analysis, benevolent dictatorship is likely sooner or later to lead to a totalitarian society. Second, he thought Keynes's economic theories appealed to a group far broader than economists primarily because of their link to his political approach.[120] Alex Tabarrok argues that Keynesian politics–as distinct from Keynesian policies–has failed pretty much whenever it's been tried, at least in liberal democracies.[121]  In response to this argument, John Quiggin,[122] wrote about these theories' implication for a liberal democratic order. He thought that if it is generally accepted that democratic politics is nothing more than a battleground for competing interest groups, then reality will come to resemble the model. Paul Krugman wrote \"I don't think we need to take that as an immutable fact of life; but still, what are the alternatives?\"[123] Daniel Kuehn, criticized James M. Buchanan. He argued, \"if you have a problem with politicians – criticize politicians,\" not Keynes.[124] He also argued that empirical evidence makes it pretty clear that Buchanan was wrong.[125][126] James Tobin argued, if advising government officials, politicians, voters, it's not for economists to play games with them.[127] Keynes implicitly rejected this argument, in \"soon or late it is ideas not vested interests which are dangerous for good or evil.\"[128][129]  Brad DeLong has argued that politics is the main motivator behind objections to the view that government should try to serve a stabilizing macroeconomic role.[130] Paul Krugman argued that a regime that by and large lets markets work, but in which the government is ready both to rein in excesses and fight slumps is inherently unstable, due to intellectual instability, political instability, and financial instability.[131]  Another influential school of thought was based on the Lucas critique of Keynesian economics. This called for greater consistency with microeconomic theory based on rational choice theory, and in particular emphasized the idea of rational expectations. Lucas and others argued that Keynesian economics required remarkably foolish and short-sighted behaviour from people, which totally contradicted the economic understanding of their behaviour at a micro level. New classical economics introduced a set of macroeconomic theories that were based on optimizing microeconomic behaviour. These models have been developed into the real business-cycle theory, which argues that business cycle fluctuations can to a large extent be accounted for by real (in contrast to nominal) shocks.  Beginning in the late 1950s new classical macroeconomists began to disagree with the methodology employed by Keynes and his successors. Keynesians emphasized the dependence of consumption on disposable income and, also, of investment on current profits and current cash flow. In addition, Keynesians posited a Phillips curve that tied nominal wage inflation to unemployment rate. To support these theories, Keynesians typically traced the logical foundations of their model (using introspection) and supported their assumptions with statistical evidence.[132] New classical theorists demanded that macroeconomics be grounded on the same foundations as microeconomic theory, profit-maximizing firms and rational, utility-maximizing consumers.[132]  The result of this shift in methodology produced several important divergences from Keynesian macroeconomics:[132]  F.A. Hayek, an Austrian-style economist described Keynesianism as a system of \"economics of abundance\" stating it is, \"a system of economics which is based on the assumption that no real scarcity exists, and that the only scarcity with which we need concern ourselves is the artificial scarcity created by the determination of people not to sell their services and products below certain arbitrarily fixed prices.\"[133] Ludwig von Mises, another Austrian economist, describes a Keynesian system as believing it can solve most problems with \"more money and credit\" which leads to a system of \"inflationism\" in which \"prices (of goods) rise higher and higher.\"[134] Murray Rothbard wrote that Keynesian-style governmental regulation of money and credit created a \"dismal monetary and banking situation,\" since it allows for the central bankers that have the exclusive ability to print money to be \"unchecked and out of control.\"[135] Rothbard went on to say in an interview that, \"There is one good thing about (Karl) Marx: he was not a Keynesian.\"[136]  The social historian C. J. Coventry argues in Keynes from Below: A Social History of Second World War Keynesian Economics (2023) that Keynes and Keynesian economics was unpopular in the United Kingdom and Australia in the 1940s. Many workers and trades unions, as well as figures in the British Labour Party and Australian Labor Party, saw Keynesianism as a means of stopping socialism. Keynes was largely supported by business leaders, bankers and conservative parties, or tripartite third way Catholics eager to avoid socialism after the Second World War.[137] While Coventry agrees that the Keynesianism has considerable benefits, he argues that these benefits arose from the next phase of capitalism with many of the disadvantages being forced onto peoples in the third world, such as in British Malaya where there was bloodshed for crucial resources. "},"meta":{},"created_at":"2025-03-22T14:25:42.290416Z","updated_at":"2025-03-22T14:25:42.290416Z","inner_id":80,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":89,"annotations":[{"id":89,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.322899Z","updated_at":"2025-03-22T14:25:42.322899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"b59e5b8a-f670-4588-b66f-18e30a3ae67c","import_id":null,"last_action":null,"bulk_created":false,"task":89,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Heterodox  Monetarism is a school of thought in monetary economics that emphasizes the role of policy-makers in controlling the amount of money in circulation. It gained prominence in the 1970s but was mostly abandoned as a direct guidance to monetary policy during the following decade because of the rise of inflation targeting through movements of the official interest rate.  The monetarist theory states that variations in the money supply have major influences on national output in the short run and on price levels over longer periods. Monetarists assert that the objectives of monetary policy are best met by targeting the growth rate of the money supply rather than by engaging in discretionary monetary policy.[1] Monetarism is commonly associated with neoliberalism.[2]  Monetarism is mainly associated with the work of Milton Friedman, who was an influential opponent of Keynesian economics, criticising Keynes's theory of fighting economic downturns using fiscal policy (e.g. government spending). Friedman and Anna Schwartz wrote an influential book, A Monetary History of the United States, 1867–1960, and argued that inflation is \"always and everywhere a monetary phenomenon\".[3]  Although opposed to the existence of the Federal Reserve,[4] Friedman advocated, given its existence, a central bank policy aimed at keeping the growth of the money supply at a rate commensurate with the growth in productivity and demand for goods. Money growth targeting was mostly abandoned by the central banks who tried it, however. Contrary to monetarist thinking, the relation between money growth and inflation proved to be far from tight. Instead, starting in the early 1990s, most major central banks turned to direct inflation targeting, relying on steering short-run interest rates as their main policy instrument.[5]: 483–485  Afterwards, monetarism was subsumed into the new neoclassical synthesis which appeared in macroeconomics around 2000.  Monetarism is an economic theory that focuses on the macroeconomic effects of the supply of money and central banking. Formulated by Milton Friedman, it argues that excessive expansion of the money supply is inherently inflationary, and that monetary authorities should focus solely on maintaining price stability.  Monetarist theory draws its roots from the quantity theory of money, a centuries-old economic theory which had been put forward by various economists, among them Irving Fisher and Alfred Marshall, before Friedman restated it in 1956.[6][7]  Monetarists argued that central banks sometimes caused major unexpected fluctuations in the money supply. Friedman asserted that actively trying to stabilize demand through monetary policy changes can have negative unintended consequences.[5]: 511–512  In part he based this view on the historical analysis of monetary policy, A Monetary History of the United States, 1867–1960, which he coauthored with Anna Schwartz in 1963. The book attributed inflation to excess money supply generated by a central bank. It attributed deflationary spirals to the reverse effect of a failure of a central bank to support the money supply during a liquidity crunch.[8] In particular, the authors argued that the Great Depression of the 1930s was caused by a massive contraction of the money supply (they deemed it \"the Great Contraction\"[9]), and not by the lack of investment that Keynes had argued. They also maintained that post-war inflation was caused by an over-expansion of the money supply. They made famous the assertion of monetarism that \"inflation is always and everywhere a monetary phenomenon.\"  Friedman proposed a fixed monetary rule, called Friedman's k-percent rule, where the money supply would be automatically increased by a fixed percentage per year. The rate should equal the growth rate of real GDP, leaving the price level unchanged. For instance, if the economy is expected to grow at 2 percent in a given year, the Fed should allow the money supply to increase by 2 percent. Because discretionary monetary policy would be as likely to destabilise as to stabilise the economy, Friedman advocated that the Fed be bound to fixed rules in conducting its policy.[10]  Most monetarists oppose the gold standard. Friedman viewed a pure gold standard as impractical. For example, whereas one of the benefits of the gold standard is that the intrinsic limitations to the growth of the money supply by the use of gold would prevent inflation, if the growth of population or increase in trade outpaces the money supply, there would be no way to counteract deflation and reduced liquidity (and any attendant recession) except for the mining of more gold. But he also admitted that if a government was willing to surrender control over its monetary policy and not to interfere with economic activities, a gold-based economy would be possible.[11]  Clark Warburton is credited with making the first solid empirical case for the monetarist interpretation of business fluctuations in a series of papers from 1945.[1]p. 493 Within mainstream economics, the rise of monetarism started with Milton Friedman's 1956 restatement of the quantity theory of money. Friedman argued that the demand for money could be described as depending on a small number of economic variables.[12]  Thus, according to Friedman, when the money supply expanded, people would not simply wish to hold the extra money in idle money balances; i.e., if they were in equilibrium before the increase, they were already holding money balances to suit their requirements, and thus after the increase they would have money balances surplus to their requirements. These excess money balances would therefore be spent and hence aggregate demand would rise. Similarly, if the money supply were reduced people would want to replenish their holdings of money by reducing their spending. In this, Friedman challenged a simplification attributed to Keynes suggesting that \"money does not matter.\"[12] Thus the word 'monetarist' was coined.  The popularity of monetarism picked up in political circles when the prevailing view of neo-Keynesian economics seemed unable to explain the contradictory problems of rising unemployment and inflation in response to the Nixon shock in 1971 and the oil shocks of 1973. On one hand, higher unemployment seemed to call for reflation, but on the other hand rising inflation seemed to call for disinflation. The social-democratic post-war consensus that had prevailed in first world countries was thus called into question by the rising neoliberal political forces.[2]  In 1979, United States President Jimmy Carter appointed as Federal Reserve Chief Paul Volcker, who made fighting inflation his primary objective, and who restricted the money supply (in accordance with the Friedman rule) to tame inflation in the economy. The result was a major rise in interest rates, not only in the United States; but worldwide. The \"Volcker shock\" continued from 1979 to the summer of 1982, decreasing inflation and increasing unemployment.[13]  In May 1979, Margaret Thatcher, Leader of the Conservative Party in the United Kingdom, won the general election, defeating the sitting Labour Government led by James Callaghan. By that time, the UK had endured several years of severe inflation, which was rarely below the 10% mark and stood at 10.3% by the time of the election.[14] Thatcher implemented monetarism as the weapon in her battle against inflation, and succeeded at reducing it to 4.6% by 1983. However, unemployment in the United Kingdom increased from 5.7% in 1979 to 12.2% in 1983, reaching 13.0% in 1982; starting with the first quarter of 1980, the UK economy contracted in terms of real gross domestic product for six straight quarters.[15]  Monetarist ascendancy was brief, however.[10] The period when major central banks focused on targeting the growth of money supply, reflecting monetarist theory, lasted only for a few years, in the US from 1979 to 1982.[16]  The money supply is useful as a policy target only if the relationship between money and nominal GDP, and therefore inflation, is stable and predictable. This implies that the velocity of money must be predictable. In the 1970s velocity had seemed to increase at a fairly constant rate, but in the 1980s and 1990s velocity became highly unstable, experiencing unpredictable periods of increases and declines. Consequently, the stable correlation between the money supply and nominal GDP broke down, and the usefulness of the monetarist approach came into question. Many economists who had been convinced by monetarism in the 1970s abandoned the approach after this experience.[10]  The changing velocity originated in shifts in the demand for money and created serious problems for the central banks. This provoked a thorough rethinking of monetary policy. In the early 1990s central banks started focusing on targeting inflation directly using the short-run interest rate as their central policy variable, abandoning earlier emphasis on money growth. The new strategy proved successful, and today most major central banks follow a flexible inflation targeting.[5]: 483–485   While monetarism's influence on policy diminished in the 1980s, subsequent research suggests that the apparent instability in money demand functions may have stemmed from measurement issues rather than a fundamental breakdown in the money-income relationship. Barnett and others argued that simple-sum monetary aggregates, which weight all monetary components equally regardless of their liquidity characteristics, introduce significant measurement error that obscures stable underlying relationships.[17]  Studies using theoretically-grounded Divisia monetary aggregates, which weight monetary components based on their \"monetary services\" or liquidity properties, have found considerably more stable money demand relationships. For instance, Belongia and Ireland demonstrated that money demand equations using Divisia measures remain stable even through periods of financial innovation and policy regime changes that destabilized traditional simple-sum specifications.[18]  This finding has important implications for monetary policy frameworks. The breakdown in simple-sum money demand relationships was a key factor in central banks abandoning monetary targeting in favor of interest rate rules. However, research using Divisia aggregates suggests that money could still serve as a useful policy indicator or intermediate target if properly measured.[19]  The stability of Divisia money demand functions has been demonstrated across different time periods and countries. For example, Hendrickson found that replacing simple-sum with Divisia measures resolves apparent instabilities in U.S. money demand, while similar results have been documented for other economies.[20] Chen and Valcarcel argued that the properly measured monetary quantities and their holding costs maintain a stable, long-term cointegration.[21]  These findings suggest that the historical shift away from monetary aggregates in policy frameworks may have been premature and based on flawed measurement rather than a true breakdown in the relationship between money and economic activity. While most central banks continue to focus primarily on interest rates, the stability of properly-measured money demand functions indicates that monetary aggregates could potentially play a more prominent role in policy frameworks.[22]  Even though monetarism failed in practical policy, and the close attention to money growth which was at the heart of monetarist analysis is rejected by most economists today, some aspects of monetarism have found their way into modern mainstream economic thinking.[10][23] Among them are the belief that controlling inflation should be a primary responsibility of the central bank.[10] It is also widely recognized that monetary policy, as well as fiscal policy, can affect output in the short run.[5]: 511  In this way, important monetarist thoughts have been subsumed into the new neoclassical synthesis or consensus view of macroeconomics that emerged in the 2000s.[24][5]: 518  "},"meta":{},"created_at":"2025-03-22T14:25:42.291408Z","updated_at":"2025-03-22T14:25:42.291408Z","inner_id":81,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":90,"annotations":[{"id":90,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.322899Z","updated_at":"2025-03-22T14:25:42.322899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"4077f03d-6436-4fca-994b-12b53bd2fa3c","import_id":null,"last_action":null,"bulk_created":false,"task":90,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A pension fund, also known as a superannuation fund in some countries, is any program, fund, or scheme which provides retirement income. The U.S. Government's Social Security Trust Fund, which oversees $2.57 trillion in assets, is the world's largest public pension fund. Pension funds typically have large amounts of money to invest and are the major investors in listed and private companies.  They are especially important to the stock market where large institutional investors dominate.  The largest 300 pension funds collectively hold about USD$6 trillion in assets.[1] In 2012, PricewaterhouseCoopers estimated that pension funds worldwide hold over $33.9 trillion in assets (and were expected to grow to more than $56 trillion by 2020), the largest for any category of institutional investor ahead of mutual funds, insurance companies, currency reserves, sovereign wealth funds, hedge funds, or private equity.[2]  Open pension funds support at least one pension plan with no restriction on membership while closed pension funds support only pension plans that are limited to certain employees.[3] Closed pension funds are further subclassified into:  A public pension fund is one that is regulated under public sector law while a private pension fund is regulated under private sector law. In certain countries, the distinction between public or government pension funds and private pension funds may be difficult to assess. In others, the distinction is made sharply in law, with very specific requirements for administration and investment.  For example, local governmental bodies in the United States are subject to laws passed by the states in which those localities exist, and these laws include provisions such as defining classes of permitted investments and a minimum municipal obligation.[4][5]  It is important to distinguish between pension plan, funds and firm. A pension plan is a benefits program set up and sustained by an employer or an employee group. They are managed by state or private firms as well as pension funds.[6]  Pension funds are financial mechanisms that provide retirement income for employees after their working life. They work by accumulating contributions from employers, and sometimes employees, which are then invested to grow over time. Upon retirement, employees receive benefits, typically calculated as a percentage of their average salary during their working years. For instance, consider a scenario where a pension scheme offers a payment equivalent to 1% of an individual's average salary over the last five years of their employment for each year they served with the employer. Thus, if an employee worked for 35 years at the company and had an average final salary of $60,000, they would be entitled to an annual pension of $21,000. It is important to point out that one cannot usually take early withdrawals or loans from pensions. Public sector pensions, like the California Public Employees’ Retirement System (CalPERS), often include cost-of-living escalators and can be more generous than private sector pensions. Private pension plans are regulated by federal laws such as the Employee Retirement Income Security Act (ERISA) and are insured by the Pension Benefit Guaranty Corporation (PBGC), which guarantees benefits if a pension plan fails.[7]  Pension funds can make investments into stocks, bond, real estate, and other assets. However, they have to be prudently managed compared to other types of funds due to their lower risk tolerance. For many years, they mainly invest into stable stocks and bond.[8] In order to keep high returns, with changing market conditions, they started to invest into other assets.[9] As of 2023, many pension funds are moving away from managing active stock portfolios towards passive investment methods, focusing on index funds and exchange-traded funds (ETFs) that replicate market indices. Additionally, there's an increasing trend to diversify into alternative assets like commodities, high-yield bonds, hedge funds, and real estate. Newer investment tools for pension funds include asset-backed securities, such as those tied to student loans or credit card debt, which are used to boost returns. Investing in private equity is also rising in popularity; these are long-term investments in non-public companies, aimed at achieving substantial profits through eventual sales when these companies reach maturity. Furthermore, real estate investment trusts (REITs) are becoming a frequent choice for pension funds due to their passive investment approach in the real estate sector. Direct investments in commercial properties like office buildings, warehouses, and industrial parks are also prevalent.[10]  Many governments around the world have established public pension systems that are partially or fully funded by investments rather than relying solely on payroll taxes. This approach helps to ensure the long-term solvency of these pension programs. Some examples of governments that use pension fund investments are:  These are just a few examples of governments that have adopted an investment-based approach to managing their public pension systems. By diversifying and growing their pension fund assets, these countries aim to mitigate the risks of running out of money in the future as their populations age.  Pension funds are important financial institutions which manage the retirement savings of millions. Effective governance in these entities is crucial not only to safeguard these funds but also to ensure that they meet their future obligations to pensioners. The governance structures, strategies, and practices of pension funds significantly influence their stability, performance, and the trust of their stakeholders. Proper governance ensures that decisions are made transparently and that fund managers are accountable to stakeholders, including employees, retirees, and employers.[11]  According to the OECD Guidelines for Pension Fund Governance, the governance structure should clearly identify and separate operational and oversight responsibilities. Every pension fund should have a governing body, accountable to the pension plan members and beneficiaries. This body is ultimately responsible for ensuring adherence to the terms of the arrangement and the protection of the best interest of plan members and beneficiaries. The governing body should also meet minimum suitability standards to ensure a high level of integrity, competence, experience, and professionalism. Additionally, there should be adequate internal controls in place to ensure compliance with the law.[12]  Many pension funds have problems with governance. In Hungary, where pension funds are established as not-for-profit institutions, there is evidence that the governing body is generally ineffective in looking after the best interests of its members. Most funds are established by financial institutions that find it easy to promote their candidates to the fund's supervisory board. Some pension funds in the United States have also been the subject of governance problems too, as well as in other countries.[13]  The first concepts of providing retirement benefits have roots in ancient civilizations such as Rome and Greece. The pension system as we know it originated in the 19th century. In 1889, German Chancellor Otto von Bismarck started an early modern pension scheme. His goal was to help older German´s citizens. However, this idea came from the United States of America. In 1875 American Express Company introduced its own pension plan. During the early 20th century pension plans for public employees were growing, which resulted in the creation of a U.S. federal retirement plan, known as Social Security, in 1935. After World War II, pension funds became the primary tool for providing retirement benefits, which was supported by the growth of labour unions. By the 1970s, they held large amounts of financial assets and had evolved to be significant participants in financial markets. But in the 1980s and 1990s pension funds faced significant challenges. The stock market crash in 1987 and the recession in the early 1990s had a negative impact on pension funds. Furthermore, demographic shifts and rising life expectancies placed pressure on these funds to sustain retirement benefits over extended durations.[14]  In the United States, pension plans are regulated mainly by The Employee Retirement Income Security Act 1974(ERISA). It provides framework for the regulation of employee pension and plans which are private pension funds offering. In 2006 was introduced The Pension Protection Act (PPA). This act come with new funding requirements for defined pension plans. As well as with new rules for calculating plan assets and liabilities.[8]  Pension funds in European Union are regulated by Directive 2003\/41\/EC, also known as the IORP directive. This directive was recast and adopted in December 2016. It should promote long-term investment via occupational pension funds. Additionally, beneficiaries and members should now be better informed about their entitlements, address challenges faced by occupational pension funds operating across borders, and foster long-term investments in economic activities that boost growth, enhance the environment, and increase employment opportunities.[15]  The following table lists largest pension funds by total assets by the SWF Institute.[16]  [44]  The contributions are invested by the EPF in various sectors, such as equities, bonds, and property, to generate returns. Members can withdraw their savings under specific conditions, such as retirement at the age of 55, for healthcare, housing, or education. The EPF also allows partial withdrawals before retirement for certain approved purposes. It has total of around 265 billion USD of asset under management as of end 2023.)[55]  The pension system in Romania is made of three pillars. One is the state pension (Pillar I – Mandatory), the second is a private mandatory pension where the state transfers a percentage of the contribution it collects for the public pension, and the third is an optional private pension (Pillar III – Voluntary). The Financial Supervisory Authority – Private Pension is responsible for the supervision and regulation of the private pension system.[57]  Social Security Institution was established by the Social Security Institution Law No:5502 which was published in the Official Gazette No: 26173 dated 20.06.2006 and brings the Social Insurance Institution, General Directorate of Bağ-kur and General Directorate of Emekli Sandığı whose historical development are summarized above under a single roof in order to transfer five different retirement regimes which are civil servants, contractual paid workers, agricultural paid workers, self-employers and agricultural self-employers into a single retirement regime that will offer equal actuarial rights and obligations.  OYAK (Ordu Yardımlaşma Kurumu\/Armed Forces Pension Fund) provides its members with \"supplementary retirement benefits\" apart from the official retirement fund, T.C.Emekli Sandığı\/SSK, to which they are primarily affiliated. In addition to the retirement benefit, OYAK pays \"disability benefits\" to the members on duty when they become partially or fully disabled as well as \"death benefits\" to the heirs of the deceased member if the death occurs during the member's subscription to the foundation. OYAK is incorporated as a private entity under its own law subject to Turkish civic and commercial codes. OYAK, while fulfilling its legal duties, as set in the law, also provides its members with social services such as loans, home loans and retirement income systems. The initial source of OYAK's funds is a compulsory 10 percent levy on the base salary of Turkey's 200,000 serving officers who, together with 25,000 current pensioners, make up OYAK's members. Some other Turkish private pension funds:  In the United States, pension funds include schemes which result in a deferral of income by employees, even if retirement income provision is not the intent.[60] The United States has $19.1 trillion in retirement and pension assets ($9.1 trillion in private funds, $10 trillion in public funds) as of 31 December 2016.[61] The largest 200 pension funds accounted for $4.540 trillion as of 30 September 2009.[62] "},"meta":{},"created_at":"2025-03-22T14:25:42.291408Z","updated_at":"2025-03-22T14:25:42.291408Z","inner_id":82,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":91,"annotations":[{"id":91,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.322899Z","updated_at":"2025-03-22T14:25:42.322899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"d3dba0a7-8e05-4db0-b8da-ddb39da8da3c","import_id":null,"last_action":null,"bulk_created":false,"task":91,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A commercial bank is a financial institution that accepts deposits from the public and gives loans for the purposes of consumption and investment to make a profit.  It can also refer to a bank or a division of a larger bank that deals with corporations or large or middle-sized businesses, to differentiate from retail banks and investment banks. Commercial banks include private sector banks and public sector banks. However, central banks function differently from commercial banks, despite a common misconception known as the \"bank analogy\". Unlike commercial banks, central banks are not primarily focused on generating profits and cannot become insolvent in the same way as commercial banks in a fiat currency system.[1]  The name bank derives from the Italian word banco 'desk\/bench', used during the Italian Renaissance era by Florentine bankers, who used to carry out their transactions on a desk covered by a green tablecloth.[2] However, traces of banking activity can be found even in ancient times.  In the United States, the term commercial bank was often used to distinguish it from an investment bank due to differences in bank regulation. After the Great Depression, through the Glass–Steagall Act, the U.S. Congress required that commercial banks only engage in banking activities, whereas investment banks were limited to capital market activities. This separation was mostly repealed in 1999 by the Gramm–Leach–Bliley Act.  The general role of commercial banks is to provide financial services to the general public and business, ensuring economic and social stability and sustainable growth of the economy.  In this respect, credit creation is the most significant function of commercial banks. While sanctioning a loan to a customer, they do not provide cash to the borrower. Instead, they open a deposit account from which the borrower can withdraw. In other words, while sanctioning a loan, they automatically create deposits.  Regulations  In most countries, commercial banks are heavily regulated and this is typically done by a country's central bank. They will impose a number of conditions on the banks that they regulate such as keeping bank reserves and to maintain minimum capital requirements. They also require some capital  Commercial banks generally provide a number of services to its clients; these can be split into core banking services such as deposits, loans, and other services which are related to payment systems and other financial services.  Along with core products and services, commercial banks perform several secondary functions. The secondary functions of commercial banks can be divided into agency functions and utility functions.  Agency functions include:  Utility functions include: "},"meta":{},"created_at":"2025-03-22T14:25:42.291408Z","updated_at":"2025-03-22T14:25:42.291408Z","inner_id":83,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":92,"annotations":[{"id":92,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.322899Z","updated_at":"2025-03-22T14:25:42.322899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"1ee4ea39-7429-48dd-8bb4-018d91c30e88","import_id":null,"last_action":null,"bulk_created":false,"task":92,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  A dividend is a distribution of profits by a corporation to its shareholders, after which the stock exchange decreases the price of the stock by the dividend to remove volatility. The market has no control over the stock price on open on the ex-dividend date, though more often than not it may open higher.[1] When a corporation earns a profit or surplus, it is able to pay a portion of the profit as a dividend to shareholders. Any amount not distributed is taken to be re-invested in the business (called retained earnings). The current year profit as well as the retained earnings of previous years are available for distribution; a corporation is usually prohibited from paying a dividend out of its capital. Distribution to shareholders may be in cash (usually by bank transfer) or, if the corporation has a dividend reinvestment plan, the amount can be paid by the issue of further shares or by share repurchase. In some cases, the distribution may be of assets.  The dividend received by a shareholder is income of the shareholder and may be subject to income tax (see dividend tax). The tax treatment of this income varies considerably between jurisdictions. The corporation does not receive a tax deduction for the dividends it pays.[2]  A dividend is allocated as a fixed amount per share, with shareholders receiving a dividend in proportion to their shareholding. Dividends can provide at least temporarily stable income and raise morale among shareholders, but are not guaranteed to continue. For the joint-stock company, paying dividends is not an expense; rather, it is the division of after-tax profits among shareholders. Retained earnings (profits that have not been distributed as dividends) are shown in the shareholders' equity section on the company's balance sheet – the same as its issued share capital. Public companies usually pay dividends on a fixed schedule, but may cancel a scheduled dividend, or declare an unscheduled dividend at any time, sometimes called a special dividend to distinguish it from the regular dividends. (more usually a special dividend is paid at the same time as the regular dividend, but for a one-off higher amount). Cooperatives, on the other hand, allocate dividends according to members' activity, so their dividends are often considered to be a pre-tax expense.  The usually fixed payments to holders of preference shares (or preferred stock in American English) are classed as dividends. The word dividend comes from the Latin word dividendum (\"thing to be divided\").[3]  In the financial history of the world, the Dutch East India Company (VOC) was the first recorded (public) company ever to pay regular dividends.[4][5] The VOC paid annual dividends worth around 18 percent of the value of the shares for almost 200 years of existence (1602–1800).[6]  In common law jurisdictions, courts have typically refused to intervene in companies' dividend policies, giving directors wide discretion as to the declaration or payment of dividends. The principle of non-interference was established in the Canadian case of Burland v Earle (1902), the British case of Bond v Barrow Haematite Steel Co (1902), and the Australian case of Miles v Sydney Meat-Preserving Co Ltd (1912). However in Sumiseki Materials Co Ltd v Wambo Coal Pty Ltd (2013) the Supreme Court of New South Wales broke with this precedent and recognised a shareholder's contractual right to a dividend.[7]  Cash dividends are the most common form of payment and are paid out in currency, usually via electronic funds transfer or a printed paper check. Such dividends are a form of investment income of the shareholder, usually treated as earned in the year they are paid (and not necessarily in the year a dividend was declared). For each share owned, a declared amount of money is distributed. Thus, if a person owns 100 shares and the cash dividend is 50 cents per share, the holder of the stock will be paid $50. Dividends paid are not classified as an expense, but rather a deduction of retained earnings. Dividends paid does not appear on an income statement, but does appear on the balance sheet.  Different classes of stocks have different priorities when it comes to dividend payments. Preferred stocks have priority claims on a company's income. A company must pay dividends on its preferred shares before distributing income to common share shareholders.  Stock or scrip dividends are those paid out in the form of additional shares of the issuing corporation, or another corporation (such as its subsidiary corporation). They are usually issued in proportion to shares owned (for example, for every 100 shares of stock owned, a 5% stock dividend will yield 5 extra shares).  Nothing tangible will be gained if the stock is split because the total number of shares increases, lowering the price of each share, without changing the total value of the shares held. (See also Stock dilution.)  Stock dividend distributions do not affect the market capitalization of a company.[8][9] Stock dividends are not includable in the gross income of the shareholder for US income tax purposes. Because the shares are issued for proceeds equal to the pre-existing market price of the shares; there is no negative dilution in the amount recoverable.[10][11]  Property dividends or dividends in specie (Latin for \"in kind\") are those paid out in the form of assets from the issuing corporation or another corporation, such as a subsidiary corporation. They are relatively rare and most frequently are securities of other companies owned by the issuer, however, they can take other forms, such as products and services.  Interim dividends are dividend payments made before a company's Annual General Meeting (AGM) and final financial statements. This declared dividend usually accompanies the company's interim financial statements.  Other dividends can be used in structured finance. Financial assets with known market value can be distributed as dividends; warrants are sometimes distributed in this way. For large companies with subsidiaries, dividends can take the form of shares in a subsidiary company. A common technique for \"spinning off\" a company from its parent is to distribute shares in the new company to the old company's shareholders. The new shares can then be traded independently.[citation needed]  A dividend payout ratio characterizes how much of a company's earnings (or its cash flow) is paid out in the form of dividends.  Most often, the payout ratio is calculated based on dividends per share and earnings per share:[12]  A payout ratio greater than 100% means the company paid out more in dividends for the year than it earned.  Since earnings are an accountancy measure, they do not necessarily closely correspond to the actual cash flow of the company. Hence another way to determine the safety of a dividend is to replace earnings in the payout ratio by free cash flow. Free cash flow is the business's operating cash flow minus its capital expenditures: this is a measure of how much incoming cash is \"free\" to pay out to stockholders and\/or to grow the business.  A free cash flow payout ratio greater than 100% means the company paid out more cash in dividends for the year than the \"free\" cash it took in.  A dividend that is declared must be approved by a company's board of directors before it is paid. For public companies in the US, four dates are relevant regarding dividends:[13] The position in the UK is very similar, except that the expression \"in-dividend date\" is not used.  Declaration date – the day the board of directors announces its intention to pay a dividend. On that day, a liability is created and the company records that liability on its books; it now owes the money to the shareholders.  In-dividend date – the last day, which is one trading day before the ex-dividend date, where shares are said to be cum dividend ('with [including] dividend'). That is, existing shareholders and anyone who buys the shares on this day will receive the dividend, and any shareholders who have sold the shares lose their right to the dividend. After this date the shares becomes ex dividend.  Ex-dividend date – the day on which shares bought and sold no longer come attached with the right to be paid the most recently declared dividend. In the United States and many European countries, it is typically one trading day before the record date. This is an important date for any company that has many shareholders, including those that trade on exchanges, to enable reconciliation of who is entitled to be paid the dividend. Existing shareholders will receive the dividend even if they sell the shares on or after that date, whereas anyone who bought the shares will not receive the dividend. It is relatively common for a share's price to decrease on the ex-dividend date by an amount roughly equal to the dividend being paid, which reflects the decrease in the company's assets resulting from the payment of the dividend.  Book closure date – when a company announces a dividend, it will also announce the date on which the company will temporarily close its books for share transfers, which is also usually the record date.  Record date – shareholders registered in the company's record as of the record date will be paid the dividend, while shareholders who are not registered as of this date will not receive the dividend. Registration in most countries is essentially automatic for shares purchased before the ex-dividend date.  Payment date – the day on which dividend cheques will actually be mailed to shareholders or the dividend amount credited to their bank account.  The dividend frequency is the number of dividend payments within a single business year.[14] The most usual dividend frequencies are yearly, semi-annually, quarterly and monthly. Some common dividend frequencies are quarterly in the US, semi-annually in Japan, UK and Australia and annually in Germany.  Some companies have dividend reinvestment plans, or DRIPs, not to be confused with scrips. DRIPs allow shareholders to use dividends to systematically buy small amounts of stock, usually with no commission and sometimes at a slight discount. In some cases, the shareholder might not need to pay taxes on these re-invested dividends, but in most cases they do. Utilizing a DRIP is a powerful investment tool because it takes advantage of both dollar cost averaging and compounding. Dollar cost averaging is the principle of investing a set amount of capital at recurring intervals. In this case, if the dividend is paid quarterly, then every quarter you are investing a set amount (the number of shares you own multiplied by the dividend per share). By doing this, you buy more shares when the price is low and fewer when the price is high. Additionally, the fractional shares that are purchased then begin paying dividends, compounding your investment and increasing the number of shares and total dividend earned each time a dividend distribution is made.  Governments may adopt policies on dividend distribution for the protection of shareholders and the preservation of company viability, as well as treating dividends as a potential source of revenue.[15]  Most countries impose a corporate tax on the profits made by a company. Many jurisdictions also impose a tax on dividends paid by a company to its shareholders (stockholders), but the tax treatment of a dividend income varies considerably between jurisdictions. The primary tax liability is that of the shareholder, although a tax obligation may also be imposed on the corporation in the form of a withholding tax. In some cases, the withholding tax may be the extent of the tax liability in relation to the dividend. A dividend tax is in addition to any tax imposed directly on the corporation on its profits.[16]  A dividend paid by a company is not an expense of the company.  Australia and New Zealand have a dividend imputation system, wherein companies can attach franking credits or imputation credits to dividends. These franking credits represent the tax paid by the company upon its pre-tax profits. One dollar of company tax paid generates one franking credit. Companies can attach any proportion of franking up to a maximum amount that is calculated from the prevailing company tax rate: for each dollar of dividend paid, the maximum level of franking is the company tax rate divided by (1 − company tax rate). At the current 30% rate, this works out at 0.30 of a credit per 70 cents of dividend, or 42.857 cents per dollar of dividend. The shareholders who are able to use them, apply these credits against their income tax bills at a rate of a dollar per credit, thereby effectively eliminating the double taxation of company profits.  In India, a company declaring or distributing dividends is required to pay a Corporate Dividend Tax in addition to the tax levied on their income. The dividend received by the shareholders is then exempt in their hands. Dividend-paying firms in India fell from 24 percent in 2001 to almost 19 percent in 2009 before rising to 19 percent in 2010.[17] However, dividend income over and above ₹1,000,000 attracts 10 percent dividend tax in the hands of the shareholder with effect from April 2016.[18] Since the Budget 2020–2021, DDT has been abolished. Now, the Indian government taxes dividend income in the hands of investor according to income tax slab rates.  The United States and Canada impose a lower tax rate on dividend income than ordinary income, on the assertion that company profits had already been taxed as corporate tax. In the United States, shareholders of corporations face double taxation – taxes on both corporate profits and taxes on distribution of dividends.  The rules in Part 23 of the Companies Act 2006 (sections 829–853) govern the payment of dividends to shareholders. The Act refers in this section to \"distribution\", covering any kind of distribution of a company's assets to its members (with some exceptions), \"whether in cash or otherwise\". A company is only able to make a distribution out of its accumulated, realised profits, \"so far as not previously utilised by distribution or capitalisation, less its accumulated, realised losses, so far as not previously written off in a reduction or reorganisation of capital duly made\".[19]  The United Kingdom government announced in 2018 that it was considering a review of the existing rules on dividend distribution following a consultation exercise on insolvency and corporate governance. The aim was to address concerns which had emerged where companies in financial distress were still able to distribute \"significant dividends\" to their shareholders.[15] A requirement has been proposed under which the largest companies would be required to publish a distribution policy statement covering dividend distribution.[20]  The law in England and Wales regarding dividend payment was clarified in 2018 by the England and Wales Court of Appeal in the case of Global Corporate Ltd v Hale [2018] EWCA Civ 2618. Certain payments made to a director\/shareholder had been treated by the High Court as quantum meruit payments to Hale in his capacity as a company director but the Appeal Court reversed this judgment and treated the payments as dividends. At the time of payment they had been treated as \"dividends\" payable from an anticipated profit. The company subsequently went into liquidation; an attempt to recharacterise the payments as payments for services rendered was held to be unlawful.[21]  After a stock goes ex-dividend (when a dividend has just been paid, so there is no anticipation of another imminent dividend payment), the stock price should drop.  To calculate the amount of the drop, the traditional method is to view the financial effects of the dividend from the perspective of the company. Since the company has paid say £x in dividends per share out of its cash account on the left hand side of the balance sheet, the equity account on the right side should decrease an equivalent amount. This means that a £x dividend should result in a £x drop in the share price.  A more accurate method of calculating the fall in price is to look at the share price and dividend from the after-tax perspective of a shareholder. The after-tax drop in the share price (or capital gain\/loss) should be equivalent to the after-tax dividend. For example, if the tax of capital gains Tcg is 35%, and the tax on dividends Td is 15%, then a £1 dividend is equivalent to £0.85 of after-tax money. To get the same financial benefit from a, the after-tax capital loss value should equal £0.85. The pre-tax capital loss would be ⁠£0.85\/1 − Tcg⁠ = ⁠£0.85\/1 − 0.35⁠ = ⁠£0.85\/0.65⁠ = £1.31. In this case, a dividend of £1 has led to a larger drop in the share price of £1.31, because the tax rate on capital losses is higher than the dividend tax rate. However in many countries the stock market is dominated by institutions which pay no additional tax on dividends received (as opposed to tax on overall profits). If that is the case, then the share price should fall by the full amount of the dividend.  Finally, security analysis that does not take dividends into account may mute the decline in share price, for example in the case of a price–earnings ratio target that does not back out cash; or amplify the decline when comparing different periods.  The effect of a dividend payment on share price is an important reason why it can sometimes be desirable to exercise an American option early.  Some[who?] believe company profits are best re-invested in the company with actions such as research and development, capital investment or expansion. Proponents of this view (and thus critics of dividends per se) suggest that an eagerness to return profits to shareholders may indicate the management having run out of good ideas for the future of the company. A counter-argument to this position came from Peter Lynch of Fidelity investments, who declared: \"One strong argument in favor of companies that pay dividends is that companies that don’t pay dividends have a sorry history of blowing the money on a string of stupid diworseifications\";[22] using his self-created term for diversification that results in worse effects, not better. Additionally, studies have demonstrated that companies that pay dividends have higher earnings growth, suggesting dividend payments may be evidence of confidence in earnings growth and sufficient profitability to fund future expansion.[23] Benjamin Graham and David Dodd wrote in Securities Analysis (1934): \"The prime purpose of a business corporation is to pay dividends to its owners. A successful company is one that can pay dividends regularly and presumably increase the rate as time goes on.\"[24]  Other studies indicate that dividend-paying stocks tend to offer superior long-term performance relative to the overall market at least in developed economies,[25][26] relative to a stock index such as the S&P 500[27][28] or Dow Jones Industrial Average[29] or relative to stocks that do not pay dividends.[28][30] Several explanations have been proposed for this outperformance such as dividends being associated with value stocks which are themselves associated with long-term outperformance;[31] being more durable in crashes or bear markets;[32][33]  being associated with profitable companies exhibiting high levels of free cashflow; and being associated with mature, unfashionable companies that are overlooked by many investors and thus an effective contrarian strategy.[34][35] Asset managers at Tweedy, Browne[36] and Capital Group[37] have suggested dividends are an effective measure of a given company's overall financial status.  Shareholders in companies that pay little or no cash dividends can potentially reap the benefit of the company's profits when they sell their shareholding, or when a company is wound down and all assets liquidated and distributed amongst shareholders. However, data from professor Jeremy Siegel found stocks that do not pay dividends tend to have worse long-term performance, as a group, than the general stock market and also perform worse than dividend-paying stocks.[35]  Taxation of dividends is often used as justification for retaining earnings, or for performing a stock buyback, in which the company buys back stock, thereby increasing the value of the stock left outstanding.  When dividends are paid, individual shareholders in many countries suffer from double taxation of those dividends:  In many countries, the tax rate on dividend income is lower than for other forms of income to compensate for tax paid at the corporate level.  A capital gain should not be confused with a dividend. Generally, a capital gain occurs where a capital asset is sold for an amount greater than the amount of its cost at the time the investment was purchased. A dividend is a parsing out a share of the profits, and is taxed at the dividend tax rate. If there is an increase of value of stock, and a shareholder chooses to sell the stock, the shareholder will pay a tax on capital gains (often taxed at a lower rate than ordinary income). If a holder of the stock chooses to not participate in the buyback, the price of the holder's shares could rise (as well as it could fall), but the tax on these gains is delayed until the sale of the shares.  Certain types of specialized investment companies (such as a REIT in the U.S.) allow the shareholder to partially or fully avoid double taxation of dividends.  Cooperative businesses may retain their earnings, or distribute part or all of them as dividends to their members. They distribute their dividends in proportion to their members' activity, instead of the value of members' shareholding. Therefore, co-op dividends are often treated as pre-tax expenses. In other words, local tax or accounting rules may treat a dividend as a form of customer rebate or a staff bonus to be deducted from turnover before profit (tax profit or operating profit) is calculated.  Consumers' cooperatives allocate dividends according to their members' trade with the co-op. For example, a credit union will pay a dividend to represent interest on a saver's deposit. A retail co-op store chain may return a percentage of a member's purchases from the co-op, in the form of cash, store credit, or equity. This type of dividend is sometimes known as a patronage dividend or patronage refund, as well as being informally named divi or divvy.[38][39][40]  Producer cooperatives, such as worker cooperatives, allocate dividends according to their members' contribution, such as the hours they worked or their salary.[41]  In real estate investment trusts and royalty trusts, the distributions paid often will be consistently greater than the company earnings. This can be sustainable because the accounting earnings do not recognize any increasing value of real estate holdings and resource reserves. If there is no economic increase in the value of the company's assets then the excess distribution (or dividend) will be a return of capital and the book value of the company will have shrunk by an equal amount. This may result in capital gains which may be taxed differently from dividends representing distribution of earnings.  The distribution of profits by other forms of mutual organization also varies from that of joint-stock companies, though may not take the form of a dividend.  In the case of mutual insurance, for example, in the United States, a distribution of profits to holders of participating life policies is called a dividend. These profits are generated by the investment returns of the insurer's general account, in which premiums are invested and from which claims are paid.[42] The participating dividend may be used to decrease premiums, or to increase the cash value of the policy.[43] Some life policies pay nonparticipating dividends. As a contrasting example, in the United Kingdom, the surrender value of a with-profits policy is increased by a bonus, which also serves the purpose of distributing profits. Life insurance dividends and bonuses, while typical of mutual insurance, are also paid by some joint stock insurers.  Insurance dividend payments are not restricted to life policies. For example, general insurer State Farm Mutual Automobile Insurance Company can distribute dividends to its vehicle insurance policyholders.[44] "},"meta":{},"created_at":"2025-03-22T14:25:42.291408Z","updated_at":"2025-03-22T14:25:42.291408Z","inner_id":84,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":93,"annotations":[{"id":93,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.322899Z","updated_at":"2025-03-22T14:25:42.322899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"fbb38b7a-cd31-4478-8ad6-4c180308c51a","import_id":null,"last_action":null,"bulk_created":false,"task":93,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"The derivatives market is the financial market for derivatives - financial instruments like futures contracts or options - which are derived from other forms of assets.  The market can be divided into two, that for exchange-traded derivatives and that for over-the-counter derivatives. The legal nature of these products is very different, as well as the way they are traded, though many market participants are active in both. The derivatives market in Europe has a notional amount of €660 trillion.[1]  Participants in a derivative market can be segregated into four sets based on their trading motives.[2]  Futures exchanges, such as Euronext.liffe and the Chicago Mercantile Exchange, trade in standardized derivative contracts. These are options contracts, swaps contracts and futures contracts on a whole range of underlying products. The members of the exchange hold positions in these contracts with the exchange, who acts as central counterparty. When one party goes long (buys a futures contract), another goes short (sells). When a new contract is introduced, the total position in the contract is zero. Therefore, the sum of all the long positions must be equal to the sum of all the short positions. In other words, risk is transferred from one party to another is a type of a zero sum game. The total notional amount of all the outstanding positions at the end of June 2004 stood at $53 trillion (source: Bank for International Settlements (BIS): [1]). That figure grew to $81 trillion by the end of March 2008 (source: BIS [2])  Tailor-made derivatives, not traded on a futures exchange are traded on over-the-counter markets, also known as the OTC market. These consist of investment banks with traders who make markets in these derivatives, and clients such as hedge funds, commercial banks, government-sponsored enterprises, etc. Products that are always traded over-the-counter are swaps, forward rate agreements, forward contracts, credit derivatives, accumulators etc. The total notional amount of all the outstanding positions at the end of June 2004 stood at $220 trillion (source: BIS: [3]). By the end of 2007 this figure had risen to $596 trillion and in 2009 it stood at $615 trillion (source: BIS: [4])  OTC Markets are generally separated into two key segments: the customer market and the interdealer market. Customers almost exclusively trade through dealers because of the high search and transaction costs. Dealers are large institutions that arrange transactions for their customers, utilizing their specialized knowledge, expertise, and access to capital. In order to hedge the risks incurred by transacting with customers, dealers turn to the interdealer market, or the exchange-traded markets. Dealers can also trade for themselves or act as market makers in the OTC market (source: Federal Reserve Bank of Chicago [5]).  US: Figures below are from the second quarter of 2008 [6] Archived 2007-12-26 at the Wayback Machine  Positions in the OTC derivatives market have increased at a rapid pace since the last triennial survey  was undertaken in 2004. Notional amounts outstanding of such instruments totalled $516 trillion at the end of June 2007 (according to the Bank for International Settlements [7]), 135% higher than the level recorded in the 2004 survey (Graph 4). This corresponds to an annualised compound rate of growth of 34%, which is higher than the approximatively 25% average annual rate of increase since positions in OTC derivatives were first surveyed by the BIS in 1995. Notional amounts outstanding provide useful information on  the structure of the OTC derivatives market but should not be interpreted as a measure of the riskiness of these positions. Gross market values, which represent the cost of replacing all open contracts at the prevailing market prices, have increased by 74% since 2004, to $11 trillion at the end of June 2007. [8] (page 28)  Notional amounts outstanding as of December 2012 are $632 trillion as per recent survey.[3]  The derivative markets played an important role in the financial crisis of 2007–2008. Credit default swaps (CDSs), financial instruments traded on the over the counter derivatives markets, and mortgage-backed securities (MBSs), a type of securitized debt were notable contributors.[4][5] The leveraged operations are said to have generated an \"irrational appeal\" for risk taking, and the lack of clearing obligations also appeared as very damaging for the balance of the market. More specifically, interdealer collateral management and risk management systems proved to be inadequate. The G-20's proposals for financial markets reform all stress these points, and suggest: "},"meta":{},"created_at":"2025-03-22T14:25:42.291408Z","updated_at":"2025-03-22T14:25:42.291408Z","inner_id":85,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":94,"annotations":[{"id":94,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.322899Z","updated_at":"2025-03-22T14:25:42.322899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"60747d3f-d5f5-46c9-8124-daa20ce65752","import_id":null,"last_action":null,"bulk_created":false,"task":94,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A short film is a film with a low running time. The Academy of Motion Picture Arts and Sciences (AMPAS) defines a short film as \"an original motion picture that has a running time of not more than 40 minutes including all credits\".[1] Other film organizations may use different definitions, however; the Academy of Canadian Cinema and Television, for example, currently defines a short film as 45 minutes or less in the case of documentaries, and 59 minutes or less in the case of scripted narrative films.[2]  In the United States, short films were generally termed short subjects from the 1920s into the 1970s when confined to two 35 mm reels or less, and featurettes for a film of three or four reels. \"Short\" was an abbreviation for either term.  The increasingly rare industry term \"short subject\" carries more of an assumption that the film is shown as part of a presentation along with a feature film. Short films are often screened at local, national, or international film festivals and made by independent filmmakers with either a low budget or no budget at all. They are usually funded by one or more film grants, nonprofit organizations, sponsors, or personal funds. Short films are generally used for industry experience and as a platform to showcase talent to secure funding for future projects from private investors, a production company, or film studios. They can also be released with feature films, and can also be included as bonus features on some home video releases.  All films in the beginning of cinema were very short, sometimes running only a minute or less. It was not until the 1910s when films started to get longer than about ten minutes. The first set of films were presented in 1894 and it was through Thomas Edison's device called a kinetoscope. It was made for individual viewing only. Comedy short films were produced in large numbers compared to lengthy features such as D. W. Griffith's 1915 The Birth of a Nation.[citation needed] By the 1920s, a ticket purchased a varied program including a feature and several supporting works from categories such as second feature, short comedy, 4–10 minute cartoon, travelogue, and newsreel.  Short comedies were especially common, and typically came in a serial or series (such as the Our Gang movies, or the many outings of Charlie Chaplin's Little Tramp character).  Animated cartoons came principally as short subjects. Virtually all major film production companies had units assigned to develop and produce shorts, and many companies, especially in the silent and very early sound era, produced mostly or only short subjects.  In the 1930s, the distribution system changed in many countries, owing to the Great Depression. Instead of the cinema owner assembling a program of their own choice, the studios sold a package centered on a main and supporting feature, a cartoon and little else. With the rise of the double feature, two-reel shorts went into decline as a commercial category.  The year 1938 proved to be a turning point in the history of film comedies. Hal Roach, for example, had discontinued all short-subject production except Our Gang, which he finally sold to Metro-Goldwyn-Mayer in 1938. The Vitaphone studio, owned by Warner Bros., discontinued its own line of two-reel comedies in 1938; Educational Pictures did as much that same year, owing to its president Earle W. Hammons unsuccessfully entering the feature-film field. With these major comedy producers out of the running, Columbia Pictures actually expanded its own operations and launched a second two-reel-comedy unit in 1938. Columbia and RKO Radio Pictures kept making two-reel comedies into the 1950s.  Theater managers found it easier and more convenient to fit shorter, one-reel (10-minute) subjects into their double-feature programs. In the live-action field, humorist Robert Benchley had been making short comedies since the dawn of sound; his various series for Fox, Vitaphone, MGM, and Paramount ran from 1928 to 1944. MGM's Pete Smith Specialties had been a standard \"added attraction\" in moviehouse programming since 1935 and lasted through 1955. RKO's Flicker Flashbacks revivals of silent films ran from 1943 to 1956, and Warner Bros.' Joe McDoakes comedies became a regular series in 1946 and lasted until 1956. By and large, however, the movies' one-reel subject of choice was the animated cartoon,  produced by Walt Disney, Warner Bros., MGM, Paramount, Walter Lantz, Columbia, and Terrytoons.  One of the movies' oldest short-subject formats was the adventure serial, first established in 1912. A serial generally ran for 12 to 15 chapters, 15 to 20 minutes each. Every episode ended with the hero or heroine trapped in a life-threatening situation; audiences would have to return the following week to see the outcome. These \"chapter plays\" remained popular through the 1950s, although both Columbia and Republic Pictures were now making them as cheaply as possible, reusing action highlights from older serials and connecting them with a few new scenes showing identically dressed actors. Even after Republic quit making serials in 1955 and Columbia stopped in 1956, faithful audiences supported them and the studios re-released older serials through the mid-1960s. The 1964 revival of Columbia's Batman serial resulted in a media frenzy, spurring a new Batman TV series and a wave of Batman merchandise.  With the rise of television, the commercial live-action short was virtually dead; most studios canceled their live-action series in 1956. Only The Three Stooges continued making two-reel comedies; their last was released in 1959. Short films had become a medium for student, independent, and specialty work.  Cartoon shorts had a longer life, due in part to the implementation of lower-cost limited animation techniques and the rise of television animation, which allowed shorts to have both theatrical runs and a syndication afterlife. Warner Bros., one of the most prolific of the golden era, underwent several reorganizations in the 1960s before exiting the short film business in 1969 (by which point the shorts had been in televised reruns for years). MGM continued Tom and Jerry (first with a series of poorly-received Eastern European shorts by Gene Deitch, then a better-received run by Warner Bros. alumnus Chuck Jones) until 1967, and Woody Woodpecker lasted to 1972; the creative team behind MGM's 1940s and 1950s cartoons formed Hanna-Barbera Productions in 1957, mainly focusing on television. The Pink Panther was the last regular theatrical cartoon short series, having begun in 1964 (and thus having spent its entire existence in the limited animation era) and ended in 1980. By the 1960s, the market for animated shorts had largely shifted to television, with existing theatrical shorts being syndicated to television.  A few animated shorts continue within the mainstream commercial distribution. For instance, Pixar has screened a short along with each of its feature films during its initial theatrical run since 1995 (producing shorts permanently since 2001).[3] Since Disney acquired Pixar in 2006, Disney has also produced animated shorts since 2007 with the Goofy short How to Hook Up Your Home Theater and produced a series of live-action ones featuring The Muppets for viewing on YouTube as viral videos to promote the 2011 movie of the same name.  In 2009 the horror short film, No Through Road, that would go viral was released, creating analog horror. The short film would spark 3 sequels, creating No Through Road (web series)  DreamWorks Animation often produces a short sequel to include in the special edition video releases of major features, and are typical of a sufficient length to be broadcast as a TV special, a few films from the studio have added theatrical shorts as well.[citation needed] Warner Bros. often includes old shorts from its considerable library, connected only thematically, on the DVD releases of classic WB movies. From 2010–2012, Warner Bros. also released new Looney Tunes shorts before family films.[citation needed]  Shorts International and Magnolia Pictures organize an annual release of Academy Award-nominated short films in theatres across the US, UK, Canada and Mexico throughout February and March.[4]  Shorts are occasionally broadcast as filler when a feature film or other work does not fit the standard broadcast schedule. ShortsTV was the first television channel dedicated to short films.[citation needed]  However, short films generally rely on film festival exhibition to reach an audience. Such movies can also be distributed via the Internet. Certain websites which encourage the submission of user-created short films, such as YouTube and Vimeo, have attracted large communities of artists and viewers.[citation needed] Sites like Omeleto, FILMSshort, Short of the Week, Short Films Matter, Short Central and some apps showcase curated shorts.[5]  Short films are a typical first stage for new filmmakers, but professional actors and crews often still choose to create short films as an alternative form of expression.[citation needed] Amateur filmmaking has grown in popularity as equipment has become more accessible.  The lower production costs of short films often mean that short films can cover alternative subject matter as compared to higher budget feature films. Similarly, unconventional filmmaking techniques such as Pixilation or narratives that are told without dialogue, are more often seen in short films than features.  Tropfest claims to be the world's largest short film festival. Tropfest now takes place in Australia (its birthplace), Arabia, the US and elsewhere. Originating in 1993, Tropfest is often credited as being at least partially responsible for the recent popularity of short films internationally.[citation needed] Also Couch Fest Films, part of Shnit Worldwide Filmfestival, claimed to be the world's largest single-day short film festival.[6]  Among the oldest film festivals dedicated to short films are Clermont-Ferrand International Short Film Festival, France (since 1979), Tampere Film Festival, Finland (since 1969) and International Short Film Festival Oberhausen, Germany (since 1954). All of them are still considered the most important short film festival in the world to date. "},"meta":{},"created_at":"2025-03-22T14:25:42.291408Z","updated_at":"2025-03-22T14:25:42.291408Z","inner_id":86,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":95,"annotations":[{"id":95,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.322899Z","updated_at":"2025-03-22T14:25:42.322899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"e055da21-c2cb-49ec-837a-7e7124ef5f28","import_id":null,"last_action":null,"bulk_created":false,"task":95,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Financial regulation is a broad set of policies that apply to the financial sector in most jurisdictions, justified by two main features of finance: systemic risk, which implies that the failure of financial firms involves public interest considerations; and information asymmetry, which justifies curbs on freedom of contract in selected areas of financial services, particularly those that involve retail clients and\/or Principal–agent problems. An integral part of financial regulation is the supervision of designated financial firms and markets by specialized authorities such as securities commissions and bank supervisors.   In some jurisdictions, certain aspects of financial supervision are delegated to self-regulatory organizations. Financial regulation forms one of three legal categories which constitutes the content of financial law, the other two being market practices and case law.[1]  In the early modern period, the Dutch were the pioneers in financial regulation.[2] The first recorded ban (regulation) on short selling was enacted by the Dutch authorities as early as 1610.  The objectives of financial regulators are usually:[3]  Acts empower organizations, government or non-government, to monitor activities and enforce actions.[4] There are various setups and combinations in place for the financial regulatory structure around the globe.[5][6]  Exchange acts ensure that trading on the floor of exchanges is conducted in a proper manner. Most prominent the pricing process, execution and settlement of  trades, direct and efficient trade monitoring.[7][8]  Financial regulators ensure that listed companies and market participants comply with various regulations under the trading acts. The trading acts demands that listed companies publish regular financial reports, ad hoc notifications or directors' dealings. Whereas market participants are required to publish major shareholder notifications. The objective of monitoring compliance by listed companies with their disclosure requirements is to ensure that investors have access to essential and adequate information for making an informed assessment of listed companies and their securities.[9][10][11]  Asset management supervision or investment acts ensures the frictionless operation of those vehicles.[12]  Banking acts lay down rules for banks which they have to observe when they are being established and when they are carrying on their business. These rules are designed to prevent unwelcome developments that might disrupt the smooth functioning of the banking system. Thus ensuring a strong and efficient banking system.[13][14] "},"meta":{},"created_at":"2025-03-22T14:25:42.291408Z","updated_at":"2025-03-22T14:25:42.291408Z","inner_id":87,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":96,"annotations":[{"id":96,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.322899Z","updated_at":"2025-03-22T14:25:42.322899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"8b6bfe63-e506-4d54-9c07-7c28bb64bd64","import_id":null,"last_action":null,"bulk_created":false,"task":96,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  The cinema of Italy (Italian: cinema italiano, pronounced [ˈtʃiːnema itaˈljaːno]) comprises the films made within Italy or by Italian directors. Since its beginning, Italian cinema has influenced film movements worldwide. Italy is one of the birthplaces of art cinema and the stylistic aspect of film has been one of the most important factors in the history of Italian film.[5][6] As of 2018, Italian films have won 14 Academy Awards for Best Foreign Language Film (the most of any country) as well as 12 Palmes d'Or (the second-most of any country), one Academy Award for Best Picture and many Golden Lions and Golden Bears.  The history of Italian cinema began a few months after the Lumière brothers began motion picture exhibitions.[7][8] The first Italian director is considered to be Vittorio Calcina, a collaborator of the Lumière Brothers, who filmed Pope Leo XIII in 1896. The first films date back to 1896 and were made in the main cities of the Italian peninsula.[7][8] These brief experiments immediately met the curiosity of the popular class, encouraging operators to produce new films until they laid the foundations for the birth of a true film industry.[7][8] In the early years of the 20th century, silent cinema developed, bringing numerous Italian stars to the forefront until the end of World War I.[9] In the early 1900s, artistic and epic films such as Otello (1906), The Last Days of Pompeii (1908), L'Inferno (1911), Quo Vadis (1913), and Cabiria (1914), were made as adaptations of books or stage plays. Italian filmmakers were using complex set designs, lavish costumes, and record budgets, to produce pioneering films.  The oldest European avant-garde cinema movement, Italian futurism, took place in the late 1910s.[10] After a period of decline in the 1920s, the Italian film industry was revitalized in the 1930s with the arrival of sound film. A popular Italian genre during this period, the Telefoni Bianchi, consisted of comedies with glamorous backgrounds. Calligrafismo was instead in sharp contrast to Telefoni Bianchi-American style comedies and is rather artistic, highly formalistic, expressive in complexity and deals mainly with contemporary literary material. While Italy's Fascist government provided financial support for the nation's film industry, notably the construction of the Cinecittà studios (the largest film studio in Europe), it also engaged in censorship, and thus many Italian films produced in the late 1930s were propaganda films. A new era took place at the end of World War II with the birth of the influential Italian neorealist movement, reaching a vast consensus of audiences and critics throughout the post-war period,[11] and which launched the directorial careers of Luchino Visconti, Roberto Rossellini, and Vittorio De Sica. Neorealism declined in the late 1950s in favour of lighter films, such as those of the Commedia all'italiana genre and important directors like Federico Fellini and Michelangelo Antonioni. Actresses such as Sophia Loren, Giulietta Masina and Gina Lollobrigida achieved international stardom during this period.[12]  From the mid-1950s to the end of the 1970s, Commedia all'italiana and many other genres arose due to auteur cinema, and Italian cinema reached a position of great prestige both nationally and abroad.[13][14] The Spaghetti Western achieved popularity in the mid-1960s, peaking with Sergio Leone's Dollars Trilogy, which featured enigmatic scores by composer Ennio Morricone, which have become popular culture icons of the Western genre. Erotic Italian thrillers, or giallo, produced by directors such as Mario Bava and Dario Argento in the 1970s, influenced the horror genre worldwide. Since the 1980s, due to multiple factors, Italian production has gone through a crisis that has not prevented the production of quality films in the 1990s and into the new millennium, thanks to a revival of Italian cinema, awarded and appreciated all over the world.[15][16][17] During the 1980s and 1990s, directors such as Ermanno Olmi, Bernardo Bertolucci, Giuseppe Tornatore, Gabriele Salvatores and Roberto Benigni brought critical acclaim back to Italian cinema,[12] while the most popular directors of the 2000s and 2010s were Matteo Garrone, Paolo Sorrentino, Marco Bellocchio, Nanni Moretti and Marco Tullio Giordana.[18]  The country is also famed for its prestigious Venice Film Festival, the oldest film festival in the world, held annually since 1932 and awarding the Golden Lion;[19] In 2008 the Venice Days (\"Giornate degli Autori\"), a section held in parallel to the Venice Film Festival, has produced in collaboration with Cinecittà studios and the Ministry of Cultural Heritage a list of a 100 films that have changed the collective memory of the country between 1942 and 1978: the \"100 Italian films to be saved\".   The David di Donatello Awards are one of the most prestigious awards at national level.[20]  Presented by the Accademia del Cinema Italiano in the Cinecittà studios, during the awards ceremony, the winners are given a miniature reproduction of the famous statue. The finalist candidates for the award, as per tradition, are first received at the Quirinal Palace by the President of Italy. The event is the Italian equivalent of the American Academy Awards.  The history of Italian cinema began a few months after the French Lumière brothers, who made the first public screening of a film on 28 December 1895, an event considered the birth of cinema, began motion picture exhibitions.[7][8] The first Italian director is considered to be Vittorio Calcina, a collaborator of the Lumière Brothers, who filmed Pope Leo XIII on 26 February 1896 in the short film Sua Santità papa Leone XIII (\"His Holiness Pope Leo XIII\").[21] He then became the official photographer of the House of Savoy,[22] the Italian ruling dynasty from 1861 to 1946. In this role he filmed the first Italian film, Sua Maestà il Re Umberto e Sua Maestà la Regina Margherita a passeggio per il parco a Monza (\"His Majesty the King Umberto and Her Majesty the Queen Margherita strolling through the Monza Park\"), believed to have been lost until it was rediscovered by the Cineteca Nazionale in 1979.[23]  The Lumière brothers commenced public screenings in Italy in 1896 starting in March, in Rome and Milan; in April in Naples, Salerno and Bari; in June in Livorno; in August in Bergamo, Bologna and Ravenna; in October in Ancona;[24] and in December in Turin, Pescara and Reggio Calabria.[25] Not long before, in 1895, Filoteo Alberini patented his \"kinetograph\", a shooting and projecting device not unlike that of the Lumières brothers.[12][26]  Italian Lumière trainees produced short films documenting everyday life and comic strips in the late 1890s and early 1900s. Before long, other pioneers made their way. Italo Pacchioni, Arturo Ambrosio, Giovanni Vitrotti and Roberto Omegna were also active. The success of the short films was immediate. The cinema fascinated with its ability to show distant geographic realities with unprecedented precision and, vice versa, to immortalize everyday moments. Sporting events, local events, intense road traffic, the arrival of a train, visits by famous people, but also natural disasters and calamities are filmed.  Titles of the time include, Arrivo del treno alla Stazione di Milano (\"Arrival of the train at Milan station\") (1896), La battaglia di neve (\"The snow battle\") (1896), La gabbia dei matti (\"The madmen's cage\") (1896), Ballo in famiglia (\"Family dance\") (1896), Il finto storpio al Castello Sforzesco (\"The fake cripple at the Castello Sforzesco\") (1896) and La Fiera di Porta Genova (\"The fair of Porta Genova\") (1898), all shot by Italo Pacchioni, who was also the inventor of a camera and projector, inspired by the cinematograph of Lumière brothers, kept at the Cineteca Italiana in Milan.[27]  If the interest of the masses were enthusiastic, the technological novelty would likely be snubbed, at least at the beginning, by intellectuals and the press.[28] Despite initial doubt, in just two years, cinema climbs the hierarchy of society, intriguing the wealthier classes. On 28 January 1897, prince Victor Emmanuel and princess Elena of Montenegro attended a screening organized by Vittorio Calcina, in a room of the Pitti Palace in Florence.[29] Interested in experimenting with the new medium, they were filmed in S.A.R. il Principe di Napoli e la Principessa Elena visitano il battistero di S. Giovanni a Firenze  (\"Their real heights the Prince of Naples and Princess Elena visit the baptistery of Saint John in Florence\") and on the day of their wedding in Dimostrazione popolare alle LL. AA. i Principi sposi (al Pantheon – Roma) (\"Popular demonstration at the their heights the princes spouses (at the Pantheon – Rome)\").[30][31]  In the early years of the 20th century, the phenomenon of itinerant cinemas developed throughout Italy, providing literacy of the visual medium.[32] This innovative form of spectacle ran out, in a short time, a number of optical attractions such as magic lanterns, cinematographers, stereoscopes, panoramas and dioramas that had fueled the European imagination and favoured the circulation of a common market for images.[33] The nascent Italian cinema, therefore, is still linked to the traditional shows of the commedia dell'arte or to those typical of circus folklore. Public screenings take place in the streets, in cafes or in variety theatres in the presence of a swindler who has the task of promoting and enriching the story.[34]  Between 1903 and 1909 the itinerant cinema Italian film was quieting, until then considered as a freak phenomenon, took on consistency assuming the characteristics of an authentic industry, led by four major organizations: Titanus (originally Monopolio Lombardo), the first italian film production company[35] and the largest and probably the most famous film house in Italy[36]  founded by Gustavo Lombardo at Naples in 1904, Cines, based in Rome; and the Turin-based companies Ambrosio Film and Itala Film.[25] Other companies soon followed in Milan, and these early companies quickly attained a respectable production quality and were able to market their products both within Italy and abroad. Early Italian films typically consisted of adaptations of books or stage plays, such as Mario Caserini's Otello (1906) and Arturo Ambrosio's 1908 The Last Days of Pompeii, an adaptation of the homonymous novel by Edward Bulwer-Lytton. Also popular during this period were films about historical figures, such as Caserini's Beatrice Cenci (1909) and Ugo Falena's Lucrezia Borgia (1910).  In 1905, Cines inaugurated the genre of the historical film, which in this decade gave a great fortune to many Italian filmmakers. One of the first of these films was La presa di Roma (1905), lasting 10 minutes, and made by Filoteo Alberini. The operator employs for the first time actors of theatrical origin, exploiting the historical argument in a popular and pedagogical key. The film, assimilating Manzoni's lesson of making historical fiction plausible, reconstructs the Capture of Rome on 20 September 1870.  The discovery of the spectacular potential of the cinematographic medium favoured the development of a cinema with great ambitions, capable of incorporating all the cultural and historical suggestions of the country.[25] Education is an inexhaustible source of ideas, ideas that are easily assimilated not only by a cultured public but also by the masses.[25] Dozens of characters from texts make their appearance on the big screen such as the Count of Monte Cristo, Giordano Bruno, Judith beheading Holofernes, Francesca da Rimini, Lorenzino de' Medici, Rigoletto, Count Ugolino and others.[25] From an iconographic point of view, the main references are the great Renaissance and neoclassical artists, as well as symbolists and popular illustrations.[37]  In the 1910s, the Italian film industry developed rapidly.[38] In 1912, the year of the greatest expansion, 569 films were produced in Turin, 420 in Rome and 120 in Milan.[39] Popular early Italian actors included Emilio Ghione, Alberto Collo, Bartolomeo Pagano, Amleto Novelli, Lyda Borelli, Ida Carloni Talli, Lidia Quaranta and Maria Jacobini.[12]  Lost in the Dark, silent drama film directed by Nino Martoglio and produced in 1914, documented life in the slums of Naples, and is considered a precursor to the Italian neorealism movement of the 1940s and 1950s.[12] The only surviving copy of this film was destroyed by Nazi German forces during the World War II.[40] This film is based on a 1901 play of the same title by Roberto Bracco.  In the three years leading up to World War I, as production consolidates, mythological, comedy and drama films are exported all over the world. In the meantime, in the actor's field, the phenomenon of stardom was born which for a few years will experience unstoppable success. With the end of the decade, Rome definitively established itself as the main production center; this will remain, despite the crises that will periodically shake the industry, right up to the present day.  The archetypes of this film genre were The Last Days of Pompeii (1908), by Arturo Ambrosio and Luigi Maggi and Nero (1909), by Maggi himself and Arrigo Frusta. This last film was inspired by the work of Pietro Cossa who is iconographically based on the etchings of Bartolomeo Pinelli, neoclassicism and the show Nero, or the Destruction of Rome represented by the Barnum circus.[41] Followed by Marin Faliero, Doge of Venice (1909), by Giuseppe De Liguoro, Otello (1909) by Yambo and L'Odissea (1911), by Bertolini, Padovan and De Liguoro.  L'Inferno, produced by Milano Films in 1911, even before being an adaptation of Dante's canticle, was a cinematic translation of Gustave Doré's engravings that experiments with the integration of optical effects and stage action, and it was the first Italian feature film ever made.[42] The Last Days of Pompeii (1913), by Eleuterio Rodolfi, used innovative special effects.  Enrico Guazzoni's 1913 film Quo Vadis was one of the first blockbusters in the history of cinema, using thousands of extras and a lavish set design.[43] The international success of the film marked the maturation of the genre and allows Guazzoni to make increasingly spectacular films such as Antony and Cleopatra (1913) and Julius Caesar (1914). Giovanni Pastrone's 1914 film Cabiria was an even larger production, requiring two years and a record budget to produce, it was the first epic film ever made and it is considered the most famous Italian silent film.[38][44] It was also the first film in history to be shown in the White House.[45][46][47] After Guazzoni came Emilio Ghione, Febo Mari, Carmine Gallone, Giulio Antamoro and many others who contributed to the expansion of the genre.  After the great success of Cabiria, with the changing tastes of the public and the first signs of the industrial crisis, the genre began to show signs of crisis. Pastrone's plan to adapt the Bible with thousands of extras remained unfulfilled. Antamoro's Christus (1916) and Guazzoni's The Crusaders (1918) remained notable for their iconographic complexity but offered no substantial novelties. Despite sporadic attempts to reconnect with the grandeur of the past, the trend of historical blockbusters was interrupted at the beginning of the 1920s.  In the first and second decade of the 20th century came a prolific film production aimed at investigative and mystery content, supported by well-assorted Italian and foreign literature that favours its transposition into film. What would later take on the synthesis of the giallo, in fact, was produced and distributed at the dawn of Italian cinema. The most prolific production houses in the 1910s were Cines, Ambrosio Film, Itala Film, Aquila Films, Milano Films and many others, while titles such as Il delitto del magistrato (1907), Il cadavere misterioso (1908), Il piccolo Sherlock Holmes (1909), L'abisso (1910) and Alibi atroce (1910), breached the imagination of the first cinema users who demanded a greater offer. The popular consensus is remarkable to the point of encouraging the film industry to invest further production resources since these films are also distributed on the French and Anglo-Saxon markets. Thus directors among the most prolific in this field such as Oreste Mentasti, Luigi Maggi, Arrigo Frusta and Ubaldo Maria Del Colle, together with many others less known, direct several dozen films where classic narrative elements of the silent proto-giallo (mystery, crime, investigation investigative and final twist) constitute the structural aspects of cinematic representation.  Elvira Notari, the first female director ever in Italy and one of the premieres in the history of world cinema, directed Carmela, la sartina di Montesanto (1916). While in Palermo, Lucarelli Film produced La cassaforte n. 8 (1914) and Ipnotismo (1914), the Azzurri Film La regina della notte (1915), the Lumen Film Il romanzo fantastico del Dr. Mercanton o il giustiziere invisibile (1915) and Profumo mortale (1915), all films ascribable to the proto-giallo that multiplied in the following decades, becoming preparatory to the subsequent birth of the giallo.  Between 1913 and 1920 there was the rise, development and decline of the phenomenon of cinematographic stardom, born with the release of Ma l'amor mio non muore (1913), by Mario Caserini. The film had great success with the public and encoded the setting and aesthetics of female stardom. Within just a few years, Eleonora Duse, Pina Menichelli, Rina De Liguoro, Leda Gys, Hesperia, Vittoria Lepanto, Mary Cleo Tarlarini and Italia Almirante Manzini established themselves.  Films such as Fior di male (1914), by Carmine Gallone, Il fuoco (1915), by Giovanni Pastrone, Rapsodia satanica (1917), by Nino Oxilia and Cenere (1917), by Febo Mari, changed the national costume, imposing canons of beauty, role models and objects of desire.[48] These models, strongly stylized according to the cultural and artistic trends of the time, moved away from naturalism in favor of melodramatic acting, pictorial gesture and theatrical pose; all favored by the incessant use of close-up which focuses the attention on the expressiveness of the actress.[49]  The most successful comedian in Italy was André Deed, better known in Italy as Cretinetti, star of comic short film for Itala Film. Its success paved the way for Marcel Fabre (Robinet), Ernesto Vaser (Fricot) and many others. The only actor of a certain substance, however, was Ferdinand Guillaume, who became famous with the stage name of Polidor.[50]  The historical interest of these films lay in their ability to reveal the aspirations and fears of a petty-bourgeois society torn between the desire for affirmation and the uncertainties of the present. It was significant that the protagonists of Italian comedians never place themselves in open contrast with society or embody the desire for social revenge (as happens for example with Charlie Chaplin), but rather tried to integrate into a strongly desired world.[51]  Italian futurist cinema was the oldest movement of European avant-garde cinema.[10] Italian futurism, an artistic and social movement, impacted the Italian film industry from 1916 to 1919.[52] It influenced Russian Futurist cinema[53] and German Expressionist cinema.[54] Its cultural importance was considerable and influenced all subsequent avant-gardes, as well as some authors of narrative cinema; its echo expands to the dreamlike visions of some films by Alfred Hitchcock.[55]  Futurism emphasized dynamism, speed, technology, youth, violence, and objects such as the car, the airplane, and the industrial city. Its key figures were the Italians Filippo Tommaso Marinetti, Umberto Boccioni, Carlo Carrà, Fortunato Depero, Gino Severini, Giacomo Balla, and Luigi Russolo. It glorified modernity and aimed to liberate Italy from the weight of its past.[56]  The 1916 Manifesto of Futuristic Cinematography was signed by Filippo Tommaso Marinetti, Armando Ginna, Bruno Corra, Giacomo Balla and others. To the Futurists, cinema was an ideal art form, being a fresh medium, and able to be manipulated by speed, special effects and editing. Most of the futuristic-themed films of this period have been lost, but critics cite Thaïs (1917) by Anton Giulio Bragaglia as one of the most influential, serving as the main inspiration for German Expressionist cinema in the following decade.  The Italian film industry struggled against rising foreign competition in the years following World War I.[12] Several major studios, among them Cines and Ambrosio, formed the Unione Cinematografica Italiana to coordinate a national strategy for film production. This effort was largely unsuccessful, however, due to a wide disconnect between production and exhibition (some movies weren't released until several years after they were produced).[57]  With the end of World War I, Italian cinema went through a period of crisis due to many factors such as production disorganization, increased costs, technological backwardness, loss of foreign markets and inability to cope with international competition, in particular with that of Hollywood.[58] The main causes included the lack of a generational change with a production still dominated by filmmakers and producers of literary training, unable to face the challenges of modernity. The first half of the 1920s marked a sharp decrease in production; from 350 films produced in 1921 to 60 in 1924.[59]  Literature and theatre are still the preferred narrative sources. The feuilletons resist, mostly taken from classical or popular texts and directed by specialists such as Roberto Roberti and the religious blockbusters of Giulio Antamoro. On the basis of the latest generation of divas, a sentimental cinema for women spread, centred on figures on the margins of society who, instead of struggling to emancipate themselves (as happens in contemporary Hollywood cinema), go through an authentic ordeal in order to preserve their own virtue. Protest and rebellion by the female protagonists are out of the question. It is a strongly conservative cinema, tied to social rules upset by the war and in the process of dissolution throughout Europe. An exemplary case is that of A Woman's Story (1920) by Eugenio Perego, which uses an original narrative construction to propose a 19th-century morality with melodramatic tones.[60]  A particular genre is that of a realist setting, due to the work of the first female director of Italian cinema, Elvira Notari, who directs numerous films influenced by popular theatre and taken from famous dramas, Neapolitan songs, appendix novels or inspired by facts of chronicle.[61] Another film with a realist setting is Lost in the Dark (1914) by director Nino Martoglio, considered by critics as a prime example of neorealist cinema.[62]  The revival of Italian cinema took place at the end of the decade with the production of larger-scale films. During this period, a group of intellectuals close to the fortnightly cinematografo led by Alessandro Blasetti launched a program that was as simple as it was ambitious. Aware of the Italian cultural backwardness, they decided to break all ties with the previous tradition through a rediscovery of the peasant world, hitherto practically absent in Italian cinema. Sun (1929) by Alessandro Blasetti shows the evident influence of the Soviet and German avant-gardes in an attempt to renew Italian cinema in accordance with the interests of the fascist regime. Rails (1929) by Mario Camerini blends the traditional genre of comedy with kammerspiel and realist film, revealing the director's ability to outline the characters of the middle class.[63] While not comparable to the best results of international cinema of the period, the works of Camerini and Blasetti testify to a generational transition between Italian directors and intellectuals, and above all an emancipation from literary models and an approach to the tastes of the public.  The sound cinema arrived in Italy in 1930, three years after the release of The Jazz Singer (1927), and immediately led to a debate on the validity of spoken cinema and its relationship with the theatre. Some directors enthusiastically face the new challenge. The advent of talkies led to stricter censorship by the Fascist government.[12]  The first Italian talking picture was The Song of Love (1930) by Gennaro Righelli, which was a great success with the public. Alessandro Blasetti also experimented with the use of an optical track for sound in the film Resurrection (1931), shot before The Song of Love but released a few months later.[64] Similar to Righelli's film is What Scoundrels Men Are! (1932) by Mario Camerini, which has the merit of making Vittorio De Sica debut on the screens. Historical films such as Blasetti's 1860 (1934) and Carmine Gallone's Scipio Africanus: The Defeat of Hannibal (1937) were also popular during this period.[12]  With the transition to sound cinema, most of the Italian silent film actors, still linked to theatrical stylization, find themselves disqualified. The era of divas, dandies and strongmen, who barely survived the 1920s, is definitely over. Even if some performers will move on to directing or producing, the arrival of sound favours the generational change and the consequent modernization of the structures.  Italian-born director Frank Capra received three Academy Awards for Best Director for the films It Happened One Night (1934, the first Big Five winner at the Academy Awards), Mr. Deeds Goes to Town (1936) and You Can't Take It with You (1938).  In 1932, the Venice Film Festival, the world's oldest film festival and one of the \"Big Three\" film festivals, alongside the Cannes Film Festival and the Berlin International Film Festival,[65][66][67][68] was established.  In 1934, the Italian government created the General Directorate for Cinema (Direzione Generale per le Cinematografia), and appointed Luigi Freddi its director. With the approval of Benito Mussolini, this directorate called for the establishment of a town southeast of Rome devoted exclusively to cinema, dubbed the Cinecittà (\"Cinema City\"), under the slogan \"Il cinema è l'arma più forte\" (\"Cinema is the most powerful weapon\").[70] The studios were constructed during the Fascist era as part of a plan to revive the Italian film industry, which had reached its low point in 1931.[71][72]  Mussolini himself inaugurated the studios on 21 April 1937.[73] Post-production units and sets were constructed and heavily used initially. Early films such as Scipio Africanus (1937) and The Iron Crown (1941) showcased the technological advancement of the studios. Seven thousand people were involved in the filming of the battle scene from Scipio Africanus, and live elephants were brought in as a part of the re-enactment of the Battle of Zama.[74]  The Cinecittà provided everything necessary for filmmaking: theatres, technical services, and even a cinematography school, the Centro Sperimentale di Cinematografia, for younger apprentices. The Cinecittà studios were Europe's most advanced production facilities and greatly boosted the technical quality of Italian films.[12] Many films are still shot entirely in Cinecittà. Benito Mussolini founded Cinecittà studio also for the production of Fascist propaganda until World War II.[75]  During this period, Mussolini's son, Vittorio, created a national production company and organized the work of noted authors, directors and actors (including even some political opponents), thereby creating an interesting communication network among them, which produced several noted friendships and stimulated cultural interaction.  With an area of 400,000 square metres (99 acres), it is still the largest film studio in Europe,[69] and is considered the hub of Italian cinema. Filmmakers such as Federico Fellini, Roberto Rossellini, Luchino Visconti, Sergio Leone, Bernardo Bertolucci, Francis Ford Coppola, Martin Scorsese, and Mel Gibson have worked at Cinecittà. More than 3,000 movies have been filmed there, of which 90 received an Academy Award nomination and 47 of these won it.[76]  During the 1930s, light comedies known as Telefoni Bianchi (\"white telephones\") were predominant in Italian cinema.[12] These films, which featured lavish set designs, promoted conservative values and respect for authority, and thus typically avoided the scrutiny of government censors. Telefoni Bianchi proved to be the testing ground of numerous screenwriters destined to impose themselves in the following decades (including Cesare Zavattini and Sergio Amidei), and above all of numerous set designers such as Guido Fiorini, Gino Carlo Sensani and Antonio Valente, who, by virtue, successful graphic inventions led these productions to become a kind of \"summa\" of the petty-bourgeois aesthetics of the time.[77][78]  The first film of the genre Telefoni Bianchi was The Private Secretary (1931), by Goffredo Alessandrini.[79] Among the authors, Mario Camerini is the most representative director of the genre. After having practiced the most diverse trends in the 1930s, he happily moved into the territory of sentimental comedy with What Scoundrels Men Are! (1932), Il signor Max (1937) and Department Store (1939). In other films, he compares himself with the Hollywood-style comedy on the model of Frank Capra (Heartbeat, 1939) and the surreal one of René Clair (I'll Give a Million, 1936). Camerini is interested in the figure of the typical and popular Italian, so much so that he anticipates some elements of the future Italian comedy.[80] His major interpreter, Vittorio De Sica, will continue his lesson in Maddalena, Zero for Conduct (1940) and Teresa Venerdì (1941), emphasizing above all the direction of the actors and the care for the settings.  Other directors include Mario Mattoli (Schoolgirl Diary, 1941), Jean de Limur (Apparition, 1944) and Max Neufeld (The House of Shame, 1938; A Thousand Lire a Month, 1939). The realist comedies of Mario Bonnard (Before the Postman, 1942; The Peddler and the Lady, 1943) are partially different in character, which partially deviate from the imprint of Telefoni Bianchi.  In the fascist propaganda cinema, at the beginning, the representations of the squads and the first fascist actions were rare. The Old Guard (1934), by Alessandro Blasetti evokes the supposed vitalistic spontaneity of squads with populist tones, but is not appreciated by official critics.[81] Black Shirt (1933), by Giovacchino Forzano, made for the 10th anniversary of the March on Rome, celebrated the regime's policies (the reclamation of the Pontine marshes and the construction of Littoria) alternating narrative sequences with documentary passages.  With political consolidation, the government authority required the film industry to strengthen the regime's identification with the country's history and culture. Hence the intention to reread Italian history in an authoritarian perspective, teleologically reducing every past event to a harbinger of the \"fascist revolution\", in continuity with the historiographical work of Gioacchino Volpe. After the first attempts in this direction, aimed above all at underlining the alleged link between the Risorgimento and Fascism (Villafranca by Forzano, 1933; 1860 by Blasetti, 1933), the trend reached its peak just before the war. Cavalry (1936), by Goffredo Alessandrini, evokes the nobility of the Savoy fighters by presenting their deeds as anticipations of squads. Condottieri (1937) by Luis Trenker, tells the story of Giovanni delle Bande Nere, explicitly establishing a parallel with Benito Mussolini, while Scipio Africanus: The Defeat of Hannibal (1937) by Carmine Gallone (one of the greatest financial efforts of the time), it celebrates the Roman Empire and indirectly the Fascist Empire.[82]  The invasion of Ethiopia gives Italian directors the opportunity to extend the horizons of the settings.[83] The Great Appeal (1936) by Mario Camerini, exalts imperialism by describing the \"new land\" as an opportunity for work and redemption, contrasting the heroism of young soldiers with bourgeois fearlessness. The anti-pacifist controversy that accompanies colonial enterprises is also evident in Lo squadrone bianco (1936) by Augusto Genina, which combines propaganda rhetoric with notable battle sequences shot in the Italian Tripolitania desert. Most of the films celebrating the empire are predominantly documentaries, aimed at disguising the war as a struggle of civilization against barbarism. The Spanish Civil War is described in the documentaries Los novios de la muerte (1936) by Romolo Marcellini and Arriba España, España una, grande, libre! (1939) by Giorgio Ferroni, and is the backdrop for another dozen films, among which the most spectacular is The Siege of the Alcazar (1940) by Augusto Genina.[82]  Films such as Pietro Micca (1938) by Aldo Vergano, Ettore Fieramosca (1938), made in the same year by Alessandro Blasetti, and Fanfulla da Lodi (1940) by Giulio Antamoro can also be counted as propaganda films (albeit indirect), in which, a pretext for the epic narration of historical events, a clear apology for dedication to the homeland (in some cases even to the point of personal sacrifice) is made in the same vein as colonial films with a contemporary setting.  With Italy's participation in World War II, the fascist regime further strengthens its control over production and requires a more decisive commitment to propaganda. In addition to the now canonical documentaries, short films and newsreels, there is also an increase in feature films in praise of Italian war enterprises. Among the most representative we find Bengasi (1942) by Genina, Gente dell'aria (1943) by Esodo Pratelli, The Three Pilots (1942) by Mario Mattoli (based on a screenplay by Vittorio Mussolini), Il treno crociato (1943) by Carlo Campogalliani, Harlem (1943) by Carmine Gallone and Men of the Mountain (1943) by Aldo Vergano under the supervision of Blasetti. Uomini sul fondo (1941) by Francesco De Robertis is also notable due to its almost documentary approach.[84]  The most successful film of the period is We the Living (1942) by Goffredo Alessandrini, made as a single film, but then distributed in two parts due to its excessive length. Referable to the genre of anti-communist drama, this sombre melodrama (set in the Soviet Union) is inspired by the novel of the same name by the writer Ayn Rand which exalts the most radical philosophical individualism. Precisely because of this generic criticism of authoritarianism, the diptych could be interpreted as a mild accusation against the fascist regime.[85]  Among the directors who give their contribution to the war propaganda, there is also Roberto Rossellini, author of a trilogy composed of The White Ship (1941), A Pilot Returns (1942) and The Man with a Cross (1943). Anticipating in some ways his works of maturity, the director adopted a modest and immediate style, which does not contrast the effectiveness of the propaganda but neither does it exalt the dominant war rhetoric; it was the same anti-spectacular approach to which he remained faithful throughout his life.[85]  By the end of World War II, the Italian \"neorealist\" movement had begun to take shape. Neorealist films typically dealt with the working class (in contrast to the Telefoni Bianchi), and were shot on location. Many neorealist films, but not all, used non-professional actors. Though the term \"neorealism\" was used for the first time to describe Luchino Visconti’s 1943 film, Ossessione, there were several important precursors to the movement, most notably Camerini's What Scoundrels Men Are! (1932), which was the first Italian film shot entirely on location, and Blasetti's 1942 film, Four Steps in the Clouds.[87]  Ossessione angered Fascist officials. Upon viewing the film, Vittorio Mussolini is reported to have shouted, \"This is not Italy!\" before walking out of the theatre.[88] The film was subsequently banned in the Fascist-controlled parts of Italy. While neorealism exploded after the war and was incredibly influential at the international level, neorealist films made up only a small percentage of Italian films produced during this period, as postwar Italian moviegoers preferred escapist comedies starring actors such as Totò and Alberto Sordi.[87]  Neorealist works such as Roberto Rossellini's trilogy Rome, Open City (1945), Paisà (1946), and Germany, Year Zero (1948), with professional actors such as Anna Magnani and a number of non-professional actors, attempted to describe the difficult economic and moral conditions of postwar Italy and the changes in public mentality in everyday life. Visconti's The Earth Trembles (1948) was shot on location in a Sicilian fishing village and used local non-professional actors. Giuseppe De Santis, on other hand, used actors such as Silvana Mangano and Vittorio Gassman in his 1949 film, Bitter Rice, which is set in the Po Valley during rice-harvesting season.  Poetry and cruelty of life were harmonically combined in the works that Vittorio De Sica wrote and directed together with screenwriter Cesare Zavattini: among them, Shoeshine (1946), The Bicycle Thief (1948) and Miracle in Milan (1951). The 1952 film Umberto D. showed a poor old man with his little dog, who must beg for alms against his dignity in the loneliness of the new society. This work is perhaps De Sica's masterpiece and one of the most important works in Italian cinema.[89] It was not a commercial success[89] and since then it has been shown on Italian television only a few times. Yet it is perhaps the most violent attack, in the apparent quietness of the action, against the rules of the new economy, the new mentality, the new values, and it embodies both a conservative and a progressive view.[89]  Although Umberto D. is considered the end of the neorealist period, later films such as Federico Fellini's La Strada (1954) and De Sica's 1960 film Two Women (for which Sophia Loren won the Oscar for Best Actress) are grouped with the genre. Director Pier Paolo Pasolini's first film, Accattone (1961), shows a strong neorealist influence.[87] Italian neorealist cinema influenced filmmakers around the world, and helped inspire other film movements, such as the French New Wave and the Polish Film School. The Neorealist period is often simply referred to as \"The Golden Age\" of Italian cinema by critics, filmmakers, and scholars.  Calligrafismo is in sharp contrast to Telefoni Bianchi-American style comedies and is rather artistic, highly formalistic, expressive in complexity and deals mainly with contemporary literary material,[92] above all the pieces of Italian realism from authors like Corrado Alvaro, Ennio Flaiano, Emilio Cecchi, Francesco Pasinetti, Vitaliano Brancati, Mario Bonfantini and Umberto Barbaro.[93]  The best-known exponent of this genre is Mario Soldati, a long-time writer and director destined to establish himself with films of literary ancestry and solid formal structure. His films put at the centre of the story characters endowed with a dramatic and psychological strength foreign to both white-phone cinema and propaganda films, and found in works such as Dora Nelson (1939), Piccolo mondo antico (1941), Tragic Night (1942), Malombra (1942) and In High Places (1943). Luigi Chiarini, already active as a critic, deepens the trend in his Sleeping Beauty (1942), Street of the Five Moons (1942) and The Innkeeper (1944). The internal conflicts of the characters and the scenographic richness are also recurrent in the first films by Alberto Lattuada (Giacomo the Idealist, 1943) and Renato Castellani (A Pistol Shot, 1942), dominated by a sense of moral and cultural decay that seems to anticipate the end of the war.  Another important example of a calligraphic film is the film version of The Betrothed (1941), by Mario Camerini (very faithful in the staging of Manzoni's masterpiece), which due to the perceived income, became the most popular feature film between 1941 and 1942.[94]  The pioneer of the Italian cartoon was Francesco Guido, better known as Gibba. Immediately after the end of World War II, he produced the first animated medium-length film of Italian cinema entitled L'ultimo sciuscià (1946), which took up themes typical of neorealism and in the following decade the feature films Rompicollo and I picchiatelli, in collaboration with Antonio Attanasi.[95] In the 1970s, after many animated documentaries, Gibba himself will return to the feature film with the erotic Il nano e la strega (1973) and Il racconto della giungla (1974). Also interesting are the contributions of the painter and set designer Emanuele Luzzati who, after some valuable short films, made in 1976 one of the masterpieces of Italian animation: Il flauto magico (\"The Magic Flute\"), based on the homonymous opera by Mozart.  In 1949, the designer Nino Pagot presented The Dynamite Brothers at the Venice Film Festival, one of the first animated feature films of the time, released in theatres in conjunction with La Rosa di Bagdad (1949), made by the animator Anton Gino Domeneghini.[95] In the early 1950s, the cartoonist Romano Scarpa created the short film La piccola fiammiferaia (1953), which remains, like the two previous films, little more than an isolated case. Apart from these examples, Italian animation in the 1950s and 1960s failed to become a major reality and remains confined to the television sector, due to the various commissions provided by the Carosello container.[96][97]  But it is with Bruno Bozzetto that the Italian cartoon reaches an international dimension: his debut feature film West and Soda (1965), an irresistible caricature of the Western genre, received acclaim from both audiences and critics.[95] A few years later his second work entitled VIP my Brother Superman was released, distributed in 1968. After many satirical short films (centred on the popular figure of \"Signor Rossi\") he returned to the feature film with what is considered his most ambitious work, Allegro Non Troppo (1977). Inspired by the well-known Disney Fantasia, it is a mixed media film, in which animated episodes are molded to the notes of many classical music pieces. Another illustrator to underline is the artist Pino Zac who in 1971 shot (again with mixed technique) The Nonexistent Knight, based on the novel of the same name by Italo Calvino.  In the 1990s, Italian animation entered a new phase of production due to the Turin Lanterna Magica studio which in 1996, under the direction of Enzo D'Alò, created the intriguing Christmas fairy tale How the Toys Saved Christmas, based on a short story by Gianni Rodari. The film was a success and paved the way for other feature films. In fact, in 1998, Lucky and Zorba based on a novel by Luis Sepúlveda was distributed, which attracted the favour of the public, reaching a new apex in the Italian animated cinema.[98]  The director Enzo d'Alò, who separated from the Lanterna Magica studio, produced other films in the following years such as Momo (2001) and Opopomoz (2003). The Turin studio distributed on its behalf the films Aida of the Trees (2001) and Totò Sapore e la magica storia della pizza (2003), accompanied by a good response at the box office. In 2003, the first entirely Italian animated film in computer graphics was released entitled L'apetta Giulia and Signora Vita, directed by Paolo Modugno.[99] To underline the work La Storia di Leo (2007) by director Mario Cambi, winner, the following year, at the Giffoni Film Festival.  In 2010, the first Italian animated film in 3D technology was made, directed by Iginio Straffi, entitled Winx Club 3D: Magical Adventure, based on the homonymous series; in the meantime Enzo D'Alò returns to theatres, presenting his Pinocchio (2012). In 2012, the film Gladiators of Rome, also shot in 3D technology, received credit from the public, followed by the feature film Winx Club: The Mystery of the Abyss (2014), both again by Iginio Straffi. Finally, The Art of Happiness (2013) by Alessandro Rak, a film made in Naples by 40 authors, including only 10 designers and animators from the Mad Entertainment studio, a true absolute record for an animated film was made.[100] Cinderella the Cat (2017), taken from the text Pentamerone by Giambattista Basile, came out of the same studio. The work won two David di Donatello's, one of which was for special effects, becoming the first animated film to be nominated, and win, in this category.  Starting from the mid-1950s, Italian cinema freed itself from neorealism by tackling purely existential topics, films with different styles and points of view, often more introspective than descriptive.[101] Thus we are witnessing a new flowering of filmmakers who contribute in a fundamental way to the development of the art.[101]  Michelangelo Antonioni is the first to establish himself, becoming a reference author for all contemporary cinema.[102] This charge of novelty is recognizable from the beginning as the director's first work, Story of a Love Affair (1950), marks an indelible break with the world of neorealism and the consequent birth of a modern cinema.[102] Antonioni investigated the world of the Italian bourgeoisie with a critical eye, left out of the post-war cinematic lens. In doing so, works of psychological research such as I Vinti (1952), The Lady Without Camelias (1953) and Le Amiche (1955), free adaptation of the short story Tra donne sole by Cesare Pavese, came to light. In 1957, he staged the unusual proletarian drama Il Grido, with which he obtained critical acclaim.  In 1955, the David di Donatello was established, with its Best Picture category being awarded for the first time only in 1970. Named after Donatello's David, a symbolic statue of the Italian Renaissance,[103] are film awards given out each year by the Accademia del Cinema Italiano (The Academy of Italian Cinema).  Federico Fellini is recognized as one of the greatest and most influential filmmakers of all time.[105] Fellini won the Palme d'Or for La Dolce Vita, was nominated for twelve Academy Awards, and won four in the category of Best Foreign Language Film, the most for any director in the history of the academy. He received an honorary award for Lifetime Achievement at the 65th Academy Awards in Los Angeles. His other well-known films include La Strada (1954), Nights of Cabiria (1957), Juliet of the Spirits (1967), Satyricon (1969), Roma (1972), Amarcord (1973), and Fellini's Casanova (1976).  Personal and highly idiosyncratic visions of society, Fellini's films are a unique combination of memory, dreams, fantasy and desire. The adjectives \"Fellinian\" and \"Felliniesque\" are \"synonymous with any kind of extravagant, fanciful, even baroque image in the cinema and in art in general\".[106] La Dolce Vita contributed the term paparazzi to the English language, derived from Paparazzo, the photographer friend of journalist Marcello Rubini (Marcello Mastroianni).[107]  Contemporary filmmakers such as Tim Burton,[108] Terry Gilliam,[109] Emir Kusturica,[110] and David Lynch[111] have cited Fellini's influence on their work.  Although Umberto D. is considered the end of the neorealist period, subsequent works turned toward lighter, sweetened and mildly optimistic atmospheres, more coherent with the improving conditions of Italy just before the economic boom; this genre became known as pink neorealism.  The precursor of pink neorealism was Renato Castellani, who helped bring realist comedy into vogue with Under the Sun of Rome (1948) and It's Forever Springtime (1949), both shot on location and with non-professional actors, and above all with public success and criticism of Two Cents Worth of Hope (1952), which laid the foundations for pink neorealism.[112]  Notable films of pink neorealism, which combine popular comedy and realist motifs, are Pane, amore e fantasia (1953) by Luigi Comencini and Poveri ma belli (1957) by Dino Risi, both works are in perfect harmony with the evolution of the Italian costume.[113] The large influx at the box office from the two films remained almost unchanged in the sequels Bread, Love and Jealousy (1954), Scandal in Sorrento (1955) and Pretty But Poor (1957), also directed by Luigi Comencini and Dino Risi.  Similarly, stories of daily life told with gentle irony (without losing sight of the social fabric) can be found in the work of the Milanese Luciano Emmer, whose films Sunday in August (1950), Three Girls from Rome (1952) and High School (1954), are the best-known examples. Another film of the pink neorealism genre was Susanna Whipped Cream (1957) by Steno.[114]  This trend allowed some actresses to become real celebrities, such as Sophia Loren, Gina Lollobrigida, Silvana Pampanini, Lucia Bosé, Barbara Bouchet, Eleonora Rossi Drago, Silvana Mangano, Virna Lisi, Claudia Cardinale and Stefania Sandrelli. Soon pink neorealism was replaced by the Commedia all'italiana, a unique genre that, born on an ideally humouristic line, talked instead very seriously about important social themes.  Commedia all'italiana (\"Comedy in the Italian way\") is an Italian film genre born in Italy in the 1950s and developed in the following 1960s and 1970s. It is widely considered to have started with Mario Monicelli's Big Deal on Madonna Street in 1958[115] and derives its name from the title of Pietro Germi's Divorce Italian Style, 1961.[116] According to most of the critics, La Terrazza by Ettore Scola (1980) is the last work considered part of the Commedia all'italiana.[117][118][119]  Rather than a specific genre, the term indicates a period (approximately from the late 1950s to the early 1970s) in which the Italian film industry was producing many successful comedies, with some common traits like satire of manners, farcical and grotesque overtones, a strong focus on \"spicy\" social issues of the period (like sexual matters, divorce, contraception, marriage of the clergy, the economic rise of the country and its various consequences, the traditional religious influence of the Catholic Church) and a prevailing middle-class setting, often characterized by a substantial background of sadness and social criticism that diluted the comic contents.[120]  The genre of Commedia all'italiana differs markedly from the light and disengaged comedy from the so-called \"pink neorealism\" trend, in vogue until all of the 1950s, since, starting from the lesson of neorealism, is based on a more frank adherence in writing to reality; therefore, alongside the comic situations and plots typical of traditional comedy, always combines, with irony, a biting and sometimes bitter satire of manners, which reflects the evolution of Italian society in those years.[120]  The success of films belonging to the \"Commedia all'italiana\" genre is due both to the presence of an entire generation of great actors, who knew how to masterfully embody the vices and virtues, and the attempts at emancipation but also the vulgarities of the Italians of the time, both to the careful work of directors, storytellers and screenwriters, who invented a real genre, with essentially new connotations, managing to find precious material for their cinematographic creations in the folds of a rapid evolution with many contradictions.[120]  Among the actors the main representatives are Alberto Sordi, Ugo Tognazzi, Vittorio Gassman, Marcello Mastroianni and Nino Manfredi,[121] while among the actresses is Monica Vitti.[122] Among directors and films, in 1961 Dino Risi directed Una vita difficile (A Difficult Life), then Il Sorpasso (The Easy Life), now a cult-movie, followed by: I Mostri (The Monsters, also known as 15 From Rome), In nome del popolo italiano (In the Name of the Italian People) and Profumo di donna (Scent of a Woman). Monicelli's works include La grande guerra (The Great War), I compagni (The Organizer), L'armata Brancaleone, Vogliamo i colonnelli (We Want the Colonels), Romanzo popolare (Come Home and Meet My Wife) and the Amici miei (My Friends) series.  For the majority of critics the true and proper \"Commedia all'italiana\" is to be considered definitively waned since the beginning of the 1980s, giving way, at most, to an \"Commedia italiana\" (\"Italian comedy\").[123]  At this time, on the more commercial side of production, the phenomenon of Totò, a Neapolitan actor who is acclaimed as the major Italian comic, exploded. His films (often with Aldo Fabrizi, Peppino De Filippo and almost always with Mario Castellani) expressed a sort of neorealistic satire, in the means of a guitto (a \"hammy\" actor) as well as with the art of the great dramatic actor he also was.[124] Totò is one of the symbols of the cinema of Naples.[125]  A \"film-machine\" who produced dozens of titles per year, his repertoire was frequently repeated. His personal story (a prince born in the poorest rione (section of the city) of Naples), his unique twisted face, his special mimic expressions and his gestures created an inimitable personage and made him one of the most beloved Italians of the 1960s.  Some of his best-known films are Fear and Sand by Mario Mattoli, Toto Tours Italy by Mario Mattoli, Toto the Sheik by Mario Mattoli, Cops and Robbers by Mario Monicelli, Toto and the Women by Mario Monicelli, Totò Tarzan by Mario Mattoli, Toto the Third Man by Mario Mattoli, Toto and the King of Rome by Mario Monicelli and Steno, Toto in Color by Steno (one of the first Italian colour movies, 1952, in Ferrania colour), Big Deal on Madonna Street by Mario Monicelli, Toto, Peppino, and the Hussy by Camillo Mastrocinque and The Law Is the Law by Christian-Jaque. Pier Paolo Pasolini's The Hawks and the Sparrows and the episode \"Che cosa sono le nuvole\" from Caprice Italian Style (the latter released after his death), showed his dramatic skills.[124]  A series of black-and-white films based on Don Camillo and Peppone characters created by the Italian writer and journalist Giovannino Guareschi were made between 1952 and 1965. These were French-Italian coproductions, and starred Fernandel as the Italian priest Don Camillo and Gino Cervi as Giuseppe 'Peppone' Bottazzi, the Communist Mayor of their rural town. The titles are: The Little World of Don Camillo (1952), The Return of Don Camillo (1953), Don Camillo's Last Round (1955), Don Camillo: Monsignor (1961), and Don Camillo in Moscow (1965).  The movies were a huge commercial success in their native countries. In 1952, Little World of Don Camillo became the highest-grossing film in both Italy and France,[126] while The Return of Don Camillo was the second most popular film of 1953 at the Italian and French box office.[127]  Mario Camerini began filming the film Don Camillo e i giovani d'oggi, but had to stop filming due to Fernandel's falling ill, which resulted in his untimely death. The film was then realized in 1972 with Gastone Moschin playing the role of Don Camillo and Lionel Stander as Peppone.  A new Don Camillo film, titled The World of Don Camillo, was also remade in 1983, an Italian production with Terence Hill directing and also starring as Don Camillo. Colin Blakely performed Peppone in one of his last film roles.  Hollywood on the Tiber is a phrase used to describe the period in the 1950s and 1960s when the Italian capital of Rome emerged as a major location for international filmmaking attracting many foreign productions to the Cinecittà studios, the largest film studio in Europe.[69] By contrast to the native Italian film industry, these movies were made in English for global release. Although the primary markets for such films were American and British audiences, they enjoyed widespread popularity in other countries, including Italy.  In the late 1940s, Hollywood studios began to shift production abroad to Europe. Italy was, along with Britain, one of the major destinations for American film companies. Large-budget films shot at Cinecittà during the \"Hollywood on the Tiber\" era such as Quo Vadis (1951), Roman Holiday (1953), Ben-Hur (1959), and Cleopatra (1963) were made in English with international casts and sometimes, but not always, Italian settings or themes.  The heyday of what was dubbed '\"Hollywood on the Tiber\" was between 1950 and 1970, during which time many of the most famous names in world cinema made films in Italy. The phrase \"Hollywood on Tiber\", a reference to the river that runs through Rome, was coined in 1950 by Time magazine during the making of Quo Vadis.[128]  Sword-and-sandal, also known as peplum (pepla plural), is a subgenre of largely Italian-made historical, mythological, or Biblical epics mostly set in the Greco-Roman antiquity or the Middle Ages. These films attempted to emulate the big-budget Hollywood historical epics of the time.[129]  With the release of 1958's Hercules, starring American bodybuilder Steve Reeves, the Italian film industry gained entree to the American film market. These films were low-budget costume\/adventure dramas, and had immediate appeal with both European and American audiences. Besides the many films starring a variety of muscle men as Hercules, heroes such as Samson and Italian fictional hero Maciste were common.  Sometimes dismissed as low-quality escapist fare, the sword-and-sandal allowed newer directors such as Sergio Leone and Mario Bava a means of breaking into the film industry. Some, such as Mario Bava's Hercules in the Haunted World (Italian: Ercole Al Centro Della Terra) are considered seminal works in their own right.  As the genre matured, budgets sometimes increased, as evidenced in 1962's I sette gladiatori (The Seven Gladiators in 1964 US release), a wide-screen epic with impressive sets and matte-painting work. Most sword-and-sandal films were in  colour, whereas previous Italian efforts had often been black and white.  Musicarello (pl. musicarelli) is a film subgenre which emerged in Italy and which is characterised by the presence in main roles of young singers, already famous among their peers, and their new record album. The genre began in the late 1950s, and had its peak of production in the 1960s.[130]  The film which started the genre is considered to be I ragazzi del Juke-Box by Lucio Fulci (1959).[131] The musicarelli were inspired by two American musicals, in particular Jailhouse Rock by Richard Thorpe (1957)  and earlier Love Me Tender by Robert D. Webb (1956), both starring Elvis Presley.[132][133][134]  At the heart of the musicarello is a hit song, or a song that the producers hoped would become a hit, that usually shares its title with the film itself and sometimes has lyrics depicting a part of the plot.[135] In the films there are almost always tender and chaste love stories accompanied by the desire to have fun and dance without thoughts.[136] Musicarelli reflect the desire and need for emancipation of young Italians, highlighting some generational frictions.[132]  With the arrival of the 1968 student protests the genre started to decline, because the generational revolt became explicitly political and at the same time there was no longer music equally directed to the whole youth audience.[137] For some time the duo Al Bano and Romina Power continued to enjoy success in musicarello films, but their films (like their songs) were a return to the traditional melody and to the musical films of the previous decades.[137]  On the heels of the sword-and-sandal craze, a related genre, the Spaghetti Western arose and was popular both in Italy and elsewhere. These films differed from traditional westerns by being filmed in Europe on limited budgets, but featured vivid cinematography. The term was used by foreign critics because most of these westerns were produced and directed by Italians.[140]  The most popular Spaghetti Westerns were those of Sergio Leone, credited as the inventor of the genre,[141][142] whose Dollars Trilogy (1964's A Fistful of Dollars, an unauthorized remake of the Japanese film Yojimbo by Akira Kurosawa; 1965's For a Few Dollars More, an original sequel; and 1966's  The Good, the Bad and the Ugly, a World-famous prequel), featuring Clint Eastwood as a character marketed as \"the Man with No Name\" and notorious scores by Ennio Morricone, came to define the genre along with Once Upon a Time in the West (1968).  Another popular Spaghetti Western film is Sergio Corbucci Django (1966), starring Franco Nero as the titular character, another Yojimbo plagiarism, produced to capitalize on the success of A Fistful of Dollars. The original Django was followed by both an authorized sequel (1987's Django Strikes Again) and an overwhelming number of unauthorized uses of the same character in other films.  Also considered Spaghetti Westerns is a film genre which combined traditional western ambience with a Commedia all'italiana-type comedy; films including They Call Me Trinity (1970) and Trinity Is Still My Name (1971), both by Enzo Barboni, which featured Bud Spencer and Terence Hill, the stage names of Carlo Pedersoli and Mario Girotti.  Terence Hill and Bud Spencer made numerous films together.[143] Most of their early films were Spaghetti Westerns, beginning with God Forgives... I Don't! (1967), the first part of a trilogy, followed by Ace High (1968) and Boot Hill (1969), but they also starred in comedies such as ... All the Way, Boys! (1972) and Watch Out, We're Mad! (1974).  The next films shot by the couple of actors, almost all comedies, were Two Missionaries (1974), Crime Busters (1977), Odds and Evens (1978), I'm for the Hippopotamus (1979), Who Finds a Friend Finds a Treasure (1981), Go for It (1983), Double Trouble (1984), Miami Supercops (1985) and Troublemakers (1994).  During the 1960s and 1970s, Italian filmmakers Mario Bava, Riccardo Freda, Antonio Margheriti and Dario Argento developed giallo (plural gialli, from giallo, Italian for \"yellow\") horror films that become classics and influenced the genre in other countries. Representative films include: The Girl Who Knew Too Much (1963), Castle of Blood (1964), The Bird with the Crystal Plumage (1970), Twitch of the Death Nerve (1971), Deep Red (1975) and Suspiria (1977).  Giallo is a genre of mystery fiction and thrillers and often contains slasher, crime fiction, psychological thriller, psychological horror, sexploitation, and, less frequently, supernatural horror elements.[148] Giallo developed in the mid-to-late 1960s, peaked in popularity during the 1970s, and subsequently declined in commercial mainstream filmmaking over the next few decades, though examples continue to be produced. It was a predecessor to, and had significant influence on, the later American slasher film genre.[149]  Giallo usually blends the atmosphere and suspense of thriller fiction with elements of horror fiction (such as slasher violence) and eroticism (similar to the French fantastique genre), and often involves a mysterious killer whose identity is not revealed until the final act of the film. Most critics agree that the giallo represents a distinct category with unique features,[150] but there is some disagreement on what exactly defines a giallo film.[151]  Giallo films are generally characterized as gruesome murder-mystery thrillers that combine the suspense elements of detective fiction with scenes of shocking horror, featuring excessive bloodletting, stylish camerawork and often jarring musical arrangements. The archetypal giallo plot involves a mysterious, black-gloved psychopathic killer who stalks and butchers a series of beautiful women.[153] While most gialli involve a human killer, some also feature a supernatural element.[154]  The typical giallo protagonist is an outsider of some type, often a traveller, tourist, outcast, or even an alienated or disgraced private investigator, and frequently a young woman, often a young woman who is lonely or alone in a strange or foreign situation or environment (gialli rarely or less frequently feature law enforcement officers as chief protagonists).[154][155]  The protagonists are generally or often unconnected to the murders before they begin and are drawn to help find the killer through their role as witnesses to one of the murders.[154] The mystery is the identity of the killer, who is often revealed in the climax to be another key character, who conceals his or her identity with a disguise (usually some combination of hat, mask, sunglasses, gloves, and trench coat).[156] Thus, the literary whodunit element of the giallo novels is retained, while being filtered through horror genre elements and Italy's long-standing tradition of opera and staged grand guignol drama. The structure of giallo films is also sometimes reminiscent of the so-called \"weird menace\" pulp magazine horror mystery genre alongside Edgar Allan Poe and Agatha Christie.[157]  Poliziotteschi (plural of poliziottesco) films constitute a subgenre of crime and action films that emerged in Italy in the late 1960s and reached the height of their popularity in the 1970s. They are also known as polizieschi all'italiana, Euro-crime, Italo-crime, spaghetti crime films', or simply Italian crime films.  Influenced by both 1970s French crime films and gritty 1960s and 1970s American cop films and vigilante films,[158] poliziotteschi films were made amidst an atmosphere of socio-political turmoil in Italy and increasing Italian crime rates.  The films generally featured graphic and brutal violence, organized crime, car chases, vigilantism, heists, gunfights, and corruption up to the highest levels. The protagonists were generally tough working-class loners, willing to act outside a corrupt or overly bureaucratic system.[159] Notable international actors who acted in this genre of films include Alain Delon, Henry Silva, Fred Williamson, Charles Bronson, Tomas Milian and others.  Franco and Ciccio were a comedy duo formed by Italian actors Franco Franchi (1928–1992) and Ciccio Ingrassia (1922–2003), particularly popular in the 1960s and 1970s. Together, they appeared in 116 films, usually as the main characters, and occasionally as supporting characters in films featuring well-known actors such as Totò, Domenico Modugno, Vittorio Gassman, Buster Keaton and Vincent Price.[160]  Their collaboration began in 1954 in the theatre field, and ended with Franchi's death in 1992. The two made their cinema debut in 1960 with the film Appuntamento a Ischia. After, seeing them in this film Modugno, who wanted them with him in his film,[161][162] and remained active until 1984 when they shot their last film together, Kaos, although there were some interruptions in 1973, and from 1975 to 1980.[163]  They acted in films certainly made in a short time and with few means, such as those shot with director Marcello Ciorciolini, sometimes even making a dozen films in a year, often without a real script and where they often improvised on the set. Also are the 13 films directed by Lucio Fulci, who was the architect of the reversal of their typical roles by making Ciccio the serious one, the sidekick, and Franco the comic one.[164]  They also worked with important directors such as Pier Paolo Pasolini and the Taviani brothers. At the time, they were considered protagonists of B movie, they were subsequently reevaluated by critics for their comedy and creative abilities, becoming the subject of study.[165][166] The huge success with the public is evidenced by the box office earnings, which in the 1960s, represented 10% of the annual earnings in Italy.[167]  The auteur cinema of the 1960s continues its path by analyzing distinct themes and problems. A new authorial vision is emancipated from the surreal and existential veins of Fellini and Antonioni which sees cinema as an ideal means of denouncing corruption and malfeasance,[168] both in the political system and in the industrial world. Thus was born the structure of the investigative film which, starting from the neorealist analysis of the facts, added to them a concise critical judgment, with the manifest intent of shaking the consciences of public opinion. This typology deliberately touches upon burning issues, often targeting the established power, with the intent of reconstructing a historical truth that is often hidden or denied.[169]  The precursor of this way of understanding the director's profession was Francesco Rosi.[170] In 1962 he inaugurated the investigation film project retracing, through a series of long flashbacks, the life of the homonym Sicilian criminal in the film Salvatore Giuliano. The following year he directed Rod Steiger in Hands over the City (1963), in which he courageously denounced the collusion existing between the various organs of the State and the building exploitation in Naples. The film was awarded the Golden Lion at the Venice Film Festival.  One of Francesco Rosi's most famous films of denunciation is The Mattei Affair (1972), a rigorous documentary into the mysterious disappearance of Enrico Mattei, manager of Eni, a large Italian state group. The film won the Palme d'Or at the Cannes Film Festival. It became (together with the tight Illustrious Corpses (1976)) a true model for similar denunciation films produced both in Italy and abroad. Famous films of denunciation by Elio Petri are The Working Class Goes to Heaven (1971), a corrosive denunciation of life in the factory (winner of the Palme d'Or at Cannes) and Investigation of a Citizen Above Suspicion (1970). The latter (accompanied by the incisive soundtrack by Ennio Morricone) is a dry psychoanalytic thriller centred on the aberrations of power, analyzed in a pathological key.[171] The film obtained a wide consensus, winning the Academy Award for Best International Feature Film the following year.  Arguments related to civilian cinema can be found in the work of Damiano Damiani, who with The Day of the Owl (1968) enjoyed considerable success. Other feature films include, Confessions of a Police Captain (1971), The Case Is Closed, Forget It (1971), How to Kill a Judge (1974) and I Am Afraid (1977). Also Pasquale Squitieri for the film Il prefetto di ferro (1977) and Giuliano Montaldo, who after some experiences as an actor, staged some historical and political films such as The Fifth Day of Peace (1970), Sacco & Vanzetti (1971) and Giordano Bruno (1973). Also Nanni Loy for the film In Prison Awaiting Trial (1971) starring Alberto Sordi.  In the 1970s the work done by the director Lina Wertmüller was influential, who together with the well-established actors Giancarlo Giannini and Mariangela Melato, gave life to successful films such as The Seduction of Mimi (1972), Love and Anarchy (1973) and Swept Away (1974). Two years later, with Seven Beauties (1976), she obtained four nominations for the Academy Awards, making her the first woman ever to receive a nomination for best director.[172]  The last protagonist of the great season of the comedy is the director Ettore Scola. Throughout the 1950s, he played the role of screenwriter, and then makes his directorial debut in 1964 with the film Let's Talk About Women. In 1974, he directed his best-known film, We All Loved Each Other So Much, which traces 30 years of Italian history through the stories of three friends: the lawyer Gianni Perego (Vittorio Gassman), the porter Antonio (Nino Manfredi) and the intellectual Nicola (Stefano Satta Flores). Other films include, Down and Dirty (1976) starring Nino Manfredi, and A Special Day (1977) starring Sophia Loren and Marcello Mastroianni.[173]  Commedia sexy all'italiana is characterized typically by both abundant female nudity and comedy, and by the minimal weight given to social criticism that was the basic ingredient of the main commedia all'italiana genre.[174] Stories are often set in affluent environments such as wealthy households. It is closely connected to the sexual revolution, which was extremely new and innovative for that period. For the first time, films with female nudity could be watched at the cinema. Pornography and scenes of explicit sex were still forbidden in Italian cinemas, but partial nudity was somewhat tolerated. The genre has been described as a cross between bawdy comedy and humorous erotic film with ample slapstick elements which follow more or less clichéd storylines.  During this time, commedia sexy all'italiana films, described by the film critics of the time as not artistic or \"trash films\", were very popular in Italy. Today they are widely re-evaluated and have become real cult movies. They also allowed the producers of Italian cinema to have enough revenue to produce successful artistic films. These comedy films were of little artistic value and reached their popularity by confronting Italian social taboos, most notably in the sexual sphere. Actors such as Lando Buzzanca, Lino Banfi, Renzo Montagnani, Alvaro Vitali, Gloria Guida, Barbara Bouchet and Edwige Fenech owe much of their popularity to these films.  The films starring Ugo Fantozzi, a character invented by Paolo Villaggio for his television sketches and newspaper short stories, also fell within the comic satirical comedy genre.[175] Although Villaggio's movies tend to bridge comedy with a more elevated social satire, this character had a great impact on Italian society, to such a degree that the adjective fantozziano entered the lexicon.[176] Ugo Fantozzi represents the archetype of the average Italian of the 1970s, middle-class with a simple lifestyle with the anxieties common to an entire class of workers,[177] being re-evaluated by critics.[178]  Of the many films telling of Fantozzi's misadventures, the most notable and famous were Fantozzi (1975) and Il secondo tragico Fantozzi (1976), both directed by Luciano Salce, but many others were produced. The other films were Fantozzi contro tutti (1980) directed by Neri Parenti, Fantozzi subisce ancora (1983)  by Neri Parenti, Superfantozzi (1986) by Neri Parenti, Fantozzi va in pensione (1988) by Neri Parenti, Fantozzi alla riscossa (1990) by Neri Parenti, Fantozzi in paradiso (1993) by Neri Parenti, Fantozzi - Il ritorno (1996) by Neri Parenti and Fantozzi 2000 - La clonazione (1999) by Domenico Saverni.  The sceneggiata (pl. sceneggiate) or sceneggiata napoletana is a form of musical drama typical of Naples. Beginning as a form of musical theatre after World War I, it was also adapted for cinema; sceneggiata films became especially popular in the 1970s, and contributed to the genre becoming more widely known outside Naples.[179] The most famous actors who played dramas were Mario Merola, Mario Trevi, and Nino D'Angelo.[180]  The sceneggiata can be roughly described as a \"musical soap opera\", where action and dialogue are interspersed with Neapolitan songs.  Plots revolve around melodramatic themes drawing from the Neapolitan culture and tradition, including passion, jealousy, betrayal, personal deceit and treachery, honour, vengeance, and life in the world of petty crime.  Songs and dialogue were originally in Neapolitan dialect, although, especially in film production, Italian has sometimes been preferred, to reach a larger audience.  Sgarro alla camorra (i.e. \"Offence to the Camorra\", 1973), written and directed by Ettore Maria Fizzarotti and starring Mario Merola at his film debut, is regarded as the first sceneggiata film and as a prototype for the genre.[181][182] It was shot in Cetara, Province of Salerno.[182] Outside Italy, sceneggiata is mostly known in areas populated by Italian immigrants. Besides Naples, the second homeland of sceneggiata is probably Little Italy in New York City.[183]  The 1980s was a period of decline for Italian filmmaking. In 1985, only 80 films were produced (the least since the postwar period)[187] and the total number of audience decreased from 525 million in 1970 to 123 million.[188] It is a physiological process that invests, in the same period as other countries, with a great cinematographic tradition such as Japan, United Kingdom and France. The era of producers ended; Carlo Ponti and Dino De Laurentiis work abroad, Goffredo Lombardo and Franco Cristaldi were no longer key figures. The crisis affects the Italian genre cinema above all, which, by virtue of the success of commercial television, is deprived of the vast majority of its audience.[189] As a result, cinemas began showing mainly Hollywood films, which steadily took over, while many other cinemas closed.  Among the major artistic films of this era were La città delle donne, E la nave va, Ginger and Fred by Fellini, L'albero degli zoccoli by Ermanno Olmi (winner of the Palme d'Or at the Cannes Film Festival), La notte di San Lorenzo by Paolo and Vittorio Taviani, Antonioni's Identificazione di una donna, and Bianca and La messa è finita by Nanni Moretti. Although not entirely Italian, Bernardo Bertolucci's The Last Emperor, winner of 9 Oscars including Best Picture and Best Director, and Once Upon a Time in America of Sergio Leone came out of this period also.  Non ci resta che piangere, directed by and starring both Roberto Benigni and Massimo Troisi, is a cult movie in Italy.  Carlo Verdone, actor, screenwriter and film director, is best known for his comedic roles in Italian classics, which he also wrote and directed. His career was jumpstarted by his first three successes, Un sacco bello (1980), Bianco, rosso e Verdone (1981) and Borotalco (1982). Since the 1990s, he has been introducing more serious subjects in his work, linked to the excesses of society and the individual's hardships in confronting it; some examples are Maledetto il giorno che t'ho incontrata (1992), Il mio miglior nemico (2006) and Io, loro e Lara (2010).  Francesco Nuti began his professional career as an actor in the late 1970s, when he formed the cabaret group Giancattivi together with Alessandro Benvenuti and Athina Cenci. The group took part in the TV shows Black Out and Non Stop for RAI TV, and shot their first feature film, West of Paperino (1981), written and directed by Benvenuti. The following year Nuti abandoned the trio and began a solo career with three movies directed by Maurizio Ponzi: What a Ghostly Silence There Is Tonight (1982), The Pool Hustlers (1982) and Son contento (1983). Starting in 1985, he began to direct his movies, scoring an immediate success with the films Casablanca, Casablanca  and  All the Fault of Paradise (1985), Stregati (1987), Caruso Pascoski, Son of a Pole (1988), Willy Signori e vengo da lontano (1990) and Women in Skirts (1991). The 1990s were however a period of decline for the Tuscan director, with poorly successful movies such as OcchioPinocchio (1994), Mr. Fifteen Balls (1998), Io amo Andrea (2000) and Caruso, Zero for Conduct (2001).  The cinepanettoni (singular: cinepanettone) are a series of farcical comedy films, one or two of which are scheduled for release annually in Italy during the Christmas period. The films were originally produced by Aurelio De Laurentiis' Filmauro studio.[190] These films are usually focused on the holidays of stereotypical Italians: bungling, wealthy and presumptuous members of the middle class who visit famous, glamorous or exotic places.  The economic crisis that emerged in the 1980s began to ease over the next decade.[191] Nonetheless, the 1992–93 and 1993–94 seasons marked an all-time low in the number of films made, in the national market share (15 per cent), in the total number of viewers (under 90 million per year) and in the number of cinemas.[192] The effect of this industrial contraction sanctions the total disappearance of Italian genre cinema in the middle of the decade, as it was no longer suitable to compete with the contemporary big Hollywood blockbusters (mainly due to the enormous budget differences available), with its directors and actors who therefore almost entirely switch to television film.  A new generation of directors has helped return Italian cinema to a healthy level since the end of the 1980s. Probably the most noted film of the period is Nuovo Cinema Paradiso, for which Giuseppe Tornatore won a 1989 Oscar (awarded in 1990) for Best Foreign Language Film. This award was followed when Gabriele Salvatores's Mediterraneo won the same prize in 1991.  Il Postino: The Postman (1994), directed by the British Michael Radford and starring Massimo Troisi, received five nominations at the Academy Awards, including Best Picture and Best Actor for Troisi, and won for Best Original Score. Another exploit was in 1998 when Roberto Benigni won three Oscars for his movie Life Is Beautiful (La vita è bella) (Best Actor for Benigni himself, Best Foreign Film, Best Music). The film was also nominated for Best Picture.  Leonardo Pieraccioni made his directorial debut with The Graduates (1995).[193] In 1996 he directed his breakthrough film The Cyclone, which grossed Lire 75 billion at the box office.[194][195]  With the new millennium, the Italian film industry regained stability and critical recognition. In 1995, 93 films were produced,[196] while in 2005, 274 films were made.[197] In 2006, the national market share reached 31 per cent.[198] In 2001, Nanni Moretti's film The Son's Room (La stanza del figlio) received the Palme d'Or at the Cannes Film Festival. Other noteworthy recent Italian films include: Jona che visse nella balena directed by Roberto Faenza, Il grande cocomero by Francesca Archibugi, The Profession of Arms (Il mestiere delle armi) by Olmi, L'ora di religione by Marco Bellocchio, Il ladro di bambini, Lamerica, The Keys to the House (Le chiavi di casa) by Gianni Amelio, I'm Not Scared (Io non-ho paura) by Gabriele Salvatores, Le Fate Ignoranti, Facing Windows (La finestra di fronte) by Ferzan Özpetek, Good Morning, Night (Buongiorno, notte) by Marco Bellocchio, The Best of Youth (La meglio gioventù) by Marco Tullio Giordana, The Beast in the Heart (La bestia nel cuore) by Cristina Comencini. In 2008 Paolo Sorrentino's Il Divo, a biographical film based on the life of Giulio Andreotti, won the Jury prize and Gomorra, a crime drama film, directed by Matteo Garrone won the Gran Prix at the Cannes Film Festival.  Paolo Sorrentino's The Great Beauty (La Grande Bellezza) won the 2014 Academy Award for Best Foreign Language Film.  The two highest-grossing Italian films in Italy have both been directed by Gennaro Nunziante and starred Checco Zalone: Sole a catinelle (2013) with €51.8 million, and Quo Vado? (2016) with €65.3 million.[200][201]  They Call Me Jeeg, a 2016 critically acclaimed superhero film directed by Gabriele Mainetti and starring Claudio Santamaria, won many awards, such as eight David di Donatello, two Nastro d'Argento, and a Globo d'oro.  Gianfranco Rosi's documentary film Fire at Sea (2016) won the Golden Bear at the 66th Berlin International Film Festival. They Call Me Jeeg and Fire at Sea were also selected as the Italian entry for the Best Foreign Language Film at the 89th Academy Awards, but they were not nominated.[202]  Other successful 2010s Italian films include: Vincere and The Traitor by Marco Bellocchio, The First Beautiful Thing (La prima cosa bella), Human Capital (Il capitale umano) and Like Crazy (La pazza gioia) by Paolo Virzì, We Have a Pope (Habemus Papam) and Mia Madre by Nanni Moretti, Caesar Must Die (Cesare deve morire) by Paolo and Vittorio Taviani, Don't Be Bad (Non essere cattivo) by Claudio Caligari, Romanzo Criminale by Michele Placido (that spawned a TV series, Romanzo criminale - La serie), Youth (La giovinezza) by Paolo Sorrentino, Suburra by Stefano Sollima, Perfect Strangers (Perfetti sconosciuti) by Paolo Genovese, Mediterranea and A Ciambra by Jonas Carpignano, Italian Race (Veloce come il vento) and The First King: Birth of an Empire (Il primo re) by Matteo Rovere, and Tale of Tales (Il racconto dei racconti), Dogman and Pinocchio by Matteo Garrone.  Call Me by Your Name (2017), the final installment in Luca Guadagnino's thematic Desire trilogy, following I Am Love (2009) and A Bigger Splash (2015), received widespread acclaim and numerous accolades, including the Academy Award for Best Adapted Screenplay and the nomination for Best Picture in 2018.  Perfect Strangers by Paolo Genovese was included in the Guinness World Records as it became the most remade film in cinema history, with a total of 18 versions of the film.[199]  Successful 2020s Italian films include: The Life Ahead by Edoardo Ponti, Hidden Away by Giorgio Diritti, Bad Tales by Damiano and Fabio D'Innocenzo, The Predators by Pietro Castellitto, Padrenostro by Claudio Noce, Notturno by Gianfranco Rosi, The King of Laughter by Mario Martone, A Chiara by Jonas Carpignano, Freaks Out by Gabriele Mainetti, The Hand of God by Paolo Sorrentino, Nostalgia by Mario Martone, Dry by Paolo Giordano, The Hanging Sun by Francesco Carrozzini, Bones and All by Luca Guadagnino, L'immensità by Emanuele Crialese and Robbing Mussolini by Renato De Maria.  The list of the 100 Italian films to be saved (Italian: 100 film italiani da salvare) was created with the aim to report \"100 films that have changed the collective memory of the country between 1942 and 1978\". The project was established in 2008 by the Venice Days festival section of the 65th Venice International Film Festival, in collaboration with Cinecittà Holding and with the support of the Ministry of Cultural Heritage.  The list was edited by Fabio Ferzetti,[203] film critic of the newspaper Il Messaggero, in collaboration with film director Gianni Amelio and the writers and film critics Gian Piero Brunetta, Giovanni De Luna, Gianluca Farinelli, Giovanna Grignaffini, Paolo Mereghetti, Morando Morandini, Domenico Starnone and Sergio Toffetti.[204][205]  Cineteca Nazionale is a film archive located in Rome. Founded in 1949, here are 80,000 films on file, 600,000 photographs, 50,000 posters and the collection of the Italian Association for the History of Cinema Research (AIRSC).[206] It arose from the archival heritage of the Centro Sperimentale di Cinematografia, which in 1943, had been removed by the Nazi occupiers, losing unique materials.[207][208][209] Cineteca Italiana is a private film archive located in Milan. Established in 1947, and as a foundation in 1996, the Cineteca Italiana houses over 20,000 films and more than 100,000 photographs from the history of Italian and international cinema.[210] Cineteca di Bologna is a film archive in Bologna. It was founded in 1962.[211]  The National Museum of Cinema (Italian: Museo Nazionale del Cinema) located in Turin is a motion picture museum inside the Mole Antonelliana tower. It is operated by the Maria Adriana Prolo Foundation, and the core of its collection is the result of the work of the historian and collector Maria Adriana Prolo. It was housed in the Palazzo Chiablese. In 2008, with 532,196 visitors, it ranked 13th among the most visited Italian museums.[212] The museum houses pre-cinematographic optical devices such as magic lanterns, earlier and current film technologies, stage items from early Italian movies and other memorabilia. Along the exhibition path of about 35,000 square feet (3,200 m2) on five levels, it is possible to visit some areas devoted to the different kinds of film crew, and in the main hall, fitted in the temple hall of the Mole (which was a building originally intended as a synagogue), a series of chapels representing several film genres.[213]  The Museum of Precinema (Italian: Museo del Precinema) is a museum in the Palazzo Angeli, Prato della Valle, Padua, related to the history of precinema, or precursors of film. It was created in 1998 to display the Minici Zotti Collection, in collaboration with the Comune of Padova. It also produces interactive touring exhibitions and makes valuable loans to other prestigious exhibitions such as Lanterne magique et film peint at the Cinémathèque Française in Paris and the National Museum of Cinema in Turin.  The Cinema Museum of Rome is located in Cinecittà. The collections consist of movie posters and playbills, cine cameras, projectors, magic lanterns, stage costumes and the patent of Filoteo Alberini's \"kinetograph\".[214] The Milan Cinema Museum, managed by the Cineteca Italiana, is divided into three sections, the precinema, animation cinema and \"Milan as a film set\", as well as multimedia and interactive stations.[215]  The Catania Cinema Museum exhibits documents concerning cinema, its techniques and its history, with particular attention to the link between cinema and Sicily.[216] The Cinema Museum of Syracuse collects more than 10,000 exhibits on display in 12 rooms.[217]  Italy is the most awarded country at the Academy Awards for Best Foreign Language Film, with 14 wins, 3 Special Awards and 31 nominations. Winners with the year of the ceremony:  In 1961, Sophia Loren won the Academy Award for Best Actress for her role as a woman who is raped during World War II, along with her adolescent daughter, in Vittorio De Sica's Two Women. She was the first actress to win an Academy Award for a performance in any foreign language, and the second Italian leading lady Oscar-winner, after Anna Magnani for The Rose Tattoo. In 1998, Roberto Benigni was the first Italian actor to win the Best Actor for Life Is Beautiful.  Italian-born filmmaker Frank Capra won three times at the Academy Award for Best Director, for It Happened One Night, Mr. Deeds Goes to Town and You Can't Take It with You. Bernardo Bertolucci won the award for The Last Emperor, and also Best Adapted Screenplay for the same movie.  Ennio De Concini, Alfredo Giannetti and Pietro Germi won the award for Best Original Screenplay for Divorce Italian Style. The Academy Award for Best Film Editing was won by Gabriella Cristiani for The Last Emperor and by Pietro Scalia for JFK and Black Hawk Down.  The award for Best Original Score was won by Nino Rota for The Godfather Part II;\tGiorgio Moroder for Midnight Express; Nicola Piovani for Life is Beautiful; Dario Marianelli for Atonement; and Ennio Morricone for The Hateful Eight. Giorgio Moroder also won the award for Best Original Song for Flashdance and Top Gun.  The Italian winners at the Academy Award for Best Production Design are Dario Simoni for Lawrence of Arabia and Doctor Zhivago; Elio Altramura and Gianni Quaranta for A Room with a View; Bruno Cesari, Osvaldo Desideri and Ferdinando Scarfiotti for The Last Emperor; Luciana Arrighi for Howards End; and Dante Ferretti and Francesca Lo Schiavo for The Aviator, Sweeney Todd: The Demon Barber of Fleet Street and Hugo.  The winners at the Academy Award for Best Cinematography are: Tony Gaudio for Anthony Adverse; Pasqualino De Santis\tfor Romeo and Juliet; Vittorio Storaro for Apocalypse Now, Reds and The Last Emperor; and Mauro Fiore for Avatar.  The winners at the Academy Award for Best Costume Design are Piero Gherardi for La dolce vita and 8½; Vittorio Nino Novarese for Cleopatra and Cromwell; Danilo Donati for The Taming of the Shrew, Romeo and Juliet, and Fellini's Casanova; Franca Squarciapino for Cyrano de Bergerac; Gabriella Pescucci\tfor The Age of Innocence; and Milena Canonero for Barry Lyndon, Chariots of Fire, Marie Antoinette and The Grand Budapest Hotel.  Special effects artist Carlo Rambaldi won three Oscars: one Special Achievement Academy Award for Best Visual Effects for King Kong[219] and two Academy Awards for Best Visual Effects for Alien[220] (1979) and E.T. the Extra-Terrestrial.[221] The Academy Award for Best Makeup and Hairstyling was won by Manlio Rocchetti for Driving Miss Daisy, and Alessandro Bertolazzi and Giorgio Gregorini for Suicide Squad.  Sophia Loren, Federico Fellini, Michelangelo Antonioni, Dino De Laurentiis, Ennio Morricone, and Piero Tosi also received the Academy Honorary Award. "},"meta":{},"created_at":"2025-03-22T14:25:42.291408Z","updated_at":"2025-03-22T14:25:42.291408Z","inner_id":88,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":97,"annotations":[{"id":97,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.322899Z","updated_at":"2025-03-22T14:25:42.322899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"05e80631-da40-4ced-bbfb-74e3c614766d","import_id":null,"last_action":null,"bulk_created":false,"task":97,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  The cinema of France comprises the film industry and its film productions, whether made within the nation of France or by French film production companies abroad. It is the oldest and largest precursor of national cinemas in Europe, with primary influence also on the creation of national cinemas in Asia.  The Lumière brothers launched cinematography in 1895 with their L'Arrivée d'un train en gare de La Ciotat. By the early 1900s, French cinema led globally, with pioneers like Méliès creating cinematic techniques and the first sci-fi film, A Trip to the Moon (1902). Studios like Pathé and Gaumont dominated, with Alice Guy-Blaché directing hundreds of films. Post-WWI, French cinema declined as U.S. films flooded Europe, leading to import quotas. Between the wars, directors like Jean Renoir, Jean Vigo and Marcel Carné shaped French Poetic Realism. Renoir’s La Règle du Jeu (1939) and Carné’s Les Enfants du Paradis (1945) remain iconic, showcasing innovation despite war challenges.  From the 1940s to the 1970s, French cinema flourished with the advent of the New Wave, led by critics-turned-directors like Jean-Luc Godard and François Truffaut, producing groundbreaking films such as Breathless (1960) and The 400 Blows (1959). The movement, which inspired global filmmakers, faded by the late 1960s. Meanwhile, commercial French cinema gained popularity with comedies like La Grande Vadrouille (1966). Stars like Brigitte Bardot, Alain Delon and Catherine Deneuve rose to international fame. Directors like Bertrand Tavernier explored political and artistic themes. By the late 1970s, films like La Cage aux Folles (1978) achieved significant global success.  France was able to produce several major box office successes into the 1990s such as Cyrano de Bergerac (1990), while certain film like La Femme Nikita (1990) and The Fifth Element (1997) reached an international audience.  In 2013, France was the second largest exporter of films in the world after the United States, and a 2014 study showed that French cinema was the most appreciated by global audiences after that of the US.[3]  According to industry tracker The Numbers, the fortunes of French film exports have since declined: in 2019, France had fallen to the position of 7th largest exporter by total box office revenue with a 2% share of the global market, and in 2023, 15th by the same metric with a 0.44% share.[4][5] Overall, France sits fourth on the tracker's all-time box office chart behind the US, UK, and China.[6]  Les frères Lumière released the first projection with the Cinematograph, in Paris on 28 December 1895, with first public showing in the Eden Theatre, La Ciotat.[7] The French film industry in the late 19th century and early 20th century was the world's most important. Auguste and Louis Lumière invented the cinématographe and their L'Arrivée d'un train en gare de La Ciotat in Paris in 1895 is considered by many historians as the official birth of cinematography. French films during this period catered to a growing middle class and were mostly shown in cafés and traveling fairs.[8]  The early days of the industry, from 1896 to 1902, saw the dominance of four firms: Pathé Frères, the Gaumont Film Company, the Georges Méliès company, and the Lumières.[9] Méliès invented many of the techniques of cinematic grammar, and among his fantastic, surreal short subjects is the first science fiction film A Trip to the Moon (Le Voyage dans la Lune) in 1902.  In 1902, the Lumières abandoned everything but the production of film stock, leaving Méliès as the weakest player of the remaining three. (He would retire in 1914.) From 1904 to 1911, the Pathé Frères company led the world in film production and distribution.[9]  At Gaumont, pioneer Alice Guy-Blaché (M. Gaumont's former secretary) was made head of production and oversaw about 400 films, from her first, La Fée aux Choux, in 1896, through 1906. She then continued her career in the United States, as did Maurice Tourneur and Léonce Perret after World War I.  In 1907, Gaumont owned and operated the biggest movie studio in the world, and along with the boom in construction of \"luxury cinemas\" like the Gaumont-Palace and the Pathé-Palace (both 1911), cinema became an economic challenger to theater by 1914.[9]  After World War I, the French film industry suffered because of a lack of capital, and film production decreased as it did in most other European countries. This allowed the United States film industry to enter the European cinema market, because American films could be sold more cheaply than European productions, since the studios already had recouped their costs in the home market. When film studios in Europe began to fail, many European countries began to set import barriers. France installed an import quota of 1:7, meaning for every seven foreign films imported to France, one French film was to be produced and shown in French cinemas.[10]  During the period between World War I and World War II, Jacques Feyder and Jean Vigo became two of the founders of poetic realism in French cinema. They also dominated French impressionist cinema, along with Abel Gance, Germaine Dulac and Jean Epstein.  In 1931, Marcel Pagnol filmed the first of his great trilogy Marius, Fanny, and César. He followed this with other films including The Baker's Wife. Other notable films of the 1930s included René Clair's Under the Roofs of Paris (1930), Jean Vigo's L'Atalante (1934), Jacques Feyder's Carnival in Flanders (1935), and Julien Duvivier's La belle equipe (1936). In 1935, renowned playwright and actor Sacha Guitry directed his first film and went on to make more than 30 films that were precursors to the New Wave era. In 1937, Jean Renoir, the son of painter Pierre-Auguste Renoir, directed La Grande Illusion (The Grand Illusion). In 1939, Renoir directed La Règle du Jeu (The Rules of the Game). Several critics have cited this film as one of the greatest of all-time, particularly for its innovative camerawork, cinematography and sound editing.  Marcel Carné's Les Enfants du Paradis (Children of Paradise) was filmed during World War II and released in 1945. The three-hour film was extremely difficult to make due to the Nazi occupation. Set in Paris in 1828, it was voted Best French Film of the Century in a poll of 600 French critics and professionals in the late 1990s.  In the magazine Cahiers du cinéma, founded by André Bazin and two other writers in 1951, film critics raised the level of discussion of the cinema, providing a platform for the birth of modern film theory. Several of the Cahiers critics, including Jean-Luc Godard, François Truffaut, Claude Chabrol, Jacques Rivette and Éric Rohmer, went on to make films themselves, creating what was to become known as the French New Wave. Some of the first films of this new movement were Godard's Breathless (À bout de souffle, 1960), starring Jean-Paul Belmondo, Rivette's Paris Belongs to Us (Paris nous appartient, 1958 – distributed in 1961), starring Jean-Claude Brialy and Truffaut's The 400 Blows (Les Quatre Cent Coups, 1959) starring Jean-Pierre Léaud. Later works are Contempt (1963) by Godard starring Brigitte Bardot and Michel Piccoli and Stolen Kisses starring Léaud and Claude Jade. Because Truffaut followed the hero of his screen debut, Antoine Doinel, for twenty years, the last post-New-Wave-film is Love on the Run in which his heroes Antoine (Léaud) and Christine (Jade) get divorced.  After World War II, the French actress Leslie Caron and the French actor Louis Jourdan enjoyed success in the United States with several musical romantic comedies, notably An American in Paris (1951) and Gigi (1958), based on the 1944 novella of the same name by Colette.  Many contemporaries of Godard and Truffaut followed suit, or achieved international critical acclaim with styles of their own, such as the minimalist films of Robert Bresson and Jean-Pierre Melville, the Hitchcockian-like thrillers of Henri-Georges Clouzot, and other New Wave films by Agnès Varda and Alain Resnais. The movement, while an inspiration to other national cinemas and unmistakably a direct influence on the future New Hollywood directors, slowly faded by the end of the 1960s.  During this period, French commercial film also made a name for itself. Immensely popular French comedies with Louis de Funès topped the French box office. The war comedy La Grande Vadrouille (1966), from Gérard Oury with Bourvil, de Funès and Terry-Thomas, was the most successful film in French theaters for more than 30 years. Another example was La Folie des grandeurs with Yves Montand. French cinema also was the birthplace for many subgenres of the crime film, most notably the modern caper film, starting with 1955's Rififi by American-born director Jules Dassin and followed by a large number of serious, noirish heist dramas as well as playful caper comedies throughout the sixties, and the \"polar,\" a typical French blend of film noir and detective fiction.  In addition, French movie stars began to claim fame abroad as well as at home. Popular actors of the period included Brigitte Bardot, Alain Delon, Romy Schneider, Catherine Deneuve, Jeanne Moreau, Simone Signoret, Yves Montand, Jean-Paul Belmondo and still Jean Gabin.  Since the Sixties and the early Seventies they are completed and followed by Michel Piccoli and Philippe Noiret as character actors, Annie Girardot, Jean-Louis Trintignant, Jean-Pierre Léaud, Claude Jade, Isabelle Huppert, Anny Duperey, Gérard Depardieu, Patrick Dewaere, Jean-Pierre Cassel, Miou-Miou, Brigitte Fossey, Stéphane Audran and Isabelle Adjani. During the Eightees they are added by a new generation including Sophie Marceau, Emmanuelle Béart, Jean-Hugues Anglade, Sabine Azema, Juliette Binoche and Daniel Auteuil.  In 1968, the May riots shook France. François Truffaut had already organised demonstrations in February against Henri Langlois's removal as head of the Cinémathèque française and dedicated his film Stolen Kisses, which was being made, to Langlois. The Cannes Film Festival is cancelled – on the initiative of Truffaut, Godard and Louis Malle. Jean-Luc Godard no longer works in the commercial film business for years. Political films such as Costa-Gavras' Z celebrate success. Chabrol continues his vivisection of the bourgeoisie (The Unfaithful Wife) and Truffaut explores the possibility of bourgeois marital happiness (Bed and Board). While Godard disappears from cinema after the Nouvelle Vague except for a few essays, Truffaut and Chabrol remain the leading directors whose artistic aspects remain commercially successful. Other directors of the 1970s in this effect are Bertrand Tavernier, Claude Sautet, Eric Rohmer, Claude Lelouch, Georges Lautner, Jean-Paul Rappeneau, Michel Deville Yves Boisset, Maurice Pialat, Bertrand Blier, Coline Serreau and André Téchiné in purely entertainment films, it is Gérard Oury and Édouard Molinaro.  The 1979 film La Cage aux Folles ran for well over a year at the Paris Theatre, an arthouse cinema in New York City, and was a commercial success at theaters throughout the country, in both urban and rural areas. It won the Golden Globe Award for Best Foreign Language Film, and for years it remained the most successful foreign film to be released in the United States.[11]  Jean-Jacques Beineix's Diva (1981) sparked the beginning of the 1980s wave of French cinema. Movies which followed in its wake included Betty Blue (37°2 le matin, 1986) by Beineix, The Big Blue (Le Grand bleu, 1988) by Luc Besson, and The Lovers on the Bridge (Les Amants du Pont-Neuf, 1991) by Léos Carax. Made with a slick commercial style and emphasizing the alienation of their main characters, these films are representative of the style known as Cinema du look.  Camille Claudel, directed by newcomer Bruno Nuytten and starring Isabelle Adjani and Gérard Depardieu, was a major commercial success in 1988, earning Adjani, who was also the film's co-producer, a César Award for best actress. The historical drama film Jean de Florette (1986) and its sequel Manon des Sources (1986) were among the highest grossing French films in history and brought Daniel Auteuil international recognition.  According to Raphaël Bassan, in his article «The Angel: Un météore dans le ciel de l'animation,» La Revue du cinéma, n° 393, avril 1984. (in French), Patrick Bokanowski's The Angel, shown in 1982 at the Cannes Film Festival, can be considered the beginnings of contemporary animation. The masks erase all human personality in the characters. Patrick Bokanowski would thus have total control over the \"matter\" of the image and its optical composition. This is especially noticeable throughout the film, with images taken through distorted objectives or a plastic work on the sets and costumes, for example in the scene of the designer. Patrick Bokanowski creates his own universe and obeys his own aesthetic logic. It takes us through a series of distorted areas, obscure visions, metamorphoses and synthetic objects. Indeed, in the film, the human may be viewed as a fetish object (for example, the doll hanging by a thread), with reference to Kafkaesque and Freudian theories on automata and the fear of man faced with something as complex as him. The ascent of the stairs would be the liberation of the ideas of death, culture, and sex that makes us reach the emblematic figure of the angel.  Jean-Paul Rappeneau's Cyrano de Bergerac was a major box-office success in 1990, earning several César Awards, including best actor for Gérard Depardieu, as well as an Academy Award nomination for best foreign picture.  Luc Besson made La Femme Nikita in 1990, a movie that inspired remakes in both United States and in Hong Kong. In 1994, he also made Léon (starring Jean Reno and a young Natalie Portman), and in 1997 The Fifth Element, which became a cult favorite and launched the career of Milla Jovovich.  Jean-Pierre Jeunet made Delicatessen and The City of Lost Children (La Cité des enfants perdus), both of which featured a distinctly fantastical style.  In 1992, Claude Sautet co-wrote (with Jacques Fieschi) and directed Un Coeur en Hiver, considered by many to be a masterpiece. Mathieu Kassovitz's 1995 film Hate (La Haine) received critical praise and made Vincent Cassel a star, and in 1997, Juliette Binoche won the Academy Award for Best Supporting Actress for her role in The English Patient.  The success of Michel Ocelot's Kirikou and the Sorceress in 1998 rejuvenated the production of original feature-length animated films by such filmmakers as Jean-François Laguionie and Sylvain Chomet.  In 2000, Philippe Binant realized the first digital cinema projection in Europe, with the DLP CINEMA technology developed by Texas Instruments, in Paris.[12][13][14]  In 2001, after a brief stint in Hollywood, Jean-Pierre Jeunet returned to France with Amélie (Le Fabuleux Destin d'Amélie Poulain) starring Audrey Tautou. It became the highest-grossing French-language film ever released in the United States. The following year, Brotherhood of the Wolf  became the sixth-highest-grossing French-language film of all time in the United States and went on to gross more than $70 million worldwide.  In 2008, Marion Cotillard won the Academy Award for Best Actress and the BAFTA Award for Best Actress in a Leading Role for her portrayal of legendary French singer Édith Piaf in La Vie en Rose, the first French-language performance to be so honored. The film won two Oscars and four BAFTAs and became the third-highest-grossing French-language film in the United States since 1980. Cotillard was the first female and second person to win both an Academy Award and César Award for the same performance.  At the 2008 Cannes Film Festival, Entre les murs (The Class) won the Palme d'Or, the 6th French victory at the festival. The 2000s also saw an increase in the number of individual competitive awards won by French artists at the Cannes Festival, for direction (Tony Gatlif, Exils, 2004), screenplay (Agnès Jaoui and Jean-Pierre Bacri, Look at Me, 2004), female acting (Isabelle Huppert, The Piano Teacher, 2001; Charlotte Gainsbourg, Antichrist, 2009) and male acting (Jamel Debbouze, Samy Naceri, Roschdy Zem, Sami Bouajila and Bernard Blancan, Days of Glory, 2006).  The 2008 rural comedy Bienvenue chez les Ch'tis drew an audience of more than 20 million, the first French film to do so. Its $193 million gross in France puts it just behind Titanic as the most successful film of all time in French theaters.  In the 2000s, several French directors made international productions, often in the action genre. These include Gérard Pirès (Riders, 2002), Pitof (Catwoman, 2004), Jean-François Richet (Assault on Precinct 13, 2005), Florent Emilio Siri (Hostage, 2005), Christophe Gans (Silent Hill, 2006), Mathieu Kassovitz (Babylon A.D., 2008), Louis Leterrier (The Transporter, 2002; Transporter 2, 2005; Olivier Megaton directed Transporter 3, 2008), Alexandre Aja (Mirrors, 2008), and Pierre Morel (Taken, 2009).  Surveying the entire range of French filmmaking today, Tim Palmer calls contemporary cinema in France a kind of eco-system, in which commercial cinema co-exists with artistic radicalism, first-time directors (who make up about 40% of all France's directors each year) mingle with veterans, and there even occasionally emerges a fascinating pop-art hybridity, in which the features of intellectual and mass cinemas are interrelated (as in filmmakers like Valeria Bruni-Tedeschi, Olivier Assayas, Maïwenn, Sophie Fillières, Serge Bozon, and others).[15]  One of the most noticed and best reviewed films of 2010 was the drama Of Gods and Men (Des hommes et des dieux), about the assassination of seven monks in Tibhirine, Algeria.  2011 saw the release of The Artist, a silent film shot in black and white by Michel Hazanavicius that reflected on the end of Hollywood's silent era.  French cinema continued its upward trend of earning awards at the Cannes Festival, including the prestigious Grand Prix for Of Gods and Men (2010) and the Jury Prize for Poliss (2011); the Best Director Award for Mathieu Amalric (On Tour, 2010); the Best Actress Award for Juliette Binoche (Certified Copy, 2010); and the Best Actor Award for Jean Dujardin (The Artist, 2011).  In 2011, the film The Intouchables became the most watched film in France (including the foreign films). After ten weeks nearly 17.5 million people had seen the film in France,[16] the film was the second most-seen French movie of all time in France, and the third including foreign movies.  In 2012, with 226 million admissions (US$1,900 million) in the world for French films (582 films released in 84 countries), including 82[17] million admissions in France (US$700 million), 2012 was the fourth best year since 1985. With 144 million admissions outside France (US$1,200 million),[18] 2012 was the best year since at least 1994 (since Unifrance collects data),[19] and the French cinema reached a market share of 2.95% of worldwide admissions and of 4.86% of worldwide sales.[20][21] Three films particularly contributed to this record year: Taken 2, The Intouchables and The Artist.[22] In 2012, films shot in French ranked 4th in admissions (145 million) behind films shot in English (more than a billion admissions in the US alone), Hindi (?: no accurate data but estimated at 3 billion for the whole India\/Indian languages) and Chinese (275 million in China plus a few million abroad), but above films shot in Korean (115 million admissions in South Korea plus a few millions abroad) and Japanese (102 million admissions in Japan plus a few million abroad,[23][24] a record since 1973 et its 104 million admissions). French-language movies ranked 2nd in export (outside of French-speaking countries) after films in English. 2012 was also the year French animation studio Mac Guff was acquired by an American studio, Universal Pictures, through its Illumination Entertainment subsidiary. Illumination Mac Guff became the animation studio for some of the top English-language animated movies of the 2010s, including The Lorax and the Despicable Me franchise.  In 2015 French cinema sold 106 million tickets and grossed €600 million outside of the country. The highest-grossing film was Taken 3 (€261.7 million) and the largest territory in admissions was China (14.7 million).[25] In that year, France produced more films than any other European country, producing a record-breaking 300 feature-length films.[26] France is one of the few countries where non-American productions have the biggest share; American films only represented 44.9% of total admissions in 2014. This is largely due to the commercial strength of domestic productions.[27]  In 2013, France was the second largest exporter of films in the world after the United States, and a 2014 study showed that French cinema was the most appreciated by global audiences after that of the US.[3]  France has had a very strong film industry, due in part to protections afforded by the French Government.[28]  The French government has implemented various measures aimed at supporting local film production and movie theaters. Canal+ has a broadcast license requiring it to support the production of movies. Some taxes are levied on movies and TV channels for use as subsidies for movie production.[citation needed]  The French national and regional governments also involve themselves in film production. For example, the award-winning documentary In the Land of the Deaf (Le Pays des sourds), created by Nicolas Philibert in 1992, was co-produced by multinational partners, reducing the financial risks inherent in the project and ensuring enhanced distribution opportunities.[29][30][31][32]  On 2 February 2000 in Paris,  Philippe Binant realized the first digital cinema projection in Europe, with the DLP Cinema technology developed by Texas Instruments.[12][13][14]  In 2011 Paris had the highest density of cinemas in the world, measured by the number of movie theaters per inhabitant,[33] In most downtown Paris movie theaters, foreign or arthouse films movies are shown alongside mainstream movies.    Paris also boasts the Cité du cinéma, a major studio north of the city, and Disney Studio, a theme park devoted to the cinema.[34]  Notable French film distribution and\/or production companies include:  In February 2024, French actress and director Judith Godrèche called for the French film industry to “face the truth” on sexual violence and physical abuse during an appearance she made during the live broadcast of the 2024 Cesar Awards ceremony.[35] Shortly before this, Godrèche claimed that she was sexually abused by prominent French directors Benoit Jacquot and Jacques Doillon.[35][36] In May 2024, the French parliament launched a Commission to investigation reports of sex abuse in the French film industry.[36] In July 2024, in the wake of the investigation, and later rape charge, against Jacquot for separate sex abuse allegations, it was reported that several men in the French filmmaking industry were facing a flurry of sex abuse allegations.[37] It was also alleged that the industry provided a cover for abuse.[37] "},"meta":{},"created_at":"2025-03-22T14:25:42.291408Z","updated_at":"2025-03-22T14:25:42.291408Z","inner_id":89,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":98,"annotations":[{"id":98,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.322899Z","updated_at":"2025-03-22T14:25:42.322899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"5d527e3f-5d5f-4b73-9501-12a7868a9068","import_id":null,"last_action":null,"bulk_created":false,"task":98,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Financial innovation is the act of creating new financial instruments as well as new financial technologies, institutions, and markets. Recent financial innovations include hedge funds, private equity, weather derivatives, retail-structured products, exchange-traded funds, multi-family offices, and Islamic bonds (Sukuk). The shadow banking system has spawned an array of financial innovations including mortgage-backed securities products and collateralized debt obligations (CDOs).[1]  There are three categories of innovation: institutional, product, and process. Institutional innovations relate to the creation of new types of financial firms such as specialist credit card firms, investment consulting firms and related services, and direct banks. Product innovation relates to new products such as derivatives, securitization, and foreign currency mortgages. Process innovations relate to new ways of doing financial business, including online banking and telephone banking.[1]  Financial innovations emerge as a result of a complex interaction between and among household savings and borrowing needs, firm financing needs, the need to identify and manage risks, advances in financial theory and information technology, financial sector profit motives, and, finally, macroeconomic and regulatory factors.[2] Furthermore, distinct financial innovations may arise in different ways depending on whether they are products, platforms, or processes. Several explanations for the emergence of financial innovation have been presented.  Economic theory has much to say about what types of securities should exist, and why some may not exist (why some markets should be \"incomplete\") but little to say about why new types of securities should come into existence.  One interpretation of the Modigliani–Miller theorem is that taxes and regulation are the only reasons for investors to care what kinds of securities firms issue, whether debt, equity, or something else. The theorem states that the structure of a firm's liabilities should have no bearing on its net worth (absent taxes). The securities may trade at different prices depending on their composition, but they must ultimately add up to the same value.  The traditional account of the determinants of financial innovation in economics is the rationalist approach, which is found in Proposition I of the Modigliani and Miller (M&M) irrelevance theory.[3] According to Proposition I, a company's worth is determined by its potential to generate profits and the risk of its underlying assets. The M&M theory remains true only when substantial assumptions about market flaws are made. These imperfections include information asymmetries, adverse selection and agency problems,[4] incomplete markets,[5] regulation and taxes,[6] and other frictions that limit market participants' ability to maximize utility and would necessitate financial innovations to reduce.[5]  Parallel to the M&M theorem go the works of Markowitz on risk modeling, Eugene Fama on efficient financial markets, William F. Sharpe on quantifying the value of an asset, and Black, Scholes, and Merton on the value of risk laid the path for financial innovations to arise.[7] Yet, the M&M concept has a fundamental problem. The dominant perspective in M&M theory is demand-driven, which overlooks that financial innovations might represent a technological push, meaning they can originate irrespective of market demand reasons. For a long period, the push-pull argument dominated technical thought.[8] Industrial technologists have determined that both elements (push and pull) are relevant.[8] Following this conclusion, the emphasis has shifted to comprehending the confluence of economic, political, institutional, and technological elements, underpinning innovations.[9]  Furthermore, there should be little demand for specific types of securities. The capital asset pricing model, first developed by Jack L. Treynor and William Sharpe, suggests that investors should fully diversify and their portfolios should be a mixture of the \"market\" and a risk-free investment. Investors with different risk\/return goals can use leverage to increase the ratio of the market return to the risk-free return in their portfolios. However, Richard Roll argued that this model was incorrect, because investors cannot invest in the entire market. This implies there should be demand for instruments that open up new types of investment opportunities (since this gets investors closer to being able to buy the entire market), but not for instruments that merely repackage existing risks (since investors already have as much exposure to those risks in their portfolio).  If the world existed as the Arrow–Debreu model posits, then there would be no need for financial innovation. The model assumes that investors are able to purchase securities that pay off if and only if a certain state of the world occurs. Investors can then combine these securities to create portfolios that have whatever payoff they desire. The fundamental theorem of finance states that the price of assembling such a portfolio will be equal to its expected value under the appropriate risk-neutral measure.  Tufano (2003) and Duffie and Rahi (1995) provide useful reviews of the literature.  The extensive literature on principal–agent problems, adverse selection, and information asymmetry points to why investors might prefer some types of securities, such as debt, over others like equity. Myers and Majluf (1984) develop an adverse selection model of equity issuance, in which firms (which are trying to maximize profits for existing shareholders) issue equity only if they are desperate. This was an early article in the pecking order literature, which states that firms prefer to finance investments out of retained earnings first, then debt, and finally equity, because investors are reluctant to trust any firm that needs to issue equity.  Duffie and Rahi also devote a considerable section to examining the utility and efficiency implications of financial innovation. This is also the topic of many of the papers in the special edition of the Journal of Economic Theory in which theirs is the lead article. The usefulness of spanning the market appears to be limited (or, equivalently, the disutility of incomplete markets is not great).  Allen and Gale (1988) is one of the first papers to endogenize security issuance contingent on financial regulation—specifically, bans on short sales. In these circumstances, they find that the traditional split of cash flows between debt and equity is not optimal, and that state-contingent securities are preferred. Ross (1989) develops a model in which new financial products must overcome marketing and distribution costs. Persons and Warther (1997) studied booms and busts associated with financial innovation.  The fixed costs of creating liquid markets for new financial instruments appears to be considerable. Black and Scholes (1974) describe some of the difficulties they encountered when trying to market the forerunners to modern index funds. These included regulatory problems, marketing costs, taxes, and fixed costs of management, personnel, and trading. Shiller (2008) describes some of the frustrations involved with creating a market for house price futures.  Some types of financial instrument became prominent after macroeconomic conditions forced investors to be more aware of the need to hedge certain types of risk.  Futures, options, and many other types of derivatives have been around for centuries: the Japanese rice futures market started trading around 1730. However, recent decades have seen an explosion use of derivatives and mathematically complicated securitization techniques. From a sociological point of view, some economists argue that mathematical formulas actually change the way that economic agents use and price assets. Economists, rather than acting as a camera taking an objective picture of the way the world works, actively change behavior by providing formulas that let dispersed agents agree on prices for new assets.[11] See Exotic derivative, Exotic option.  Miller (1986) placed great emphasis on the role of taxes and government regulation in stimulating financial innovation.[6] The Modigliani–Miller theorem explicitly considered taxes as a reason to prefer one type of security over another, despite that corporations and investors should be indifferent to capital structure in a fractionless world.  The development of checking accounts at U.S. banks was in order to avoid punitive taxes on state bank notes that were part of the National Banking Act.  Some investors use total return swaps to convert dividends into capital gains, which are taxed at a lower rate.[12]  Many times, regulators have explicitly discouraged or outlawed trading in certain types of financial securities. In the United States, gambling is mostly illegal, and it can be difficult to tell whether financial contracts are illegal gambling instruments or legitimate tools for investment and risk-sharing. The Commodity Futures Trading Commission (CFTC) is in charge of making this determination. The difficulty that the Chicago Board of Trade faced in attempting to trade futures on stocks and stock indexes is described in Melamed (1996).  In the United States, Regulation Q drove several types of financial innovation to get around its interest rate ceilings, including eurodollars and NOW accounts.  Some types of financial innovation are driven by improvements in computer and telecommunication technology. For example, Paul Volcker suggested that for most people, the creation of the ATM was a greater financial innovation than asset-backed securitization.[13] Other types of financial innovation affecting the payments system include credit and debit cards and online payment systems like PayPal.  These types of innovations are notable because they reduce transaction costs. Households need to keep lower cash balances—if the economy exhibits cash-in-advance constraints then these kinds of financial innovations can contribute to greater efficiency. One study of Italian households' use of debit cards found that ownership of an ATM card resulted in benefits worth €17 annually.[14]  These types of innovations may also affect monetary policy by reducing real household balances. Especially with the increased popularity of online banking, households are able to keep greater percentages of their wealth in non-cash instruments. In a special edition of International Finance devoted to the interaction of e-commerce and central banking, Goodhart (2000) and Woodford (2000) express confidence in the ability of a central bank to maintain its policy goals by affecting the short-term interest rate even if electronic money has eliminated the demand for central bank liabilities,[15][16] while Friedman (2000) is less sanguine.[17]  A 2016 PwC report pointed to the \"accelerating pace of technological change\" as the \"most creative force—and also the most destructive—in the financial services ecosystem\".[18]  The advancement of technology has enabled a segment of underserved clients to access more complex investing alternatives, such as social trading tools and platforms, and retail algorithmic trading.[19] The first ones help inexperienced investors gain expertise and knowledge, for example, by copy trading, which allows them to imitate top-performing traders' portfolios (e.g., eToro, Estimize, Stocktwits). The second option allows investors with minimum technical skills to build, backtest, and implement trading algorithms, which they may then share with others (Streak, Quantopian & Zipline, Numerai).[20] These solutions, mostly provided by FinTechs, provide simple and fast ways to optimize returns. They are also less expensive than traditional investment management since, unlike traditional investment management, most social trading platforms do not demand a minimum investment to get started.[20]  In developed markets, the amount of algorithm trading is now approximately 70-80%.[21] Advances in computer computing power, data collecting, and telecommunications all contributed to the creation of algorithmic trading.[22]  Financial innovations may influence economic or financial systems. For instance, financial innovation may affect monetary policy effectiveness and the ability of central banks to stabilize the economy. The relationship between money and interest rates, which can define monetary policy effectiveness, is affected by financial innovation. Financial innovation also influences firm profitability, transactions, and social welfare.[23]  According to the traditional innovation-growth theory, financial innovations assist in increasing the quality and diversity of banking services, allow risk sharing, complete the market, and, ultimately, improve allocative efficiency. Thus, concentrating on the positive aspects of financial innovation.[24][25][26][27]  The innovation fragility perspective, on the other hand, focuses on the \"dark\" side of innovation. It specifically identified financial innovations as the root cause of the recent Global Financial Crisis, leading to unprecedented credit expansion that fueled the boom and subsequent bust in housing prices, engineering securities perceived to be safe but exposed to overlooked risks, and assisting banks in developing structured products to capitalize on investors' misunderstandings of financial markets.[28][29][30]  There is no definitive evidence of whether financial innovation benefits or damages the financial industry. Nevertheless, there is compelling evidence that financial innovation is linked to higher levels of economic growth.[31] Similarly, there is evidence that financial innovation promotes bank expansion and financial depth.[32]  Some economists argue that financial innovation has little to no productivity benefit: Paul Volcker stated that \"there is little correlation between sophistication of a banking system and productivity growth\",[13] that there is no \"neutral evidence that financial innovation has led to economic growth\",[33] and that financial innovation was a cause of the financial crisis of 2007–2010,[34] while Paul Krugman states that \"the rapid growth in finance since 1980 has largely been a matter of rent-seeking, rather than true productivity\".[35] "},"meta":{},"created_at":"2025-03-22T14:25:42.291408Z","updated_at":"2025-03-22T14:25:42.291408Z","inner_id":90,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":99,"annotations":[{"id":99,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.322899Z","updated_at":"2025-03-22T14:25:42.322899Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"34e71ecc-6c2a-4483-8882-2ba4fb2b5db2","import_id":null,"last_action":null,"bulk_created":false,"task":99,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A financial institution, sometimes called a banking institution, is a business entity that provides service as an intermediary for different types of financial monetary transactions. Broadly speaking, there are three major types of financial institution:[1][2]  Financial institutions can be distinguished broadly into two categories according to ownership structure:  Some experts see a trend toward homogenisation of financial institutions, meaning a tendency to invest in similar areas and have similar business strategies. A consequence of this might be fewer banks serving specific target groups, and small-scale producers may be under-served.[3] This is why a target of the United Nations Sustainable Development Goal 10 is to improve the regulation and monitoring of global financial institutions and strengthen such regulations.[4]  Standard Settlement Instructions (SSIs) are the agreements between two financial institutions which fix the receiving agents of each counterparty in ordinary trades of some type. These agreements allow the related counterparties to make faster operations since the time used to settle the receiving agents is conserved. Limiting each subject to an SSI also lowers the likelihood of a fraud. SSIs are used by financial institutions to facilitate fast and accurate cross-border payments.  Financial institutions in most countries operate in a heavily regulated environment because they are critical parts of countries' economies, due to economies' dependence on them to grow the money supply via fractional-reserve banking. Regulatory structures differ in each country, but typically involve prudential regulation as well as consumer protection and market stability. Some countries have one consolidated agency that regulates all financial institutions while others have separate agencies for different types of institutions such as banks, insurance companies and brokers.  Countries that have separate agencies include the United States, where the key governing bodies are the Federal Financial Institutions Examination Council (FFIEC), Office of the Comptroller of the Currency – National Banks, Federal Deposit Insurance Corporation (FDIC) State \"non-member\" banks, National Credit Union Administration (NCUA) – Credit Unions, Federal Reserve (Fed) – \"member\" banks, Office of Thrift Supervision – National Savings & Loan Association, State governments each often regulate and charter financial institutions.  Countries that have one consolidated financial regulator include: Norway with the Financial Supervisory Authority of Norway, Germany with Federal Financial Supervisory Authority and Russia with Central Bank of Russia.  Merits of raising funds through financial institutions are as follows: "},"meta":{},"created_at":"2025-03-22T14:25:42.291408Z","updated_at":"2025-03-22T14:25:42.291408Z","inner_id":91,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":100,"annotations":[{"id":100,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.323896Z","updated_at":"2025-03-22T14:25:42.323896Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"069a62ab-6efa-4dfc-9198-d968ed1f0767","import_id":null,"last_action":null,"bulk_created":false,"task":100,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A bank run or run on the bank occurs when many clients withdraw their money from a bank, because they believe the bank may fail in the near future. In other words, it is when, in a fractional-reserve banking system (where banks normally only keep a small proportion of their assets as cash), numerous customers withdraw cash from deposit accounts with a financial institution at the same time because they believe that the financial institution is, or might become, insolvent. When they transfer funds to another institution, it may be characterized as a capital flight. As a bank run progresses, it may become a self-fulfilling prophecy: as more people withdraw cash, the likelihood of default increases, triggering further withdrawals. This can destabilize the bank to the point where it runs out of cash and thus faces sudden bankruptcy.[1] To combat a bank run, a bank may acquire more cash from other banks or from the central bank, or limit the amount of cash customers may withdraw, either by imposing a hard limit or by scheduling quick deliveries of cash, encouraging high-return term deposits to reduce on-demand withdrawals or suspending withdrawals altogether.  A banking panic or bank panic is a financial crisis that occurs when many banks suffer runs at the same time, as people suddenly try to convert their threatened deposits into cash or try to get out of their domestic banking system altogether. A systemic banking crisis is one where all or almost all of the banking capital in a country is wiped out.[2] The resulting chain of bankruptcies can cause a long economic recession as domestic businesses and consumers are starved of capital as the domestic banking system shuts down.[3]  According to former U.S. Federal Reserve chairman Ben Bernanke, the Great Depression was caused by the failure of the Federal Reserve System to prevent deflation,[4] and much of the economic damage was caused directly by bank runs.[5] The cost of cleaning up a systemic banking crisis can be huge, with fiscal costs averaging 13% of GDP and economic output losses averaging 20% of GDP for important crises from 1970 to 2007.[2]  Several techniques have been used to try to prevent bank runs or mitigate their effects. They have included a higher reserve requirement (requiring banks to keep more of their reserves as cash), government bailouts of banks, supervision and regulation of commercial banks, the organization of central banks that act as a lender of last resort, the protection of deposit insurance systems such as the U.S. Federal Deposit Insurance Corporation,[1] and after a run has started, a temporary suspension of withdrawals.[6] These techniques do not always work: for example, even with deposit insurance, depositors may still be motivated by beliefs they may lack immediate access to deposits during a bank reorganization.[7]  Bank runs first appeared as a part of cycles of credit expansion and its subsequent contraction. From the 16th century onwards, English goldsmiths issuing promissory notes suffered severe failures due to bad harvests, plummeting parts of the country into famine and unrest. Other examples are the Dutch tulip manias (1634–37), the British South Sea Bubble (1717–19), the French Mississippi Company (1717–20), the post-Napoleonic depression (1815–30), and the Great Depression (1929–39).  Bank runs have also been used to blackmail individuals and governments. In 1832, for example, the British government under the Duke of Wellington overturned a majority government on the orders of the king, William IV, to prevent reform (the later Reform Act 1832 (2 & 3 Will. 4. c. 45)). Wellington's actions angered reformers, and they threatened a run on the banks under the rallying cry \"Stop the Duke, go for gold!\".[8]  Many of the recessions in the United States were caused by banking panics. The Great Depression contained several banking crises consisting of runs on multiple banks from 1929 to 1933; some of these were specific to regions of the U.S.[3] Bank runs were most common in states whose laws allowed banks to operate only a single branch, dramatically increasing risk compared to banks with multiple branches particularly when single-branch banks were located in areas economically dependent on a single industry.[9]  Banking panics began in the Southern United States in November 1930, one year after the stock market crash, triggered by the collapse of a string of banks in Tennessee and Kentucky, which brought down their correspondent networks. In December, New York City experienced massive bank runs that were contained to the many branches of a single bank. Philadelphia was hit a week later by bank runs that affected several banks, but were successfully contained by quick action by the leading city banks and the Federal Reserve Bank.[10] Withdrawals became worse after financial conglomerates in New York and Los Angeles failed in prominently-covered scandals.[11] Much of the US Depression's economic damage was caused directly by bank runs,[5] though Canada had no bank runs during this same era due to different banking regulations.[9]  Milton Friedman and Anna Schwartz argued that steady withdrawals from banks by nervous depositors (\"hoarding\") were inspired by news of the fall 1930 bank runs and forced banks to liquidate loans, which directly caused a decrease in the money supply, shrinking the economy.[12] Bank runs continued to plague the United States for the next several years. Citywide runs hit Boston (December 1931), Chicago (June 1931 and June 1932), Toledo (June 1931), and St. Louis (January 1933), among others.[13] Institutions put into place during the Depression have prevented runs on U.S. commercial banks since the 1930s,[14] even under conditions such as the U.S. savings and loan crisis of the 1980s and 1990s.[15]  The 2007–2008 financial crisis was centered around market-liquidity failures that were comparable to a bank run. The crisis contained a wave of bank nationalizations, including those associated with Northern Rock of the UK and IndyMac of the U.S. This crisis was caused by low real interest rates stimulating an asset price bubble fuelled by new financial products that were not stress tested and that failed in the downturn.[16]  Under fractional-reserve banking, the type of banking currently used in most developed countries,  banks retain only a fraction of their demand deposits as cash. The remainder is invested in securities and loans, whose terms are typically longer than the demand deposits, resulting in an asset–liability mismatch. No bank has enough reserves on hand to cope with all deposits being taken out at once.[17][better source needed]  Diamond and Dybvig developed an influential model to explain why bank runs occur and why banks issue deposits that are more liquid than their assets. According to the model, the bank acts as an intermediary between borrowers who prefer long-maturity loans and depositors who prefer liquid accounts.[1][14] The Diamond–Dybvig model provides an example of an economic game with more than one Nash equilibrium, where it is logical for individual depositors to engage in a bank run once they suspect one might start, even though that run will cause the bank to collapse.[1]  In the model, business investment requires expenditures in the present to obtain returns that take time in coming, for example, spending on machines and buildings now for production in future years. A business or entrepreneur that needs to borrow to finance investment will want to give their investments a long time to generate returns before full repayment, and will prefer long maturity loans, which offer little liquidity to the lender. The same principle applies to individuals and households seeking financing to purchase large-ticket items such as housing or automobiles. The households and firms who have the money to lend to these businesses may have sudden, unpredictable needs for cash, so they are often willing to lend only on the condition of being guaranteed immediate access to their money in the form of liquid demand deposit accounts, that is, accounts with shortest possible maturity. Since borrowers need money and depositors fear to make these loans individually, banks provide a valuable service by aggregating funds from many individual deposits, portioning them into loans for borrowers, and spreading the risks both of default and sudden demands for cash.[1] Banks can charge much higher interest on their long-term loans than they pay out on demand deposits, allowing them to earn a profit.  If only a few depositors withdraw at any given time, this arrangement works well. Barring some major emergency on a scale matching or exceeding the bank's geographical area of operation, depositors' unpredictable needs for cash are unlikely to occur at the same time; that is, by the law of large numbers, banks can expect only a small percentage of accounts withdrawn on any one day because individual expenditure needs are largely uncorrelated. A bank can make loans over a long horizon, while keeping only relatively small amounts of cash on hand to pay any depositors who may demand withdrawals.[1]  However, if many depositors withdraw all at once, the bank itself (as opposed to individual investors) may run short of liquidity, and depositors will rush to withdraw their money, forcing the bank to liquidate many of its assets at a loss, and eventually to fail. If such a bank were to attempt to call in its loans early, businesses might be forced to disrupt their production while individuals might need to sell their homes and\/or vehicles, causing further losses to the larger economy.[1] Even so, many, if not most, debtors would be unable to pay the bank in full on demand and would be forced to declare bankruptcy, possibly affecting other creditors in the process.  A bank run can occur even when started by a false story. Even depositors who know the story is false will have an incentive to withdraw, if they suspect other depositors will believe the story. The story becomes a self-fulfilling prophecy.[1] Indeed, Robert K. Merton, who coined the term self-fulfilling prophecy, mentioned bank runs as a prime example of the concept in his book Social Theory and Social Structure.[18] Mervyn King, governor of the Bank of England, once noted that it may not be rational to start a bank run, but it is rational to participate in one once it had started.[19]  A bank run is the sudden withdrawal of deposits of just one bank. A banking panic or bank panic is a financial crisis that occurs when many banks suffer runs at the same time, as a cascading failure. In a systemic banking crisis, all or almost all of the banking capital in a country is wiped out; this can result when regulators ignore systemic risks and spillover effects.[2]  Systemic banking crises are associated with substantial fiscal costs and large output losses. Frequently, emergency liquidity support and blanket guarantees have been used to contain these crises, not always successfully. Although fiscal tightening may help contain market pressures if a crisis is triggered by unsustainable fiscal policies, expansionary fiscal policies are typically used. In crises of liquidity and solvency, central banks can provide liquidity to support illiquid banks. Depositor protection can help restore confidence, although it tends to be costly and does not necessarily speed up economic recovery. Intervention is often delayed in the hope that recovery will occur, and this delay increases the stress on the economy.[2]  Some measures are more effective than others in containing economic fallout and restoring the banking system after a systemic crisis.[2][20] These include establishing the scale of the problem, targeted debt relief programs to distressed borrowers, corporate restructuring programs, recognizing bank losses, and adequately capitalizing banks. Speed of intervention appears to be crucial; intervention is often delayed in the hope that insolvent banks will recover if given liquidity support and relaxation of regulations, and in the end this delay increases stress on the economy. Programs that are targeted, that specify clear quantifiable rules that limit access to preferred assistance, and that contain meaningful standards for capital regulation, appear to be more successful. According to IMF, government-owned asset management companies (bad banks) are largely ineffective due to political constraints.[2]  A silent run occurs when the implicit fiscal deficit from a government's unbooked loss exposure[clarification needed] to zombie banks is large enough to deter depositors of those banks. As more depositors and investors begin to doubt whether a government can support a country's banking system, the silent run on the system can gather steam, causing the zombie banks' funding costs to increase. If a zombie bank sells some assets at market value, its remaining assets contain a larger fraction of unbooked losses; if it rolls over its liabilities at increased interest rates, it squeezes its profits along with the profits of healthier competitors. The longer the silent run goes on, the more benefits are transferred from healthy banks and taxpayers to the zombie banks.[21] The term is also used when many depositors in countries with deposit insurance draw down their balances below the limit for deposit insurance.[22]  The cost of cleaning up after a crisis can be huge. In systemically important banking crises in the world from 1970 to 2007, the average net recapitalization cost to the government was 6% of GDP, fiscal costs associated with crisis management averaged 13% of GDP (16% of GDP if expense recoveries are ignored), and economic output losses averaged about 20% of GDP during the first four years of the crisis.[2]  Several techniques have been used to help prevent or mitigate bank runs.  Some prevention techniques apply to individual banks, independently of the rest of the economy.  Some prevention techniques apply across the whole economy, though they may still allow individual institutions to fail.  The role of the lender of last resort, and the existence of deposit insurance, both create moral hazard, since they reduce banks' incentive to avoid making risky loans. They are nonetheless standard practice, as the benefits of collective prevention are commonly believed to outweigh the costs of excessive risk-taking.[29]  Techniques to deal with a banking panic when prevention have failed:  The bank panic of 1933 is the setting of Archibald MacLeish's 1935 play, Panic. Motion picture depictions of bank runs include those in American Madness (1932), It's a Wonderful Life (1946, set in 1932), Silver River (1948), Mary Poppins (1964, set in 1910 London), Rollover (1981), Noble House (1988) and The Pope Must Die (1991).  Arthur Hailey's novel The Moneychangers includes a potentially fatal run on a fictional American bank.  A run on a bank is one of the many causes of the characters' suffering in Upton Sinclair's The Jungle.  In The Simpsons episode \"The PTA Disbands\", Bart Simpson starts a whisper campaign at the Bank of Springfield as a prank to instigate a bank run. The episode spoofs It's a Wonderful Life. "},"meta":{},"created_at":"2025-03-22T14:25:42.292408Z","updated_at":"2025-03-22T14:25:42.292408Z","inner_id":92,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":101,"annotations":[{"id":101,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.323896Z","updated_at":"2025-03-22T14:25:42.323896Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"9366ea1d-73ce-4ad0-a270-2ad15e74b1d1","import_id":null,"last_action":null,"bulk_created":false,"task":101,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Heterodox  Macroeconomics is a branch of economics that deals with the performance, structure, behavior, and decision-making of an economy as a whole.[1] This includes regional, national, and global economies.[2][3] Macroeconomists study topics such as output\/GDP (gross domestic product) and national income, unemployment (including unemployment rates), price indices and inflation, consumption, saving, investment, energy, international trade, and international finance.  Macroeconomics and microeconomics are the two most general fields in economics.[4] The focus of macroeconomics is often on a country (or larger entities like the whole world) and how its markets interact to produce large-scale phenomena that economists refer to as aggregate variables. In microeconomics the focus of analysis is often a single market, such as whether changes in supply or demand are to blame for price increases in the oil and automotive sectors.  From introductory classes in \"principles of economics\" through doctoral studies, the macro\/micro divide is institutionalized in the field of economics. Most economists identify as either macro- or micro-economists.  Macroeconomics is traditionally divided into topics along different time frames: the analysis of short-term fluctuations over the business cycle, the determination of structural levels of variables like inflation and unemployment in the medium (i.e. unaffected by short-term deviations) term, and the study of long-term economic growth. It also studies the consequences of policies targeted at mitigating fluctuations like fiscal or monetary policy, using taxation and government expenditure or interest rates, respectively, and of policies that can affect living standards in the long term, e.g. by affecting growth rates.  Macroeconomics as a separate field of research and study is generally recognized to start in 1936, when John Maynard Keynes published his The General Theory of Employment, Interest and Money, but its intellectual predecessors are much older. Since World War II, various macroeconomic schools of thought like Keynesians, monetarists, new classical and new Keynesian economists have made contributions to the development of the macroeconomic research mainstream.  Macroeconomics encompasses a variety of concepts and variables, but above all the three central macroeconomic variables are output, unemployment, and inflation.[5]: 39  Besides, the time horizon varies for different types of macroeconomic topics, and this distinction is crucial for many research and policy debates.[5]: 54  A further important dimension is that of an economy's openness, economic theory distinguishing sharply between closed economies and open economies.[5]: 373   It is usual to distinguish between three time horizons in macroeconomics, each having its own focus on e.g. the determination of output:[5]: 54   National output is the total amount of everything a country produces in a given period of time. Everything that is produced and sold generates an equal amount of income. The total net output of the economy is usually measured as gross domestic product (GDP). Adding net factor incomes from abroad to GDP produces gross national income (GNI), which measures total income of all residents in the economy. In most countries, the difference between GDP and GNI are modest so that GDP can approximately be treated as total income of all the inhabitants as well, but in some countries, e.g. countries with very large net foreign assets (or debt), the difference may be considerable.[5]: 385   Economists interested in long-run increases in output study economic growth. Advances in technology, accumulation of machinery and other capital, and better education and human capital, are all factors that lead to increased economic output over time. However, output does not always increase consistently over time. Business cycles can cause short-term drops in output called recessions. Economists look for macroeconomic policies that prevent economies from slipping into either recessions or overheating and that lead to higher productivity levels and standards of living.  The amount of unemployment in an economy is measured by the unemployment rate, i.e. the percentage of persons in the labor force who do not have a job, but who are actively looking for one. People who are retired, pursuing education, or discouraged from seeking work by a lack of job prospects are not part of the labor force and consequently not counted as unemployed, either.[5]: 156   Unemployment has a short-run cyclical component which depends on the business cycle, and a more permanent structural component, which can be loosely thought of as the average unemployment rate in an economy over extended periods,[6] and which is often termed the natural[6] or structural[7][5]: 167  rate of unemployment.  Cyclical unemployment occurs when growth stagnates. Okun's law represents the empirical relationship between unemployment and short-run GDP growth.[8] The original version of Okun's law states that a 3% increase in output would lead to a 1% decrease in unemployment.[9]  The structural or natural rate of unemployment is the level of unemployment that will occur in a medium-run equilibrium, i.e. a situation with a cyclical unemployment rate of zero. There may be several reasons why there is some positive unemployment level even in a cyclically neutral situation, which all have their foundation in some kind of market failure:[6]  A general price increase across the entire economy is called inflation. When prices decrease, there is deflation. Economists measure these changes in prices with price indexes. Inflation will increase when an economy becomes overheated and grows too quickly. Similarly, a declining economy can lead to decreasing inflation and even in some cases deflation.  Central bankers conducting monetary policy usually have as a main priority to avoid too high inflation, typically by adjusting interest rates. High inflation as well as deflation can lead to increased uncertainty and other negative consequences, in particular when the inflation (or deflation) is unexpected. Consequently, most central banks aim for a positive, but stable and not very high inflation level.[5]  Changes in the inflation level may be the result of several factors. Too much aggregate demand in the economy will cause an overheating, raising inflation rates via the Phillips curve because of a tight labor market leading to large wage increases which will be transmitted to increases in the price of the products of employers. Too little aggregate demand will have the opposite effect of creating more unemployment and lower wages, thereby decreasing inflation. Aggregate supply shocks will also affect inflation, e.g. the oil crises of the 1970s and the 2021–2023 global energy crisis. Changes in inflation may also impact the formation of inflation expectations, creating a self-fulfilling inflationary or deflationary spiral.[5]  The monetarist quantity theory of money holds that changes in the price level are directly caused by changes in the money supply.[16] Whereas there is empirical evidence that there is a long-run positive correlation between the growth rate of the money stock and the rate of inflation, the quantity theory has proved unreliable in the short- and medium-run time horizon relevant to monetary policy and is abandoned as a practical guideline by most central banks today.[17]  Open economy macroeconomics deals with the consequences of international trade in goods, financial assets and possibly factor markets like labor migration and international relocation of firms (physical capital). It explores what determines import, export, the balance of trade and over longer horizons the accumulation of net foreign assets. An important topic is the role of exchange rates and the pros and cons of maintaining a fixed exchange rate system or even a currency union like the Economic and Monetary Union of the European Union, drawing on the research literature on optimum currency areas.[5]  One way to calculate Gross Domestic Product, or total net output, is the expenditure method. The GDP essentially tells you how big the economy is. The larger the GDP value, the bigger the economy. The expenditure approach involves looking at four main components: Consumer Spending, Government Spending, Investment Spending, and Net Exports.[18] Consumer Spending is made up of ordinary consumers spending money on different kinds of products and also investing their money in residential markets. Government Spending involves the government spending money on goods and services and they may assist consumers or businesses with spending as well. For instance, purchasing physical capital for businesses. While transfer payments, which includes things like welfare or social security payments), are things that a government pays, it is not included in the final calculation of the expenditure approach because it is not paying for any final goods and services. Investment spending involves businesses spending money on physical capital\/equipment to help with producing goods and services. Lastly, net exports is just exports minus imports. Exports are goods and services that a country is selling to people abroad and imports are goods and services that people from a country are receiving from abroad.  Hence, the equation for the expenditure approach to calculating the Gross Domestic Product is  GDP = Consumer Spending(CS) + Government Spending(GS) + Investment Spending(IS) + Net Exports(EXP-IMP).   Another concern with measuring a country's economic growth is that even though we see the GDP growing, that does not inherently mean the economy is growing. Most of the increase in GDP may just be due to inflation. To know whether this is the case, we have to calculate the GDP Deflator which adjusts the GDP for inflation.  GDP Deflator = (Nominal GDP\/Real GDP) x 100[19]  Nominal GDP is GDP that includes inflation and Real GDP is GDP adjusted for inflation. To adjust for inflation means that the effect of inflation on the value was removed.  A GDP Deflator of 100 indicates that there is no inflation nor deflation. A GDP Deflator value that is greater than 100 indicates that there is inflation. A GDP Deflator value that is less than 100 indicates that there is deflation.  Macroeconomics as a separate field of research and study is generally recognized to start with the publication of John Maynard Keynes' The General Theory of Employment, Interest, and Money in 1936.[20][21][5]: 526  The terms \"macrodynamics\" and \"macroanalysis\" were introduced by Ragnar Frisch in 1933, and Lawrence Klein in 1946 used the word \"macroeconomics\" itself in a journal title in 1946.[20] but naturally several of the themes which are central to macroeconomic research had been discussed by thoughtful economists and other writers long before 1936.[20]  In particular, macroeconomic questions before Keynes were the topic of the two long-standing traditions of business cycle theory and monetary theory. William Stanley Jevons was one of the pioneers of the first tradition, whereas the quantity theory of money, labelled the oldest surviving theory in economics, as an example of the second was described already in the 16th century by Martín de Azpilcueta and later discussed by personalities like John Locke and David Hume. In the first decades of the 20th century monetary theory was dominated by the eminent economists Alfred Marshall, Knut Wicksell and Irving Fisher.[20]  When the Great Depression struck, the reigning economists had difficulty explaining how goods could go unsold and workers could be left unemployed. In the prevailing neoclassical economics paradigm, prices and wages would drop until the market cleared, and all goods and labor were sold. Keynes in his main work, the General Theory, initiated what is known as the Keynesian Revolution. He offered a new interpretation of events and a whole intellectural framework - a novel theory of economics that explained why markets might not clear, which would evolve into a school of thought known as Keynesian economics, also called Keynesianism or Keynesian theory.[5]: 526   In Keynes' theory, aggregate demand - by Keynes called \"effective demand\" - was key to determining output. Even if Keynes conceded that output might eventually return to a medium-run equilibrium (or \"potential\") level, the process would be slow at best. Keynes coined the term liquidity preference (his preferred name for what is also known as money demand) and explained how monetary policy might affect aggregate demand, at the same time offering clear policy recommendations for an active role of fiscal policy in stabilizing aggregate demand and hence output and employment. In addition, he explained how the multiplier effect would magnify a small decrease in consumption or investment and cause declines throughout the economy, and noted the role that uncertainty and animal spirits can play in the economy.[5]: 526   The generation following Keynes combined the macroeconomics of the General Theory with neoclassical microeconomics to create the neoclassical synthesis. By the 1950s, most economists had accepted the synthesis view of the macroeconomy.[5]: 526  Economists like Paul Samuelson, Franco Modigliani, James Tobin, and Robert Solow developed formal Keynesian models and contributed formal theories of consumption, investment, and money demand that fleshed out the Keynesian framework.[5]: 527   Milton Friedman updated the quantity theory of money to include a role for money demand. He argued that the role of money in the economy was sufficient to explain the Great Depression, and that aggregate demand oriented explanations were not necessary. Friedman also argued that monetary policy was more effective than fiscal policy; however, Friedman doubted the government's ability to \"fine-tune\" the economy with monetary policy. He generally favored a policy of steady growth in money supply instead of frequent intervention.[5]: 528   Friedman also challenged the original simple Phillips curve relationship between inflation and unemployment. Friedman and Edmund Phelps (who was not a monetarist) proposed an \"augmented\" version of the Phillips curve that excluded the possibility of a stable, long-run tradeoff between inflation and unemployment.[22] When the oil shocks of the 1970s created a high unemployment and high inflation, Friedman and Phelps were vindicated. Monetarism was particularly influential in the early 1980s, but fell out of favor when central banks found the results disappointing when trying to target money supply instead of interest rates as monetarists recommended, concluding that the relationships between money growth, inflation and real GDP growth are too unstable to be useful in practical monetary policy making.[23]  New classical macroeconomics further challenged the Keynesian school. A central development in new classical thought came when Robert Lucas introduced rational expectations to macroeconomics. Prior to Lucas, economists had generally used adaptive expectations where agents were assumed to look at the recent past to make expectations about the future. Under rational expectations, agents are assumed to be more sophisticated.[5]: 530  Consumers will not simply assume a 2% inflation rate just because that has been the average the past few years; they will look at current monetary policy and economic conditions to make an informed forecast. In the new classical models with rational expectations, monetary policy only had a limited impact.  Lucas also made an influential critique of Keynesian empirical models. He argued that forecasting models based on empirical relationships would keep producing the same predictions even as the underlying model generating the data changed. He advocated models based on fundamental economic theory (i.e. having an explicit microeconomic foundation) that would, in principle, be structurally accurate as economies changed.[5]: 530   Following Lucas's critique, new classical economists, led by Edward C. Prescott and Finn E. Kydland, created real business cycle (RBC) models of the macro economy. RBC models were created by combining fundamental equations from neo-classical microeconomics to make quantitative models. In order to generate macroeconomic fluctuations, RBC models explained recessions and unemployment with changes in technology instead of changes in the markets for goods or money. Critics of RBC models argue that technological changes, which typically diffuse slowly throughout the economy, could hardly generate the large short-run output fluctuations that we observe. In addition, there is strong empirical evidence that monetary policy does affect real economic activity, and the idea that technological regress can explain recent recessions seems implausible.[5]: 533 [6]: 195   Despite criticism of the realism in the RBC models, they have been very influential in economic methodology by providing the first examples of general equilibrium models based on microeconomic foundations and a specification of underlying shocks that aim to explain the main features of macroeconomic fluctuations, not only qualitatively, but also quantitatively. In this way, they were forerunners of the later DSGE models.[6]: 194   New Keynesian economists responded to the new classical school by adopting rational expectations and focusing on developing micro-founded models that were immune to the Lucas critique. Like classical models, new classical models had assumed that prices would be able to adjust perfectly and monetary policy would only lead to price changes. New Keynesian models investigated sources of sticky prices and wages due to imperfect competition,[24] which would not adjust, allowing monetary policy to impact quantities instead of prices. Stanley Fischer and John B. Taylor produced early work in this area by showing that monetary policy could be effective even in models with rational expectations when contracts locked in wages for workers. Other new Keynesian economists, including Olivier Blanchard, Janet Yellen, Julio Rotemberg, Greg Mankiw, David Romer, and Michael Woodford, expanded on this work and demonstrated other cases where various market imperfections caused inflexible prices and wages leading in turn to monetary and fiscal policy having real effects. Other researchers focused on imperferctions in labor markets, developing models of efficiency wages or search and matching (SAM) models, or imperfections in credit markets like Ben Bernanke.[5]: 532–36   By the late 1990s, economists had reached a rough consensus.[25] The market imperfections and nominal rigidities of new Keynesian theory was combined with rational expectations and the RBC methodology to produce a new and popular type of models called dynamic stochastic general equilibrium (DSGE) models. The fusion of elements from different schools of thought has been dubbed the new neoclassical synthesis.[26][27] These models are now used by many central banks and are a core part of contemporary macroeconomics.[5]: 535–36   The 2007–2008 financial crisis, which led to the Great Recession, led to major reassessment of macroeconomics, which as a field generally had neglected the potential role of financial institutions in the economy. After the crisis, macroeconomic researchers have turned their attention in several new directions:  Research in the economics of the determinants behind long-run economic growth has followed its own course.[32] The Harrod-Domar model from the 1940s attempted to build a long-run growth model inspired by Keynesian demand-driven considerations.[33] The Solow–Swan model worked out by Robert Solow and, independently, Trevor Swan in the 1950s achieved more long-lasting success, however, and is still today a common textbook model for explaining economic growth in the long-run.[34] The model operates with a production function where national output is the product of two inputs: capital and labor. The Solow model assumes that labor and capital are used at constant rates without the fluctuations in unemployment and capital utilization commonly seen in business cycles.[35] In this model, increases in output, i.e. economic growth, can only occur because of an increase in the capital stock, a larger population, or technological advancements that lead to higher productivity (total factor productivity). An increase in the savings rate leads to a temporary increase as the economy creates more capital, which adds to output. However, eventually the depreciation rate will limit the expansion of capital: savings will be used up replacing depreciated capital, and no savings will remain to pay for an additional expansion in capital. Solow's model suggests that economic growth in terms of output per capita depends solely on technological advances that enhance productivity.[36] The Solow model can be interpreted as a special case of the more general Ramsey growth model, where households' savings rates are not constant as in the Solow model, but derived from an explicit intertemporal utility function.  In the 1980s and 1990s endogenous growth theory arose to challenge the neoclassical growth theory of Ramsey and Solow. This group of models explains economic growth through factors such as increasing returns to scale for capital and learning-by-doing that are endogenously determined instead of the exogenous technological improvement used to explain growth in Solow's model.[37] Another type of endogenous growth models endogenized the process of technological progress by modelling research and development activities by profit-maximizing firms explicitly within the growth models themselves.[7]: 280–308   Since the 1970s, various environmental problems have been integrated into growth and other macroeconomic models to study their implications more thoroughly. During the oil crises of the 1970s when scarcity problems of natural resources were high on the public agenda, economists like Joseph Stiglitz and Robert Solow introduced non-renewable resources into neoclassical growth models to study the possibilities of maintaining growth in living standards under these conditions.[7]: 201–39  More recently, the issue of climate change and the possibilities of a sustainable development are examined in so-called integrated assessment models, pioneered by William Nordhaus.[38] In macroeconomic models in environmental economics, the economic system is dependant upon the environment. In this case, the circular flow of income diagram may be replaced by a more complex flow diagram reflecting the input of solar energy, which sustains natural inputs and environmental services which are then used as units of production. Once consumed, natural inputs pass out of the economy as pollution and waste. The potential of an environment to provide services and materials is referred to as an \"environment's source function\", and this function is depleted as resources are consumed or pollution contaminates the resources. The \"sink function\" describes an environment's ability to absorb and render harmless waste and pollution: when waste output exceeds the limit of the sink function, long-term damage occurs.[39]: 8   The division into various time frames of macroeconomic research leads to a parallel division of macroeconomic policies into short-run policies aimed at mitigating the harmful consequences of business cycles (known as stabilization policy) and medium- and long-run policies targeted at improving the structural levels of macroeconomic variables.[7]: 18   Stabilization policy is usually implemented through two sets of tools: fiscal and monetary policy. Both forms of policy are used to stabilize the economy, i.e. limiting the effects of the business cycle by conducting expansive policy when the economy is in a recession or contractive policy in the case of overheating.[5][40]  Structural policies may be labor market policies which aim to change the structural unemployment rate or policies which affect long-run propensities to save, invest, or engage in education or research and development.[7]: 19   Central banks conduct monetary policy mainly by adjusting short-term interest rates.[41] The actual method through which the interest rate is changed differs from central bank to central bank, but typically the implementation happens either directly via administratively changing the central bank's own offered interest rates or indirectly via open market operations.[42]  Via the monetary transmission mechanism, interest rate changes affect investment, consumption, asset prices like stock prices and house prices, and through exchange rate reactions export and import. In this way aggregate demand, employment and ultimately inflation is affected.[43] Expansionary monetary policy lowers interest rates, increasing economic activity, whereas contractionary monetary policy raises interest rates. In the case of a fixed exchange rate system, interest rate decisions together with direct intervention by central banks on exchange rate dynamics are major tools to control the exchange rate.[44]  In developed countries, most central banks follow inflation targeting, focusing on keeping medium-term inflation close to an explicit target, say 2%, or within an explicit range. This includes the Federal Reserve and the European Central Bank, which are generally considered to follow a strategy very close to inflation targeting, even though they do not officially label themselves as inflation targeters.[45] In practice, an official inflation targeting often leaves room for the central bank to also help stabilize output and employment, a strategy known as \"flexible inflation targeting\".[46] Most emerging economies focus their monetary policy on maintaining a fixed exchange rate regime, aligning their currency with one or more foreign currencies, typically the US dollar or the euro.[47]  Conventional monetary policy can be ineffective in situations such as a liquidity trap. When nominal interest rates are near zero, central banks cannot loosen monetary policy through conventional means. In that situation, they may use unconventional monetary policy such as quantitative easing to help stabilize output. Quantity easing can be implemented by buying not only government bonds, but also other assets such as corporate bonds, stocks, and other securities. This allows lower interest rates for a broader class of assets beyond government bonds. A similar strategy is to lower long-term interest rates by buying long-term bonds and selling short-term bonds to create a flat yield curve, known in the US as Operation Twist.[48]  Fiscal policy is the use of government's revenue (taxes) and expenditure as instruments to influence the economy.  For example, if the economy is producing less than potential output, government spending can be used to employ idle resources and boost output, or taxes could be lowered to boost private consumption which has a similar effect. Government spending or tax cuts do not have to make up for the entire output gap. There is a multiplier effect that affects the impact of government spending. For instance, when the government pays for a bridge, the project not only adds the value of the bridge to output, but also allows the bridge workers to increase their consumption and investment, which helps to close the output gap.  The effects of fiscal policy can be limited by partial or full crowding out. When the government takes on spending projects, it limits the amount of resources available for the private sector to use. Full crowding out occurs in the extreme case when government spending simply replaces private sector output instead of adding additional output to the economy. A crowding out effect may also occur if government spending should lead to higher interest rates, which would limit investment.[49]  Some fiscal policy is implemented through automatic stabilizers without any active decisions by politicians. Automatic stabilizers do not suffer from the policy lags of discretionary fiscal policy. Automatic stabilizers use conventional fiscal mechanisms, but take effect as soon as the economy takes a downturn: spending on unemployment benefits automatically increases when unemployment rises, and tax revenues decrease, which shelters private income and consumption from part of the fall in market income.[7]: 657   There is a general consensus that both monetary and fiscal instruments may affect demand and activity in the short run (i.e. over the business cycle).[7]: 657  Economists usually favor monetary over fiscal policy to mitigate moderate fluctuations, however, because it has two major advantages. First, monetary policy is generally implemented by independent central banks instead of the political institutions that control fiscal policy. Independent central banks are less likely to be subject to political pressures for overly expansionary policies. Second, monetary policy may suffer shorter inside lags and outside lags than fiscal policy.[40] There are some exceptions, however: Firstly, in the case of a major shock, monetary stabilization policy may not be sufficient and should be supplemented by active fiscal stabilization.[7]: 659  Secondly, in the case of a very low interest level, the economy may be in a liquidity trap in which monetary policy becomes ineffective, which makes fiscal policy the more potent tool to stabilize the economy.[5] Thirdly, in regimes where monetary policy is tied to fulfilling other targets, in particular fixed exchange rate regimes, the central bank cannot simultaneously adjust its interest rates to mitigate domestic business cycle fluctuations, making fiscal policy the only usable tool for such countries.[44]  Macroeconomic teaching, research and informed debates normally evolve around formal (diagrammatic or equational) macroeconomic models to clarify assumptions and show their consequences in a precise way. Models include simple theoretical models, often containing only a few equations, used in teaching and research to highlight key basic principles, and larger applied quantitative models used by e.g. governments, central banks, think tanks and international organisations to predict effects of changes in economic policy or other exogenous factors or as a basis for making economic forecasting.[50]  Well-known specific theoretical models include short-term models like the Keynesian cross, the IS–LM model and the Mundell–Fleming model, medium-term models like the AD–AS model, building upon a Phillips curve, and long-term growth models like the Solow–Swan model, the Ramsey–Cass–Koopmans model and Peter Diamond's overlapping generations model. Quantitative models include early large-scale macroeconometric model, the new classical real business cycle models, microfounded computable general equilibrium (CGE) models used for medium-term (structural) questions like international trade or tax reforms, Dynamic stochastic general equilibrium (DSGE) models used to analyze business cycles, not least in many central banks, or integrated assessment models like DICE.  The IS–LM model, invented by John Hicks in 1936, gives the underpinnings of aggregate demand (itself discussed below). It answers the question \"At any given price level, what is the quantity of goods demanded?\" The graphic model shows combinations of interest rates and output that ensure equilibrium in both the goods and money markets under the model's assumptions.[51] The goods market is modeled as giving equality between investment and public and private saving (IS), and the money market is modeled as giving equilibrium between the money supply and liquidity preference (equivalent to money demand).[52]  The IS curve consists of the points (combinations of income and interest rate) where investment, given the interest rate, is equal to public and private saving, given output.[53] The IS curve is downward sloping because output and the interest rate have an inverse relationship in the goods market: as output increases, more income is saved, which means interest rates must be lower to spur enough investment to match saving.[53]  The traditional LM curve is upward sloping because the interest rate and output have a positive relationship in the money market: as income (identically equal to output in a closed economy) increases, the demand for money increases, resulting in a rise in the interest rate in order to just offset the incipient rise in money demand.[54]  The IS-LM model is often used in elementary textbooks to demonstrate the effects of monetary and fiscal policy, though it ignores many complexities of most modern macroeconomic models.[51] A problem related to the LM curve is that modern central banks largely ignore the money supply in determining policy, contrary to the model's basic assumptions.[6]: 262  In some modern textbooks, consequently, the traditional IS-LM model has been modified by replacing the traditional LM curve with an assumption that the central bank simply determines the interest rate of the economy directly.[6]: 194 [5]: 113   The AD–AS model is a common textbook model for explaining the macroeconomy.[55] The original version of the model shows the price level and level of real output given the equilibrium in aggregate demand and aggregate supply. The aggregate demand curve's downward slope means that more output is demanded at lower price levels.[56] The downward slope can be explained as the result of three effects: the Pigou or real balance effect, which states that as real prices fall, real wealth increases, resulting in higher consumer demand of goods; the Keynes or interest rate effect, which states that as prices fall, the demand for money decreases, causing interest rates to decline and borrowing for investment and consumption to increase; and the net export effect, which states that as prices rise, domestic goods become comparatively more expensive to foreign consumers, leading to a decline in exports.[56]  In many representations of the AD–AS model, the aggregate supply curve is horizontal at low levels of output and becomes inelastic near the point of potential output, which corresponds with full employment.[55] Since the economy cannot produce beyond the potential output, any AD expansion will lead to higher price levels instead of higher output.  In modern textbooks, the AD–AS model is often presented slightly differently, however, in a diagram showing not the price level, but the inflation rate along the vertical axis,[6]: 263 [11]: 399–428 [7]: 595  making it easier to relate the diagram to real-world policy discussions.[7]: vii  In this framework, the AD curve is downward sloping because higher inflation will cause the central bank, which is assumed to follow an inflation target, to raise the interest rate which will dampen economic activity, hence reducing output. The AS curve is upward sloping following a standard modern Phillips curve thought, in which a higher level of economic activity lowers unemployment, leading to higher wage growth and in turn higher inflation.[6]: 263   In early February of 2025, United States of America President Donald Trump stated that he would be imposing a 25% tariff on imported goods from Mexico and Canada and a 10% tariff on imported goods from China for US consumers.[57] A tariff in this case is essentially a tax on imported goods and services. US consumers will be less likely to buy imports from those three countries due to the higher price they would have to pay. This was projected to reduce US imports by 15% and generate federal revenue of $100 billion.[58]  While imports from Mexico and Canada are important to the US, the US does not rely as much on Canadian and Mexican imports compared to Mexico's and Canada's economies being highly reliant on their exports to the USA. There would be higher production and grocery costs for the US but Mexico would have its economy reduced by 16% as the US takes in 80% of their car exports and 60% of their petroleum exports. In addition, Canada would have its economy reduced by similar amounts as the US takes in 70% of all of their exports.[59]  Going back to our earlier expenditure approach to calculating GDP, we can see that (Exports - Imports) would reduce significantly due to reduced exports which means a negative Net Exports and a lower GDP. Hence, we see the projections showing that Canada's and Mexico's economies would be reduced greatly.  When looking at data presented by the Bureau of Economic Analysis, with a base year of 2017, we see that the GDP deflator has been trending upwards since then.[60] The base year serves as the standard year to which we can compare whether GDP increased or decreased. The base year's prices are used when calculating Real GDP for a specific year. For instance, calculating 2020's GDP Deflator would be = 2020's Nominal GDP\/2020's Real GDP(Using 2017 Prices). The GDP Deflator has risen from 100 to 126.22 in 2024 Q4. So we see with real-life data that there has been a lot of inflation over the past decade. This trend was followed during the COVID-19 pandemic as well.   "},"meta":{},"created_at":"2025-03-22T14:25:42.292408Z","updated_at":"2025-03-22T14:25:42.292408Z","inner_id":93,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":102,"annotations":[{"id":102,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.323896Z","updated_at":"2025-03-22T14:25:42.323896Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"ffe2b922-72da-41bc-8c1c-3416b062a091","import_id":null,"last_action":null,"bulk_created":false,"task":102,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  South Korean films have been heavily influenced by such events and forces as the Korea under Japanese rule, the Korean War, government censorship, the business sector, globalization, and the democratization of South Korea.[5][6]  The golden age of South Korean cinema in the mid-20th century produced what are considered two of the best South Korean films of all time, The Housemaid (1960) and Obaltan (1961),[7] while the industry's revival with the Korean New Wave from the late 1990s to the present produced both of the country's highest-grossing films, The Admiral: Roaring Currents (2014) and Extreme Job (2019), as well as prize winners on the festival circuit including Golden Lion recipient Pietà (2012) and Palme d'Or recipient and Academy Award winner Parasite (2019) and international cult classics including Oldboy (2003),[8] Snowpiercer (2013),[9] and Train to Busan (2016).[10]  With the increasing global success and globalization of the Korean film industry, the past two decades have seen Korean actors like Lee Byung-hun and Bae Doona star in American films, Korean auteurs such as Park Chan-wook and Bong Joon-ho direct English-language works, Korean American actors crossover to star in Korean films as with Steven Yeun and Ma Dong-seok, and Korean films be remade in the United States, China, and other markets. The Busan International Film Festival has also grown to become Asia's largest and most important film festival.  American film studios have also set up local subsidiaries like Warner Bros. Korea and 20th Century Fox Korea to finance Korean films like The Age of Shadows (2016) and The Wailing (2016), putting them in direct competition with Korea's Big Four vertically integrated domestic film production and distribution companies: Lotte Cultureworks (formerly Lotte Entertainment), CJ Entertainment, Next Entertainment World (NEW), and Showbox. Netflix has also entered Korea as a film producer and distributor as part of both its international growth strategy in search of new markets and its drive to find new content for consumers in the U.S. market amid the \"streaming wars\" with Disney, which has a Korean subsidiary, and other competitors.  The earliest movie theaters in the country opened during the late Joseon to Korean Empire periods. The first was Ae Kwan Theater,[11] followed by Dansungsa.[12]  With the surrender of Japan in 1945 and the subsequent liberation of Korea, freedom became the predominant theme in South Korean cinema in the late 1940s and early 1950s.[5] One of the most significant films from this era is director Choi In-gyu's Viva Freedom! (1946), which is notable for depicting the Korean independence movement. The film was a major commercial success because it tapped into the public's excitement about the country's recent liberation.[13]  However, during the Korean War, the South Korean film industry stagnated, and only 14 films were produced from 1950 to 1953. All of the films from that era have since been lost.[14] Following the Korean War armistice in 1953, South Korean president Syngman Rhee attempted to rejuvenate the film industry by exempting it from taxation. Additionally foreign aid arrived in the country after the war that provided South Korean filmmakers with equipment and technology to begin producing more films.[15]  Though filmmakers were still subject to government censorship, South Korea experienced a golden age of cinema, mostly consisting of melodramas, starting in the mid-1950s.[5] The number of films made in South Korea increased from only 15 in 1954 to 111 in 1959.[16]  One of the most popular films of the era, director Lee Kyu-hwan's now lost remake of Chunhyang-jeon (1955), drew 10 percent of Seoul's population to movie theaters[15] However, while Chunhyang-jeon re-told a traditional Korean story, another popular film of the era, Han Hyung-mo's Madame Freedom (1956), told a modern story about female sexuality and Western values.[17]  South Korean filmmakers enjoyed a brief freedom from censorship in the early 1960s, between the administrations of Syngman Rhee and Park Chung Hee.[18] Kim Ki-young's The Housemaid (1960) and Yu Hyun-mok's Obaltan (1960), now considered among the best South Korean films ever made, were produced during this time.[7] Kang Dae-jin's The Coachman (1961) became the first South Korean film to win an award at an international film festival when it took home the Silver Bear Jury Prize at the 1961 Berlin International Film Festival.[19][20]  When Park Chung Hee became acting president in 1962, government control over the film industry increased substantially. Under the Motion Picture Law of 1962, a series of increasingly restrictive measures was enacted that limited imported films under a quota system. The new regulations also reduced the number of domestic film-production companies from 71 to 16 within a year. Government censorship targeted obscenity, communism, and unpatriotic themes in films.[21][22] Nonetheless, the Motion Picture Law's limit on imported films resulted in a boom of domestic films. South Korean filmmakers had to work quickly to meet public demand, and many films were shot in only a few weeks. During the 1960s, the most popular South Korean filmmakers released six to eight films per year. Notably, director Kim Soo-yong released ten films in 1967, including Mist, which is considered to be his greatest work.[19]  In 1967, South Korea's first animated feature film, Hong Kil-dong, was released. A handful of animated films followed including Golden Iron Man (1968), South Korea's first science-fiction animated film.[19]  Government control of South Korea's film industry reached its height during the 1970s under President Park Chung Hee's authoritarian \"Yusin System.\" The Korean Motion Picture Promotion Corporation was created in 1973, ostensibly to support and promote the South Korean film industry, but its primary purpose was to control the film industry and promote \"politically correct\" support for censorship and government ideals.[23] According to the 1981 International Film Guide, \"No country has a stricter code of film censorship than South Korea – with the possible exception of the North Koreans and some other Communist bloc countries.\"[24]  Only filmmakers who had previously produced \"ideologically sound\" films and who were considered to be loyal to the government were allowed to release new films. Members of the film industry who tried to bypass censorship laws were blacklisted and sometimes imprisoned.[25] One such blacklisted filmmaker, the prolific director Shin Sang-ok, was kidnapped by the North Korean government in 1978 after the South Korean government revoked his film-making license in 1975.[26]  The propaganda-laden movies (or \"policy films\") produced in the 1970s were unpopular with audiences who had become accustomed to seeing real-life social issues onscreen during the 1950s and 1960s. In addition to government interference, South Korean filmmakers began losing their audience to television, and movie-theater attendance dropped by over 60 percent from 1969 to 1979.[27]  Films that were popular among audiences during this era include Yeong-ja's Heydays (1975) and Winter Woman (1977), both box office hits directed by Kim Ho-sun.[26] Yeong-ja's Heydays and Winter Women are classified as \"hostess films,\" which are movies about prostitutes and bargirls. Despite their overt sexual content, the government allowed the films to be released, and the genre was extremely popular during the 1970s and 1980s.[22]  In the 1980s, the South Korean government began to relax its censorship and control of the film industry. The Motion Picture Law of 1984 allowed independent filmmakers to begin producing films, and the 1986 revision of the law allowed more films to be imported into South Korea.[21]  Meanwhile, South Korean films began reaching international audiences for the first time in a significant way. Director Im Kwon-taek's Mandala (1981) won the Grand Prix at the 1981 Hawaii Film Festival, and he soon became the first Korean director in years to have his films screened at European film festivals. His film Gilsoddeum (1986) was shown at the 36th Berlin International Film Festival, and actress Kang Soo-yeon won Best Actress at the 1987 Venice International Film Festival for her role in Im's film, The Surrogate Woman.[28]  In 1988, the South Korean government lifted all restrictions on foreign films, and American film companies began to set up offices in South Korea. In order for domestic films to compete, the government once again enforced a screen quota that required movie theaters to show domestic films for at least 146 days per year. However, despite the quota, the market share of domestic films was only 16 percent by 1993.[21]  The South Korean film industry was once again changed in 1992 with Kim Ui-seok's hit film Marriage Story, released by Samsung. It was the first South Korean movie to be released by business conglomerate known as a chaebol, and it paved the way for other chaebols to enter the film industry, using an integrated system of financing, producing, and distributing films.[29]  It is important to note that until 1996, when the Film Promotion Law was passed,[30] the film industry was still subject to censorship. Censoring of scripts in pre-production was officially dismissed in the late 1980s, still producers were unofficially expected to present two copies to the Public Performance Ethics Committee,[31] who had the power to modify by completely cutting scenes.[32]  As a result of the 1997 Asian financial crisis, many chaebols began to scale back their involvement in the film industry. However, they had already laid the groundwork for a renaissance in South Korean film-making by supporting young directors and introducing good business practices into the industry.[29] \"New Korean Cinema,\" including glossy blockbusters and creative genre films, began to emerge in the late 1990s and 2000s.[6] At the same time, representation of women in visual media drastically declined in the aftermath of the 1997 IMF Crisis.[33]  South Korean cinema saw domestic box-office success exceeding that of Hollywood films in the late 1990s largely due to screen quota laws that limited the public showing foreign films.[34] First enacted in 1967, South Korea's screen quota placed restrictions on the number of days per year that foreign films could be shown at any given theater—garnering criticism from film distributors outside South Korea as unfair. As a prerequisite for negotiations with the United States for a free-trade agreement, the Korean government cut its annual screen quota for domestic films from 146 days to 73 (allowing more foreign films to enter the market).[35] In February 2006, South Korean movie workers responded to the reduction by staging mass rallies in protest.[36] According to Kim Hyun, \"South Korea's movie industry, like that of most countries, is grossly overshadowed by Hollywood. The nation exported US$2 million-worth of movies to the United States last year and imported $35.9 million-worth\".[37]  One of the first blockbusters was Kang Je-gyu's Shiri (1999), a film about a North Korean spy in Seoul. It was the first film in South Korean history to sell more than two million tickets in Seoul alone.[38] Shiri was followed by other blockbusters including Park Chan-wook's Joint Security Area (2000), Kwak Jae-yong's My Sassy Girl (2001), Kwak Kyung-taek's Friend (2001), Kang Woo-suk's Silmido (2003), and Kang Je-gyu's Taegukgi (2004). In fact, both Silmido and Taegukgi were seen by 10 million people domestically—about one-quarter of South Korea's entire population.[39]  South Korean films began attracting significant international attention in the 2000s, due in part to filmmaker Park Chan-wook, whose movie Oldboy (2003) won the Grand Prix at the 2004 Cannes Film Festival and was praised by American directors including Quentin Tarantino and Spike Lee, the latter of whom directed the remake Oldboy (2013).[8][40]  Director Bong Joon-ho's The Host (2006) and later the English-language film Snowpiercer (2013), are among the highest-grossing films of all time in South Korea and were praised by foreign film critics.[41][9][42] Yeon Sang-ho's Train to Busan (2016), also one of the highest-grossing films of all time in South Korea, became the second highest-grossing film in Hong Kong in 2016.[43]  In 2019, Bong Joon-ho's Parasite became the first film from South Korea to win the prestigious Palme d'Or at the Cannes Film Festival.[44] At the 92nd Academy Awards, Parasite became the first South Korean film to receive any sort of Academy Awards recognition, receiving six nominations. It won Best Picture, Best Director, Best International Feature Film and Best Original Screenplay, becoming the first film produced entirely by an Asian country to receive a nomination for the Academy Award for Best Picture since Crouching Tiger, Hidden Dragon, as well as the first film not in English ever to win the Oscar for Best Picture.[45]  The Korean Film Council has published box office data on South Korean films since 2004. As of March 2025, the top ten highest-grossing domestic films in South Korea since 2004 are as follows.[41]  South Korea's first film awards ceremonies were established in the 1950s, but have since been discontinued. The longest-running and most popular film awards ceremonies are the Grand Bell Awards, which were established in 1962, and the Blue Dragon Film Awards, which were established in 1963. Other awards ceremonies include the Baeksang Arts Awards, the Korean Association of Film Critics Awards, and the Busan Film Critics Awards.[48]  Founded in 1996, the Busan International Film Festival is South Korea's major film festival and has grown to become one of the largest and most prestigious film events in Asia.[49]  The first South Korean film to win an award at an international film festival was Kang Dae-jin's The Coachman (1961), which was awarded the Silver Bear Jury Prize at the 1961 Berlin International Film Festival.[19][20] The tables below list South Korean films that have since won major international film festival prizes. "},"meta":{},"created_at":"2025-03-22T14:25:42.292408Z","updated_at":"2025-03-22T14:25:42.292408Z","inner_id":94,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":103,"annotations":[{"id":103,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.326896Z","updated_at":"2025-03-22T14:25:42.326896Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"fd1959e0-f22d-4495-96ec-d748f855e8bb","import_id":null,"last_action":null,"bulk_created":false,"task":103,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Empirical methods  Prescriptive and policy  Financial economics is the branch of economics characterized by a \"concentration on monetary activities\", in which \"money of one type or another is likely to appear on both sides of a trade\".[1]  Its concern is thus the interrelation of financial variables, such as share prices, interest rates and exchange rates, as opposed to those concerning the real economy.  It has two main areas of focus:[2] asset pricing and corporate finance; the first being the perspective of providers of capital, i.e. investors, and the second of users of capital. It thus provides the theoretical underpinning for much of finance.  The subject is concerned with \"the allocation and deployment of economic resources, both spatially and across time, in an uncertain environment\".[3][4] It therefore centers on decision making under uncertainty in the context of the financial markets, and the resultant economic and financial models and principles, and is concerned with deriving testable or policy implications from acceptable assumptions.  It thus also includes a formal study of the financial markets themselves, especially market microstructure and market regulation. It is built on the foundations of microeconomics and decision theory.  Financial econometrics is the branch of financial economics that uses econometric techniques to parameterise the relationships identified. Mathematical finance is related in that it will derive and extend the mathematical or numerical models suggested by financial economics. Whereas financial economics has a primarily microeconomic focus, monetary economics is primarily macroeconomic in nature.  Four equivalent formulations,[6] where:  Financial economics studies how rational investors would apply decision theory to investment management. The subject is thus built on the foundations of microeconomics and derives several key results for the application of decision making under uncertainty to the financial markets. The underlying economic logic yields the fundamental theorem of asset pricing, which gives the conditions for arbitrage-free asset pricing.[6][5] The various \"fundamental\" valuation formulae result directly.  Underlying all of financial economics are the concepts of present value and expectation.[6]  Calculating their present value,      X  s j    \/  r   {\\displaystyle X_{sj}\/r}   in the first formula, allows the decision maker to aggregate the cashflows (or other returns) to be produced by the asset in the future to a single value at the date in question, and to thus more readily compare two opportunities; this concept is then the starting point for financial decision making.  [note 1] (Note that here, \"    r   {\\displaystyle r}  \" represents a generic (or arbitrary) discount rate applied to the cash flows, whereas in the valuation formulae, the risk-free rate is applied once these have been \"adjusted\" for their riskiness; see below.)  An immediate extension is to combine probabilities with present value, leading to the expected value criterion which sets asset value as a function of the sizes of the expected payouts and the probabilities of their occurrence,      X  s     {\\displaystyle X_{s}}   and      p  s     {\\displaystyle p_{s}}   respectively. [note 2]  This decision method, however, fails to consider risk aversion. In other words, since individuals receive greater utility from an extra dollar when they are poor and less utility when comparatively rich, the approach is therefore to \"adjust\" the weight assigned to the various outcomes, i.e. \"states\", correspondingly:      Y  s     {\\displaystyle Y_{s}}  . See indifference price. (Some investors may in fact be risk seeking as opposed to risk averse, but the same logic would apply.)  Choice under uncertainty here may then be defined as the maximization of expected utility. More formally, the resulting expected utility hypothesis states that, if certain axioms are satisfied, the subjective value associated with a gamble by an individual is that individual's statistical expectation of the valuations of the outcomes of that gamble.  The impetus for these ideas arises from various inconsistencies observed under the expected value framework, such as the St. Petersburg paradox and the Ellsberg paradox. [note 3]  The New Palgrave Dictionary of Economics (2008, 2nd ed.) also uses the JEL codes to classify its entries in v. 8, Subject Index, including Financial Economics at pp. 863–64. The below have links to entry abstracts of The New Palgrave Online for each primary or secondary JEL category (10 or fewer per page, similar to Google searches):  Tertiary category entries can also be searched.[10]  The concepts of arbitrage-free, \"rational\", pricing and equilibrium are then coupled  [11] with the above to derive various of the \"classical\"[12]  (or \"neo-classical\"[13]) financial economics models.  Rational pricing is the assumption that asset prices (and hence asset pricing models) will reflect the arbitrage-free price of the asset, as any deviation from this price will be \"arbitraged away\". This assumption is useful in pricing fixed income securities, particularly bonds, and is fundamental to the pricing of derivative instruments.  Economic equilibrium is a state in which economic forces such as supply and demand are balanced, and in the absence of external influences these equilibrium values of economic variables will not change. General equilibrium deals with the behavior of supply, demand, and prices in a whole economy with several or many interacting markets, by seeking to prove that a set of prices exists that will result in an overall equilibrium. (This is in contrast to partial equilibrium, which only analyzes single markets.)  The two concepts are linked as follows: where market prices are complete and do not allow profitable arbitrage, i.e. they comprise an arbitrage-free market, then these prices are also said to constitute an \"arbitrage equilibrium\". Intuitively, this may be seen by considering that where an arbitrage opportunity does exist, then prices can be expected to change, and they are therefore not in equilibrium.[14] An arbitrage equilibrium is thus a precondition for a general economic equilibrium.  \"Complete\" here means that there is a price for every asset in every possible state of the world,     s   {\\displaystyle s}  , and that the complete set of possible bets on future states-of-the-world can therefore be constructed with existing assets (assuming no friction): essentially solving simultaneously for n (risk-neutral) probabilities,      q  s     {\\displaystyle q_{s}}  , given n prices. For a simplified example see Rational pricing § Risk neutral valuation, where the economy has only two possible states – up and down – and where      q  u p     {\\displaystyle q_{up}}   and      q  d o w n     {\\displaystyle q_{down}}   (=    1 −  q  u p     {\\displaystyle 1-q_{up}}  ) are the two corresponding probabilities, and in turn, the derived distribution, or \"measure\".  The formal derivation will proceed by arbitrage arguments.[6][14][11] The analysis here is often undertaken assuming a representative agent,[15] essentially treating all market participants, \"agents\", as identical (or, at least, assuming that they act in such a way that the sum of their choices is equivalent to the decision of one individual) with the effect that the problems are then mathematically tractable.  With this measure in place, the expected, i.e. required, return of any security (or portfolio) will then equal the risk-free return, plus an \"adjustment for risk\",[6] i.e. a security-specific risk premium, compensating for the extent to which its cashflows are unpredictable. All pricing models are then essentially variants of this, given specific assumptions or conditions.[6][5][16] This approach is consistent with the above, but with the expectation based on \"the market\" (i.e. arbitrage-free, and, per the theorem, therefore in equilibrium) as opposed to individual preferences.  Continuing the example, in pricing a derivative instrument, its forecasted cashflows in the abovementioned up- and down-states      X  u p     {\\displaystyle X_{up}}   and      X  d o w n     {\\displaystyle X_{down}}  , are multiplied through by      q  u p     {\\displaystyle q_{up}}   and      q  d o w n     {\\displaystyle q_{down}}  , and are then discounted at the risk-free interest rate; per the second equation above.  In pricing a \"fundamental\", underlying, instrument (in equilibrium), on the other hand, a risk-appropriate premium over risk-free is required in the discounting, essentially employing the first equation with     Y   {\\displaystyle Y}   and     r   {\\displaystyle r}   combined. This premium may be derived by the CAPM (or extensions) as will be seen under § Uncertainty.  The difference is explained as follows: By construction, the value of the derivative will (must) grow at the risk free rate, and, by arbitrage arguments, its value must then be discounted correspondingly; in the case of an option, this is achieved by \"manufacturing\" the instrument as a combination of the underlying and a risk free \"bond\"; see Rational pricing § Delta hedging (and § Uncertainty below). Where the underlying is itself being priced, such \"manufacturing\" is of course not possible – the instrument being \"fundamental\", i.e. as opposed to \"derivative\" – and a premium is then required for risk.  (Correspondingly, mathematical finance separates into two analytic regimes: risk and portfolio management (generally) use physical (or actual or actuarial) probability, denoted by \"P\"; while derivatives pricing uses risk-neutral probability (or arbitrage-pricing probability), denoted by \"Q\". In specific applications the lower case is used, as in the above equations.)  With the above relationship established, the further specialized Arrow–Debreu model may be derived.  [note 4] This result suggests that, under certain economic conditions, there must be a set of prices such that aggregate supplies will equal aggregate demands for every commodity in the economy.  The Arrow–Debreu model applies to economies with maximally complete markets, in which there exists a market for every time period and forward prices for every commodity at all time periods.  A direct extension, then, is the concept of a state price security, also called an Arrow–Debreu security, a contract that agrees to pay one unit of a numeraire (a currency or a commodity) if a particular state occurs (\"up\" and \"down\" in the simplified example above) at a particular time in the future and pays zero numeraire in all the other states. The price of this security is the state price      π  s     {\\displaystyle \\pi _{s}}   of this particular state of the world; the collection of these is also referred to as a \"Risk Neutral Density\".[20]  In the above example, the state prices,      π  u p     {\\displaystyle \\pi _{up}}  ,       π  d o w n     {\\displaystyle \\pi _{down}}  would equate to the present values of     $  q  u p     {\\displaystyle \\$q_{up}}   and     $  q  d o w n     {\\displaystyle \\$q_{down}}  : i.e. what one would pay today, respectively, for the up- and down-state securities; the state price vector is the vector of state prices for all states. Applied to derivative valuation, the price today would simply be [     π  u p     {\\displaystyle \\pi _{up}}  ×     X  u p     {\\displaystyle X_{up}}   +      π  d o w n     {\\displaystyle \\pi _{down}}  ×     X  d o w n     {\\displaystyle X_{down}}  ]: the fourth formula (see above regarding the absence of a risk premium here). For a continuous random variable indicating a continuum of possible states, the value is found by integrating over the state price \"density\".  State prices find immediate application as a conceptual tool (\"contingent claim analysis\");[6] but can also be applied to valuation problems.[21] Given the pricing mechanism described, one can decompose the derivative value – true in fact for \"every security\"[2] – as a linear combination of its state-prices; i.e. back-solve for the state-prices corresponding to observed derivative prices.[22][21] [20]  These recovered state-prices can then be used for valuation of other instruments with exposure to the underlyer, or for other decision making relating to the underlyer itself.  Using the related stochastic discount factor - also called the pricing kernel - the asset price is computed by \"discounting\" the future cash flow by the stochastic factor        m ~      {\\displaystyle {\\tilde {m}}}  , and then taking the expectation;[16] the third equation above. Essentially, this factor divides expected utility at the relevant future period - a function of the possible asset values realized under each state -  by the utility due to today's wealth, and is then also referred to as \"the intertemporal marginal rate of substitution\".  Applying the above economic concepts, we may then derive various economic- and financial models and principles. As above, the two usual areas of focus are Asset Pricing and Corporate Finance, the first being the perspective of providers of capital, the second of users of capital. Here, and for (almost) all other financial economics models, the questions addressed are typically framed in terms of \"time, uncertainty, options, and information\",[1][15] as will be seen below.  Applying this framework, with the above concepts, leads to the required models. This derivation begins with the assumption of \"no uncertainty\" and is then expanded to incorporate the other considerations.[4] (This division sometimes denoted \"deterministic\" and \"random\",[23] or \"stochastic\".)  Bond valuation formula where Coupons and Face value are discounted at the appropriate rate, \"i\": typically reflecting a spread over the risk free rate as a function of credit risk; often quoted as a \"yield to maturity\". See body for discussion re the relationship with the above pricing formulae.  DCF valuation formula, where the value of the firm, is its forecasted free cash flows discounted to the present using the weighted average cost of capital, i.e. cost of equity and cost of debt, with the former (often) derived using the below CAPM. For share valuation investors use the related dividend discount model.   The starting point here is \"Investment under certainty\", and usually framed in the context of a corporation. The Fisher separation theorem, asserts that the objective of the corporation will be the maximization of its present value, regardless of the preferences of its shareholders.  Related is the Modigliani–Miller theorem, which shows that, under certain conditions, the value of a firm is unaffected by how that firm is financed, and depends neither on its dividend policy nor its decision to raise capital by issuing stock or selling debt. The proof here proceeds using arbitrage arguments, and acts as a benchmark [11] for evaluating the effects of factors outside the model that do affect value. [note 5]  The mechanism for determining (corporate) value is provided by [26] [27] John Burr Williams' The Theory of Investment Value, which proposes that the value of an asset should be calculated using \"evaluation by the rule of present worth\". Thus, for a common stock, the \"intrinsic\", long-term worth is the present value of its future net cashflows, in the form of dividends. What remains to be determined is the appropriate discount rate. Later developments show that, \"rationally\", i.e. in the formal sense, the appropriate discount rate here will (should) depend on the asset's riskiness relative to the overall market, as opposed to its owners' preferences; see below. Net present value (NPV) is the direct extension of these ideas typically applied to Corporate Finance decisioning. For other results, as well as specific models developed here, see the list of \"Equity valuation\" topics under Outline of finance § Discounted cash flow valuation. [note 6]  Bond valuation, in that cashflows (coupons and return of principal, or \"Face value\") are deterministic, may proceed in the same fashion.[23] An immediate extension, Arbitrage-free bond pricing, discounts each cashflow at the market derived rate – i.e. at each coupon's corresponding zero rate, and of equivalent credit worthiness – as opposed to an overall rate. In many treatments bond valuation precedes equity valuation, under which cashflows (dividends) are not \"known\" per se. Williams and onward allow for forecasting as to these – based on historic ratios or published dividend policy – and cashflows are then treated as essentially deterministic; see below under § Corporate finance theory.  For both stocks and bonds, \"under certainty, with the focus on cash flows from securities over time,\" valuation based on a term structure of interest rates is in fact consistent with arbitrage-free pricing.[28] Indeed, a corollary of the above is that \"the law of one price implies the existence of a discount factor\";[29] correspondingly, as formulated,      ∑  s    π  s   = 1  \/  r   {\\textstyle \\sum _{s}\\pi _{s}=1\/r}  .  Whereas these \"certainty\" results are all commonly employed under corporate finance, uncertainty is the focus of \"asset pricing models\" as follows. Fisher's formulation of the theory here - developing an intertemporal equilibrium model - underpins also [26] the below applications to uncertainty;  [note 7] see [30] for the development.  The expected return used when discounting cashflows on an asset     i   {\\displaystyle i}  , is the risk-free rate plus the market premium multiplied by beta (     ρ  i , m      σ  i    σ  m       {\\displaystyle \\rho _{i,m}{\\frac {\\sigma _{i}}{\\sigma _{m}}}}  ), the asset's correlated volatility relative to the overall market     m   {\\displaystyle m}  .  For \"choice under uncertainty\" the twin assumptions of rationality and market efficiency, as more closely defined, lead to modern portfolio theory (MPT) with its capital asset pricing model (CAPM) – an equilibrium-based result – and to the Black–Scholes–Merton theory (BSM; often, simply Black–Scholes) for option pricing – an arbitrage-free result. As above, the (intuitive) link between these, is that the latter derivative prices are calculated such that they are arbitrage-free with respect to the more fundamental, equilibrium determined, securities prices; see Asset pricing § Interrelationship.  Briefly, and intuitively – and consistent with § Arbitrage-free pricing and equilibrium above – the relationship between rationality and efficiency is as follows.[31]  Given the ability to profit from private information, self-interested traders are motivated to acquire and act on their private information. In doing so, traders contribute to more and more \"correct\", i.e. efficient, prices: the efficient-market hypothesis, or EMH. Thus, if prices of financial assets are (broadly) efficient, then deviations from these (equilibrium) values could not last for long. (See earnings response coefficient.) The EMH (implicitly) assumes that average expectations constitute an \"optimal forecast\", i.e. prices using all available information  are identical to the best guess of the future: the assumption of rational expectations.  The EMH does allow that when faced with new information, some investors may overreact and some may underreact,  [32] but what is required, however, is that investors' reactions follow a normal distribution – so that the net effect on market prices cannot be reliably exploited [32] to make an abnormal profit. In the competitive limit, then, market prices will reflect all available information and prices can only move in response to news:[33] the random walk hypothesis.  This news, of course, could be \"good\" or \"bad\", minor or, less common, major; and these moves are then, correspondingly, normally distributed; with the price therefore following a log-normal distribution.  [note 8]  Under these conditions, investors can then be assumed to act rationally: their investment decision must be calculated or a loss is sure to follow;[32] correspondingly, where an arbitrage opportunity presents itself, then arbitrageurs will exploit it, reinforcing this equilibrium. Here, as under the certainty-case above, the specific assumption as to pricing is that prices are calculated as the present value of expected future dividends, [5] [33] [15]  as based on currently available information. What is required though, is a theory for determining the appropriate discount rate, i.e. \"required return\", given this uncertainty: this is provided by the MPT and its CAPM. Relatedly, rationality – in the sense of arbitrage-exploitation – gives rise to Black–Scholes; option values here ultimately consistent with the CAPM.  In general, then, while portfolio theory studies how investors should balance risk and return when investing in many assets or securities, the CAPM is more focused, describing how, in equilibrium, markets set the prices of assets in relation to how risky they are. [note 9] This result will be independent of the investor's level of risk aversion and assumed utility function, thus providing a readily determined discount rate for corporate finance decision makers as above,[36] and for other investors. The argument proceeds as follows: [37]  If one can construct an efficient frontier – i.e. each combination of assets offering the best possible expected level of return for its level of risk, see diagram – then mean-variance efficient portfolios can be formed simply as a combination of holdings of the risk-free asset and the \"market portfolio\" (the Mutual fund separation theorem), with the combinations here plotting as the capital market line, or CML.  Then, given this CML, the required return on a risky security will be independent of the investor's utility function, and solely determined by its covariance (\"beta\") with aggregate, i.e. market, risk.  This is because investors here can then maximize utility through leverage as opposed to stock selection; see Separation property (finance), Markowitz model § Choosing the best portfolio and CML diagram aside.  As can be seen in the formula aside, this result is consistent with the preceding, equaling the riskless return plus an adjustment for risk.[5]  A more modern, direct, derivation is as described at the bottom of this section; which can be generalized to derive other equilibrium-pricing models.  Black–Scholes provides a mathematical model of a financial market containing derivative instruments, and the resultant formula for the price of European-styled options. [note 10] The model is expressed as the Black–Scholes equation, a partial differential equation describing the changing price of the option over time; it is derived assuming log-normal, geometric Brownian motion (see Brownian model of financial markets). The key financial insight behind the model is that one can perfectly hedge the option by buying and selling the underlying asset in just the right way and consequently \"eliminate risk\", absenting the risk adjustment from the pricing (    V   {\\displaystyle V}  , the value, or price, of the option, grows at     r   {\\displaystyle r}  , the risk-free rate).[6][5] This hedge, in turn, implies that there is only one right price – in an arbitrage-free sense – for the option. And this price is returned by the Black–Scholes option pricing formula. (The formula, and hence the price, is consistent with the equation, as the formula is the solution to the equation.) Since the formula is without reference to the share's expected return, Black–Scholes inheres risk neutrality; intuitively consistent with the \"elimination of risk\" here, and mathematically consistent with § Arbitrage-free pricing and equilibrium above. Relatedly, therefore, the pricing formula may also be derived directly via risk neutral expectation. Itô's lemma provides the underlying mathematics, and, with Itô calculus more generally, remains fundamental in quantitative finance. [note 11]  As implied by the Fundamental Theorem, the two major results are consistent.  Here, the Black-Scholes equation can alternatively be derived from the CAPM, and the price obtained from the Black–Scholes model is thus consistent with the assumptions of the CAPM.[46][13]  The Black–Scholes theory, although built on Arbitrage-free pricing, is therefore consistent with the equilibrium based capital asset pricing.  Both models, in turn, are ultimately consistent with the Arrow–Debreu theory, and can be derived via state-pricing – essentially, by expanding the above fundamental equations – further explaining, and if required demonstrating, this consistency.[6]  Here, the CAPM is derived by linking     Y   {\\displaystyle Y}  , risk aversion, to overall market return, and setting the return on security     j   {\\displaystyle j}   as      X  j    \/  P r i c  e  j     {\\displaystyle X_{j}\/Price_{j}}  ; see Stochastic discount factor § Properties. The Black–Scholes formula is found, in the limit,[47] by attaching a binomial probability[11] to each of numerous possible spot-prices (i.e. states) and then rearranging for the terms corresponding to     N (  d  1   )   {\\displaystyle N(d_{1})}   and     N (  d  2   )   {\\displaystyle N(d_{2})}  , per the boxed description; see Binomial options pricing model § Relationship with Black–Scholes.  More recent work further generalizes and extends these models.  As regards asset pricing, developments in equilibrium-based pricing are discussed under \"Portfolio theory\" below, while \"Derivative pricing\" relates to risk-neutral, i.e. arbitrage-free, pricing.  As regards the use of capital, \"Corporate finance theory\" relates, mainly, to the application of these models.  The majority of developments here relate to required return, i.e. pricing, extending the basic CAPM. Multi-factor models such as the Fama–French three-factor model and the Carhart four-factor model, propose factors other than market return as relevant in pricing. The intertemporal CAPM and consumption-based CAPM similarly extend the model. With intertemporal portfolio choice, the investor now repeatedly optimizes her portfolio; while the inclusion of consumption (in the economic sense) then incorporates all sources of wealth, and not just market-based investments, into the investor's calculation of required return.  Whereas the above extend the CAPM, the single-index model is a more simple model. It assumes, only, a correlation between security and market returns, without (numerous) other economic assumptions. It is useful in that it simplifies the estimation of correlation between securities, significantly reducing the inputs for building the correlation matrix required for portfolio optimization. The arbitrage pricing theory (APT)  similarly differs as regards its assumptions. APT \"gives up the notion that there is one right portfolio for everyone in the world, and ...replaces it with an explanatory model of what drives asset returns.\"[48] It returns the required (expected) return of a financial asset as a linear function of various macro-economic factors, and assumes that arbitrage should bring incorrectly priced assets back into line.[note 12] The linear factor model structure of the APT is used as the basis for many of the commercial risk systems employed by asset managers.  As regards portfolio optimization, the Black–Litterman model[51]  departs from the original Markowitz model – i.e. of constructing portfolios via an efficient frontier. Black–Litterman instead starts with an equilibrium assumption, and is then modified to take into account the 'views' (i.e., the specific opinions about asset returns) of the investor in question to arrive at a bespoke [52] asset allocation. Where factors additional to volatility are considered (kurtosis, skew...) then multiple-criteria decision analysis can be applied; here deriving a Pareto efficient portfolio. The universal portfolio algorithm applies machine learning to asset selection, learning adaptively from historical data. Behavioral portfolio theory recognizes that investors have varied aims and create an investment portfolio that meets a broad range of goals.  Copulas have lately been applied here; recently this is the case also for genetic algorithms and Machine learning, more generally.   (Tail) risk parity focuses on allocation of risk, rather than allocation of capital. [note 13] See Portfolio optimization § Improving portfolio optimization for other techniques and objectives, and Financial risk management § Investment management for discussion.  Interpretation: Analogous to Black–Scholes,  [53] arbitrage arguments describe the instantaneous change in the bond price     P   {\\displaystyle P}   for changes in the (risk-free) short rate     r   {\\displaystyle r}  ; the analyst selects the specific short-rate model to be employed.   In pricing derivatives, the binomial options pricing model provides a discretized version of Black–Scholes, useful for the valuation of American styled options. Discretized models of this type are built – at least implicitly – using state-prices (as above); relatedly, a large number of researchers have used options to extract state-prices for a variety of other applications in financial economics.[6][46][22] For path dependent derivatives, Monte Carlo methods for option pricing are employed; here the modelling is in continuous time, but similarly uses risk neutral expected value. Various other numeric techniques have also been developed. The theoretical framework too has been extended such that martingale pricing is now the standard approach. [note 14]  Drawing on these techniques, models for various other underlyings and applications have also been developed, all based on the same logic (using \"contingent claim analysis\"). Real options valuation allows that option holders can influence the option's underlying; models for employee stock option valuation explicitly assume non-rationality on the part of option holders; Credit derivatives allow that payment obligations or delivery requirements might not be honored. Exotic derivatives are now routinely valued. Multi-asset underlyers are handled via simulation or copula based analysis.  Similarly, the various short-rate models allow for an extension of these techniques to fixed income- and interest rate derivatives. (The Vasicek and CIR models are equilibrium-based, while Ho–Lee and subsequent models are based on arbitrage-free pricing.) The more general HJM Framework describes the dynamics of the full forward-rate curve – as opposed to working with short rates – and is then more widely applied. The valuation of the underlying instrument – additional to its derivatives – is relatedly extended, particularly for hybrid securities, where credit risk is combined with uncertainty re future rates;  see Bond valuation § Stochastic calculus approach and Lattice model (finance) § Hybrid securities.  [note 15]  Following the Crash of 1987, equity options traded in American markets began to exhibit what is known as a \"volatility smile\";  that is, for a given expiration, options whose strike price differs substantially from the underlying asset's price command higher prices, and thus implied volatilities, than what is suggested by BSM. (The pattern differs across various markets.) Modelling the volatility smile is an active area of research, and developments here – as well as implications re the standard theory – are discussed in the next section.  After the 2007–2008 financial crisis, a further development:[62] as outlined, (over the counter) derivative pricing had relied on the BSM risk neutral pricing framework, under the assumptions of funding at the risk free rate and the ability to perfectly replicate cashflows so as to fully hedge. This, in turn, is built on the assumption of a credit-risk-free environment – called into question during the crisis.  Addressing this, therefore, issues such as counterparty credit risk, funding costs and costs of capital are now additionally considered when pricing,[63] and a credit valuation adjustment, or CVA – and potentially other valuation adjustments, collectively xVA – is generally added to the risk-neutral derivative value. The standard economic arguments can be extended to incorporate these various adjustments.[64]  A related, and perhaps more fundamental change, is that discounting is now on the Overnight Index Swap (OIS) curve, as opposed to LIBOR as used previously.[62] This is because post-crisis, the overnight rate  is considered a better proxy for the \"risk-free rate\".[65] (Also, practically, the interest paid on cash collateral is usually the overnight rate; OIS discounting is then, sometimes, referred to as \"CSA discounting\".) Swap pricing – and, therefore, yield curve construction – is further modified: previously, swaps were valued off a single \"self discounting\" interest rate curve; whereas post crisis, to accommodate OIS discounting, valuation is now under a \"multi-curve framework\" where \"forecast curves\" are constructed for each floating-leg LIBOR tenor, with discounting on the common OIS curve.  Mirroring the above developments, corporate finance valuations and decisioning no longer need assume \"certainty\". Monte Carlo methods in finance allow financial analysts to construct \"stochastic\" or probabilistic corporate finance models, as opposed to the traditional static and deterministic models;[66] see Corporate finance § Quantifying uncertainty.  Relatedly, Real Options theory allows for owner – i.e. managerial – actions that impact underlying value: by incorporating option pricing logic, these actions are then applied to a distribution of future outcomes, changing with time, which then determine the \"project's\" valuation today.[67]  More traditionally, decision trees – which are complementary – have been used to evaluate projects, by incorporating in the valuation (all) possible events (or states) and consequent management decisions;[68][66] the correct discount rate here reflecting each decision-point's \"non-diversifiable risk looking forward.\"[66] [note 16]  Related to this, is the treatment of forecasted cashflows in equity valuation. In many cases, following Williams above, the average (or most likely) cash-flows were discounted,[70] as opposed to a theoretically correct state-by-state treatment under uncertainty; see comments under Financial modeling § Accounting.  In more modern treatments, then, it is the expected cashflows (in the mathematical sense:      ∑  s    p  s    X  s j     {\\textstyle \\sum _{s}p_{s}X_{sj}}  ) combined into an overall value per forecast period which are discounted. [71] [72] [73] [66] And using the CAPM – or extensions – the discounting here is at the risk-free rate plus a premium linked to the uncertainty of the entity or project cash flows [66] (essentially,     Y   {\\displaystyle Y}   and     r   {\\displaystyle r}   combined).  Other developments here include[74] agency theory, which analyses the difficulties in motivating corporate management (the \"agent\"; in a different sense to the above) to act in the best interests of shareholders (the \"principal\"), rather than in their own interests; here emphasizing the issues interrelated with capital structure. [75] Clean surplus accounting and the related residual income valuation provide a model that returns price as a function of earnings, expected returns, and change in book value, as opposed to dividends. This approach, to some extent, arises due to the implicit contradiction of seeing value as a function of dividends, while also holding that dividend policy cannot influence value per Modigliani and Miller's \"Irrelevance principle\"; see Dividend policy § Relevance of dividend policy.  \"Corporate finance\" as a discipline more generally, building on Fisher above, relates to the long term objective of maximizing the value of the firm - and its return to shareholders - and thus also incorporates the areas of capital structure and dividend policy.  [76] Extensions of the theory here then also consider these latter, as follows: (i) optimization re capitalization structure, and theories here as to corporate choices and behavior: Capital structure substitution theory, Pecking order theory, Market timing hypothesis, Trade-off theory; (ii) considerations and analysis re dividend policy, additional to - and sometimes contrasting with - Modigliani-Miller, include:  the Walter model, Lintner model, Residuals theory and signaling hypothesis, as well as discussion re the observed clientele effect and dividend puzzle.  As described, the typical application of real options is to capital budgeting type problems.  However, here, they are also applied to problems of capital structure and dividend policy, and to the related design of corporate securities; [77]   and since stockholder and bondholders have different objective functions, in the analysis of the related agency problems. [67]  In all of these cases, state-prices can provide the market-implied information relating to the corporate, as above, which is then applied to the analysis. For example, convertible bonds can (must) be priced consistent with the (recovered) state-prices of the corporate's equity.[21][71]  The discipline, as outlined, also includes a formal study of financial markets. Of interest especially are market regulation and market microstructure, and their relationship to price efficiency.  Regulatory economics studies, in general, the economics of regulation. In the context of finance, it will address the impact of financial regulation on the functioning of markets and the efficiency of prices, while also weighing the corresponding increases in market confidence and financial stability. Research here considers how, and to what extent, regulations relating to disclosure (earnings guidance, annual reports), insider trading, and short-selling will impact price efficiency, the cost of equity, and market liquidity.[78]  Market microstructure is concerned with the details of how exchange occurs in markets  (with Walrasian-, matching-, Fisher-, and  Arrow-Debreu markets as prototypes),  and \"analyzes how specific trading mechanisms affect the price formation process\",[79] examining the ways in which the processes of a market affect determinants of transaction costs, prices, quotes, volume, and trading behavior. It has been used, for example, in providing explanations for long-standing exchange rate puzzles,[80] and for the equity premium puzzle.[81]  In contrast to the above classical approach, models here explicitly allow for (testing the impact of) market frictions and other imperfections; see also market design.  For both regulation [82] and microstructure,[83] and generally,[84] agent-based models can be developed [85] to examine any impact due to a change in structure or policy - or to make inferences re market dynamics - by testing these in an artificial financial market, or AFM. [note 17] This approach, essentially simulated trade between numerous agents, \"typically uses artificial intelligence technologies [often genetic algorithms and neural nets] to represent the adaptive behaviour of market participants\".[85]  These 'bottom-up' models \"start from first principals of agent behavior\",[86] with participants modifying their trading strategies having learned over time, and \"are able to describe macro features [i.e. stylized facts] emerging from a soup of individual interacting strategies\".[86] Agent-based models depart further from the classical approach — the representative agent, as outlined — in that they introduce heterogeneity into the environment (thereby addressing, also, the aggregation problem).  As above, there is a very close link between: the random walk hypothesis, with the associated belief that price changes should follow a normal distribution, on the one hand;  and market efficiency and rational expectations, on the other. Wide departures from these are commonly observed, and there are thus, respectively, two main sets of challenges.  As discussed, the assumptions that market prices follow a random walk and that asset returns are normally distributed are fundamental. Empirical evidence, however, suggests that these assumptions may not hold, and that in practice, traders, analysts and risk managers frequently modify the \"standard models\" (see Kurtosis risk, Skewness risk, Long tail, Model risk).  In fact, Benoit Mandelbrot had discovered already in the 1960s  [87]  that changes in financial prices do not follow a normal distribution, the basis for much option pricing theory, although this observation was slow to find its way into mainstream financial economics. [88]  Financial models with long-tailed distributions and volatility clustering have been introduced to overcome problems with the realism of the above \"classical\" financial models; while jump diffusion models allow for (option) pricing incorporating \"jumps\" in the spot price.[89]  Risk managers, similarly, complement (or substitute) the standard value at risk models with historical simulations, mixture models, principal component analysis, extreme value theory, as well as models for volatility clustering.[90]  For further discussion see Fat-tailed distribution § Applications in economics, and Value at risk § Criticism. Portfolio managers, likewise, have modified their optimization criteria and algorithms; see § Portfolio theory above.  Closely related is the volatility smile, where, as above, implied volatility – the volatility corresponding to the BSM price – is observed to differ as a function of strike price (i.e. moneyness), true only if the price-change distribution is non-normal, unlike that assumed by BSM. The term structure of volatility describes how (implied) volatility differs for related options with different maturities. An implied volatility surface is then a three-dimensional surface plot of volatility smile and term structure. These empirical phenomena negate the assumption of constant volatility – and log-normality – upon which Black–Scholes is built.[40][89]   Within institutions, the function of Black–Scholes is now, largely, to communicate prices via implied volatilities, much like bond prices are communicated via YTM; see Black–Scholes model § The volatility smile.  In consequence traders (and risk managers) now, instead, use \"smile-consistent\" models, firstly, when valuing derivatives not directly mapped to the surface, facilitating the pricing of other, i.e. non-quoted, strike\/maturity combinations, or of non-European derivatives, and generally for hedging purposes.   The two main approaches are local volatility and stochastic volatility. The first returns the volatility which is \"local\" to each spot-time point of the finite difference- or simulation-based valuation; i.e. as opposed to implied volatility, which holds overall.  In this way calculated prices – and numeric structures – are market-consistent in an arbitrage-free sense.  The second approach assumes that the volatility of the underlying price is a stochastic process rather than a constant. Models here are first calibrated to observed prices, and are then applied to the valuation or hedging in question; the most common are Heston, SABR and CEV. This approach addresses certain problems identified with hedging under local volatility.[91]  Related to local volatility are the lattice-based implied-binomial and -trinomial trees – essentially a discretization of the approach – which are similarly, but less commonly,[20] used for pricing; these are built on state-prices recovered from the surface. Edgeworth binomial trees allow for a specified (i.e. non-Gaussian) skew and kurtosis in the spot price; priced here, options with differing strikes will return differing implied volatilities, and the tree can be calibrated to the smile as required.[92]  Similarly purposed (and derived) closed-form models were also developed. [93]  As discussed, additional to assuming log-normality in returns, \"classical\" BSM-type models also (implicitly) assume the existence of a credit-risk-free environment, where one can perfectly replicate cashflows so as to fully hedge, and then discount at \"the\" risk-free-rate.   And therefore, post crisis, the various x-value adjustments must be employed, effectively correcting the risk-neutral value for counterparty- and funding-related risk. These xVA are additional to any smile or surface effect. This is valid as the surface is built on price data relating to fully collateralized positions, and there is therefore no \"double counting\" of credit risk (etc.) when appending xVA. (Were this not the case, then each counterparty would have its own surface...)  As mentioned at top, mathematical finance (and particularly financial engineering) is more concerned with mathematical consistency (and market realities) than compatibility with economic theory, and the above \"extreme event\" approaches, smile-consistent modeling, and valuation adjustments should then be seen in this light.  Recognizing this, critics of financial economics - especially vocal since the 2007–2008 financial crisis - suggest that instead, the theory needs revisiting almost entirely: [note 18]  As seen, a common assumption is that financial decision makers act rationally; see Homo economicus. Recently, however, researchers in experimental economics and experimental finance have challenged this assumption empirically. These assumptions are also challenged theoretically, by behavioral finance, a discipline primarily concerned with the limits to rationality of economic agents. [note 19] For related criticisms re corporate finance theory vs its practice see:.[95]  Various persistent market anomalies have also been documented as consistent with and complementary to price or return distortions – e.g. size premiums – which appear to contradict the efficient-market hypothesis. Within these market anomalies, calendar effects are the most commonly referenced group. Related to these are various of the economic puzzles, concerning phenomena similarly contradicting the theory. The equity premium puzzle, as one example, arises in that the difference between the observed returns on stocks as compared to government bonds is consistently higher than the risk premium rational equity investors should demand, an \"abnormal return\". For further context see Random walk hypothesis § A non-random walk hypothesis, and sidebar for specific instances.  More generally, and, again, particularly following the 2007–2008 financial crisis, financial economics (and mathematical finance) has been subjected to deeper criticism. Notable here is Nassim Nicholas Taleb, whose critique overlaps the above, but extends  [96]  also to the institutional  [97] aspects of finance - including academic. [98] His Black swan theory posits that although events of large magnitude and consequence play a major role in finance, since these are (statistically) unexpected, they are \"ignored\" by economists and traders.  Thus, although a \"Taleb distribution\" - which normally provides a payoff of small positive returns, while carrying a small but significant risk of catastrophic losses - more realistically describes markets than current models, the latter continue to be preferred (even with professionals here acknowledging that it only \"generally works\" or only \"works on average\"). [99]  Here,[97] financial crises have been a topic of interest  [100]  and, in particular, the failure of (financial) economists - as well as [97] bankers and regulators - to model and predict these. See Financial crisis § Theories. The related problem of systemic risk, has also received attention. Where companies hold securities in each other, then this interconnectedness may entail a \"valuation chain\" – and the performance of one company, or security, here will impact all, a phenomenon not easily modeled, regardless of whether the individual models are correct. See: Systemic risk § Inadequacy of classic valuation models; Cascades in financial networks; Flight-to-quality.  Areas of research attempting to explain (or at least model) these phenomena, and crises, include [15] market microstructure and Heterogeneous agent models, as above. The latter is extended to agent-based computational models; here,[84] as mentioned, price is treated as an emergent phenomenon, resulting from the interaction of the various market participants (agents). The noisy market hypothesis argues that prices can be influenced by speculators and momentum traders, as well as by insiders and institutions that often buy and sell stocks for reasons unrelated to fundamental value; see Noise (economic) and  Noise trader. The adaptive market hypothesis is an attempt to reconcile the efficient market hypothesis with behavioral economics, by applying the principles of evolution to financial interactions. An information cascade, alternatively, shows market participants engaging in the same acts as others (\"herd behavior\"), despite contradictions with their private information. Copula-based modelling has similarly been applied. See also Hyman Minsky's \"financial instability hypothesis\", as well as  George Soros' application of \"reflexivity\". In the alternative, institutionally inherent limits to arbitrage - i.e. as opposed to factors directly contradictory to the theory - are sometimes referenced.  Note however, that despite the above inefficiencies, asset prices do effectively follow a random walk - i.e. in the sense that \"changes in the stock market are unpredictable, lacking any pattern that can be used by an investor to beat the overall market\".  [101] Thus after fund costs - and given other considerations -  it is difficult to consistently outperform market averages [102]   and achieve \"alpha\". The practical implication [103] is that passive investing, i.e. via low-cost index funds, should, on average, serve better than any other active strategy -  and, in fact, this practice is now widely adopted. [note 20] Here, however, the following concern is posited: although in concept, it is \"the research undertaken by active managers [that] keeps prices closer to value... [and] thus there is a fragile equilibrium in which some investors choose to index while the rest continue to search for mispriced securities\"; [103] in practice, as more investors \"pour money into index funds tracking the same stocks, valuations for those companies become inflated\",[104] potentially leading to asset bubbles.  Financial economics  Asset pricing  Corporate finance  Course material  Links and portals  Actuarial resources "},"meta":{},"created_at":"2025-03-22T14:25:42.292408Z","updated_at":"2025-03-22T14:25:42.292408Z","inner_id":95,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":104,"annotations":[{"id":104,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.326896Z","updated_at":"2025-03-22T14:25:42.326896Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"275a5936-ea75-4974-a99b-e77f80ba6d84","import_id":null,"last_action":null,"bulk_created":false,"task":104,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  A stock exchange, securities exchange, or bourse is an exchange where stockbrokers and traders can buy and sell securities, such as shares of stock, bonds and other financial instruments. Stock exchanges may also provide facilities for the issue and redemption of such securities and instruments and capital events including the payment of income and dividends. Securities traded on a stock exchange include stock issued by listed companies, unit trusts, derivatives, pooled investment products and bonds. Stock exchanges often function as \"continuous auction\" markets with buyers and sellers consummating transactions via open outcry at a central location such as the floor of the exchange or by using an electronic system to process financial transactions.[2]  To be able to trade a security on a particular stock exchange, the security must be listed there. Usually, there is a central location for record keeping, but trade is increasingly less linked to a physical place as modern markets use electronic communication networks, which give them advantages of increased speed and reduced cost of transactions. Trade on an exchange is restricted to brokers who are members of the exchange. In recent years, various other trading venues such as electronic communication networks, alternative trading systems and \"dark pools\" have taken much of the trading activity away from traditional stock exchanges.[3]  Initial public offerings of stocks and bonds to investors is done in the primary market and subsequent trading is done in the secondary market. A stock exchange is often the most important component of a stock market. Supply and demand in stock markets are driven by various factors that, as in all free markets, affect the price of stocks (see stock valuation).  There is usually no obligation for stock to be issued through the stock exchange itself, nor must stock be subsequently traded on an exchange. Such trading may be off exchange or over-the-counter. This is the usual way that derivatives and bonds are traded. Increasingly, stock exchanges are part of a global securities market. Stock exchanges also serve an economic function in providing liquidity to shareholders in providing an efficient means of disposing of shares. In recent years, as the ease and speed of exchanging stocks over digital platforms has increased, volatility in the day-to-day market has increased, too.  The beginnings of lending were in Italy in the late Middle Ages. In the 14th century, Venetian lenders would carry slates with information on the various issues for sale and meet with clients, much like a broker does today.[4] Venetian merchants introduced the principle of exchanging debts between moneylenders; a lender looking to unload a high-risk, high-interest loan might exchange it for a different loan with another lender. These lenders also bought government debt issues.[5] As the natural evolution of their business continued, the lenders began to sell debt issues to the first individual investors. The Venetians were the leaders in the field and the first to start trading securities from other governments, yet did not embark on private trade with India. Nor did the Italians connect on land with the Chinese Silk Road. Along the potential overland trade route, Holy Roman Emperor Frederick II repulsed advances by Mongol Batu Kahn (Golden Horde) in 1241.[6] There is little consensus among scholars as to when corporate stock was first traded. Some view the key event as the Dutch East India Company's founding in 1602,[7] while others point to much earlier developments (Bruges, Antwerp in 1531 and in Lyon in 1548). The first book in history of securities exchange, the Confusion of Confusions, was written by the Dutch-Jewish trader Joseph de la Vega and the Amsterdam Stock Exchange is often considered the oldest \"modern\" securities market in the world.[8] On the other hand, economist Ulrike Malmendier of the University of California at Berkeley argues that a share market existed as far back as ancient Rome, that derives from Etruscan \"Argentari\". In the Roman Republic, which existed for centuries before the Empire was founded, there were societates publicanorum, organizations of contractors or leaseholders who performed temple-building and other services for the government. One such service was the feeding of geese on the Capitoline Hill as a reward to the birds after their honking warned of a Gallic invasion in 390 B.C. Participants in such organizations had partes or shares, a concept mentioned various times by the statesman and orator Cicero. In one speech, Cicero mentions \"shares that had a very high price at the time\". Such evidence, in Malmendier's view, suggests the instruments were tradable, with fluctuating values based on an organization's success. The societas declined into obscurity in the time of the emperors, as most of their services were taken over by direct agents of the state.  Tradable bonds as a commonly used type of security were a more recent innovation, spearheaded by the Italian city-states of the late medieval and early Renaissance periods.[9]  Joseph de la Vega, also known as Joseph Penso de la Vega and by other variations of his name, was an Amsterdam trader from a Spanish Jewish family and a prolific writer as well as a successful businessman in 17th-century Amsterdam. His 1688 book Confusion of Confusions[10] explained the workings of the city's stock market. It was the earliest book about stock trading and inner workings of a stock market, taking the form of a dialogue between a merchant, a shareholder and a philosopher, the book described a market that was sophisticated but also prone to excesses, and de la Vega offered advice to his readers on such topics as the unpredictability of market shifts and the importance of patience in investment.  In England, the Dutch King William III sought to modernize the kingdom's finances to pay for its wars, and thus the first government bonds were issued in 1693 and the Bank of England was set up the following year. Soon thereafter, English joint-stock companies began going public.  London's first stockbrokers, however, were barred from the old commercial center known as the Royal Exchange, reportedly because of their rude manners. Instead, the new trade was conducted from coffee houses along Exchange Alley. By 1698, a broker named John Castaing, operating out of Jonathan's Coffee House, was posting regular lists of stock and commodity prices. Those lists mark the beginning of the London Stock Exchange.[11]  One of history's greatest financial bubbles occurred around 1720. At the center of it were the South Sea Company, set up in 1711 to conduct English trade with South America, and the Mississippi Company, focused on commerce with France's Louisiana colony and touted by transplanted Scottish financier John Law, who was acting in effect as France's central banker. Investors snapped up shares in both, and whatever else was available. In 1720, at the height of the mania, there was even an offering of \"a company for carrying out an undertaking of great advantage, but nobody to know what it is\".  By the end of that same year, share prices had started collapsing, as it became clear that expectations of imminent wealth from the Americas were overblown. In London, Parliament passed the Bubble Act, which stated that only royally chartered companies could issue public shares. In Paris, Law was stripped of office and fled the country. Stock trading was more limited and subdued in subsequent decades. Yet the market survived, and by the 1790s shares were being traded in the young United States. On May 17, 1792, the New York Stock Exchange opened under a Platanus occidentalis (buttonwood tree) in New York City, as 24 stockbrokers signed the Buttonwood Agreement, agreeing to trade five securities under that buttonwood tree.[12]  Bombay Stock Exchange was started by Premchand Roychand in 1875.[13] While BSE Limited is now synonymous with Dalal Street, it was not always so. In the 1850s, five stock brokers gathered together under a Banyan tree in front of Mumbai Town Hall, where Horniman Circle is now situated.[14] A decade later, the brokers moved their location to another leafy setting, this time under banyan trees at the junction of Meadows Street and what was then called Esplanade Road, now Mahatma Gandhi Road. With a rapid increase in the number of brokers, they had to shift places repeatedly. At last, in 1874, the brokers found a permanent location, the one that they could call their own. The brokers group became an official organization known as \"The Native Share & Stock Brokers Association\" in 1875.[15]  The Bombay Stock Exchange continued to operate out of a building near the Town Hall until 1928. The present site near Horniman Circle was acquired by the exchange in 1928, and a building was constructed and occupied in 1930. The street on which the site is located came to be called Dalal Street in Hindi (meaning \"Broker Street\") due to the location of the exchange.  On 31 August 1957, the BSE became the first stock exchange to be recognized by the Indian Government under the Securities Contracts Regulation Act. Construction of the present building, the Phiroze Jeejeebhoy Towers at Dalal Street, Fort area, began in the late 1970s and was completed and occupied by the BSE in 1980. Initially named the BSE Towers, the name of the building was changed soon after occupation, in memory of Sir Phiroze Jamshedji Jeejeebhoy, chairman of the BSE since 1966, following his death.  In 1986, the BSE developed the S&P BSE SENSEX index, giving the BSE a means to measure the overall performance of the exchange. In 2000, the BSE used this index to open its derivatives market, trading S&P BSE SENSEX futures contracts. The development of S&P BSE SENSEX options along with equity derivatives followed in 2001 and 2002, expanding the BSE's trading platform.  Historically an open outcry floor trading exchange, the Bombay Stock Exchange switched to an electronic trading system developed by Cmc ltd. in 1995. It took the exchange only 50 days to make this transition. This automated, screen-based trading platform called BSE On-Line Trading (BOLT) had a capacity of 8 million orders per day. Now BSE has raised capital by issuing shares and as on 3 May 2017 the BSE share which is traded in NSE only closed with ₹999.[16]  Stock exchanges have multiple roles in the economy. This may include the following:[17]  Besides the borrowing capacity provided to an individual or firm by the banking system, in the form of credit or a loan, a stock exchange provides companies with the facility to raise capital for expansion through selling shares to the investing public.[18]  Capital intensive companies, particularly high tech companies, typically need to raise high volumes of capital in their early stages. For this reason, the public market provided by the stock exchanges has been one of the most important funding sources for many capital intensive startups. In the 1990s and early 2000s, hi-tech listed companies experienced a boom and bust in the world's major stock exchanges.[19] Since then, it has been much more demanding for the high-tech entrepreneur to take his\/her company public, unless either the company is already generating sales and earnings, or the company has demonstrated credibility and potential from successful outcomes: clinical trials, market research, patent registrations, etc. This shift in market expectations has led to an increased reliance on private equity and venture capital funding in the early stages of high-tech companies.[20] This is quite different from the situation of the 1990s to early-2000s period, when a number of companies (particularly Internet boom and biotechnology companies) went public in the most prominent stock exchanges around the world in the total absence of sales, earnings, or any type of well-documented promising outcome. Though it is not as common, it still happens that highly speculative and financially unpredictable hi-tech startups are listed for the first time in a major stock exchange. Additionally, there are smaller, specialized entry markets for these kind of companies with stock indexes tracking their performance (examples include the Alternext, CAC Small, SDAX, TecDAX).  Alternative investment funds refer to funds that include hedge funds, venture capital, private equity, angel funds, real estate, commodities, collectibles, structured products, etc. Alternative investment funds are an alternative to traditional investment options (stocks, bonds, and cash).  Companies have also raised significant amounts of capital through R&D limited partnerships. Tax law changes that were enacted in 1987 in the United States changed the tax deductibility of investments in R&D limited partnerships.[21] In order for a partnership to be of interest to investors today, the cash on cash return must be high enough to entice investors.  A general source of capital for startup companies has been venture capital. This source remains largely available today, but the maximum statistical amount that the venture company firms in aggregate will invest in any one company is not limitless (it was approximately $15 million in 2001 for a biotechnology company).[22]  Another alternative source of cash for a private company is a corporate partner, usually an established multinational company, which provides capital for the smaller company in return for marketing rights, patent rights, or equity. Corporate partnerships have been used successfully in a large number of cases.  When people draw their savings and invest in shares (through an initial public offering or the seasoned equity offering of an already listed company), it usually leads to rational allocation of resources because funds, which could have been consumed, or kept in idle deposits with banks, are mobilized and redirected to help companies' management boards finance their organizations. This may promote business activity with benefits for several economic sectors such as agriculture, commerce and industry, resulting in stronger economic growth and higher productivity levels of firms.  Companies view acquisitions as an opportunity to expand product lines, increase distribution channels, hedge against volatility, increase their market share, or acquire other necessary business assets. A takeover bid or mergers and acquisitions through the stock market is one of the simplest and most common ways for a company to grow by acquisition or fusion.  By going public and listing on a stock exchange, companies gain access to a broader pool of investors, which can provide the necessary funds for expansion, research and development, and other growth initiatives. Additionally, being listed on a stock exchange enhances a company's visibility and credibility, making it more attractive to potential partners, customers, and employees. According to a report by the World Federation of Exchanges (WFE), stock exchanges contribute to economic growth by enabling companies to access long-term capital, thereby fostering innovation and job creation.[23]  While stock exchanges are not designed to be platforms for the redistribution of wealth,[24] they play a significant role in allowing both casual and professional stock investors to partake in the wealth generated by profitable businesses. This is achieved through the distribution of dividends and the potential for stock price increases leading to capital gains. As a result, individuals who invest in stocks have the opportunity to share in the prosperity of successful companies,[25] effectively participating in a form of wealth redistribution through their investment activities. Thus, while not the primary purpose of stock exchanges, the opportunity for individuals to benefit from the success of businesses can be seen as a form of wealth redistribution within the financial markets.  Both casual and professional stock investors, as large as institutional investors or as small as an ordinary middle-class family, through dividends and stock price increases that may result in capital gains, share in the wealth of profitable businesses. Unprofitable and troubled businesses may result in capital losses for shareholders.  By having a wide and varied scope of owners, companies generally tend to improve management standards and efficiency to satisfy the demands of these shareholders and the more stringent rules for public corporations imposed by public stock exchanges and the government. This improvement can be attributed in some cases to the price mechanism exerted through shares of stock, wherein the price of the stock falls when management is considered poor (making the firm vulnerable to a takeover by new management) or rises when management is doing well (making the firm less vulnerable to a takeover). In addition, publicly listed shares are subject to greater transparency so that investors can make informed decisions about a purchase. Consequently, it is alleged that public companies (companies that are owned by shareholders who are members of the general public and trade shares on public exchanges) tend to have better management records than privately held companies (those companies where shares are not publicly traded, often owned by the company founders, their families and heirs, or otherwise by a small group of investors).[26]  Despite this claim, some well-documented cases are known where it is alleged that there has been considerable slippage in corporate governance on the part of some public companies, particularly in the cases of accounting scandals. The policies that led to the dot-com bubble in the late 1990s and the subprime mortgage crisis in 2007–08 are also examples of corporate mismanagement. The mismanagement of companies such as Pets.com (2000), Enron (2001), One.Tel (2001), Sunbeam Products (2001), Webvan (2001), Adelphia Communications Corporation (2002), MCI WorldCom (2002), Parmalat (2003), American International Group (2008), Bear Stearns (2008), Lehman Brothers (2008), General Motors (2009) and Satyam Computer Services (2009) all received plenty of media attention.  Many banks and companies worldwide utilize securities identification numbers (ISIN) to identify, uniquely, their stocks, bonds and other securities. Adding an ISIN code helps to distinctly identify securities and the ISIN system is used worldwide by funds, companies, and governments.  However, when poor financial, ethical or managerial records become public, stock investors tend to lose money as the stock and the company tend to lose value. In the stock exchanges, shareholders of underperforming firms are often penalized by significant share price decline, and they tend as well to dismiss incompetent management teams.  As opposed to other businesses that require huge capital outlay, investing in shares is open to both the large and small stock investors as minimum investment amounts are minimal. Therefore, the stock exchange provides the opportunity for small investors to own shares of the same companies as large investors.  Governments at various levels may decide to borrow money to finance infrastructure projects such as sewage and water treatment works or housing estates by selling another category of securities known as bonds. These bonds can be raised through the stock exchange whereby members of the public buy them, thus loaning money to the government. The issuance of such bonds can obviate, in the short term, direct taxation of citizens to finance development—though by securing such bonds with the full faith and credit of the government instead of with collateral, the government must eventually tax citizens or otherwise raise additional funds to make any regular coupon payments and refund the principal when the bonds mature.  At the stock exchange, share prices rise and decreases depending, largely, on economic forces. Share prices tend to rise or remain stable when companies and the economy in general show signs of stability and growth. A recession, depression, or financial crisis could eventually lead to a stock market crash. Therefore, the movement of share prices and in general of the stock indexes can be an indicator of the general trend in the economy.  Stock exchanges offer employment opportunities to various individuals such as jobbers and other members who perform activities within the stock exchange. This makes the stock exchange a source of employment, not only for investors but also for the members and their employees. The diverse range of roles within the stock exchange, including trading, analysis, compliance, and administrative functions, creates an ecosystem of employment opportunities that support the operations and functions of the exchange. Additionally, the stock exchange's role in facilitating capital formation and investment in businesses also indirectly contributes to job creation and economic growth, making it a significant player in the employment landscape.[27]  The stock exchange plays a role in regulating companies by exerting a significant influence on their management practices.[28][23] To be listed on a stock exchange, a company is required to adhere to a set of rules and regulations established by the exchange itself. These regulations serve as a framework for corporate governance, financial transparency, and accountability, thereby ensuring that listed companies operate in a manner that is conducive to investor confidence and market stability. By imposing these standards, stock exchanges contribute to the overall integrity and reliability of the financial markets, fostering an environment where companies are held accountable for their actions and decisions, ultimately benefiting both investors and the broader economy.  Each stock exchange imposes its own listing requirements upon companies that want to be listed on that exchange. Such conditions may include minimum number of shares outstanding, minimum market capitalization, and minimum annual income.  The listing requirements imposed by some stock exchanges include:  Stock exchanges originated as mutual organizations, owned by its member stockbrokers. However, the major stock exchanges have demutualized, where the members sell their shares in an initial public offering. In this way the mutual organization becomes a corporation, with shares that are listed on a stock exchange. Examples are Australian Securities Exchange (1998), Euronext (merged with New York Stock Exchange), NASDAQ (2002), Bursa Malaysia (2004), the New York Stock Exchange (2005), Bolsas y Mercados Españoles, and the São Paulo Stock Exchange (2007).  The Shenzhen Stock Exchange and Shanghai Stock Exchange can be characterized as quasi-state institutions insofar as they were created by government bodies in China and their leading personnel are directly appointed by the China Securities Regulatory Commission.  Another example is Tashkent Stock Exchange established in 1994, three years after the collapse of the Soviet Union, mainly state-owned but has a form of a public corporation (joint-stock company). Korea Exchange (KRX) owns 25% less one share of the Tashkent Stock Exchange.[35]  In 2018, there were 15 licensed stock exchanges in the United States, of which 13 actively traded securities. All of these exchanges were owned by three publicly traded multinational companies, Intercontinental Exchange, Nasdaq, Inc., and Cboe Global Markets, except one, IEX.[36][37] In 2019, a group of financial corporations announced plans to open a members owned exchange, MEMX, an ownership structure similar to the mutual organizations of earlier exchanges.[38][36]  Top ten traditional stock exchanges by total market capitalization ( As of December 2024)[39]  July 2024  (in billions of dollars)  In the 19th century, exchanges were opened to trade forward contracts on commodities. Exchange traded forward contracts are called futures contracts. These commodity markets later started offering future contracts on other products, such as interest rates and shares, as well as options contracts. They are now generally known as futures exchanges.  Lists: "},"meta":{},"created_at":"2025-03-22T14:25:42.292408Z","updated_at":"2025-03-22T14:25:42.292408Z","inner_id":96,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":105,"annotations":[{"id":105,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.326896Z","updated_at":"2025-03-22T14:25:42.327895Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"f004a476-aa93-493a-abf0-a17a3a7f70fd","import_id":null,"last_action":null,"bulk_created":false,"task":105,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Empirical methods  Prescriptive and policy  The economy of governments covers the systems for setting levels of taxation, government budgets, the money supply and interest rates as well as the labour market, national ownership, and many other areas of government interventions into the economy.  Most factors of economic policy can be divided into either fiscal policy, which deals with government actions regarding taxation and spending, or monetary policy, which deals with central banking actions regarding the money supply and interest rates.  Such policies are often influenced by international institutions like the International Monetary Fund or World Bank as well as political beliefs and the consequent policies of parties.  Almost every aspect of government has an important economic component. A few examples of the kinds of economic policies that exist include:[1]  Stabilization policy attempts to stimulate an economy out of recession or constrain the money supply to prevent excessive inflation.  Policy is generally directed to achieve particular objectives, like targets for inflation, unemployment, or economic growth. Sometimes other objectives, like military spending or nationalization are important.  These are referred to as the policy goals: the outcomes which the economic policy aims to achieve.  To achieve these goals, governments use policy tools which are under the control of the government. These generally include the interest rate and money supply, tax and government spending, tariffs, exchange rates, labor market regulations, and many other aspects of government.  Government and central banks are limited in the number of goals they can achieve in the short term. For instance, there may be pressure on the government to reduce inflation, reduce unemployment, and reduce interest rates while maintaining currency stability. If all of these are selected as goals for the short term, then policy is likely to be incoherent, because a normal consequence of reducing inflation and maintaining currency stability is increasing unemployment and increasing interest rates.  This dilemma can in part be resolved by using microeconomic supply-side policy to help adjust markets. For instance, unemployment could potentially be reduced by altering laws relating to trade unions or unemployment insurance, as well as by macroeconomic (demand-side) factors like interest rates.  For much of the 20th century, governments adopted discretionary policies like demand management designed to correct the business cycle. These typically used fiscal and monetary policy to adjust inflation, output and unemployment.  However, following the stagflation of the 1970s, policymakers began to be attracted to policy rules.   A discretionary policy is supported because it allows policymakers to respond quickly to events. However, discretionary policy can be subject to dynamic inconsistency: a government may say it intends to raise interest rates indefinitely to bring inflation under control, but then relax its stance later. This makes policy non-credible and ultimately ineffective.  A rule-based policy can be more credible, because it is more transparent and easier to anticipate. Examples of rule-based policies are fixed exchange rates, interest rate rules, the stability and growth pact and the Golden Rule. Some policy rules can be imposed by external bodies, for instance, the Exchange Rate Mechanism for currency.   A compromise between strict discretionary and strict rule-based policy is to grant discretionary power to an independent body. For instance, the Federal Reserve Bank, European Central Bank, Bank of England and Reserve Bank of Australia all set interest rates without government interference, but do not adopt rules.  Another type of non-discretionary policy is a set of policies that are imposed by an international body. This can occur (for example) as a result of intervention by the International Monetary Fund.  The first economic problem was how to gain the resources it needed to be able to perform the functions of an early government: the military, roads and other projects like building the Pyramids.  Early governments generally relied on tax in kind and forced labor for their economic resources. However, with the development of money came the first policy choice. A government could raise money through taxing its citizens. However, it could now also debase the coinage and so increase the money supply.   Early civilizations also made decisions about whether to permit and how to tax trade. Some early civilizations, such as Ptolemaic Egypt adopted a closed currency policy whereby foreign merchants had to exchange their coin for local money. This effectively levied a very high tariff on foreign trade.  By the early modern age, more policy choices had been developed. There was considerable debate about mercantilism and other restrictive trade practices like the Navigation Acts, as trade policy became associated with both national wealth and with foreign and colonial policy.  Throughout the 19th century, monetary standards became an important issue. Gold and silver were in supply in different proportions. Which metal was adopted influenced the wealth of different groups in society.  With the accumulation of private capital in the Renaissance, states developed methods of financing deficits without debasing their coin. The development of capital markets meant that a government could borrow money to finance war or expansion while causing less economic hardship.   This was the beginning of modern fiscal policy.  The same markets made it easy for private entities to raise bonds or sell stock to fund private initiatives.  The business cycle became a predominant issue in the 19th century, as it became clear that industrial output, employment, and profit behaved in a cyclical manner. One of the first proposed policy solutions to the problem came with the work of Keynes, who proposed that fiscal policy could be used actively to ward off depressions, recessions and slumps.  The Austrian School of economics argues that central banks create the business cycle. After the dominance of monetarism[2] and neoclassical thought that advised limiting the role of government in the economy in the second half of the twentieth century, the interventionist view has once more dominated the economic policy debate in response to the 2007-2008 financial crisis,[3]  A recent trend originating from medicine is to justify economic policy decisions with best available evidence.[4] While the previous approaches have been focused on macroeconomic policymaking aimed at sustaining promoting economic development and counteracting recessions, EBP is oriented towards all types of decisions concerned not only with anti-cyclical development but primarily with the growth-promoting policies. To gather evidence for such decisions, economists conduct randomized field experiments. The work of Banerjee, Duflo, and Kremer, the 2019 Nobel Prize laureates[5] exemplifies the gold type of evidence. However, the emphasis put on experimental evidence by the movement of evidence-based policy (and evidence-based medicine) results from the narrowly construed notion of intervention, which encompasses only policy decisions concerned with policymaking aimed at modifying causes to influence effects. In contrast to this idealized view of evidence-based policy movement, economic policymaking is a broader term that includes also institutional reforms and actions that do not require causal claims to be neutral under interventions. Such policy decisions can be grounded in, respectively, mechanistic evidence and correlational (econometric) studies.[6] "},"meta":{},"created_at":"2025-03-22T14:25:42.292408Z","updated_at":"2025-03-22T14:25:42.292408Z","inner_id":97,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":106,"annotations":[{"id":106,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.327895Z","updated_at":"2025-03-22T14:25:42.327895Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"42b02b97-6368-4fc7-a002-a6a9543b4af0","import_id":null,"last_action":null,"bulk_created":false,"task":106,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"This is an accepted version of this page   Empirical methods  Prescriptive and policy  Capitalism is an economic system based on the private ownership of the means of production and their operation for profit.[a] It is characterized by private property, capital accumulation, competitive markets, commodification, wage labor, and an emphasis on innovation and economic growth.[b] However, such economies tend to experience a business cycle of economic growth followed by recessions.[13]  Economists, historians, political economists, and sociologists have adopted different perspectives in their analyses of capitalism and have recognized various forms of it in practice. These include laissez-faire or free-market capitalism, anarcho-capitalism, state capitalism, and welfare capitalism. Different forms of capitalism feature varying degrees of free markets, public ownership,[14] obstacles to free competition, and state-sanctioned social policies. The degree of competition in markets and the role of intervention and regulation, as well as the scope of state ownership, vary across different models of capitalism.[15][16] The extent to which different markets are free and the rules defining private property are matters of politics and policy. Most of the existing capitalist economies are mixed economies that combine elements of free markets with state intervention and in some cases economic planning.[17]  Capitalism in its modern form emerged from agrarianism in England, as well as mercantilist practices by European countries between the 16th and 18th centuries. The Industrial Revolution of the 18th century established capitalism as a dominant mode of production, characterized by factory work, and a complex division of labor. Through the process of globalization, capitalism spread across the world in the 19th and 20th centuries, especially before World War I and after the end of the Cold War. During the 19th century, capitalism was largely unregulated by the state, but became more regulated in the post–World War II period through Keynesianism, followed by a return of more unregulated capitalism starting in the 1980s through neoliberalism.  The term \"capitalist\", meaning an owner of capital, appears earlier than the term \"capitalism\" and dates to the mid-17th century. \"Capitalism\" is derived from capital, which evolved from capitale, a late Latin word based on caput, meaning \"head\"—which is also the origin of \"chattel\" and \"cattle\" in the sense of movable property (only much later to refer only to livestock). Capitale emerged in the 12th to 13th centuries to refer to funds, stock of merchandise, sum of money or money carrying interest.[18]: 232 [19] By 1283, it was used in the sense of the capital assets of a trading firm and was often interchanged with other words—wealth, money, funds, goods, assets, property and so on.[18]: 233   The Hollantse (German: holländische) Mercurius uses \"capitalists\" in 1633 and 1654 to refer to owners of capital.[18]: 234  In French, Étienne Clavier referred to capitalistes in 1788,[20] four years before its first recorded English usage by Arthur Young in his work Travels in France (1792).[19][21] In his Principles of Political Economy and Taxation (1817), David Ricardo referred to \"the capitalist\" many times.[22] English poet Samuel Taylor Coleridge used \"capitalist\" in his work Table Talk (1823).[23] Pierre-Joseph Proudhon used the term in his first work, What is Property? (1840), to refer to the owners of capital. Benjamin Disraeli used the term in his 1845 work Sybil.[19] Alexander Hamilton used \"capitalist\" in his Report of Manufactures presented to the United States Congress in 1791.  The initial use of the term \"capitalism\" in its modern sense is attributed to Louis Blanc in 1850 (\"What I call 'capitalism' that is to say the appropriation of capital by some to the exclusion of others\") and Pierre-Joseph Proudhon in 1861 (\"Economic and social regime in which capital, the source of income, does not generally belong to those who make it work through their labor\").[18]: 237  Karl Marx frequently referred to the \"capital\" and to the \"capitalist mode of production\" in Das Kapital (1867).[24][25] Marx did not use the form capitalism but instead used capital, capitalist and capitalist mode of production, which appear frequently.[25][26] Due to the word being coined by socialist critics of capitalism, economist and historian Robert Hessen stated that the term \"capitalism\" itself is a term of disparagement and a misnomer for economic individualism.[27] Bernard Harcourt agrees with the statement that the term is a misnomer, adding that it misleadingly suggests that there is such a thing as \"capital\" that inherently functions in certain ways and is governed by stable economic laws of its own.[28]  In the English language, the term \"capitalism\" first appears, according to the Oxford English Dictionary (OED), in 1854, in the novel The Newcomes by novelist William Makepeace Thackeray, where the word meant \"having ownership of capital\".[29] Also according to the OED, Carl Adolph Douai, a German American socialist and abolitionist, used the term \"private capitalism\" in 1863.  Other terms sometimes used for capitalism are:  There is no universally agreed upon definition of capitalism; it is unclear whether or not capitalism characterizes an entire society, a specific type of social order, or crucial components or elements of a society.[37] Societies officially founded in opposition to capitalism (such as the Soviet Union) have sometimes been argued to actually exhibit characteristics of capitalism.[38] Nancy Fraser describes usage of the term \"capitalism\" by many authors as \"mainly rhetorical, functioning less as an actual concept than as a gesture toward the need for a concept\".[8] Scholars who are uncritical of capitalism rarely actually use the term \"capitalism\".[39] Some doubt that the term \"capitalism\" possesses valid scientific dignity,[37] and it is generally not discussed in mainstream economics,[8] with economist Daron Acemoglu suggesting that the term \"capitalism\" should be abandoned entirely.[40] Consequently, understanding of the concept of capitalism tends to be heavily influenced by opponents of capitalism and by the followers and critics of Karl Marx.[39]  Capitalism, in its modern form, can be traced to the emergence of agrarian capitalism and mercantilism in the early Renaissance, in city-states like Florence.[42] Capital has existed incipiently on a small scale for centuries[43] in the form of merchant, renting and lending activities and occasionally as small-scale industry with some wage labor. Simple commodity exchange and consequently simple commodity production, which is the initial basis for the growth of capital from trade, have a very long history. During the Islamic Golden Age, Arabs promulgated capitalist economic policies such as free trade and banking. Their use of Indo-Arabic numerals facilitated bookkeeping. These innovations migrated to Europe through trade partners in cities such as Venice and Pisa. Italian mathematicians traveled the Mediterranean talking to Arab traders and returned to popularize the use of Indo-Arabic numerals in Europe.[44]  The economic foundations of the feudal agricultural system began to shift substantially in 16th-century England as the manorial system had broken down and land began to become concentrated in the hands of fewer landlords with increasingly large estates. Instead of a serf-based system of labor, workers were increasingly employed as part of a broader and expanding money-based economy. The system put pressure on both landlords and tenants to increase the productivity of agriculture to make profit; the weakened coercive power of the aristocracy to extract peasant surpluses encouraged them to try better methods, and the tenants also had incentive to improve their methods in order to flourish in a competitive labor market. Terms of rent for land were becoming subject to economic market forces rather than to the previous stagnant system of custom and feudal obligation.[45][46]  The economic doctrine prevailing from the 16th to the 18th centuries is commonly called mercantilism.[47][48] This period, the Age of Discovery, was associated with the geographic exploration of foreign lands by merchant traders, especially from England and the Low Countries. Mercantilism was a system of trade for profit, although commodities were still largely produced by non-capitalist methods.[49] Most scholars consider the era of merchant capitalism and mercantilism as the origin of modern capitalism,[48][50] although Karl Polanyi argued that the hallmark of capitalism is the establishment of generalized markets for what he called the \"fictitious commodities\", i.e. land, labor and money. Accordingly, he argued that \"not until 1834 was a competitive labor market established in England, hence industrial capitalism as a social system cannot be said to have existed before that date\".[51]  England began a large-scale and integrative approach to mercantilism during the Elizabethan Era (1558–1603). A systematic and coherent explanation of balance of trade was made public through Thomas Mun's argument England's Treasure by Forraign Trade, or the Balance of our Forraign Trade is The Rule of Our Treasure. It was written in the 1620s and published in 1664.[52]  European merchants, backed by state controls, subsidies and monopolies, made most of their profits by buying and selling goods. In the words of Francis Bacon, the purpose of mercantilism was \"the opening and well-balancing of trade; the cherishing of manufacturers; the banishing of idleness; the repressing of waste and excess by sumptuary laws; the improvement and husbanding of the soil; the regulation of prices...\".[53]  After the period of the proto-industrialization, the British East India Company and the Dutch East India Company, after massive contributions from the Mughal Bengal,[54][55] inaugurated an expansive era of commerce and trade.[56][57] These companies were characterized by their colonial and expansionary powers given to them by nation-states.[56] During this era, merchants, who had traded under the previous stage of mercantilism, invested capital in the East India Companies and other colonies, seeking a return on investment.  In the mid-18th century a group of economic theorists, led by David Hume (1711–1776)[59] and Adam Smith (1723–1790), challenged fundamental mercantilist doctrines—such as the belief that the world's wealth remained constant and that a state could only increase its wealth at the expense of another state.  During the Industrial Revolution, industrialists replaced merchants as a dominant factor in the capitalist system and effected the decline of the traditional handicraft skills of artisans, guilds and journeymen. Industrial capitalism marked the development of the factory system of manufacturing, characterized by a complex division of labor between and within work process and the routine of work tasks; and eventually established the domination of the capitalist mode of production.[60]  Industrial Britain eventually abandoned the protectionist policy formerly prescribed by mercantilism. In the 19th century, Richard Cobden (1804–1865) and John Bright (1811–1889), who based their beliefs on the Manchester School, initiated a movement to lower tariffs.[61] In the 1840s Britain adopted a less protectionist policy, with the 1846 repeal of the Corn Laws and the 1849 repeal of the Navigation Acts.[62] Britain reduced tariffs and quotas, in line with David Ricardo's advocacy of free trade.  Broader processes of globalization carried capitalism across the world. By the beginning of the nineteenth century, a series of loosely connected market systems had come together as a relatively integrated global system, in turn intensifying processes of economic and other globalization.[63][64] Late in the 20th century, capitalism overcame a challenge by centrally-planned economies and is now the encompassing system worldwide,[33][65] with the mixed economy as its dominant form in the industrialized Western world.  Industrialization allowed cheap production of household items using economies of scale, while rapid population growth created sustained demand for commodities. The imperialism of the 18th-century decisively shaped globalization.[63][66][67][68]  After the First and Second Opium Wars (1839–60) by Britain and France and the completion of the British conquest of India by 1858 and the French conquest of Africa, Polynesia and Indochina by 1887, vast populations of Asia became consumers of European exports. Europeans colonized areas of Africa and the Pacific islands. Colonisation by Europeans, notably of Africa by the British and French, yielded valuable natural resources such as rubber, diamonds and coal and helped fuel trade and investment between the European imperial powers, their colonies and the United States:  The inhabitant of London could order by telephone, sipping his morning tea, the various products of the whole earth, and reasonably expect their early delivery upon his doorstep. Militarism and imperialism of racial and cultural rivalries were little more than the amusements of his daily newspaper. What an extraordinary episode in the economic progress of man was that age which came to an end in August 1914.[69] From the 1870s to the early 1920s, the global financial system was mainly tied to the gold standard.[70][71] The United Kingdom first formally adopted this standard in 1821. Soon to follow were Canada in 1853, Newfoundland in 1865, the United States and Germany (de jure) in 1873. New technologies, such as the telegraph, the transatlantic cable, the radiotelephone, the steamship and railways allowed goods and information to move around the world to an unprecedented degree.[72]  In the United States, the term \"capitalist\" primarily referred to powerful businessmen[73] until the 1920s due to widespread societal skepticism and criticism of capitalism and its most ardent supporters.  Contemporary capitalist societies developed in the West from 1950 to the present and this type of system continues throughout the world—relevant examples started in the United States after the 1950s, France after the 1960s, Spain after the 1970s, Poland after 2015, and others. At this stage most capitalist markets are considered[by whom?] developed and characterized by developed private and public markets for equity and debt, a high standard of living (as characterized by the World Bank and the IMF), large institutional investors and a well-funded banking system. A significant managerial class has emerged[when?] and decides on a significant proportion of investments and other decisions. A different future than that envisioned by Marx has started to emerge—explored and described by Anthony Crosland in the United Kingdom in his 1956 book The Future of Socialism[74] and by John Kenneth Galbraith in North America in his 1958 book The Affluent Society,[75] 90 years after Marx's research on the state of capitalism in 1867.[76]  The postwar boom ended in the late 1960s and early 1970s and the economic situation grew worse with the rise of stagflation.[77] Monetarism, a modification of Keynesianism that is more compatible with laissez-faire analyses, gained increasing prominence in the capitalist world, especially under the years in office of Ronald Reagan in the United States (1981–1989) and of Margaret Thatcher in the United Kingdom (1979–1990). Public and political interest began shifting away from the so-called collectivist concerns of Keynes's managed capitalism to a focus on individual choice, called \"remarketized capitalism\".[78]  The end of the Cold War and the dissolution of the Soviet Union allowed for capitalism to become a truly global system in a way not seen since before World War I. The development of the neoliberal global economy would have been impossible without the fall of communism.[79][80][81]  Harvard Kennedy School economist Dani Rodrik distinguishes between three historical variants of capitalism:[82]  The relationship between democracy and capitalism is a contentious area in theory and in popular political movements.[83] The extension of adult-male suffrage in 19th-century Britain occurred along with the development of industrial capitalism and representative democracy became widespread at the same time as capitalism, leading capitalists to posit a causal or mutual relationship between them. However, according to some authors in the 20th-century, capitalism also accompanied a variety of political formations quite distinct from liberal democracies, including fascist regimes, absolute monarchies and single-party states.[48] Democratic peace theory asserts that democracies seldom fight other democracies, but others suggest this may be because of political similarity or stability, rather than because they are \"democratic\" or \"capitalist\". Critics argue that though economic growth under capitalism has led to democracy, it may not do so in the future as authoritarian régimes have been able to manage economic growth using some of capitalism's competitive principles[84][85] without making concessions to greater political freedom.[86][87]  Political scientists Torben Iversen and David Soskice see democracy and capitalism as mutually supportive.[88] Robert Dahl argued in On Democracy that capitalism was beneficial for democracy because economic growth and a large middle class were good for democracy.[89] He also argued that a market economy provided a substitute for government control of the economy, which reduces the risks of tyranny and authoritarianism.[89]  In his book The Road to Serfdom (1944), Friedrich Hayek (1899–1992) asserted that the free-market understanding of economic freedom as present in capitalism is a requisite of political freedom. He argued that the market mechanism is the only way of deciding what to produce and how to distribute the items without using coercion. Milton Friedman and Ronald Reagan also promoted this view.[90] Friedman claimed that centralized economic operations are always accompanied by political repression. In his view, transactions in a market economy are voluntary and the wide diversity that voluntary activity permits is a fundamental threat to repressive political leaders and greatly diminishes their power to coerce. Some of Friedman's views were shared by John Maynard Keynes, who believed that capitalism was vital for freedom to survive and thrive.[91][92] Freedom House, an American think-tank that conducts international research on, and advocates for, democracy, political freedom and human rights, has argued that \"there is a high and statistically significant correlation between the level of political freedom as measured by Freedom House and economic freedom as measured by the Wall Street Journal\/Heritage Foundation survey\".[93]  In Capital in the Twenty-First Century (2013), Thomas Piketty of the Paris School of Economics asserted that inequality is the inevitable consequence of economic growth in a capitalist economy and the resulting concentration of wealth can destabilize democratic societies and undermine the ideals of social justice upon which they are built.[94]  States with capitalistic economic systems have thrived under political regimes deemed to be authoritarian or oppressive. Singapore has a successful open market economy as a result of its competitive, business-friendly climate and robust rule of law. Nonetheless, it often comes under fire for its style of government which, though democratic and consistently one of the least corrupt,[95] operates largely under a one-party rule. Furthermore, it does not vigorously defend freedom of expression as evidenced by its government-regulated press, and its penchant for upholding laws protecting ethnic and religious harmony, judicial dignity and personal reputation. The private (capitalist) sector in the People's Republic of China has grown exponentially and thrived since its inception, despite having an authoritarian government. Augusto Pinochet's rule in Chile led to economic growth and high levels of inequality[96] by using authoritarian means to create a safe environment for investment and capitalism. Similarly, Suharto's authoritarian reign and extirpation of the Communist Party of Indonesia allowed for the expansion of capitalism in Indonesia.[97][98]  The term \"capitalism\" in its modern sense is often attributed to Karl Marx.[49][99] In Das Kapital, Marx analyzed the \"capitalist mode of production\" using a method of critique that later became known as Marxism. However, while Marx did discuss capitalism extensively, he used the term \"capitalism\" less frequently than \"capitalist mode of production.\" His collaborator, Friedrich Engels, played a significant role in popularizing the term in more political interpretations of their work. In the 20th century, supporters of the capitalist system often replaced the term \"capitalism\" with phrases such as \"free enterprise\" or \"private enterprise\" to avoid its negative connotations. Similarly, the term \"capitalist\" was sometimes substituted with \"investor\" or \"entrepreneur\" to emphasize productive roles rather than passive wealth accumulation.[100]  In general, capitalism as an economic system and mode of production can be summarized by the following:[101]  In free market and laissez-faire forms of capitalism, markets are used most extensively with minimal or no regulation over the pricing mechanism. In mixed economies, which are almost universal today,[111] markets continue to play a dominant role, but they are regulated to some extent by the state in order to correct market failures, promote social welfare, conserve natural resources, fund defense and public safety or other rationale. In state capitalist systems, markets are relied upon the least, with the state relying heavily on state-owned enterprises or indirect economic planning to accumulate capital.  Competition arises when more than one producer is trying to sell the same or similar products to the same buyers. Adherents of the capitalist theory believe that competition leads to innovation and more affordable prices. Monopolies or cartels can develop, especially if there is no competition. A monopoly occurs when a firm has exclusivity over a market. Hence, the firm can engage in rent seeking behaviors such as limiting output and raising prices because it has no fear of competition.  Governments have implemented legislation for the purpose of preventing the creation of monopolies and cartels. In 1890, the Sherman Antitrust Act became the first legislation passed by the United States Congress to limit monopolies.[112]  Wage labor, usually referred to as paid work, paid employment, or paid labor, refers to the socioeconomic relationship between a worker and an employer in which the worker sells their labor power under a formal or informal employment contract.[105] These transactions usually occur in a labor market where wages or salaries are market-determined.[113]  In exchange for the money paid as wages (usual for short-term work-contracts) or salaries (in permanent employment contracts), the work product generally becomes the undifferentiated property of the employer. A wage laborer is a person whose primary means of income is from the selling of their labor in this way.[114]  The profit motive, in the theory of capitalism, is the desire to earn income in the form of profit. Stated differently, the reason for a business's existence is to turn a profit.[115] The profit motive functions according to rational choice theory, or the theory that individuals tend to pursue what is in their own best interests. Accordingly, businesses seek to benefit themselves and\/or their shareholders by maximizing profit.  In capitalist theoretics, the profit motive is said to ensure that resources are being allocated efficiently. For instance, Austrian economist Henry Hazlitt explains: \"If there is no profit in making an article, it is a sign that the labor and capital devoted to its production are misdirected: the value of the resources that must be used up in making the article is greater than the value of the article itself\".[116]  Socialist theorists note that, unlike mercantilists, capitalists accumulate their profits while expecting their profit rates to remain the same.  This causes problems as earnings in the rest of society do not increase in the same proportion.[117]  The relationship between the state, its formal mechanisms, and capitalist societies has been debated in many fields of social and political theory, with active discussion since the 19th century. Hernando de Soto is a contemporary Peruvian economist who has argued that an important characteristic of capitalism is the functioning state protection of property rights in a formal property system where ownership and transactions are clearly recorded.[118]  According to de Soto, this is the process by which physical assets are transformed into capital, which in turn may be used in many more ways and much more efficiently in the market economy. A number of Marxian economists have argued that the inclosure acts in England and similar legislation elsewhere were an integral part of capitalist primitive accumulation and that specific legal frameworks of private land ownership have been integral to the development of capitalism.[119][120]  Private property rights are not absolute, as in many countries the state has the power to seize private property, typically for public use, under the powers of eminent domain.  In capitalist economics, market competition is the rivalry among sellers trying to achieve such goals as increasing profits, market share and sales volume by varying the elements of the marketing mix: price, product, distribution and promotion. Merriam-Webster defines competition in business as \"the effort of two or more parties acting independently to secure the business of a third party by offering the most favourable terms\".[121] It was described by Adam Smith in The Wealth of Nations (1776) and later economists as allocating productive resources to their most highly valued uses[122] and encouraging efficiency. Smith and other classical economists before Antoine Augustine Cournot were referring to price and non-price rivalry among producers to sell their goods on best terms by bidding of buyers, not necessarily to a large number of sellers nor to a market in final equilibrium.[123] Competition is widespread throughout the market process. It is a condition where \"buyers tend to compete with other buyers, and sellers tend to compete with other sellers\".[124] In offering goods for exchange, buyers competitively bid to purchase specific quantities of specific goods which are available, or might be available if sellers were to choose to offer such goods. Similarly, sellers bid against other sellers in offering goods on the market, competing for the attention and exchange resources of buyers. Competition results from scarcity, as it is not possible to satisfy all conceivable human wants, and occurs as people try to meet the criteria being used to determine allocation.[124]: 105   In the works of Adam Smith, the idea of capitalism is made possible through competition which creates growth. Although capitalism had not entered mainstream economics at the time of Smith, it is vital to the construction of his ideal society. One of the foundational blocks of capitalism is competition. Smith believed that a prosperous society is one where \"everyone should be free to enter and leave the market and change trades as often as he pleases.\"[125] He believed that the freedom to act in one's self-interest is essential for the success of a capitalist society. In response to the idea that if all participants focus on their own goals, society's well-being will be water under the bridge, Smith maintains that despite the concerns of intellectuals, \"global trends will hardly be altered if they refrain from pursuing their personal ends.\"[126] He insisted that the actions of a few participants cannot alter the course of society. Instead, Smith maintained that they should focus on personal progress instead and that this will result in overall growth to the whole.  Competition between participants, \"who are all endeavoring to justle one another out of employment, obliges every man to endeavor to execute his work\" through competition towards growth.[125]  Economic growth is a characteristic tendency of capitalist economies.[127][128] However, capitalist economies may experience fluctuations in growth that cannot be accounted for by demographic or technological changes. These fluctuations, which involve sustained periods of economic growth and recession, are referred to as business cycles in macroeconomics. Economic growth is measured as growth in investment, economic output, and economic consumption per capita. Changes in hours of employment on their own are not considered as a factor of economic growth.[13]  The capitalist mode of production refers to the systems of organising production and distribution within capitalist societies. Private money-making in various forms (renting, banking, merchant trade, production for profit and so on) preceded the development of the capitalist mode of production as such.  The term capitalist mode of production is defined by private ownership of the means of production, extraction of surplus value by the owning class for the purpose of capital accumulation, wage-based labor and, at least as far as commodities are concerned, being market-based.[129]  Capitalism in the form of money-making activity has existed in the shape of merchants and money-lenders who acted as intermediaries between consumers and producers engaging in simple commodity production (hence the reference to \"merchant capitalism\") since the beginnings of civilisation. What is specific about the \"capitalist mode of production\" is that most of the inputs and outputs of production are supplied through the market (i.e. they are commodities) and essentially all production is in this mode.[15] By contrast, in flourishing feudalism most or all of the factors of production, including labor, are owned by the feudal ruling class outright and the products may also be consumed without a market of any kind, it is production for use within the feudal social unit and for limited trade.[102] This has the important consequence that, under capitalism, the whole organisation of the production process is reshaped and re-organised to conform with economic rationality as bounded by capitalism, which is expressed in price relationships between inputs and outputs (wages, non-labor factor costs, sales and profits) rather than the larger rational context faced by society overall—that is, the whole process is organised and re-shaped in order to conform to \"commercial logic\". Essentially, capital accumulation comes to define economic rationality in capitalist production.[103]  A society, region or nation is capitalist if the predominant source of incomes and products being distributed is capitalist activity, but even so this does not yet mean necessarily that the capitalist mode of production is dominant in that society.[130]  Mixed economies rely on the nation they are in to provide some goods or services, while the free market produces and maintains the rest.[110]  Government agencies regulate the standards of service in many industries, such as airlines and broadcasting, as well as financing a wide range of programs. In addition, the government regulates the flow of capital and uses financial tools such as the interest rate to control such factors as inflation and unemployment.[131]  In capitalist economic structures, supply and demand is an economic model of price determination in a market. It postulates that in a perfectly competitive market, the unit price for a particular good will vary until it settles at a point where the quantity demanded by consumers (at the current price) will equal the quantity supplied by producers (at the current price), resulting in an economic equilibrium for price and quantity.  The \"basic laws\" of supply and demand, as described by David Besanko and Ronald Braeutigam, are the following four:[132]: 37   A supply schedule is a table that shows the relationship between the price of a good and the quantity supplied.[133]  A demand schedule, depicted graphically as the demand curve, represents the amount of some goods that buyers are willing and able to purchase at various prices, assuming all determinants of demand other than the price of the good in question, such as income, tastes and preferences, the price of substitute goods and the price of complementary goods, remain the same. According to the law of demand, the demand curve is almost always represented as downward sloping, meaning that as price decreases, consumers will buy more of the good.[134]  Just like the supply curves reflect marginal cost curves, demand curves are determined by marginal utility curves.[135]  In the context of supply and demand, economic equilibrium refers to a state where economic forces such as supply and demand are balanced and in the absence of external influences the (equilibrium) values of economic variables will not change. For example, in the standard text-book model of perfect competition equilibrium occurs at the point at which quantity demanded and quantity supplied are equal.[136] Market equilibrium, in this case, refers to a condition where a market price is established through competition such that the amount of goods or services sought by buyers is equal to the amount of goods or services produced by sellers. This price is often called the competitive price or market clearing price and will tend not to change unless demand or supply changes.  Partial equilibrium, as the name suggests, takes into consideration only a part of the market to attain equilibrium. Jain proposes (attributed to George Stigler): \"A partial equilibrium is one which is based on only a restricted range of data, a standard example is price of a single product, the prices of all other products being held fixed during the analysis\".[137]  According to Hamid S. Hosseini, the \"power of supply and demand\" was discussed to some extent by several early Muslim scholars, such as fourteenth century Mamluk scholar Ibn Taymiyyah, who wrote: \"If desire for goods increases while its availability decreases, its price rises. On the other hand, if availability of the good increases and the desire for it decreases, the price comes down\".[138]  John Locke's 1691 work Some Considerations on the Consequences of the Lowering of Interest and the Raising of the Value of Money[139] includes an early and clear description[non-primary source needed] of supply and demand and their relationship. In this description, demand is rent: \"The price of any commodity rises or falls by the proportion of the number of buyer and sellers\" and \"that which regulates the price... [of goods] is nothing else but their quantity in proportion to their rent\".  David Ricardo titled one chapter of his 1817 work Principles of Political Economy and Taxation \"On the Influence of Demand and Supply on Price\".[140] In Principles of Political Economy and Taxation, Ricardo more rigorously laid down the idea of the assumptions that were used to build his ideas of supply and demand.  In his 1870 essay \"On the Graphical Representation of Supply and Demand\", Fleeming Jenkin in the course of \"introduc[ing] the diagrammatic method into the English economic literature\" published the first drawing of supply and demand curves therein,[141] including comparative statics from a shift of supply or demand and application to the labor market.[142] The model was further developed and popularized by Alfred Marshall in the 1890 textbook Principles of Economics.[140]  There are many variants of capitalism in existence that differ according to country and region.[143] They vary in their institutional makeup and by their economic policies. The common features among all the different forms of capitalism are that they are predominantly based on the private ownership of the means of production and the production of goods and services for profit; the market-based allocation of resources; and the accumulation of capital.  They include advanced capitalism, corporate capitalism, finance capitalism, free-market capitalism, mercantilism, state capitalism and welfare capitalism. Other theoretical variants of capitalism include anarcho-capitalism, community capitalism, humanistic capitalism, neo-capitalism, state monopoly capitalism, and technocapitalism.  Advanced capitalism is the situation that pertains to a society in which the capitalist model has been integrated and developed deeply and extensively for a prolonged period. Various writers identify Antonio Gramsci as an influential early theorist of advanced capitalism, even if he did not use the term himself. In his writings, Gramsci sought to explain how capitalism had adapted to avoid the revolutionary overthrow that had seemed inevitable in the 19th century. At the heart of his explanation was the decline of raw coercion as a tool of class power, replaced by use of civil society institutions to manipulate public ideology in the capitalists' favour.[144][145][146]  Jürgen Habermas has been a major contributor to the analysis of advanced-capitalistic societies. Habermas observed four general features that characterise advanced capitalism:  Corporate capitalism is a free or mixed-market capitalist economy characterized by the dominance of hierarchical and bureaucratic corporations.[148]  Finance capitalism is the subordination of processes of production to the accumulation of money profits in a financial system. In their critique of capitalism, Marxism and Leninism both emphasise the role of finance capital as the determining and ruling-class interest in capitalist society, particularly in the latter stages.[149][150]  Rudolf Hilferding is credited with first bringing the term finance capitalism into prominence through Finance Capital, his 1910 study of the links between German trusts, banks and monopolies—a study subsumed by Vladimir Lenin into Imperialism, the Highest Stage of Capitalism (1917), his analysis of the imperialist relations of the great world powers.[151] Lenin concluded that the banks at that time operated as \"the chief nerve centres of the whole capitalist system of national economy\".[152] For the Comintern (founded in 1919), the phrase \"dictatorship of finance capitalism\"[153] became a regular one.  Fernand Braudel would later point to two earlier periods when finance capitalism had emerged in human history—with the Genoese in the 16th century and with the Dutch in the 17th and 18th centuries—although at those points it developed from commercial capitalism.[154][need quotation to verify] Giovanni Arrighi extended Braudel's analysis to suggest that a predominance of finance capitalism is a recurring, long-term phenomenon, whenever a previous phase of commercial\/industrial capitalist expansion reaches a plateau.[155]  A capitalist free-market economy is an economic system where prices for goods and services are set entirely by the forces of supply and demand and are expected, by its adherents, to reach their point of equilibrium without intervention by government policy. It typically entails support for highly competitive markets and private ownership of the means of production.[156] Laissez-faire capitalism is a more extensive form of this free-market economy, but one in which the role of the state is limited to protecting property rights.[157] In anarcho-capitalist theory, property rights are protected by private firms and market-generated law. According to anarcho-capitalists, this entails property rights without statutory law through market-generated tort, contract and property law, and self-sustaining private industry.[158][159]  Fernand Braudel argued that free market exchange and capitalism are to some degree opposed; free market exchange involves transparent public transactions and a large number of equal competitors, while capitalism involves a small number of participants using their capital to control the market via private transactions, control of information, and limitation of competition.[160]  Mercantilism is a nationalist form of early capitalism that came into existence approximately in the late 16th century. It is characterized by the intertwining of national business interests with state-interest and imperialism. Consequently, the state apparatus is used to advance national business interests abroad. An example of this is colonists living in America who were only allowed to trade with and purchase goods from their respective mother countries (e.g., United Kingdom, France and Portugal). Mercantilism was driven by the belief that the wealth of a nation is increased through a positive balance of trade with other nations—it corresponds to the phase of capitalist development sometimes called the primitive accumulation of capital.[161]  A social market economy is a free-market or mixed-market capitalist system, sometimes classified as a coordinated market economy, where government intervention in price formation is kept to a minimum, but the state provides significant services in areas such as social security, health care, unemployment benefits and the recognition of labor rights through national collective bargaining arrangements.[162]  This model is prominent in Western and Northern European countries as well as Japan, albeit in slightly different configurations. The vast majority of enterprises are privately owned in this economic model. Rhine capitalism is the contemporary model of capitalism and adaptation of the social market model that exists in continental Western Europe today.[163]  State capitalism is a capitalist market economy dominated by state-owned enterprises, where the state enterprises are organized as commercial, profit-seeking businesses. The designation has been used broadly throughout the 20th century to designate a number of different economic forms, ranging from state-ownership in market economies to the command economies of the former Eastern Bloc. According to Aldo Musacchio, a professor at Harvard Business School, state capitalism is a system in which governments, whether democratic or autocratic, exercise a widespread influence on the economy either through direct ownership or various subsidies. Musacchio notes a number of differences between today's state capitalism and its predecessors. In his opinion, gone are the days when governments appointed bureaucrats to run companies: the world's largest state-owned enterprises are now traded on the public markets and kept in good health by large institutional investors. Contemporary state capitalism is associated with the East Asian model of capitalism, dirigisme and the economy of Norway.[164] Alternatively, Merriam-Webster defines state capitalism as \"an economic system in which private capitalism is modified by a varying degree of government ownership and control\".[165]  In Socialism: Utopian and Scientific, Friedrich Engels argued that state-owned enterprises would characterize the final stage of capitalism, consisting of ownership and management of large-scale production and communication by the bourgeois state.[166] In his writings, Vladimir Lenin characterized the economy of Soviet Russia as state capitalist, believing state capitalism to be an early step toward the development of socialism.[167][168]  Some economists and left-wing academics including Richard D. Wolff and Noam Chomsky, as well as many Marxist philosophers and revolutionaries such as Raya Dunayevskaya and C.L.R. James, argue that the economies of the former Soviet Union and Eastern Bloc represented a form of state capitalism because their internal organization within enterprises and the system of wage labor remained intact.[169][170][171][172][173]  The term is not used by Austrian School economists to describe state ownership of the means of production. The economist Ludwig von Mises argued that the designation of state capitalism was a new label for the old labels of state socialism and planned economy and differed only in non-essentials from these earlier designations.[174]  Welfare capitalism is capitalism that includes social welfare policies. Today, welfare capitalism is most often associated with the models of capitalism found in Central and Northern Europe such as the Nordic model, social market economy and Rhine capitalism. In some cases, welfare capitalism exists within a mixed economy, but welfare states can and do exist independently of policies common to mixed economies such as state interventionism and extensive regulation.[175]  A mixed economy is a largely market-based capitalist economy consisting of both private and public ownership of the means of production and economic interventionism through macroeconomic policies intended to correct market failures, reduce unemployment and keep inflation low. The degree of intervention in markets varies among different countries. Some mixed economies such as France under dirigisme also featured a degree of indirect economic planning over a largely capitalist-based economy.  Most modern capitalist economies are defined as mixed economies to some degree, however French economist Thomas Piketty state that capitalist economies might shift to a much more laissez-faire approach in the near future.[176]  Eco-capitalism, also known as \"environmental capitalism\" or (sometimes[177]) \"green capitalism\", is the view that capital exists in nature as \"natural capital\" (ecosystems that have ecological yield) on which all wealth depends. Therefore, governments should use market-based policy-instruments (such as a carbon tax) to resolve environmental problems.[178]  The term \"Blue Greens\" is often applied to those who espouse eco-capitalism. Eco-capitalism can be thought of as the right-wing equivalent to Red Greens.[179][need quotation to verify]  Sustainable capitalism is a conceptual form of capitalism based upon sustainable practices that seek to preserve humanity and the planet, while reducing externalities and bearing a resemblance of capitalist economic policy. A capitalistic economy must expand to survive and find new markets to support this expansion.[180] Capitalist systems are often destructive to the environment as well as certain individuals without access to proper representation. However, sustainability provides quite the opposite; it implies not only a continuation, but a replenishing of resources.[181] Sustainability is often thought of to be related to environmentalism, and sustainable capitalism applies sustainable principles to economic governance and social aspects of capitalism as well.  The importance of sustainable capitalism has been more recently recognized, but the concept is not new. Changes to the current economic model would have heavy social environmental and economic implications and require the efforts of individuals, as well as compliance of local, state and federal governments. Controversy surrounds the concept as it requires an increase in sustainable practices and a marked decrease in current consumptive behaviors.[182]  This is a concept of capitalism described in Al Gore and David Blood's manifesto for the Generation Investment Management to describe a long-term political, economic and social structure which would mitigate current threats to the planet and society.[183] According to their manifesto, sustainable capitalism would integrate the environmental, social and governance (ESG) aspects into risk assessment in attempt to limit externalities.[184] Most of the ideas they list are related to economic changes, and social aspects, but strikingly few are explicitly related to any environmental policy change.[183]  The accumulation of capital is the process of \"making money\" or growing an initial sum of money through investment in production. Capitalism is based on the accumulation of capital, whereby financial capital is invested in order to make a profit and then reinvested into further production in a continuous process of accumulation. In Marxian economic theory, this dynamic is called the law of value. Capital accumulation forms the basis of capitalism, where economic activity is structured around the accumulation of capital, defined as investment in order to realize a financial profit.[185] In this context, \"capital\" is defined as money or a financial asset invested for the purpose of making more money (whether in the form of profit, rent, interest, royalties, capital gain or some other kind of return).[186]  In mainstream economics, accounting and Marxian economics, capital accumulation is often equated with investment of profit income or savings, especially in real capital goods. The concentration and centralisation of capital are two of the results of such accumulation. In modern macroeconomics and econometrics, the phrase \"capital formation\" is often used in preference to \"accumulation\", though the United Nations Conference on Trade and Development (UNCTAD) refers nowadays to \"accumulation\". The term \"accumulation\" is occasionally used in national accounts.  Wage labor refers to the sale of labor under a formal or informal employment contract to an employer.[105] These transactions usually occur in a labor market where wages are market determined.[187] In Marxist economics, these owners of the means of production and suppliers of capital are generally called capitalists. The description of the role of the capitalist has shifted, first referring to a useless intermediary between producers, then to an employer of producers, and finally to the owners of the means of production.[100] Labor includes all physical and mental human resources, including entrepreneurial capacity and management skills, which are required to produce products and services. Production is the act of making goods or services by applying labor power.[188][189]  Criticism of capitalism comes from various political and philosophical approaches, including anarchist, socialist, religious and nationalist viewpoints.[190] Of those who oppose it or want to modify it, some believe that capitalism should be removed through revolution while others believe that it should be changed slowly through political reforms.[191][192]  Prominent critiques of capitalism allege that it is inherently exploitative,[193][194][195] alienating,[196] unstable,[197][198] unsustainable,[199][200][201] and economically inefficient[202][203][204]—and that it creates massive economic inequality,[205][206] commodifies people,[207][208] degrades the environment,[199][209] is undemocratic,[210][211][212] embeds uneven and underdevelopment between nation states,[213][214][215][216]  and leads to an erosion of human rights[217] because of its incentivization of imperialist expansion and war.[218][219][220]  Other critics argue that such inequities are not due to the ethic-neutral construct of the economic system commonly known as capitalism, but to the ethics of those who shape and execute the system. For example, some contend that Milton Friedman's (human) ethic of 'maximizing shareholder value' creates a harmful form of capitalism,[221][222] while a Millard Fuller or John Bogle (human) ethic of 'enough' creates a sustainable form.[223][224] Equitable ethics and unified ethical decision-making is theorized to create a less damaging form of capitalism.[225]  Inheritance has been argued to not be a fundamental part of capitalism,[226] instead part of nepotism.[227] "},"meta":{},"created_at":"2025-03-22T14:25:42.292408Z","updated_at":"2025-03-22T14:25:42.292408Z","inner_id":98,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":107,"annotations":[{"id":107,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.327895Z","updated_at":"2025-03-22T14:25:42.327895Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"f326033a-f632-44b7-9a87-d7b3e898b9d7","import_id":null,"last_action":null,"bulk_created":false,"task":107,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"A soundtrack[2] is a recorded audio signal accompanying and synchronised to the images of a book, drama, motion picture, radio program, television program, or video game; colloquially, a commercially released soundtrack album of music as featured in the soundtrack of a film, video, or television presentation; or the physical area of a film that contains the synchronised recorded sound.[1]  In movie industry terminology usage, a sound track is an audio recording created or used in film production or post-production. Initially, the dialogue, sound effects, and music in a film each has its own separate track, and these are mixed together to make what is called the composite track, which is heard in the film. A dubbing track is often later created when films are dubbed into another language.  This is also known as an M&E (music and effects) track. M&E tracks contain all sound elements minus dialogue, which is then supplied by the foreign distributor in the native language of its territory.  Current dictionary entries for soundtrack document soundtrack as a noun, and as a verb.[3][4] An early attempt at popularizing the term sound track was printed in the magazine Photoplay in 1929.[5] A 1992 technical dictionary entry in the Academic Press Dictionary of Science and Technology does not distinguish between the form sound track and soundtrack.[6]  The contraction soundtrack came into public consciousness with the advent of so-called soundtrack albums in the late 1940s. First conceived by movie companies as a promotional gimmick for new films, these commercially available recordings were labeled and advertised as \"music from the original motion picture soundtrack\", or \"music from and inspired by the motion picture.\" These phrases were soon shortened to just \"original motion picture soundtrack.\" More accurately, such recordings are made from a film's music track, because they usually consist of isolated music from a film, not the composite (sound) track with dialogue and sound effects.  The term original soundtrack (OST), often used for titles of albums of soundtrack music, is sometimes also used to differentiate the original music heard and recorded versus a rerecording or cover version.  Types of soundtrack recordings include:  The soundtrack to the 1937 Walt Disney animated film Snow White and the Seven Dwarfs was the first commercially issued film soundtrack.[8] It was released by RCA Victor Records on multiple 78 RPM discs in January 1938 as Songs from Walt Disney's Snow White and the Seven Dwarfs (with the Same Characters and Sound Effects as in the Film of That Title) and has since seen numerous expansions and reissues.  The first live-action musical film to have a commercially issued soundtrack album was MGM's 1946 film biography of Show Boat composer Jerome Kern, Till the Clouds Roll By. The album was originally issued as a set of four 10-inch 78-rpm records. Only eight selections from the film were included in this first edition of the album. In order to fit the songs onto the record sides the musical material needed editing and manipulation. This was before tape existed, so the record producer needed to copy segments from the playback discs used on set, then copy and re-copy them from one disc to another adding transitions and cross-fades until the final master was created. Needless to say, it was several generations removed from the original and the sound quality suffered for it. The playback recordings were purposely recorded very dry (without reverberation); otherwise it would come across as too hollow sounding in large movie theatres. This made these albums sound flat and boxy.  MGM Records called these \"original cast albums\" in the style of Decca Broadway show cast albums mostly because the material on the discs would not lock to picture, thereby creating the largest distinction between 'Original Motion Picture Soundtrack' which, in its strictest sense would contain music that would lock to picture if the home user would play one alongside the other and 'Original Cast Soundtrack' which in its strictest sense would refer to studio recordings of film music by the original film cast, but which had been edited or rearranged for time and content and would not lock to picture.[9]  In reality, however, soundtrack producers remain ambiguous about this distinction, and titles in which the music on the album does lock to picture may be labeled as OCS and music from an album that does not lock to picture may be referred to as OMPS.  The phrase \"recorded directly from the soundtrack\" was used for a while in the 1970s, 1980s and 1990s to differentiate material that would lock to picture from that which would not (excluding alternate masters and alternate vocals or solos). However, partly because many 'film takes' actually consisted of several different attempts at the song edited together, over time that term became nebulous as well. For example, in cases where the master take used in the film could not be found in its isolated form (without the M&E), the aforementioned alternate masters and alternate vocal and solo performances might be used instead.  As a result of all this ambiguity, over the years the term soundtrack began to be commonly applied to any recording from a film, whether taken from the actual film soundtrack or re-recorded in the studio at an earlier or later time. The term is also sometimes used for Broadway cast recordings.  Contributing to the vagueness of the term are projects such as The Sound of Music Live!, which was filmed live on the set for an NBC holiday season special first broadcast in 2013. The album, released three days before the broadcast, contained studio pre-recordings by the original cast of all the songs used in the special, but because only the orchestral portion of the material from the album is the same as that used in the special (i.e., the vocals were sung live over a prerecorded track), this creates a similar technicality. Although the instrumental music bed from the CD will lock to picture, the vocal performances will not (although it is possible to create a complete soundtrack recording by lifting the vocal performances from the DVD, erasing the alternate vocal masters from the CD and combining the two).  Among MGM's most notable soundtrack albums were those of the films Good News, Easter Parade, Annie Get Your Gun, Singin' in the Rain,[10] Show Boat, The Band Wagon, Seven Brides for Seven Brothers, and Gigi.  Film score albums did not really become popular until the LP era, although a few were issued in 78-rpm albums.  Alex North's score for the 1951 film version of A Streetcar Named Desire was released on a 10-inch LP by Capitol Records and sold so well that the label rereleased it on one side of a 12-inch LP with some of Max Steiner's film music on the reverse.  Steiner's score for Gone with the Wind has been recorded many times, but when the film was reissued in 1967, MGM Records finally released an album of the famous score recorded directly from the soundtrack. Like the 1967 rerelease of the film, this version of the score was artificially enhanced for stereo. In recent years, Rhino Records has released a 2-CD set of the complete Gone With the Wind score, restored to its original mono sound.  One of the biggest-selling film scores of all time was John Williams' music from the movie Star Wars. Many film score albums go out of print after the films finish their theatrical runs and some have become extremely rare collectors' items.  In a few rare instances an entire film dialogue track was issued on records. The 1968 Franco Zeffirelli film of Romeo and Juliet was issued as a 4-LP set, as a single LP with musical and dialogue excerpts, and as an album containing only the film's musical score. The ground-breaking film Who's Afraid of Virginia Woolf? was issued by Warner Bros Records as a 2-LP set containing virtually all the dialogue from the film. RCA Victor also issued a double-album set what was virtually all the dialogue from the film soundtrack of A Man for All Seasons, Decca Records issued a double-album for Man of La Mancha and Disney Music Group (formerly Buena Vista Records) issued a similar double-album for its soundtrack for The Hobbit.  When a blockbuster film is released, or during and after a television series airs, an album in the form of a soundtrack is typically released alongside that.  A soundtrack typically contains instrumentation or alternatively a film score. But it can also feature songs that were sung or performed by characters in a scene (or a cover version of a song in the media, re-recorded by a popular artist), songs that were used as intentional or unintentional background music in important scenes, songs that were heard in the closing credits, or songs for no apparent reason related to the media other than for promotion, that were included in a soundtrack.  Soundtracks are usually released on major record labels (just as if they were released by a musical artist), and the songs and the soundtrack itself can also be on music charts, and win musical awards.  By convention, a soundtrack record can contain all kinds of music including music inspired by but not actually appearing in the movie; the score contains only music by the original film's composers.[11]  Contemporaneously, a soundtrack can go against normality, (most typically used in popular culture franchises) and contains recently released and\/or exclusive never before released original pop music selections, (some of which become high charting records on their own, which due to being released on another franchises title, peaked because of that) and is simply used for promotional purposes for well known artists, or new or unknown artists. These soundtracks contain music not at all heard in the film\/television series, and any artistic or lyrical connection is purely coincidental.  However depending on the genre of the media the soundtrack of popular songs would have a set pattern; a lighthearted romance might feature easy listening love songs, whilst a more dark thriller would compose of hard rock or urban music.  In 1908, Camille Saint-Saëns composed the first music specifically for use in a motion picture (L'assasinat du duc de Guise), and releasing recordings of songs used in films became prevalent in the 1930s. Henry Mancini, who won an Emmy Award and two Grammys for his soundtrack to Peter Gunn, was the first composer to have a widespread hit with a song from a soundtrack.  Before the 1970s, soundtracks (with a few exceptions), accompanied towards musicals, and was an album that featured vocal and instrumental, (and instrumental versions of vocal songs) musical selections performed by cast members. Or cover versions of songs sung by another artist.  After the 1970s, soundtracks started to include more diversity, and music consumers would anticipate a motion picture or television soundtrack. Majority of top charting songs were those featured or released on a film or television soundtrack album.  In recent years the term soundtrack sort of subsided. It now mostly commonly refers to instrumental background music used in that media. Popular songs featured in a film or television series are instead highlighted and referenced in the credits, not as part of a soundtrack.  In the late 1980s, cognitive psychology and psychology of music started an investigation on the impact that the soundtrack exerts on the interpretation of audiovisual stimuli. Canadian psychologist Annabel J. Cohen is one of the first scholars who systematically studied the relationship between music and moving image within the interpretation process of brief animated videos. Her studies converged in the Congruence-Association Model of music and multimedia.[12][13] More recent empirical studies proved that the film music goes far beyond the role of an emotionalizing accessory in film contexts;[14] contrarily, it can radically alter the empathy experienced by the viewers toward the characters on screen, attributed emotions (e.g., whether a character is happy or sad),[15] evaluation of the scenic environments, plot anticipations,[16] and moral judgement of the characters.[17] Furthermore, eyetracking and pupillometry studies found that film music is able to influence gaze direction and pupil dilation depending on its emotional valence and semantic information conveyed.[18][16][19] Recently, new experiments showed that film music can alter time perception while watching movies; in particular, soundtracks deemed as activating and arousing lead to time overestimation as opposed to more relaxing or sad music.[20] Lastly, soundtracks have been proved to shape the memory of the scene that the viewers form, to the point of biasing their recall coherently with the music's semantic contents.[21][22][23]  Soundtrack may also refer to music used in video games. While sound effects were nearly universally used for action happening in the game, music to accompany the gameplay was a later development. Rob Hubbard and Martin Galway were early composers of music specifically for video games for the 1980s Commodore 64 computer. Koji Kondo was an early and important composer for Nintendo games. As the technology improved, polyphonic and often orchestral soundtracks replaced simple monophonic melodies starting in the late 1980s and the soundtracks to popular games such as the Dragon Quest and Final Fantasy series began to be released separately. In addition to compositions written specifically for video games, the advent of CD technology allowed developers to incorporate licensed songs into their soundtrack (the Grand Theft Auto series is a good example of this). Furthermore, when Microsoft released the Xbox in 2001, it featured an option allowing users to customize the soundtrack for certain games by ripping a CD to the hard-drive.  As in Sound of Music Live! the music or dialogue in question was prepared specifically for use in or at an event such as that described above.  In the case of theme parks, actors may be ensconced in large costumes where their faces may be obscured. They mime along to a prerecorded music, effects and narration track that may sound as if it was lifted from a movie, or may sound as if it had been overly dramatized for effect.  In the case of cruise ships, the small stage spaces do not allow for full orchestration, so that possibly the larger instruments may be pre-recorded onto a backing track and the remaining instruments may play live, or the reverse may occur in such instances as Elvis: The Concert or Sinatra: His Voice. His World. His Way both of which use isolated vocal and video performances accompanied by a live band.  In the case of event soundtracks, large public gatherings such as Hands Across America, The Live Aid Concert, the 200th Anniversary Celebration of the U.S. Constitution in Philadelphia, The MUSE Concerts or the various Greenpeace events (i.e. The First International Greenpeace Record Project, Rainbow Warriors and Alternative NRG) all had special music, effects and dialogue written especially for the event which later went on sale to the record and later video-buying public.  Only a few cases exist of an entire soundtrack being written specifically for a book.  'Kaladin', a book soundtrack to popular fantasy novelist Brandon Sanderson's book, 'The Way of Kings', was written by The Black Piper. The Black Piper, hailing from Provo, Utah,[24] is a combined group of composers who share a love for fantasy literature. 'Kaladin' was funded through Kickstarter and raised over $112,000. It was released December 2017.[25]  A New York Times Bestselling series, Green Rider by Kristen Britain, celebrated its 25th anniversary with the release of a book soundtrack by the same name. It was recorded in Utah, featuring artist Jenny Oaks Baker and William Arnold and was released in 2018.  A soundtrack for J. R. R. Tolkien's The Hobbit and The Lord of the Rings was composed by Craig Russell for the San Luis Obispo Youth Symphony. Commissioned in 1995, it was finally put on disk in 2000 by the San Luis Obispo Symphony. [citation needed]  For the 1996 Star Wars novel Shadows of the Empire (written by author Steve Perry), Lucasfilm chose Joel McNeely to write a score. This was an eccentric, experimental project, in contrast to all other soundtracks, as the composer was allowed to convey general moods and themes, rather than having to write music to flow for specific scenes. A project called \"Sine Fiction\"[26] has made some soundtracks to novels by science fiction writers like Isaac Asimov and Arthur C. Clarke, and has thus far released 19 soundtracks to science-fiction novels or short stories. All of them are available for free download.  Author L. Ron Hubbard composed and recorded a soundtrack album to his novel Battlefield Earth entitled Space Jazz. He marketed the concept album as \"the only original sound track ever produced for a book before it becomes a movie\". There are two other soundtracks to Hubbard novels, being Mission Earth by Edgar Winter and To the Stars by Chick Corea.  The 1985 novel Always Coming Home by Ursula K. Le Guin originally came in a box set with an audiocassette entitled Music and Poetry of the Kesh, featuring three performances of poetry, and ten musical compositions by Todd Barton.  In comics, Daniel Clowes' graphic novel Like a Velvet Glove Cast in Iron had an official soundtrack album.  The original black-and-white Nexus #3 from Capitol comics included the Flexi-Nexi which was a soundtrack flexi-disc for the issue. Trosper by Jim Woodring included a soundtrack album composed and performed by Bill Frisell,[27] and the Absolute Edition of The League of Extraordinary Gentlemen: Black Dossier is planned to include an original vinyl record. The Crow released a soundtrack album called Fear and Bullets to coincide with the limited edition hardcover copy of the graphic novel. The comic book Hellblazer released an annual with a song called Venus of the Hardsell, which was then recorded and a music video to accompany with.  The Brazilian graphic novel Achados e Perdidos (Lost and Found), by Eduardo Damasceno and Luís Felipe Garrocho, had an original soundtrack composed by musician Bruno Ito. The book was self-published in 2011 after a crowdfunding campaign and was accompanied by a CD with eight songs (one for each chapter of the story). In 2012, this graphic novel won the Troféu HQ Mix (Brazilian most important comic book award) in the category \"Special Homage\".[28][29]  As Internet access became more widespread, a similar practice developed of accompanying a printed work with a downloadable theme song, rather than a complete and physically published album. The theme songs for Nextwave,[30] Runaways,[31] Achewood, and Dinosaur Comics are examples of this. The novella Chasing Homer (2019) by László Krasznahorkai was published with an original soundtrack by Miklos Szilveszter, accessible through a QR code at the start of each chapter.[32]  In Japan, such examples of music inspired by a work and not intended to soundtrack a radio play or motion picture adaptation of it are known as an image album or image song, though this definition also includes such things as film score demos inspired by concept art and songs inspired by a TV series that are not featured in them. Many audiobooks have some form of musical accompaniment, but these are generally not extensive enough to be released as a separate soundtrack. "},"meta":{},"created_at":"2025-03-22T14:25:42.292408Z","updated_at":"2025-03-22T14:25:42.292408Z","inner_id":99,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":108,"annotations":[{"id":108,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.327895Z","updated_at":"2025-03-22T14:25:42.327895Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"b5b130a9-574b-4414-bfab-c0b586a12bac","import_id":null,"last_action":null,"bulk_created":false,"task":108,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Mergers and acquisitions (M&A) are business transactions in which the ownership of a company, business organization, or one of their operating units is transferred to or consolidated with another company or business organization. They may happen through direct absorption, a merger, a tender offer or a hostile takeover.[1] As an aspect of strategic management, M&A can allow enterprises to grow or downsize, and change the nature of their business or competitive position.  Technically, a merger is the legal consolidation of two business entities into one, whereas an acquisition occurs when one entity takes ownership of another entity's share capital, equity interests or assets. From a legal and financial point of view, both mergers and acquisitions generally result in the consolidation of assets and liabilities under one entity, and the distinction between the two is not always clear.  Most countries require mergers and acquisitions to comply with antitrust or competition law. In the United States, for example, the Clayton Act outlaws any merger or acquisition that may \"substantially lessen competition\" or \"tend to create a monopoly\", and the Hart–Scott–Rodino Act requires notifying the U.S. Department of Justice's Antitrust Division and the Federal Trade Commission about any merger or acquisition over a certain size.  An acquisition\/takeover is the purchase of one business or company by another company or other business entity. Specific acquisition targets can be identified through myriad avenues, including market research, trade expos, sent up from internal business units, or supply chain analysis.[2] Such purchase may be of 100%, or nearly 100%, of the assets or ownership equity of the acquired entity.  A consolidation\/amalgamation occurs when two companies combine to form a new enterprise altogether, and neither of the previous companies remains independently owned. Acquisitions are divided into \"private\" and \"public\" acquisitions, depending on whether the acquiree or merging company (also termed a target) is or is not publicly listed. Some public companies rely on acquisitions as an important value creation strategy.[3] An additional dimension or categorization consists of whether an acquisition is friendly or hostile.[4]  Achieving acquisition success has proven to be very difficult, while various studies have shown that 50% of acquisitions were unsuccessful.[5] \"Serial acquirers\"[6] appear to be more successful with M&A than companies who make acquisitions only occasionally (see Douma & Schreuder, 2013, chapter 13). The new forms of buy out created since the crisis[clarification needed] are based on serial type acquisitions known as an ECO Buyout which is a co-community ownership buy out and the new generation buy outs of the MIBO (Management Involved or Management & Institution Buy Out) and MEIBO (Management & Employee Involved Buy Out).  Whether a purchase is perceived as being \"friendly\" or \"hostile\" depends significantly on how the proposed acquisition is communicated to and perceived by the target company's board of directors, employees, and shareholders. It is normal for M&A deal communications to take place in a so-called \"confidentiality bubble\", wherein the flow of information is restricted pursuant to confidentiality agreements.[7] In the case of a friendly transaction, the companies cooperate in negotiations; in the case of a hostile deal, the board and\/or management of the target is unwilling to be bought or the target's board has no prior knowledge of the offer. Hostile acquisitions can, and often do, ultimately become \"friendly\" as the acquirer secures endorsement of the transaction from the board of the acquiree company. This usually requires an improvement in the terms of the offer and\/or through negotiation.  \"Acquisition\" usually refers to a purchase of a smaller firm by a larger one. Sometimes, however, a smaller firm will acquire management control of a larger and\/or longer-established company and retain the name of the latter for the post-acquisition combined entity. This is known as a reverse takeover. Another type of acquisition is the reverse merger, a form of transaction that enables a private company to be publicly listed in a relatively short time frame. A reverse merger is a type of merger where a privately held company, typically one with promising prospects and a need for financing, acquires a publicly listed shell company that has few assets and no significant business operations.  The combined evidence suggests that the shareholders of acquired firms realize significant positive \"abnormal returns,\" while shareholders of the acquiring company are most likely to experience a negative wealth effect.[8] Most studies indicate that M&A transactions have a positive net effect, with investors in both the buyer and target companies seeing positive returns. This suggests that M&A creates economic value, likely by transferring assets to more efficient management teams who can better utilize them. (See Douma & Schreuder, 2013, chapter 13).  There are also a variety of structures used in securing control over the assets of a company, which have different tax and regulatory implications:  The terms \"demerger\", \"spin-off\" and \"spin-out\" are sometimes used to indicate a situation where one company splits into two, generating a second company which may or may not become separately listed on a stock exchange.  As per knowledge-based views, firms can generate greater values through the retention of knowledge-based resources which they generate and integrate.[9] Extracting technological benefits during and after acquisition is an ever-challenging issue because of organizational differences. Based on the content analysis of seven interviews, the authors concluded the following components for their grounded model of acquisition:  An increase in acquisitions in the global business environment requires enterprises to evaluate the key stake holders of acquisitions very carefully before implementation. It is imperative for the acquirer to understand this relationship and apply it to its advantage. Employee retention is possible only when resources are exchanged and managed without affecting their independence.[10]  A corporate acquisition can be structured legally as either an \"asset purchase\" in which the seller sells business assets and liabilities to the buyer, an \"equity purchase\" in which the buyer purchases equity interests in a target company from one or more selling shareholders or a \"merger\" in which one legal entity is combined into another entity by operation of the corporate law statute(s) of the jurisdiction of the merging entities.[11] In a transaction structured as a merger or an equity purchase, the buyer acquires all of the assets and liabilities of the acquired entity. In a transaction structured as an asset purchase, the buyer and seller agree on which assets and liabilities the buyer will acquire from the seller.  Asset purchases are common in technology transactions in which the buyer is most interested in particular intellectual property but does not want to acquire liabilities or other contractual relationships.[12] An asset purchase structure may also be used when the buyer wishes to buy a particular division or unit of a company that is not a separate legal entity. Divestitures present a variety of unique challenges, such as identifying the assets and liabilities that pertain solely to the unit being sold, determining whether the unit relies on services from other parts of the seller's organization, transferring employees, moving permits and licenses, and safeguarding against potential competition from the seller in the same business sector after the transaction is completed.[13]  From an economic point of view, business combinations can also be classified as horizontal, vertical and conglomerate mergers (or acquisitions). A horizontal merger is between two competitors in the same industry. A vertical merger occurs when two firms combine across the value chain, such as when a firm buys a former supplier (backward integration) or a former customer (forward integration). When there is no strategic relatedness between an acquiring firm and its target, this is called a conglomerate merger (Douma & Schreuder, 2013).[14]  The form of merger most often employed is a triangular merger, where the target company merges with a shell company wholly owned by the buyer, thus becoming a subsidiary of the buyer.  In a \"forward triangular merger\", the target company merges into the subsidiary, with the subsidiary as the surviving company of the merger; a \"reverse triangular merger\" is similar except that the subsidiary merges into the target company, with the target company surviving the merger.[11]  Mergers, asset purchases and equity purchases are each taxed differently, and the most beneficial structure for tax purposes is highly situation-dependent. Under the U.S. Internal Revenue Code, a forward triangular merger is taxed as if the target company sold its assets to the shell company and then liquidated, them whereas a reverse triangular merger is taxed as if the target company's shareholders sold their stock in the target company to the buyer.[15]  The documentation of an M&A transaction often begins with a letter of intent. The letter of intent generally does not bind the parties to commit to a transaction, but may bind the parties to confidentiality and exclusivity obligations so that the transaction can be considered through a due diligence process involving lawyers, accountants, tax advisors, and other professionals, as well as business people from both sides.[13]  After due diligence is complete, the parties may proceed to draw up a definitive agreement, known as a \"merger agreement\", \"share purchase agreement,\" or \"asset purchase agreement\" depending on the structure of the transaction. Such contracts are typically 80 to 100 pages long and focus on five key types of terms:[16]  Following the closing of a deal, adjustments may be made to some of the provisions outlined in the purchase agreement, such as the purchase price. These adjustments are subject to enforceability issues in certain situations. Alternatively, certain transactions use the 'locked box' approach, where the purchase price is fixed at signing and based on the seller's equity value at a pre-signing date and an interest charge.  The assets of a business are pledged to two categories of stakeholders: equity owners and owners of the business' outstanding debt. The core value of a business, which accrues to both categories of stakeholders, is called the Enterprise Value (EV), whereas the value which accrues just to shareholders is the Equity Value (also called market capitalization for publicly listed companies). Enterprise Value reflects a capital structure neutral valuation and is frequently a preferred way to compare value as it is not affected by a company's, or management's, strategic decision to fund the business either through debt, equity, or a portion of both.[17] Five common ways to \"triangulate\" the enterprise value of a business are:  Professionals who value businesses generally do not use just one method, but a combination. Valuations implied using these methodologies can prove different to a company's current trading valuation. For public companies, the market based enterprise value and equity value can be calculated by referring to the company's share price and components on its balance sheet. The valuation methods described above represent ways to determine value of a company independently from how the market currently, or historically, has determined value based on the price of its outstanding securities.  Most often value is expressed in a Letter of Opinion of Value  (LOV) when the business is being valued informally. Formal valuation reports generally get more detailed and expensive as the size of a company increases, but this is not always the case as the nature of the business and the industry it is operating in can influence the complexity of the valuation task.  Objectively evaluating the historical and prospective performance of a business is a challenge faced by many. Generally, parties rely on independent third parties to conduct due diligence studies or business assessments. To yield the most value from a business assessment, objectives should be clearly defined and the right resources should be chosen to conduct the assessment in the available timeframe.  As synergy plays a large role in the valuation of acquisitions, it is paramount to get the value of synergies right, as briefly alluded to regarding DCF valuations. Synergies are different from the \"sales price\" valuation of the firm, as they will accrue to the buyer. Hence, the analysis should be done from the acquiring firm's point of view. Synergy-creating investments are initiated by the choice of the acquirer, and therefore they are not obligatory, making them essentially real options. To include this real options aspect into the analysis of acquisition targets is an issue that has been studied lately.[19] See also contingent value rights.  Valuation can be influenced by a range of factors beyond financial performance, including cross-border and cross-industry considerations, sustainability initiatives, and regulatory environments. Cross-border transactions may involve exchange rate risks, differing accounting standards, and geopolitical factors, which can lead to valuation adjustments.[20] Cross-industry acquisitions can introduce complexities in synergies and market positioning, as firms may face integration challenges and differences in business models.[21] Additionally, sustainability-related factors, such as a company's environmental, social, and governance performance, can impact valuation by influencing investor sentiment, access to capital, and long-term risk exposure.[22]  Mergers are generally differentiated from acquisitions partly by the way in which they are financed and partly by the relative size of the companies. Various methods of financing an M&A deal exist:  Payment by cash. Such transactions are usually termed acquisitions rather than mergers because the shareholders of the target company are removed from the picture and the target comes under the (indirect) control of the bidder's shareholders.  Payment in the form of the acquiring company's stock, issued to the shareholders of the acquired company at a given ratio proportional to the valuation of the latter. They receive stock in the company that is purchasing the smaller subsidiary.  There are some elements to think about when choosing the form of payment. When submitting an offer, the acquiring firm should consider other potential bidders and think strategically. The form of payment might be decisive for the seller. With pure cash deals, there is no doubt on the real value of the bid (without considering an eventual earnout). The contingency of the share payment is indeed removed. Thus, a cash offer preempts competitors better than securities. Taxes are a second element to consider and should be evaluated with the counsel of competent tax and accounting advisers. Third, with a share deal the buyer's capital structure might be affected and the control of the buyer modified. If the issuance of shares is necessary, shareholders of the acquiring company might prevent such capital increase at the general meeting of shareholders. The risk is removed with a cash transaction. Then, the balance sheet of the buyer will be modified and the decision maker should take into account the effects on the reported financial results. For example, in a pure cash deal (financed from the company's current account), liquidity ratios might decrease. On the other hand, in a pure stock for stock transaction (financed from the issuance of new shares), the company might show lower profitability ratios (e.g. ROA). However, economic dilution must prevail towards accounting dilution when making the choice. The form of payment and financing options are tightly linked. If the buyer pays cash, there are three main financing options:  M&A advice is provided by full-service investment banks- who often advise and handle the biggest deals in the world (called bulge bracket) - and specialist M&A firms, who provide M&A only advisory, generally to mid-market, select industries and SBEs.  Highly focused and specialized M&A advice investment banks are called boutique investment banks.  The dominant rationale used to explain M&A activity is that acquiring firms seek improved financial performance or reduce risk. The following motives are considered to improve financial performance or reduce risk:  Megadeals—deals of at least one $1 billion in size—tend to fall into four discrete categories: consolidation, capabilities extension, technology-driven market transformation, and going private.  On average and across the most commonly studied variables, acquiring firms' financial performance does not positively change as a function of their acquisition activity.[29] Therefore, additional motives for merger and acquisition that may not add shareholder value include:  The M&A process itself is a multifaceted which depends upon the type of merging companies.  The M&A process results in the restructuring of a business's purpose, corporate governance and brand identity.  An arm's length merger is a merger:   ″The two elements are complementary and not substitutes. The first element is important because the directors have the capability to act as effective and active bargaining agents, which disaggregated stockholders do not. But, because bargaining agents are not always effective or faithful, the second element is critical, because it gives the minority stockholders the opportunity to reject their agents' work. Therefore, when a merger with a controlling stockholder was: 1) negotiated and approved by a special committee of independent directors; and 2) conditioned on an affirmative vote of a majority of the minority stockholders, the business judgment standard of review should presumptively apply, and any plaintiff ought to have to plead particularized facts that, if true, support an inference that, despite the facially fair process, the merger was tainted because of fiduciary wrongdoing.″[38]  A Strategic merger usually refers to long-term strategic holding of target (Acquired) firm. This type of M&A process aims at creating synergies in the long run by increased market share, broad customer base, and corporate strength of business. A strategic acquirer may also be willing to pay a premium offer to target firm in the outlook of the synergy value created after M&A process.  The term \"acqui-hire\" is used to refer to acquisitions where the acquiring company seeks to obtain the target company's talent, rather than their products (which are often discontinued as part of the acquisition so the team can focus on projects for their new employer). In recent years, these types of acquisitions have become common in the technology industry, where major web companies such as Facebook, Twitter, and Yahoo! have frequently used talent acquisitions to add expertise in particular areas to their workforces.[39][40]  Merger of equals is often a combination of companies of a similar size. Since 1990, there have been more than 625 M&A transactions announced as mergers of equals with a total value of US$2,164.4 bil.[41] Some of the largest mergers of equals took place during the dot-com bubble of the late 1990s and in the year 2000: AOL and Time Warner (US$164 bil.), SmithKline Beecham and Glaxo Wellcome (US$75 bil.), Citicorp and Travelers Group (US$72 bil.). More recent examples this type of combinations are DuPont and Dow Chemical (US$62 bil.) and Praxair and Linde (US$35 bil.).  An analysis of 1,600 companies across industries revealed the rewards for M&A activity were greater for consumer products companies than the average company. For the period 2000–2010, consumer products companies turned in an average annual TSR of 7.4%, while the average for all companies was 4.8%.  Given that the cost of replacing an executive can run over 100% of his or her annual salary, any investment of time and energy in re-recruitment will likely pay for itself many times over if it helps a business retain just a handful of key players that would have otherwise left.  Organizations should move rapidly to re-recruit key managers. It's much easier to succeed with a team of quality players that one selects deliberately rather than try to win a game with those who randomly show up to play.  Mergers and acquisitions often create brand problems, beginning with what to call the company after the transaction and going down into detail about what to do about overlapping and competing product brands. Decisions about what brand equity to write off are not inconsequential. And, given the ability for the right brand choices to drive preference and earn a price premium, the future success of a merger or acquisition depends on making wise brand choices. Brand decision-makers essentially can choose from four different approaches to dealing with naming issues, each with specific pros and cons:[42]  The factors influencing brand decisions in a merger or acquisition transaction can range from political to tactical. Ego can drive choice just as well as rational factors such as brand value and costs involved with changing brands.[43]  Beyond the bigger issue of what to call the company after the transaction comes the ongoing detailed choices about what divisional, product and service brands to keep. The detailed decisions about the brand portfolio are covered under the topic brand architecture.  Most histories of M&A begin in the late 19th century United States. However, mergers coincide historically with the existence of companies. In 1708, for example, the East India Company merged with an erstwhile competitor to restore its monopoly over the Indian trade. In 1784, the Italian Monte dei Paschi and Monte Pio banks were united as the Monti Reuniti.[44] In 1821, the Hudson's Bay Company merged with the rival North West Company.  The Great Merger Movement was a predominantly U.S. business phenomenon that happened from 1895 to 1905. During this time, small firms with little market share consolidated with similar firms to form large, powerful institutions that dominated their markets, such as the Standard Oil Company, which at its height controlled nearly 90% of the global oil refinery industry. It is estimated that more than 1,800 of these firms disappeared into consolidations, many of which acquired substantial shares of the markets in which they operated. The vehicle used were so-called trusts. In 1900 the value of firms acquired in mergers was 20% of GDP. In 1990 the value was only 3% and from 1998 to 2000 it was around 10–11% of GDP. Companies such as DuPont, U.S. Steel, and General Electric that merged during the Great Merger Movement were able to keep their dominance in their respective sectors through 1929, and in some cases today, due to growing technological advances of their products, patents, and brand recognition by their customers. There were also other companies that held the greatest market share in 1905 but at the same time did not have the competitive advantages of the companies like DuPont and General Electric. These companies such as International Paper and American Chicle saw their market share decrease significantly by 1929 as smaller competitors joined forces with each other and provided much more competition. The companies that merged were mass producers of homogeneous goods that could exploit the efficiencies of large volume production. In addition, many of these mergers were capital-intensive. Due to high fixed costs, when demand fell, these newly merged companies had an incentive to maintain output and reduce prices. However more often than not mergers were \"quick mergers\". These \"quick mergers\" involved mergers of companies with unrelated technology and different management. As a result, the efficiency gains associated with mergers were not present. The new and bigger company would actually face higher costs than competitors because of these technological and managerial differences. Thus, the mergers were not done to see large efficiency gains, they were in fact done because that was the trend at the time. Companies which had specific fine products, like fine writing paper, earned their profits on high margin rather than volume and took no part in the Great Merger Movement.[citation needed]  One of the major short run factors that sparked the Great Merger Movement was the desire to keep prices high. However, high prices attracted the entry of new firms into the industry.  A major catalyst behind the Great Merger Movement was the Panic of 1893, which led to a major decline in demand for many homogeneous goods. For producers of homogeneous goods, when demand falls, these producers have more of an incentive to maintain output and cut prices, in order to spread out the high fixed costs these producers faced (i.e. lowering cost per unit) and the desire to exploit efficiencies of maximum volume production. However, during the Panic of 1893, the fall in demand led to a steep fall in prices.  Another economic model proposed by Naomi R. Lamoreaux for explaining the steep price falls is to view the involved firms acting as monopolies in their respective markets. As quasi-monopolists, firms set quantity where marginal cost equals marginal revenue and price where this quantity intersects demand. When the Panic of 1893 hit, demand fell and along with demand, the firm's marginal revenue fell as well. Given high fixed costs, the new price was below average total cost, resulting in a loss. However, also being in a high fixed costs industry, these costs can be spread out through greater production (i.e. higher quantity produced). To return to the quasi-monopoly model, in order for a firm to earn profit, firms would steal part of another firm's market share by dropping their price slightly and producing to the point where higher quantity and lower price exceeded their average total cost. As other firms joined this practice, prices began falling everywhere and a price war ensued.[45]  One strategy to keep prices high and to maintain profitability was for producers of the same good to collude with each other and form associations, also known as cartels. These cartels were thus able to raise prices right away, sometimes more than doubling prices. However, these prices set by cartels provided only a short-term solution because cartel members would cheat on each other by setting a lower price than the price set by the cartel. Also, the high price set by the cartel would encourage new firms to enter the industry and offer competitive pricing, causing prices to fall once again. As a result, these cartels did not succeed in maintaining high prices for a period of more than a few years. The most viable solution to this problem was for firms to merge, through horizontal integration, with other top firms in the market in order to control a large market share and thus successfully set a higher price.[46]  In the long run, due to desire to keep costs low, it was advantageous for firms to merge and reduce their transportation costs thus producing and transporting from one location rather than various sites of different companies as in the past. Low transport costs, coupled with economies of scale also increased firm size by two- to fourfold during the second half of the nineteenth century. In addition, technological changes prior to the merger movement within companies increased the efficient size of plants with capital intensive assembly lines allowing for economies of scale. Thus improved technology and transportation were forerunners to the Great Merger Movement. In part due to competitors as mentioned above, and in part due to the government, however, many of these initially successful mergers were eventually dismantled. The U.S. government passed the Sherman Act in 1890, setting rules against price fixing and monopolies. Starting in the 1890s with such cases as Addyston Pipe and Steel Company v. United States, the courts attacked large companies for strategizing with others or within their own companies to maximize profits. Price fixing with competitors created a greater incentive for companies to unite and merge under one name so that they were not competitors anymore and technically not price fixing.  The economic history has been divided into Merger Waves based on the merger activities in the business world as:  During the third merger wave (1965–1989), corporate marriages involved more diverse companies. Acquirers more frequently bought into different industries. Sometimes this was done to smooth out cyclical bumps, to diversify, the hope being that it would hedge an investment portfolio.  Starting in the fifth merger wave (1992–1998) and continuing today, companies are more likely to acquire in the same business, or close to it, firms that complement and strengthen an acquirer's capacity to serve customers.  In recent decades however, cross-sector convergence[48] has become more common. For example, retail companies are buying tech or e-commerce firms to acquire new markets and revenue streams. It has been reported that convergence will remain a key trend in M&A activity through 2015 and onward.  Buyers are not necessarily hungry for the target companies' hard assets. Some are more interested in acquiring thoughts, methodologies, people and relationships. Paul Graham recognized this in his 2005 essay \"Hiring is Obsolete\", in which he theorizes that the free market is better at identifying talent, and that traditional hiring practices do not follow the principles of free market because they depend a lot upon credentials and university degrees. Graham was probably the first to identify the trend in which large companies such as Google, Yahoo! or Microsoft were choosing to acquire startups instead of hiring new recruits,[49] a process known as acqui-hiring.  Many companies are being bought for their patents, licenses, market share, name brand, research staff, methods, customer base, or culture.[28] Soft capital, like this, is very perishable, fragile, and fluid. Integrating it usually takes more finesse and expertise than integrating machinery, real estate, inventory and other tangibles.  The top ten largest deals in M&A history cumulate to a total value of 1,118,963 mil. USD. (1.118 tril. USD).[50]  In a study conducted in 2000 by Lehman Brothers, it was found that, on average, large M&A deals cause the domestic currency of the target corporation to appreciate by 1% relative to the acquirer's local currency. Until 2018, around 280,472 cross-border deals have been conducted, which cumulates to a total value of almost US$24,069 billion.[41]  The rise of globalization has exponentially increased the necessity for agencies such as the Mergers and Acquisitions International Clearing (MAIC), trust accounts and securities clearing services for Like-Kind Exchanges for cross-border M&A.[citation needed] On a global basis, the value of cross-border mergers and acquisitions rose seven-fold during the 1990s.[51] In 1997 alone, there were over 2,333 cross-border transactions, worth a total of approximately $298 billion. The vast literature on empirical studies over value creation in cross-border M&A is not conclusive, but points to higher returns in cross-border M&As compared to domestic ones when the acquirer firm has the capability to exploit resources and knowledge of the target's firm and of handling challenges. In China, for example, securing regulatory approval can be complex due to an extensive group of various stakeholders at each level of government. In the United Kingdom, acquirers may face pension regulators with significant powers, in addition to an overall M&A environment that is generally more seller-friendly than the U.S. Nonetheless, the current surge in global cross-border M&A has been called the \"New Era of Global Economic Discovery\".[52]  In little more than a decade, M&A deals in China increased by a factor of 20, from 69 in 2000 to more than 1,300 in 2013.  In 2014, Europe registered its highest levels of M&A deal activity since the financial crisis. Driven by U.S. and Asian acquirers, inbound M&A, at $320.6 billion, reached record highs by both deal value and deal count since 2001.  Approximately 23 percent of the 416 M&A deals announced in the U.S. M&A market in 2014 involved non-U.S. acquirers.  For 2016, market uncertainties, including Brexit and the potential reform from a U.S. presidential election, contributed to cross-border M&A activity lagging roughly 20% behind 2015 activity.  In 2017, the controverse trend which started in 2015, decreasing total value but rising total number of cross border deals, kept going. Compared on a year on year basis (2016–2017), the total number of cross border deals decreased by −4.2%, while cumulated value increased by 0.6%.[41]  Even mergers of companies with headquarters in the same country can often be considered international in scale and require MAIC custodial services. For example, when Boeing acquired McDonnell Douglas, the two American companies had to integrate operations in dozens of countries around the world (1997). This is just as true for other apparently \"single-country\" mergers, such as the $29-billion merger of Swiss drug makers Sandoz and Ciba-Geigy (now Novartis).  M&A practice in emerging countries differs from more mature economies, although transaction management and valuation tools (e.g. DCF, comparables) share a common basic methodology. In China, India or Brazil for example, differences affect the formation of asset price and on the structuring of deals. Profitability expectations (e.g. shorter time horizon, no terminal value due to low visibility) and risk represented by a discount rate must both be properly adjusted.[53] In a M&A perspective, differences between emerging and more mature economies include: i) a less developed system of property rights, ii) less reliable financial information, iii) cultural differences in negotiations, and iv) a higher degree of competition for the best targets.  If not properly dealt with, these factors will likely have adverse consequences on return-on-investment (ROI) and create difficulties in day-to-day business operations. It is advisable that M&A tools designed for mature economies are not directly used in emerging markets without some adjustment. M&A teams need time to adapt and understand the key operating differences between their home environment and their new market.  Despite the goal of performance improvement, results from mergers and acquisitions (M&A) are often disappointing compared with results predicted or expected. Numerous empirical studies show high failure rates of M&A deals. Studies are mostly focused on individual determinants. A book by Thomas Straub (2007) \"Reasons for frequent failure in Mergers and Acquisitions\"[57] develops a comprehensive research framework that bridges different perspectives and promotes an understanding of factors underlying M&A performance in business research and scholarship. The study should help managers in the decision-making process. The first important step towards this objective is the development of a common frame of reference that spans conflicting theoretical assumptions from different perspectives. On this basis, a comprehensive framework is proposed with which to understand the origins of M&A performance better and address the problem of fragmentation by integrating the most important competing perspectives in respect of studies on M&A. Furthermore, according to the existing literature, relevant determinants of firm performance are derived from each dimension of the model. For the dimension strategic management, the six strategic variables: market similarity, market complementarities, production operation similarity, production operation complementarities, market power, and purchasing power were identified as having an important effect on M&A performance. For the dimension organizational behavior, the variables acquisition experience, relative size, and cultural differences were found to be important. Finally, relevant determinants of M&A performance from the financial field were acquisition premium, bidding process, and due diligence. Three different ways in order to best measure post M&A performance are recognized: synergy realization, absolute performance, and finally relative performance.  Employee turnover contributes to M&A failures. The turnover in target companies is double the turnover experienced in non-merged firms for the ten years after the merger.[citation needed]  M&As involving small businesses are particularly problematic and have been found to take longer and cost more than expected with organisation cultural and effective communication with employees being key determinants of success and failure [58]  Many M&A fail due to lack of planning or execution of the plan. An empirical research study conducted between 1988 and 2002 found that \"Successful acquisitions, as defined by return on investment and time to market, are more likely to involve complex products but minimal uncertainty about whether the product is functional and whether there is an appetite in the market.\"[59][60] But failed mergers and acquisitions are caused by \"hasty purchases where information platforms between companies were incompatible and the product was not yet tested for release.\"[59] A recommendation to resolve these failed mergers is to wait for the product to become established in the market and research has been completed.  Deloitte[61] determines most companies do not do their due diligence in determining whether a M&A is the correct move due to these four reasons:  Transactions that undergo a due diligence process are more likely to be successful.[62]  A considerable body of research suggests that many mergers fail due to human factors such as issues with trust between employees of the two organizations or trust between employees and their leaders.[63]  Any M&A transaction, no matter the size or structure, can have a significant impact on the acquiring company. Developing and implementing a robust due diligence process can lead to a much better assessment of the risks and potential benefits of a transaction, enable the renegotiation of pricing and other key terms, and smooth the way towards a more effective integration.[61]  M&A can hinder innovation by mismanagement or cultural differences between companies. They can also create bottlenecks when they disrupt the flow of innovation with too many company policies and procedures. Market dominant companies can also be their own demise when presented with an M&A opportunity. Complacency and lack of due diligence may cause the market dominant company to miss the value of an innovative product or service. "},"meta":{},"created_at":"2025-03-22T14:25:42.292408Z","updated_at":"2025-03-22T14:25:42.292408Z","inner_id":100,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":109,"annotations":[{"id":109,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.327895Z","updated_at":"2025-03-22T14:25:42.327895Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"20b9f9ef-5cbc-41a0-917f-31f78cc8223f","import_id":null,"last_action":null,"bulk_created":false,"task":109,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"An independent film, independent movie, indie film, or indie movie is a feature film or short film that is produced outside the major film studio system in addition to being produced and distributed by independent entertainment companies (or, in some cases, distributed by major companies). Independent films are sometimes distinguishable by their content and style and how the filmmakers' artistic vision is realized. Sometimes, independent films are made with considerably lower budgets than major studio films.[1]  It is not unusual for well-known actors who are cast in independent features to take substantial pay cuts for a variety of reasons: if they truly believe in the message of the film, they feel indebted to a filmmaker for a career break; their career is otherwise stalled, or they feel unable to manage a more significant commitment to a studio film; the film offers an opportunity to showcase a talent that has not gained traction in the studio system; or simply because they want to work with a particular director they admire. Examples of the latter include John Travolta and Bruce Willis taking less than their usual pay to work with Quentin Tarantino on Pulp Fiction.[2]  Generally, the marketing of independent films is characterized by limited release, often at independent movie theaters, but they can also have major marketing campaigns and a wide release. Independent films are often screened at local, national, or international film festivals before distribution (theatrical or retail release). An independent film production can rival a mainstream film production if it has the necessary funding and distribution.  In 1908, the Motion Picture Patents Company or \"Edison Trust\" was formed as a trust. The Trust was a cartel that held a monopoly on film production and distribution comprising all the major film companies of the time (Edison, Biograph, Vitagraph, Essanay, Selig, Lubin, Kalem, American Star, American Pathé), the leading distributor (George Kleine) and the biggest supplier of raw film, Eastman Kodak. A number of filmmakers declined or were refused membership to the trust and came to be described as \"independent\".  At the time of the formation of the MPPC, Thomas Edison owned most of the major patents relating to motion pictures, including that for raw film. The MPPC vigorously enforced its patents, constantly bringing suits and receiving injunctions against independent filmmakers. Because of this, a number of filmmakers responded by building their own cameras and moving their operations to Hollywood, California, where the distance from Edison's home base of New Jersey made it more difficult for the MPPC to enforce its patents.[3]  The Edison Trust was soon ended by two decisions of the Supreme Court of the United States: one in 1912, which canceled the patent on raw film, and a second in 1915, which cancelled all MPPC patents. Though these decisions succeeded at legalizing independent film, they would do little to remedy the de facto ban on small productions; the independent filmmakers who had fled to Southern California during the enforcement of the trust had already laid the groundwork for the studio system of classical Hollywood cinema.  In early 1910, director D.W. Griffith was sent by the Biograph Company to the west coast with his acting troupe, consisting of performers Blanche Sweet, Lillian Gish, Mary Pickford, Lionel Barrymore, and others. They began filming on a vacant lot near Georgia Street in downtown Los Angeles. While there, the company decided to explore new territories, traveling several miles north to Hollywood, a little village that was friendly and positive about the movie company filming there. Griffith then filmed the first movie ever shot in Hollywood, In Old California, a Biograph melodrama about California in the 1800s, while it belonged to Mexico. Griffith stayed there for months and made several films before returning to New York.  During the Edison era of the early 1900s, many Jewish immigrants had found jobs in the U.S. film industry. Under the Edison Trust, they were able to make their mark in a brand-new business: the exhibition of films in storefront theaters called nickelodeons. Within a few years, ambitious men like Samuel Goldwyn, Carl Laemmle, Adolph Zukor, Louis B. Mayer, and the Warner Brothers (Harry, Albert, Samuel, and Jack) had switched to the production side of the business. After hearing about Biograph's success in Hollywood, in 1913 many such would-be movie-makers headed west to avoid the fees imposed by Edison. Soon they were the heads of a new kind of enterprise: the movie studio.  By establishing a new system of production, distribution, and exhibition which was independent of The Edison Trust in New York, these studios opened up new horizons for cinema in the United States. The Hollywood oligopoly replaced the Edison monopoly. Within this new system, a pecking order was soon established which left little room for any newcomers. By the mid-1930s, at the top were the five major studios, 20th Century Fox, Metro-Goldwyn-Mayer, Paramount Pictures, RKO Pictures, and Warner Bros. Then came three smaller companies, Columbia Pictures, United Artists, and Universal Studios. Finally there was \"Poverty Row\", a catch-all term used to encompass any other smaller studio that managed to fight their way up into the increasingly exclusive movie business.  While the small studios that made up Poverty Row could be characterized as existing \"independently\" of any major studio, they utilized the same kind of vertically and horizontally integrated systems of business as the larger players in the game. Though the eventual breakup of the studio system and its restrictive chain-theater distribution network would leave independent movie houses eager for the kind of populist, seat-filling product of the Poverty Row studios, that same paradigm shift would also lead to the decline and ultimate disappearance of \"Poverty Row\" as a Hollywood phenomenon. While the kinds of films produced by Poverty Row studios only grew in popularity, they would eventually become increasingly available both from major production companies and from independent producers who no longer needed to rely on a studio's ability to package and release their work.  This table lists the companies active in late 1935 illustrates the categories commonly used to characterize the Hollywood system.  The studio system quickly became so powerful that some filmmakers once again sought independence. On May 24, 1916, the Lincoln Motion Picture Company was formed, the first movie studio owned and controlled by independent filmmakers.[4] In 1919, four of the leading figures in American silent cinema (Mary Pickford, Charles Chaplin, Douglas Fairbanks, and D. W. Griffith) formed United Artists. Each held a 20% stake, with the remaining 20% held by lawyer William Gibbs McAdoo.[5] The idea for the venture originated with Fairbanks, Chaplin, Pickford, and cowboy star William S. Hart a year earlier as they were traveling around the U.S. selling Liberty bonds to help the World War I effort. Already veterans of Hollywood, the four film stars began to talk of forming their own company to better control their own work as well as their futures. They were spurred on by the actions of established Hollywood producers and distributors, who were making moves to tighten their control over their stars' salaries and creative license. With the addition of Griffith, planning began, but Hart bowed out before things had formalized. When he heard about their scheme, Richard A. Rowland, head of Metro Pictures, is said to have observed, \"The inmates are taking over the asylum.\"  The four partners, with advice from McAdoo (son-in-law and former Treasury Secretary of then-President Woodrow Wilson), formed their distribution company, with Hiram Abrams as its first managing director. The original terms called for Pickford, Fairbanks, Griffith, and Chaplin to independently produce five pictures each year, but by the time the company got underway in 1920–1921, feature films were becoming more expensive and more polished, and running times had settled at around ninety minutes (or eight reels). It was believed that no one, no matter how popular, could produce and star in five quality feature films a year. By 1924, Griffith had dropped out and the company was facing a crisis: either bring in others to help support a costly distribution system or concede defeat. The veteran producer Joseph Schenck was hired as president. Not only had he been producing pictures for a decade, but he brought along commitments for films starring his wife, Norma Talmadge, his sister-in-law, Constance Talmadge, and his brother-in-law, Buster Keaton. Contracts were signed with a number of independent producers, especially Samuel Goldwyn, Howard Hughes and later Alexander Korda. Schenck also formed a separate partnership with Pickford and Chaplin to buy and build theaters under the United Artists name.  Still, even with a broadening of the company, UA struggled. The coming of sound ended the careers of Pickford and Fairbanks. Chaplin, rich enough to do what he pleased, worked only occasionally. Schenck resigned in 1933 to organize a new company with Darryl F. Zanuck, Twentieth Century Pictures, which soon provided four pictures a year to UA's schedule. He was replaced as president by sales manager Al Lichtman who himself resigned after only a few months. Pickford produced a few films, and at various times Goldwyn, Korda, Walt Disney, Walter Wanger, and David O. Selznick were made \"producing partners\" (i.e., sharing in the profits), but ownership still rested with the founders. As the years passed and the dynamics of the business changed, these \"producing partners\"  drifted away. Goldwyn and Disney left for RKO, Wanger for Universal Pictures, Selznick and Korda for retirement. By the late 1940s, United Artists had virtually ceased to exist as either a producer or distributor.  In 1941, Mary Pickford, Charles Chaplin, Walt Disney, Orson Welles, Samuel Goldwyn, David O. Selznick, Alexander Korda, and Walter Wanger—many of the same people who were members of United Artists—founded the Society of Independent Motion Picture Producers. Later members included William Cagney, Sol Lesser, and Hal Roach. The Society aimed to preserve the rights of independent producers in an industry overwhelmingly controlled by the studio system. SIMPP fought to end monopolistic practices by the five major Hollywood studios which controlled the production, distribution, and exhibition of films. In 1942, the SIMPP filed an antitrust suit against Paramount's United Detroit Theatres. The complaint accused Paramount of conspiracy to control first-run and subsequent-run theaters in Detroit. It was the first antitrust suit brought by producers against exhibitors alleging monopoly and restraint of trade. In 1948, the United States Supreme Court Paramount Decision ordered the Hollywood movie studios to sell their theater chains and to eliminate certain anti-competitive practices. This effectively brought an end to the studio system of Hollywood's Golden Age. By 1958, many of the reasons for creating the SIMPP had been corrected and SIMPP closed its offices.  The efforts of the SIMPP and the advent of inexpensive portable cameras during World War II effectively made it possible for any person in America with an interest in making films to write, produce, and direct one without the aid of any major film studio. These circumstances soon resulted in a number of critically acclaimed and highly influential works, including Maya Deren's Meshes of the Afternoon in 1943, Kenneth Anger's Fireworks in 1947, and Morris Engel, Ruth Orkin and Ray Abrashkin's Little Fugitive in 1953. Filmmakers such as Ken Jacobs, with little or no formal training, began to experiment with new ways of making and shooting films.  Little Fugitive became the first independent film to be nominated for Academy Award for Best Original Screenplay at the American Academy Awards.[6] It also received Silver Lion at Venice. Both Engel and Anger's films won acclaim overseas from the burgeoning French New Wave, with Fireworks inspiring praise and an invitation to study under him in Europe from Jean Cocteau, and François Truffaut citing Little Fugitive as an essential inspiration to his seminal work, The 400 Blows. As the 1950s progressed, the new low-budget paradigm of filmmaking gained increased recognition internationally, with films such as Satyajit Ray's critically acclaimed[7][8][9][10] Apu Trilogy (1955–1959).  Unlike the films made within the studio system, these new low-budget films could afford to take risks and explore new artistic territory outside the classical Hollywood narrative. Maya Deren was soon joined in New York by a crowd of like-minded avant-garde filmmakers who were interested in creating films as works of art rather than entertainment. Based upon a common belief that the \"official cinema\" was \"running out of breath\" and had become \"morally corrupt, aesthetically obsolete, thematically superficial, [and] temperamentally boring\",[11] this new crop of independents formed The Film-Makers' Cooperative, an artist-run, non-profit organization which they would use to distribute their films through a centralized archive. Founded in 1962 by Jonas Mekas, Stan Brakhage, Shirley Clarke, Gregory Markopoulos, and others, the Cooperative provided an important outlet for many of cinema's creative luminaries in the 1960s, including Jack Smith and Andy Warhol. When he returned to America, Ken Anger would debut many of his most important works there. Mekas and Brakhage would go on to found the Anthology Film Archives in 1970, which would likewise prove essential to the development and preservation of independent films, even to this day.  Not all low-budget films existed as non-commercial art ventures. The success of films like Little Fugitive, which had been made with low (or sometimes non-existent) budgets encouraged a huge boom in popularity for non-studio films. Low-budget film making promised exponentially greater returns (in terms of percentages) if the film could have a successful run in the theaters. During this time, independent producer\/director Roger Corman began a sweeping body of work that would become legendary for its frugality and grueling shooting schedule. Until his so-called \"retirement\" as a director in 1971 (he continued to produce films even after this date), he would produce up to seven movies a year, matching and often exceeding the five-per-year schedule that the executives at United Artists had once thought impossible.  Like those of the avant-garde, the films of Roger Corman took advantage of the fact that unlike the studio system, independent films had never been bound by its self-imposed production code. Corman's example (and that of others like him) would help start a boom in independent B-movies in the 1960s, the principal aim of which was to bring in the youth market which the major studios had lost touch with. By promising sex, wanton violence, drug use, and nudity, these films hoped to draw audiences to independent theaters by offering to show them what the major studios could not. Horror and science fiction films experienced a period of tremendous growth during this time. As these tiny producers, theaters, and distributors continued to attempt to undercut one another, the B-grade shlock film soon fell to the level of the Z movie, a niche category of films with production values so low that they became a spectacle in their own right. The cult audiences these pictures attracted soon made them ideal candidates for midnight movie screenings revolving around audience participation and cosplay.  In 1968, a young filmmaker named George A. Romero shocked audiences with Night of the Living Dead, a new kind of intense and unforgiving independent horror film. This film was released just after the abandonment of the production code, but before the adoption of the MPAA rating system. As such, it was the first and last film of its kind to enjoy a completely unrestricted screening, in which young children were able to witness Romero's new brand of highly realistic gore. This film would help to set the climate of independent horror for decades to come, as films like The Texas Chain Saw Massacre (1974) and Cannibal Holocaust (1980) continued to push the envelope.  With the production code abandoned and violent and disturbing films like Romero's gaining popularity, Hollywood opted to placate the uneasy filmgoing public with the MPAA ratings system, which would place restrictions on ticket sales to young people. Unlike the production code, this rating system posed a threat to independent films in that it would affect the number of tickets they could sell and cut into the grindhouse theaters' share of the youth market. This change would further widen the divide between commercial and non-commercial films.  However, having a film audience-classified is strictly voluntary for independents and there is no legal impediment to releasing movies on an unrated basis. However, unrated movies face obstacles in marketing because media outlets, such as TV channels, newspapers and websites, often place their own restrictions on movies that do not come with a built-in national rating, in order to avoid presenting movies to inappropriately young audiences.[12]  Following the advent of television and the Paramount Case, the major studios attempted to lure audiences with spectacle. Widescreen processes and technical improvements, such as Cinemascope, stereo sound, 3-D and others, were developed in an attempt to retain the dwindling audience by giving them a larger-than-life experience. The 1950s and early 1960s saw a Hollywood dominated by musicals, historical epics, and other films which benefited from these advances. This proved commercially viable during most of the 1950s. However, by the late 1960s, audience share was dwindling at an alarming rate. Several costly flops, including Cleopatra (1963) and Hello, Dolly! (1969) put severe strain on the studios. Meanwhile, in 1951, lawyers-turned-producers Arthur Krim and Robert Benjamin had made a deal with the remaining stockholders of United Artists which would allow them to make an attempt to revive the company and, if the attempt was successful, buy it after five years.  The attempt was a success, and in 1955 United Artists became the first \"studio\" without an actual studio. UA leased space at the Pickford\/Fairbanks Studio, but did not own a studio lot as such. Because of this, many of their films would be shot on location. Primarily acting as bankers, they offered money to independent producers. Thus UA did not have the overhead, the maintenance or the expensive production staff which ran up costs at other studios. UA went public in 1956, and as the other mainstream studios fell into decline, UA prospered, adding relationships with the Mirisch brothers, Billy Wilder, Joseph E. Levine and others.  By the late 1950s, RKO had ceased film production, and the remaining four of the big five had recognized that they did not know how to reach the youth audience. In an attempt to capture this audience, the Studios hired a host of young filmmakers (many of whom were mentored by Roger Corman) and allowed them to make their films with relatively little studio control. Warner Brothers offered first-time producer Warren Beatty 40% of the gross on his film Bonnie and Clyde (1967) instead of a minimal fee. The movie had grossed over $70 million worldwide by 1973. These initial successes paved the way for the studio to relinquish almost complete control to the film school generation and began what the media dubbed \"New Hollywood.\"  Dennis Hopper, the American actor, made his writing and directing debut with Easy Rider (1969). Along with his producer\/co-star\/co-writer Peter Fonda, Hopper was responsible for one of the first completely independent films of New Hollywood. Easy Rider debuted at Cannes and garnered the \"First Film Award\" (French: Prix de la premiere oeuvre) after which it received two Oscar nominations, one for best original screenplay and one for Corman-alum Jack Nicholson's breakthrough performance in the supporting role of George Hanson, an alcoholic lawyer for the American Civil Liberties Union.[13] Following on the heels of Easy Rider shortly afterward was the revived United Artists' Midnight Cowboy (also 1969), which, like Easy Rider, took numerous cues from Kenneth Anger and his influences in the French New Wave. It became the first and only X rated film to win the Academy Award for Best Picture. Midnight Cowboy also held the distinction of featuring cameo roles by many of the top Warhol superstars, who had already become symbols of the militantly anti-Hollywood climate of NYC's independent film community.  Within a month, another young Corman trainee, Francis Ford Coppola, made his debut in Spain at the Donostia-San Sebastian International Film Festival with The Rain People (1969), a film he had produced through his own company, American Zoetrope. Though The Rain People was largely overlooked by American audiences, Zoetrope would become a powerful force in New Hollywood. Through Zoetrope, Coppola formed a distribution agreement with studio giant Warner Bros., which he would exploit to achieve wide releases for his films without making himself subject to their control. These three films provided the major Hollywood studios with both an example to follow and a new crop of talent to draw from. Zoetrope co-founder George Lucas made his feature film debut with THX 1138 (1971), also released by Zoetrope through their deal with Warner Bros., announcing himself as another major talent of New Hollywood. By the following year, two New Hollywood directors had become sufficiently established for Coppola to be offered oversight of Paramount's The Godfather (1972) and Lucas had obtained studio funding for American Graffiti (1973) from Universal. In the mid-1970s, the major Hollywood studios continued to tap these new filmmakers for both ideas and personnel, producing films such as Paper Moon (1973) and Taxi Driver (1976), all of which met with critical and commercial success. These successes by the members of New Hollywood led each of them, in turn, to make more and more extravagant demands, both on the studio and eventually on the audience.  While most members of the New Hollywood generation were, or started out as, independent filmmakers, a number of their projects were produced and released by major studios. The New Hollywood generation soon became firmly entrenched in a revived incarnation of the studio system, which financed the development, production and distribution of their films. Very few of these filmmakers ever independently financed or independently released a film of their own, or ever worked on an independently financed production during the height of the generation's influence. Seemingly independent films such as Taxi Driver, The Last Picture Show and others were studio films: the scripts were based on studio pitches and subsequently paid for by the studios, the production financing was from the studio, and the marketing and distribution of the films were designed and controlled by the studio's advertising agency. Though Coppola made considerable efforts to resist the influence of the studios, opting to finance his risky 1979 film Apocalypse Now himself rather than compromise with skeptical studio executives, he, and filmmakers like him, had saved the old studios from financial ruin by providing them with a new formula for success.  Indeed, it was during this period that the very definition of an independent film became blurred. Though Midnight Cowboy was financed by United Artists, the company was certainly a studio. Likewise, Zoetrope was another \"independent studio\" which worked within the system to make a space for independent directors who needed funding. George Lucas would leave Zoetrope in 1971 to create his own independent studio, Lucasfilm, which would produce the blockbuster Star Wars and Indiana Jones franchises. In fact, the only two movies of the movement which can be described as uncompromisingly independent are Easy Rider at the beginning, and Peter Bogdanovich's They All Laughed, at the end. Peter Bogdanovich bought back the rights from the studio to his 1980 film and paid for its distribution out of his own pocket, convinced that the picture was better than what the studio believed — he eventually went bankrupt because of this.  In retrospect, it can be seen that Steven Spielberg's Jaws (1975) and George Lucas's Star Wars (1977) marked the beginning of the end for the New Hollywood. With their unprecedented box-office successes, these movies jump-started Hollywood's blockbuster mentality, giving studios a new paradigm as to how to make money in this changing commercial landscape. The focus on high-concept premises, with greater concentration on tie-in merchandise (such as toys), spin-offs into other media (such as soundtracks), and the use of sequels (which had been made more respectable by Coppola's The Godfather Part II), all showed the studios how to make money in the new environment.  On realizing how much money could potentially be made in films, major corporations started buying up the remaining Hollywood studios, saving them from the oblivion which befell RKO in the 50s. Eventually, even RKO was revived, the corporate mentality these companies brought to the filmmaking business would slowly squeeze out the more idiosyncratic of these young filmmakers, while ensconcing the more malleable and commercially successful of them.[14]  Film critic Manohla Dargis described this era as the \"halcyon age\" of the decade's filmmaking that \"was less revolution than business as usual, with rebel hype\".[15] She also pointed out in her New York Times article that enthusiasts insisted this era was \"when American movies grew up (or at least starred underdressed actresses); when directors did what they wanted (or at least were transformed into brands); when creativity ruled (or at least ran gloriously amok, albeit often on the studio's dime).\"[16]  During the 1970s, shifts in thematic depictions of sexuality and violence occurred in American cinema, prominently featuring heightened depictions of realistic sex and violence. Directors who wished to reach mainstream audiences of Old Hollywood quickly learned to stylize these themes to make their films appealing and attractive rather than repulsive or obscene. However, at the same time that the maverick film students of the American New Wave were developing the skills they would use to take over Hollywood, many of their peers had begun to develop their style of filmmaking in a different direction. Influenced by foreign and art house directors such as Ingmar Bergman and Federico Fellini, exploitation shockers (i.e. Joseph P. Mawra, Michael Findlay, and Henri Pachard) and avant-garde cinema, (Kenneth Anger, Maya Deren and Bruce Conner[17][18]) a number of young film makers began to experiment with transgression not as a box-office draw, but as an artistic act. Directors such as John Waters and David Lynch would make a name for themselves by the early 1970s for the bizarre and often disturbing imagery which characterized their films.  When Lynch's first feature film, Eraserhead (1977), brought Lynch to the attention of producer Mel Brooks, he soon found himself in charge of the $5 million film The Elephant Man (1980) for Paramount. Though Eraserhead was strictly an out-of-pocket, low-budget, independent film, Lynch made the transition with unprecedented grace. The film was a huge commercial success, and earned eight Academy Award nominations, including Best Director and Best Adapted Screenplay nods for Lynch.[19] It also established his place as a commercially viable, if somewhat dark and unconventional, Hollywood director. Seeing Lynch as a fellow studio convert, George Lucas, a fan of Eraserhead and now the darling of the studios, offered Lynch the opportunity to direct his next Star Wars sequel, Return of the Jedi (1983). However, Lynch had seen what had happened to Lucas and his comrades in arms after their failed attempt to do away with the studio system. He refused the opportunity, stating that he would rather work on his own projects.[20]  Lynch instead chose to direct a big budget adaptation of Frank Herbert's science fiction novel Dune for Italian producer Dino De Laurentiis's De Laurentiis Entertainment Group, on the condition that the company release a second Lynch project, over which the director would have complete creative control. Although De Laurentiis hoped it would be the next Star Wars, Lynch's Dune (1984) was a critical and commercial flop, grossing a mere $27.4 million domestically against a $45 million budget. De Laurentiis, furious that the film had been a commercial disaster, was then forced to produce any film Lynch desired. He offered Lynch only $6 million in order to minimize the risk if the film had failed to recoup its costs; however, the film, Blue Velvet (1986), was a resounding success, earning him another Academy Award for Best Director nod.[21] Lynch subsequently returned to independent filmmaking, and did not work with another major studio for over a decade.  Unlike the former, John Waters released most of his films during his early life through his own production company, Dreamland Productions. In the early 1980s, New Line Cinema agreed to work with him on Polyester (1981). During the 1980s, Waters would become a pillar of the New York–based independent film movement known as the \"Cinema of Transgression\", a term coined by Nick Zedd in 1985 to describe a loose-knit group of like-minded New York artists using shock value and humor in their Super 8 mm films and video art. Other key players in this movement included Kembra Pfahler, Casandra Stark, Beth B, Tommy Turner, Richard Kern and Lydia Lunch. Rallying around such institutions as the Film-Makers' Cooperative and Anthology Film Archives, this new generation of independents devoted themselves to the defiance of the now-establishment New Hollywood, proposing that \"all film schools be blown up and all boring films never be made again.\"[22]  In 1978, Sterling Van Wagenen and Charles Gary Allison, with Chairperson Robert Redford, (veteran of New Hollywood and star of Butch Cassidy and the Sundance Kid) founded the Utah\/US Film Festival in an effort to attract more filmmakers to Utah and showcase what the potential of independent film could be. At the time, the main focus of the event was to present a series of retrospective films and filmmaker panel discussions; however it also included a small program of new independent films. The jury of the 1978 festival was headed by Gary Allison, and included Verna Fields, Linwood Gale Dunn, Katherine Ross, Charles E. Sellier Jr., Mark Rydell, and Anthea Sylbert. In 1981, the same year that United Artists, bought out by MGM after the financial failure of Michael Cimino's Heaven's Gate (1980),[23] ceased to exist as a venue for independent filmmakers, Sterling Van Wagenen left the film festival to help found the Sundance Institute with Robert Redford. In 1985, the now well-established Sundance Institute, headed by Sterling Van Wagenen, took over management of the US Film Festival, which was experiencing financial difficulties. Gary Beer and Sterling Van Wagenen spearheaded production of the inaugural Sundance Film Festival which included Program Director Tony Safford and Administrative Director Jenny Walz Selby.  In 1991, the festival was officially renamed the Sundance Film Festival, after Redford's famous role as The Sundance Kid.[24] Through this festival the Independent Cinema movement was launched. Such notable figures as Kevin Smith, Robert Rodriguez, Quentin Tarantino, David O. Russell, Paul Thomas Anderson, Steven Soderbergh, James Wan, Hal Hartley, Joel and Ethan Coen and Jim Jarmusch garnered resounding critical acclaim and unprecedented box office sales. The significance of the Sundance Film Festival is documented in the work of Professor Emanuel Levy Cinema of Outsiders: The Rise of American Independent Film, (NYU Press, 1999; 2001).  In 2005, about 15% of the U.S. domestic box office revenue was from independent studios.[25]  The 1990s saw the rise and success of independent films not only through the film festival circuit but at the box office as well while established actors, such as Bruce Willis, John Travolta, and Tim Robbins, found success themselves both in independent films and Hollywood studio films.[26] Teenage Mutant Ninja Turtles in 1990 from New Line Cinema grossed over $100 million in the United States making it the most successful indie film in box-office history to that point.[27] Miramax Films had a string of hits with Sex, Lies, and Videotape, My Left Foot, and  Clerks, putting Miramax and New Line Cinema in the sights of big companies looking to cash in on the success of independent studios. In 1993, Disney bought Miramax for $60 million. Turner Broadcasting, in a billion-dollar deal, acquired New Line Cinema, Fine Line Features, and Castle Rock Entertainment in 1994. The acquisitions proved to be a good move for Turner Broadcasting as New Line released The Mask and Dumb & Dumber, Castle Rock released The Shawshank Redemption, and Miramax released Pulp Fiction, all in 1994.[27]  The acquisitions of the smaller studios by conglomerate Hollywood was a plan in part to take over the independent film industry and at the same time start \"independent\" studios of their own. The following are all \"indie\" studios owned by conglomerate Hollywood:  By the early 2000s, Hollywood was producing three different classes of films: 1) big-budget blockbusters, 2) art films, specialty films and niche-market films produced by the conglomerate-owned \"indies\" and 3) genre and specialty films coming from true indie studios and producers. The third category comprised over half the features released in the United States and usually cost between $5 and $10 million to produce.[29]  Hollywood was producing these three different classes of feature films by means of three different types of producers. The superior products were the large, budget blockbusters and high-cost star vehicles marketed by the six major studio producer-distributors. Budgets on the major studios' pictures averaged $100 million, with approximately one-third of it spent on marketing because of the large release campaigns. Another class of Hollywood feature film included art films, specialty films, and other niche-market fare controlled by the conglomerates' indie subsidiaries. Budgets on these indie films averaged $40 million per release in the early 2000s, with $10 million to $15 million spent on marketing (MPA, 2006:12). The final class of film consisted of genre and specialty films whose release campaigns were administered by independent producer-distributors with only a few dozen or possibly a few hundred screens in select urban markets. Films like these usually cost less than $10 million, but frequently less than $5 million, with small marketing budgets that escalate if and when a particular film performs.[30]  The independent film industry exists globally. Many of the most prestigious film festivals are hosted in various cities around the world.[31] The Berlin International Film Festival attracts over 130 countries, making it the largest film festival in the world.[32] Other large events include the Toronto International Film Festival, Hong Kong International Film Festival, and the Panafrican Film and TV Festival of Ouagadougou.[31]  The European Union, specifically through the European Cinema and VOD Initiative (ECVI), has established programs that attempt to adapt the film industry to an increasing digital demand for film on video on demand services, outside of theatrical screenings. With this program, VOD offerings are paired with traditional movie screenings.[33] There is also more of a push from EU National governments to fund all aspects of the arts, including film.[34] The European Commission for Culture has an Audiovisual sector, for example, whose role is most notably to help distribute and promote films and festivals across Europe. Additionally, the Commission organizes policymaking, research, and reporting on \"media literacy\" and \"digital distribution.\"[34]  As with other media, the availability of new technologies has fueled the democratization of filmmaking and the growth of independent film. In the late forties and fifties, new inexpensive portable cameras made it easier for independent filmmakers to produce content without studio backing. The emergence of camcorders in the eighties broadened the pool of filmmakers experimenting with the newly available technology. More recently, the switch from film to digital cameras, inexpensive non-linear editing and the move to distribution via the internet have led to more people being able to make and exhibit movies of their own, including young people and individuals from marginalized communities. These people may have little to no formal technical or academic training, but instead are autodidactic filmmakers, using online sources to learn the craft. Aspiring filmmakers can range from those simply with access to a smartphone or digital camera, to those who write \"spec\" scripts (to pitch to studios), actively network, and use crowdsourcing and other financing to get their films professionally produced. Oftentimes, aspiring filmmakers have other day-jobs to support themselves financially while they pitch their scripts and ideas to independent film production companies, talent agents, and wealthy investors. This recent technology-fueled renaissance has helped fuel other supporting industries such as the \"prosumer\" camera segment and film schools for those who are less autodidactic. Film programs in universities such as NYU in New York and USC in Los Angeles have benefited from this transitional growth.[35]  The economic side of filmmaking is also less of an obstacle than before, because the backing of a major studio is no longer needed to access necessary movie funding. Crowdfunding services like Kickstarter, Pozible, and Tubestart have helped people raise thousands of dollars; enough to fund their own, low-budget productions.[36] As a result of the falling cost of technology to make, edit and digitally distribute films, filmmaking is more widely accessible than ever before.  Full-length films are often showcased at film festivals such as the Sundance Film Festival, Slamdance Film Festival, South by Southwest Festival, Raindance Film Festival, Telluride Film Festival, and Palm Springs Film Festival.[37] Award winners from these exhibitions are more likely to get picked up for distribution by major film distributors. Film festivals and screenings like these are just one of the options in which movies can be independently produced\/released.    The development of independent film in the 1990s and 21st century has been stimulated by a range of technical innovations, including the development of affordable digital cinematography cameras that can exceed the quality of film and easy-to-use computer editing software. Until digital alternatives became available, the cost of professional film equipment and stock was a major obstacle to independent filmmakers who wanted to make their own films. Successful films such as The Blair Witch Project (which grossed over US$248.6 million while only spending US$60,000) have emerged from this new accessibility to filmmaking tools. In 2002, the cost of 35 mm film stock went up 23%, according to Variety.[38] The advent of consumer camcorders in 1985, and more importantly, the arrival of digital video in the early 1990s, lowered the technology barrier to movie production. The personal computer and non-linear editing system have taken away the use of editing stands such as the KEM, dramatically reducing the costs of post-production, while technologies such as DVD, Blu-ray Disc and online video services have simplified distribution; video streaming services have made it possible to distribute a digital version of a film to an entire country or even the world, without involving shipping or warehousing of physical DVDs or film reels. 3-D technology is available to low-budget, independent filmmakers. By the second decade of the 21st century high-quality cellphone cameras allowed people to make, edit and distribute films on a single inexpensive device.  One of the examples of such a new indie approach to filmmaking is the 1999 Oscar-nominated documentary film Genghis Blues that was shot by the Belic brothers on two Hi8 consumer camcorders and won that year's Sundance Film Festival Audience Award for a Documentary.[39] At the time, distribution was still film-based so the movie had to be \"filmed out\" from interlaced digital video format to film running at traditional 24-frame per second rate, so interlacing artefacts are noticeable at times. In 2004 Panasonic released the DVX100 camcorder, which featured film-like 24-frame per second shooting rate. This gave independent filmmakers the ability to shoot video at a frame rate considered standard for movies at the time[40] and opened the possibility of clean digital frame to film frame conversion. Several acclaimed films were made with this camera, for example Iraq in Fragments.[41] More recent devices allow \"filming\" at very high frame rates to facilitate distribution into a number of frame rates without artifacts.  Even though new cinema cameras such as the Arri Alexa, RED Epic, and the many new DSLRs cost thousands of dollars to purchase, independent films are still cheaper than ever, creating footage that looks like 35 mm film without the same high cost. These cameras also perform better than traditional film because of its ability to perform in extremely dark\/low light situations relative to film. In 2008, Nikon released the first DSLR camera that could also shoot video, the Nikon D90. With the sensor larger than on a traditional camcorder, these DSLRs allow for a greater control over depth of field, great low-light capabilities, and a large variety of exchangeable lenses, including lenses from old film cameras – things which independent filmmakers have been longing for years. With the creation of new, light-weight and accessible cinema cameras, documentaries have also benefitted greatly. It was previously impossible to capture the extreme wild of mother nature because of the lack of maneuverability with film cameras; however, with the creation of DSLRs, documentary filmmakers were able to reach hard-to-get places in order to capture what they couldn't have with film cameras.[42] Cameras have even been attached to animals to allow them to help film never-before-seen scenes.  New technologies have also allowed the development of new cinematic techniques originating in independent films, such as the development of the zoom lens in the early 20th century. The use of the (controversial) hand-held shot made popular in the groundbreaking The Blair Witch Project also led to an entirely new subgenre: the found-footage film.   Independent filmmaking has also benefited from the new editing software. Instead of needing a post-house to do the editing, independent film makers use a personal computer or even just a cellphone with editing software to edit their films. Editing software available include Avid Media Composer, Adobe Premiere Pro, Final Cut Pro, (Color Grading Software) DaVinci Resolve, and many more. There are also many free tutorials and courses available online to teach different post production skills needed to use these programs. These new technologies allow independent film makers to create films that are comparable to high-budget films. Computer-generated imaging (CGI) has also become more accessible, transitioning from a highly specialized process done by post-production companies into a task that can be performed by independent artists.  Francis Ford Coppola, long an advocate of new technologies like non-linear editing and digital cameras, said in 2007 that \"cinema is escaping being controlled by the financier, and that's a wonderful thing. You don't have to go hat-in-hand to some film distributor and say, 'Please will you let me make a movie?'\"[43]  Nowadays, high pixel camera phones are widely using also for mainstream film cinematography. New Love Meetings, a documentary shot on the Nokia N90, directed by Barbara Seghezzi and Marcello Mencarini in 2005 from Italy; Why Didn't Anybody Tell Me It Would Become This Bad in Afghanistan, a docufiction film  shot on Samsung, directed by Cyrus Frisch in 2007 from the Netherlands; SMS Sugar Man, a narrative film shot on the Sony Ericsson W900i, directed by Aryan Kaganof in 2008 from South Africa; Veenavaadanam, a documentary shot on Nokia N70, directed by Sathish Kalathil in 2008 from India; and Jalachhayam, a narrative film shot on the Nokia N95, directed by Sathish Kalathil in 2010 from India are the first noted experimental works with the first generation Camera phones.  Hooked Up, To Jennifer, Tangerine, 9 Rides, Unsane, High Flying Bird, Ghost, Pondicherry, I WeirDo and Banger are some other examples shot on iPhones. "},"meta":{},"created_at":"2025-03-22T14:25:42.292408Z","updated_at":"2025-03-22T14:25:42.292408Z","inner_id":101,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":110,"annotations":[{"id":110,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.327895Z","updated_at":"2025-03-22T14:25:42.327895Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"ed66156e-136b-4c99-bdcc-65e75e19b738","import_id":null,"last_action":null,"bulk_created":false,"task":110,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"    Hindi cinema, popularly known as Bollywood and formerly as Bombay cinema,[1] is primarily produced in Mumbai. The popular term Bollywood is a portmanteau of \"Bombay\" (former name of Mumbai) and \"Hollywood\". The industry, producing films in the Hindi language, is a part of the larger Indian cinema industry, which also includes South Indian cinema and other smaller film industries.[2][3][4] The term 'Bollywood', often mistakenly used to refer to Indian cinema as a whole, only refers to Hindi-language films, with Indian cinema being an umbrella term that includes all the film industries in the country, each offering films in diverse languages and styles.  In 2017, Indian cinema produced 1,986 feature films, of which the largest number, 364, have been in Hindi.[2] In 2022, Hindi cinema represented 33% of box office revenue, followed by Telugu and Tamil representing 20% and 16% respectively.[5] Mumbai is one of the largest centres for film production in the world.[6][7][8] Hindi films sold an estimated 341 million tickets in India in 2019.[9][10] Earlier Hindi films tended to use vernacular Hindustani, mutually intelligible by speakers of either Hindi or Urdu, while modern Hindi productions increasingly incorporate elements of Hinglish.[11]  The most popular commercial genre in Hindi cinema since the 1970s has been the masala film, which freely mixes different genres including action, comedy, romance, drama and melodrama along with musical numbers.[12][13] Masala films generally fall under the musical film genre, of which Indian cinema has been the largest producer since the 1960s when it exceeded the American film industry's total musical output after musical films declined in the West. The first Indian talkie, Alam Ara (1931), was produced in the Hindustani language, four years after Hollywood's first sound film, The Jazz Singer (1927).  Alongside commercial masala films, a distinctive genre of art films known as parallel cinema has also existed, presenting realistic content and avoidance of musical numbers. In more recent years, the distinction between commercial masala and parallel cinema has been gradually blurring, with an increasing number of mainstream films adopting the conventions which were once strictly associated with parallel cinema.  \"Bollywood\" is a portmanteau derived from Bombay (the former name of Mumbai) and \"Hollywood\", a shorthand reference for the American film industry which is based in Hollywood, California.[14]  The term \"Tollywood\", for the Tollygunge-based cinema of West Bengal, predated \"Bollywood\".[15] It was used in a 1932 American Cinematographer article by Wilford E. Deming, an American engineer who helped produce the first Indian sound picture.[15]  \"Bollywood\" was probably invented in Bombay-based film trade journals in the 1960s or 1970s, though the exact inventor varies by account.[16][17] Film journalist Bevinda Collaco claims she coined the term for the title of her column in Screen magazine.[18] Her column entitled \"On the Bollywood Beat\" covered studio news and celebrity gossip.[18] Other sources state that lyricist, filmmaker and scholar Amit Khanna was its creator.[19] It is unknown if it was derived from \"Hollywood\" through \"Tollywood\", or was inspired directly by \"Hollywood\".  The term has been criticised by some film journalists and critics, who believe it implies that the industry is a poor cousin of Hollywood.[14][20]  In 1897, a film presentation by Professor Stevenson featured a stage show at Calcutta's Star Theatre. With Stevenson's encouragement and camera, Hiralal Sen, an Indian photographer, made a film of scenes from that show, The Flower of Persia (1898).[21] The Wrestlers (1899) by H. S. Bhatavdekar showed a wrestling match at the Hanging Gardens in Bombay.[22]  Dadasaheb Phalke's silent film Raja Harishchandra (1913) is the first feature-length film made in India.[26] The film, being silent, had English, Marathi, and Hindi-language intertitles. By the 1930s, the Indian film industry as a whole was producing over 200 films per year.[27] The first Indian sound film, Ardeshir Irani's Alam Ara (1931), made in Hindustani language, was commercially successful.[28] With a great demand for talkies and musicals, Hindustani cinema (as Hindi cinema was then known as)[29] and the other language film industries quickly switched to sound films.  The 1930s and 1940s were tumultuous times; India was buffeted by the Great Depression, World War II, the Indian independence movement, and the violence of the Partition. Although most early Bombay films were unabashedly escapist, a number of filmmakers tackled tough social issues or used the struggle for Indian independence as a backdrop for their films.[30] Irani made the first Hindi colour film, Kisan Kanya, in 1937. The following year, he made a colour version of Mother India. However, colour did not become a popular feature until the late 1950s. At this time, lavish romantic musicals and melodramas were cinematic staples.  The decade of the 1940s saw an expansion of Bombay cinema's commercial market and its presence in the national consciousness. The year 1943 saw the arrival of Indian cinema's first 'blockbuster' offering, the movie Kismet, which grossed in excess of the important barrier of one crore (10 million) rupees, made on a budget of only two lakh (200,000) rupees.[31] The film tackled contemporary issues, especially those arising from the Indian Independence movement, and went on to become \"the longest running hit of Indian cinema\", a title it held till the 1970s.[32] Film personalities like Bimal Roy, Sahir Ludhianvi and Prithviraj Kapoor participated in the creation of a national movement against colonial rule in India, while simultaneously leveraging the popular political movement to increase their own visibility and popularity.[33][34] Themes from the Independence Movement deeply influenced Bombay film directors, screen-play writers, and lyricists, who saw their films in the context of social reform and the problems of the common people.[35]  Before the Partition, the Bombay film industry was closely linked to the Lahore film industry (known as \"Lollywood\"; now part of the Pakistani film industry); both produced films in Hindustani (also known as Hindi-Urdu), the lingua franca of northern and central India.[36] Another centre of Hindustani-language film production was the Bengal film industry in Calcutta, Bengal Presidency (now Kolkata, West Bengal), which produced Hindustani-language films and local Bengali language films.[37][38] Many actors, filmmakers and musicians from the Lahore industry migrated to the Bombay industry during the 1940s, including actors K. L. Saigal, Prithviraj Kapoor, Dilip Kumar and Dev Anand as well as playback singers Mohammed Rafi, Noorjahan and Shamshad Begum. Around the same time, filmmakers and actors from the Calcutta film industry began migrating to Bombay; as a result, Bombay became the center of Hindustani-language film production.[38]  The 1947 partition of India divided the country into the Republic of India and Pakistan, which precipitated the migration of filmmaking talent from film production centres like Lahore and Calcutta, which bore the brunt of the partition violence.[36][39][38] This included actors, filmmakers and musicians from Bengal, Punjab (particularly the present-day Pakistani Punjab),[36] and the North-West Frontier Province (present-day Khyber Pakhtunkhwa).[40] These events further consolidated the Bombay film industry's position as the preeminent center for film production in India.  The period from the late 1940s to the early 1960s, after India's independence, is regarded by film historians as the Golden Age of Hindi cinema.[41][42][43] Some of the most critically acclaimed Hindi films of all time were produced during this time. Examples include Pyaasa (1957) and Kaagaz Ke Phool (1959), directed by Guru Dutt and written by Abrar Alvi; Awaara (1951) and Shree 420 (1955), directed by Raj Kapoor and written by Khwaja Ahmad Abbas, and Aan (1952), directed by Mehboob Khan and starring Dilip Kumar. The films explored social themes, primarily dealing with working-class life in India (particularly urban life) in the first two examples. Awaara presented the city as both nightmare and dream, and Pyaasa critiqued the unreality of urban life.[44]  Mehboob Khan's Mother India (1957), a remake of his earlier Aurat (1940), was the first Indian film nominated for the Academy Award for Best Foreign Language Film; it lost by a single vote.[45] Mother India defined conventional Hindi cinema for decades.[46][47][48] It spawned a genre of dacoit films, in turn defined by Gunga Jumna (1961).[49] Written and produced by Dilip Kumar, Gunga Jumna was a dacoit crime drama about two brothers on opposite sides of the law (a theme which became common in Indian films during the 1970s).[50] Some of the best-known epic films of Hindi cinema were also produced at this time, such as K. Asif's Mughal-e-Azam (1960).[51] Other acclaimed mainstream Hindi filmmakers during this period included Kamal Amrohi and Vijay Bhatt.  The three most popular male Indian actors of the 1950s and 1960s were Dilip Kumar, Raj Kapoor, and Dev Anand, each with a unique acting style. Kapoor adopted Charlie Chaplin's tramp persona; Anand modeled himself on suave Hollywood stars like Gregory Peck and Cary Grant, and Kumar pioneered a form of method acting which predated Hollywood method actors such as Marlon Brando. Kumar, who was described as \"the ultimate method actor\" by Satyajit Ray, inspired future generations of Indian actors. Much like Brando's influence on Robert De Niro and Al Pacino, Kumar had a similar influence on Amitabh Bachchan, Naseeruddin Shah, Shah Rukh Khan and Nawazuddin Siddiqui.[52][53] Veteran actresses such as Suraiya, Nargis, Sumitra Devi, Madhubala, Meena Kumari, Waheeda Rehman, Nutan, Sadhana, Mala Sinha and Vyjayanthimala have had their share of influence on Hindi cinema.[54]  While commercial Hindi cinema was thriving, the 1950s also saw the emergence of a parallel cinema movement.[44] Although the movement (emphasising social realism) was led by Bengali cinema, it also began gaining prominence in Hindi cinema. Early examples of parallel cinema include Dharti Ke Lal (1946), directed by Khwaja Ahmad Abbas and based on the Bengal famine of 1943,[55] Neecha Nagar (1946) directed by Chetan Anand and written by Khwaja Ahmad Abbas,[56] and Bimal Roy's Do Bigha Zamin (1953). Their critical acclaim and the latter's commercial success paved the way for Indian neorealism and the Indian New Wave (synonymous with parallel cinema).[57] Internationally acclaimed Hindi filmmakers involved in the movement included Mani Kaul, Kumar Shahani, Ketan Mehta, Govind Nihalani, Shyam Benegal, and Vijaya Mehta.[44]  After the social-realist film Neecha Nagar received the Palme d'Or at the inaugural 1946 Cannes Film Festival,[56] Hindi films were frequently in competition for Cannes' top prize during the 1950s and early 1960s and some won major prizes at the festival.[58] Guru Dutt, overlooked during his lifetime, received belated international recognition during the 1980s.[58][59] Film critics polled by the British magazine Sight & Sound included several of Dutt's films in a 2002 list of greatest films,[60] and Time's All-Time 100 Movies lists Pyaasa as one of the greatest films of all time.[61]  During the late 1960s and early 1970s, the industry was dominated by musical romance films with romantic-hero leads.[62]  By 1970, Hindi cinema was thematically stagnant[65] and dominated by musical romance films.[62] The arrival of screenwriting duo Salim–Javed (Salim Khan and Javed Akhtar) was a paradigm shift, revitalising the industry.[65] They began the genre of gritty, violent, Bombay underworld crime films early in the decade with films such as Zanjeer (1973) and Deewaar (1975).[66][67] Salim-Javed reinterpreted the rural themes of Mehboob Khan's Mother India (1957) and Dilip Kumar's Gunga Jumna (1961) in a contemporary urban context, reflecting the socio-economic and socio-political climate of 1970s India[65][68] and channeling mass discontent, disillusionment[65] and the unprecedented growth of slums[69] with anti-establishment themes and those involving urban poverty, corruption and crime.[70][71] Their \"angry young man\", personified by Amitabh Bachchan,[71] reinterpreted Dilip Kumar's performance in Gunga Jumna in a contemporary urban context[65][68] and anguished urban poor.[69]  By the mid-1970s, romantic confections had given way to gritty, violent crime films and action films about gangsters (the Bombay underworld) and bandits (dacoits). Salim-Javed's writing and Amitabh Bachchan's acting popularised the trend with films such as Zanjeer and (particularly) Deewaar, a crime film inspired by Gunga Jumna[50] which pitted \"a policeman against his brother, a gang leader based on real-life smuggler Haji Mastan\" (Bachchan); according to Danny Boyle, Deewaar was \"absolutely key to Indian cinema\".[72] In addition to Bachchan, several other actors followed by riding the crest of the trend (which lasted into the early 1990s).[73] Actresses from the era include Hema Malini, Jaya Bachchan, Raakhee, Shabana Azmi, Zeenat Aman, Parveen Babi, Rekha, Dimple Kapadia, Smita Patil, Jaya Prada and Padmini Kolhapure.[54]  The name \"Bollywood\" was coined during the 1970s,[17][18] when the conventions of commercial Hindi films were defined.[74] Key to this was the masala film, which combines a number of genres (action, comedy, romance, drama, melodrama, and musical). The masala film was pioneered early in the decade by filmmaker Nasir Hussain,[75] and the Salim-Javed screenwriting duo,[74] pioneering the Bollywood-blockbuster format.[74] Yaadon Ki Baarat (1973), directed by Hussain and written by Salim-Javed, has been identified as the first masala film and the first quintessentially \"Bollywood\" film.[74][76] Salim-Javed wrote more successful masala films during the 1970s and 1980s.[74] Masala films made Amitabh Bachchan the biggest star of the period. A landmark of the genre was Amar Akbar Anthony (1977),[76][77] directed by Manmohan Desai and written by Kader Khan, and Desai continued successfully exploiting the genre.  Both genres (masala and violent-crime films) are represented by the blockbuster Sholay (1975), written by Salim-Javed and starring Dharmendra and Amitabh Bachchan. It combined the dacoit film conventions of Mother India and Gunga Jumna with spaghetti Westerns, spawning the Dacoit Western (also known as the curry Western) which was popular during the 1970s.[49]  Some Hindi filmmakers, such as Shyam Benegal, Mani Kaul, Kumar Shahani, Ketan Mehta, Govind Nihalani and Vijaya Mehta, continued to produce realistic parallel cinema throughout the 1970s.[44][78] Although the art film bent of the Film Finance Corporation was criticised during a 1976 Committee on Public Undertakings investigation which accused the corporation of not doing enough to encourage commercial cinema, the decade saw the rise of commercial cinema with films such as Sholay (1975) which consolidated Amitabh Bachchan's position as a star. The devotional classic Jai Santoshi Ma was also released that year.[79]  By 1983, the Bombay film industry was generating an estimated annual revenue of ₹700 crore (₹ 7 billion,[80] $693.14 million),[81] equivalent to $2.19 billion (₹12,667 crore, ₹ 111.33 billion) when adjusted for inflation. By 1986, India's annual film output had increased from 741 films produced annually to 833 films annually, making India the world's largest film producer.[82] The most internationally acclaimed Hindi film of the 1980s was Mira Nair's Salaam Bombay! (1988), which won the Camera d'Or at the 1988 Cannes Film Festival and was nominated for the Academy Award for Best Foreign Language Film.  Hindi cinema experienced another period of box-office decline during the late 1980s with due to concerns by audiences over increasing violence and a decline in musical quality, and a rise in video piracy. One of the turning points came with such films as Qayamat Se Qayamat Tak (1988), presenting a blend of youthfulness, family entertainment, emotional intelligence and strong melodies, all of which lured audiences back to the big screen.[83][84] It brought back the template for Bollywood musical romance films which went on to define 1990s Hindi cinema.[84]  Known since the 1990s as \"New Bollywood\",[85] contemporary Bollywood is linked to economic liberalization in India during the early 1990s.[86] Early in the decade, the pendulum swung back toward family-centered romantic musicals. Qayamat Se Qayamat Tak (1988) was followed by blockbusters such as Maine Pyar Kiya (1989), Hum Aapke Hain Kaun (1994), Dilwale Dulhania Le Jayenge (1995), Raja Hindustani (1996), Dil To Pagal Hai (1997) and Kuch Kuch Hota Hai (1998), introducing a new generation of popular actors, including the three Khans: Aamir Khan, Shah Rukh Khan, and Salman Khan,[87][88] who have starred in most of the top ten highest-grossing Bollywood films. The Khans and have had successful careers since the late 1980s and early 1990s,[87] and have dominated the Indian box office for three decades.[89][90] Shah Rukh Khan was the most successful Indian actor for most of the 1990s and 2000s, and Aamir Khan has been the most successful Indian actor since the mid 2000s.[54][91] Action and comedy films, starring such actors as Akshay Kumar and Govinda.[92]  The decade marked the entrance of new performers in art and independent films, some of which were commercially successful. The most influential example was Satya (1998), directed by Ram Gopal Varma and written by Anurag Kashyap. Its critical and commercial success led to the emergence of a genre known as Mumbai noir:[93] urban films reflecting the city's social problems.[94] This led to a resurgence of parallel cinema by the end of the decade.[93] The films featured actors whose performances were often praised by critics.  The 2000s saw increased Bollywood recognition worldwide due to growing (and prospering) NRI and South Asian diaspora communities overseas. The growth of the Indian economy and a demand for quality entertainment in this era led the country's film industry to new heights in production values, cinematography and screenwriting as well as technical advances in areas such as special effects and animation.[95] Some of the largest production houses, among them Yash Raj Films and Dharma Productions were the producers of new modern films.[95] Some popular films of the decade were Kaho Naa... Pyaar Hai (2000), Kabhi Khushi Kabhie Gham... (2001), Gadar: Ek Prem Katha (2001), Lagaan (2001), Koi... Mil Gaya (2003), Kal Ho Naa Ho (2003), Veer-Zaara (2004), Rang De Basanti (2006), Lage Raho Munna Bhai (2006), Dhoom 2 (2006), Krrish (2006), and Jab We Met (2007), among others, showing the rise of new movie stars.  During the 2010s, the industry saw established stars such as making big-budget masala films like Dabangg (2010), Singham (2011), Ek Tha Tiger (2012), Son of Sardaar (2012), Rowdy Rathore (2012), Chennai Express (2013), Kick (2014) and Happy New Year (2014) with much-younger actresses. Although the films were often not praised by critics, they were commercially successful. Some of the films starring Aamir Khan, from Taare Zameen Par (2007) and 3 Idiots (2009) to Dangal (2016) and Secret Superstar (2018), have been credited with redefining and modernising the masala film with a distinct brand of socially conscious cinema.[96][97]  Most stars from the 2000s continued successful careers into the next decade,[citation needed] and the 2010s saw a new generation of popular actors in different films. Among new conventions, female-centred films such as The Dirty Picture (2011), Kahaani (2012), and Queen (2014), Pink (2016), Raazi (2018), Gangubai Kathiawadi (2022) and Crew started gaining wide financial success.[98]  Moti Gokulsing and Wimal Dissanayake identify six major influences which have shaped Indian popular cinema:[99]  Sharmistha Gooptu identifies Indo-Persian-Islamic culture as a major influence. During the early 20th century, Urdu was the lingua franca of popular cultural performance across northern India and established in popular performance art traditions such as nautch dancing, Urdu poetry, and Parsi theater. Urdu and related Hindi dialects were the most widely understood across northern India, and Hindustani became the standard language of early Indian talkies. Films based on \"Persianate adventure-romances\" led to a popular genre of \"Arabian Nights cinema\".[101]  Scholars Chaudhuri Diptakirti and Rachel Dwyer and screenwriter Javed Akhtar identify Urdu literature as a major influence on Hindi cinema.[102][103][104] Most of the screenwriters and scriptwriters of classic Hindi cinema came from Urdu literary backgrounds,[102][103][105] from Khwaja Ahmad Abbas and Akhtar ul Iman to Salim–Javed and Rahi Masoom Raza; a handful came from other Indian literary traditions, such as Bengali and Hindi literature.[103] Most of Hindi cinema's classic scriptwriters wrote primarily in Urdu, including Salim-Javed, Gulzar, Rajinder Singh Bedi, Inder Raj Anand, Rahi Masoom Raza and Wajahat Mirza.[102][105] Urdu poetry and the ghazal tradition strongly influenced filmi (Bollywood lyrics).[102][104] Javed Akhtar was also greatly influenced by Urdu novels by Pakistani author Ibn-e-Safi, such as the Jasoosi Dunya and Imran series of detective novels;[106] they inspired, for example, famous Bollywood characters such as Gabbar Singh in Sholay (1975) and Mogambo in Mr. India (1987).[107]  Todd Stadtman identifies several foreign influences on 1970s commercial Bollywood masala films, including New Hollywood, Italian exploitation films, and Hong Kong martial arts cinema.[73] After the success of Bruce Lee films (such as Enter the Dragon) in India,[108] Deewaar (1975) and other Bollywood films incorporated fight scenes inspired by 1970s martial arts films from Hong Kong cinema until the 1990s.[109] Bollywood action scenes emulated Hong Kong rather than Hollywood, emphasising acrobatics and stunts and combining kung fu (as perceived by Indians) with Indian martial arts such as pehlwani.[110]  Perhaps Hindi cinema's greatest influence has been on India's national identity, where (with the rest of Indian cinema) it has become part of the \"Indian story\".[111] In India, Bollywood is often associated with India's national identity. According to economist and Bollywood biographer Meghnad Desai, \"Cinema actually has been the most vibrant medium for telling India its own story, the story of its struggle for independence, its constant struggle to achieve national integration and to emerge as a global presence\".[111]  Scholar Brigitte Schulze has written that Indian films, most notably Mehboob Khan's Mother India (1957), played a key role in shaping the Republic of India's national identity in the early years after independence from the British Raj; the film conveyed a sense of Indian nationalism to urban and rural citizens alike.[112] Bollywood has long influenced Indian society and culture as the biggest entertainment industry; many of the country's musical, dancing, wedding and fashion trends are Bollywood-inspired. Bollywood fashion trendsetters have included Madhubala in Mughal-e-Azam (1960) and Madhuri Dixit in Hum Aapke Hain Koun..! (1994).[87]  Hindi films have also had a socio-political impact on Indian society, reflecting Indian politics.[113] In classic 1970s Bollywood films, Bombay underworld crime films written by Salim–Javed and starring Amitabh Bachchan such as Zanjeer (1973) and Deewaar (1975) reflected the socio-economic and socio-political realities of contemporary India. They channeled growing popular discontent and disillusionment and state failure to ensure welfare and well-being at a time of inflation, shortages, loss of confidence in public institutions, increasing crime[65] and the unprecedented growth of slums.[69] Salim-Javed and Bachchan's films dealt with urban poverty, corruption and organised crime;[70] they were perceived by audiences as anti-establishment, often with an \"angry young man\" protagonist presented as a vigilante or anti-hero[71] whose suppressed rage voiced the anguish of the urban poor.[69]  Hindi films have been a significant form of soft power for India, increasing its influence and changing overseas perceptions of India.[114][115] In Germany, Indian stereotypes included bullock carts, beggars, sacred cows, corrupt politicians, and catastrophes before Bollywood and the IT industry transformed global perceptions of India.[116] According to author Roopa Swaminathan, \"Bollywood cinema is one of the strongest global cultural ambassadors of a new India.\"[115][117] Its role in expanding India's global influence is comparable to Hollywood's similar role with American influence.[87] Monroe Township, Middlesex County, New Jersey, in the New York metropolitan area, has been profoundly impacted by Bollywood; this U.S. township has displayed one of the fastest growth rates of its Indian population in the Western Hemisphere, increasing from 256 (0.9%) as of the 2000 Census[118] to an estimated 5,943 (13.6%) as of 2017,[119] representing a 2,221.5% (a multiple of 23) numerical increase over that period, including many affluent professionals and senior citizens as well as charitable benefactors to the COVID-19 relief efforts in India in official coordination with Monroe Township, as well as actors with second homes.  During the 2000s, Hindi cinema began influencing musical films in the Western world and was instrumental role in reviving the American musical film. Baz Luhrmann said that his musical film, Moulin Rouge! (2001), was inspired by Bollywood musicals;[120] the film incorporated a Bollywood-style dance scene with a song from the film China Gate. The critical and financial success of Moulin Rouge! began a renaissance of Western musical films such as Chicago, Rent, and Dreamgirls.[121]  Indian film composer A. R. Rahman wrote the music for Andrew Lloyd Webber's Bombay Dreams, and a musical version of Hum Aapke Hain Koun was staged in London's West End. The sports film Lagaan (2001) was nominated for the Academy Award for Best Foreign Language Film, and two other Hindi films (2002's Devdas and 2006's Rang De Basanti) were nominated for the BAFTA Award for Best Film Not in the English Language.  Danny Boyle's Slumdog Millionaire (2008), which won four Golden Globes and eight Academy Awards, was inspired by mainstream Hindi films[72][122] and is considered an \"homage to Hindi commercial cinema\".[123] It was also inspired by Mumbai-underworld crime films, such as Deewaar (1975), Satya (1998), Company (2002) and Black Friday (2007).[72] Deewaar had a Hong Kong remake, The Brothers (1979),[124] which inspired John Woo's internationally acclaimed breakthrough A Better Tomorrow (1986);[124][125] the latter was a template for Hong Kong action cinema's heroic bloodshed genre.[126][127] \"Angry young man\" 1970s epics such as Deewaar and Amar Akbar Anthony (1977) also resemble the heroic-bloodshed genre of 1980s Hong Kong action cinema.[128]  The influence of filmi may be seen in popular music worldwide. Technopop pioneers Haruomi Hosono and Ryuichi Sakamoto of the Yellow Magic Orchestra produced a 1978 electronic album, Cochin Moon, based on an experimental fusion of electronic music and Bollywood-inspired Indian music.[129] Truth Hurts' 2002 song \"Addictive\", produced by DJ Quik and Dr. Dre, was lifted[clarification needed] from Lata Mangeshkar's \"Thoda Resham Lagta Hai\" in Jyoti (1981).[130] The Black Eyed Peas' Grammy Award winning 2005 song \"Don't Phunk with My Heart\" was inspired by two 1970s Bollywood songs: \"Ye Mera Dil Yaar Ka Diwana\" from Don (1978) and \"Ae Nujawan Hai Sub\" from Apradh (1972).[131] Both songs were composed by Kalyanji Anandji, sung by Asha Bhosle, and featured the dancer Helen.[132]  The Kronos Quartet re-recorded several R. D. Burman compositions sung by Asha Bhosle for their 2005 album, You've Stolen My Heart: Songs from R.D. Burman's Bollywood, which was nominated for Best Contemporary World Music Album at the 2006 Grammy Awards. Filmi music composed by A. R. Rahman (who received two Academy Awards for the Slumdog Millionaire soundtrack) has frequently been sampled by other musicians, including the Singaporean artist Kelly Poon, the French rap group La Caution and the American artist Ciara. Many Asian Underground artists, particularly those among the overseas Indian diaspora, have also been inspired by Bollywood music.[133]  Hindi films are primarily musicals, and are expected to have catchy song-and-dance numbers woven into the script. A film's success often depends on the quality of such musical numbers.[134] A film's music and song and dance portions are usually produced first and these are often released before the film itself, increasing its audience.[135]  Indian audiences expect value for money, and a good film is generally referred to as paisa vasool, (literally \"money's worth\").[136] Songs, dances, love triangles, comedy and dare-devil thrills are combined in a three-hour show (with an intermission). These are called masala films, after the Hindi word for a spice mixture. Like masalas, they are a mixture of action, comedy and romance; most have heroes who can fight off villains single-handedly. Bollywood plots have tended to be melodramatic, frequently using formulaic ingredients such as star-crossed lovers, angry parents, love triangles, family ties, sacrifice, political corruption, kidnapping, villains, kind-hearted courtesans, long-lost relatives and siblings, reversals of fortune and serendipity.  Parallel cinema films tended to be less popular at the box office. A large Indian diaspora in English-speaking countries and increased Western influence in India have nudged Bollywood films closer to Hollywood.[137]  According to film critic Lata Khubchandani, \"Our earliest films ... had liberal doses of sex and kissing scenes in them. Strangely, it was after Independence the censor board came into being and so did all the strictures.\"[138] Although Bollywood plots feature Westernised urbanites dating and dancing in clubs rather than pre-arranged marriages, traditional Indian culture continues to exist outside the industry and is an element of resistance by some to Western influences.[137] Bollywood plays a major role, however, in Indian fashion.[137] Studies have indicated that some people, unaware that changing fashion in Bollywood films is often influenced by globalisation, consider the clothes worn by Bollywood actors as authentically Indian.[137]  Film scripts (known as dialogues in Indian English) and their song lyrics are often written by different people. Earlier, scripts were usually written in an unadorned Hindustani, which would be understood by the largest possible audience.[139] Post-Independence, Hindi films tended to use a colloquial register of Hindustani, mutually intelligible by Hindi and Urdu speakers, but the use of the latter has declined over years.[11][140] Some films have used regional dialects to evoke a village setting, or archaic Urdu in medieval historical films. A number of the dominant early scriptwriters of Hindi cinema primarily wrote in Urdu; Salim-Javed wrote in Urdu script, which was then transcribed by an assistant into Devanagari script so Hindi readers could read them.[102] During the 1970s, Urdu writers Krishan Chander and Ismat Chughtai said that \"more than seventy-five per cent of films are made in Urdu\" but were categorised as Hindi films by the government.[141] Encyclopedia of Hindi Cinema noted a number of top Urdu writers for preserving the language through film.[142] Urdu poetry has strongly influenced Hindi film songs, whose lyrics also draw from the ghazal tradition (filmi-ghazal).[104] According to Javed Akhtar in 1996, despite the loss of Urdu in Indian society, Urdu diction dominated Hindi film dialogue and lyrics.[143]  In her book, The Cinematic ImagiNation, Jyotika Virdi wrote about the presence and decline of Urdu in Hindi films. Virdi notes that although Urdu was widely used in classic Hindi cinema decades after partition because it was widely taught in pre-partition India, its use has declined in modern Hindi cinema: \"The extent of Urdu used in commercial Hindi cinema has not been stable ... the ultimate victory of Hindi in the official sphere has been more or less complete. This decline of Urdu is mirrored in Hindi films ... It is true that many Urdu words have survived and have become part of Hindi cinema's popular vocabulary. But that is as far as it goes. The fact is, for the most part, popular Hindi cinema has forsaken the florid Urdu that was part of its extravagance and retained a 'residual' Urdu\", affected by an aggressive state policy that promoted a Sanskritized version of Hindi as the national language.\"[144]  Contemporary mainstream films also use English; according to the article \"Bollywood Audiences Editorial\", \"English has begun to challenge the ideological work done by Urdu.\"[11][145] Some film scripts are first written in Latin script.[146] Characters may shift from one language to the other to evoke a particular atmosphere (for example, English in a business setting and Hindi in an informal one). The blend of Hindi and English sometimes heard in modern Hindi films, known as Hinglish, has become increasingly common.[140]  For years before the turn of the millennium and even after, cinematic language (in dialogues or lyrics) would often be melodramatic, invoking God, family, mother, duty, and self-sacrifice. Song lyrics are often about love and, especially in older films, frequently used the poetic vocabulary of court Urdu, with a number of Persian loanwords.[12] Another source for love lyrics in films such as Jhanak Jhanak Payal Baje and Lagaan is the long Hindu tradition of poetry about the loves of Krishna, Radha, and the gopis.  Music directors often prefer working with certain lyricists, and the lyricist and composer may be seen as a team. This phenomenon has been compared to the pairs of American composers and songwriters who created classic Broadway musicals.  In 2008 and before, Bollywood scripts were often handwritten because, in the industry, there is a perception that manual writing is the quickest way to create scripts.[147]  Sound in early Bollywood films was usually not recorded on location (sync sound). It was usually created (or re-created) in the studio,[148] with the actors speaking their lines in the studio and sound effects added later; this created synchronisation problems.[148] Commercial Indian films are known for their lack of ambient sound, and the Arriflex 3 camera necessitated dubbing. Lagaan (2001) was filmed with sync sound,[148] and several Bollywood films have recorded on-location sound since then.  In 1955, the Bollywood Cine Costume Make-Up Artist & Hair Dressers' Association (CCMAA) ruled that female makeup artists were barred from membership.[149] The Supreme Court of India ruled in 2014 that the ban violated Indian constitutional guarantees under Article 14 (right to equality), 19(1)(g) (freedom to work) and Article 21 (right to liberty).[149] According to the court, the ban had no \"rationale nexus\" to the cause sought to be achieved and was \"unacceptable, impermissible and inconsistent\" with the constitutional rights guaranteed to India's citizens.[149] The court also found illegal the rule which mandated that for any artist to work in the industry, they must have lived for five years in the state where they intend to work.[149] In 2015, it was announced that Charu Khurana was the first woman registered by the Cine Costume Make-Up Artist & Hair Dressers' Association.[150]  Bollywood film music is called filmi (from the Hindi \"of films\"). Bollywood songs were introduced with Ardeshir Irani's Alam Ara (1931) song, \"De De Khuda Ke Naam pay pyaare\".[151] Bollywood songs are generally pre-recorded by professional playback singers, with the actors then lip syncing the words to the song on-screen (often while dancing). Although most actors are good dancers, few are also singers; a notable exception was Kishore Kumar, who starred in several major films during the 1950s while having a rewarding career as a playback singer. K. L. Saigal, Suraiyya, and Noor Jehan were known as singers and actors, and some actors in the last thirty years have sung one or more songs themselves.  Songs can make and break a film, determining whether it will be a flop or a hit: \"Few films without successful musical tracks, and even fewer without any songs and dances, succeed\".[152] Globalization has changed Bollywood music, with lyrics an increasing mix of Hindi and English. Global trends such as salsa, pop and hip hop have influenced the music heard in Bollywood films.[152]  Playback singers are featured in the opening credits, and have fans who will see an otherwise-lackluster film to hear their favourites. Notable singers are Lata Mangeshkar, Asha Bhosle, Geeta Dutt, Shamshad Begum, Kavita Krishnamurthy, Sadhana Sargam, Alka Yagnik and Shreya Goshal (female), and K. L. Saigal, Kishore Kumar, Talat Mahmood, Mukesh, Mohammed Rafi, Manna Dey, Hemant Kumar, Kumar Sanu, Udit Narayan and Sonu Nigam (male). Composers of film music, known as music directors, are also well-known. Remixing of film songs with modern rhythms is common, and producers may release remixed versions of some of their films' songs with the films' soundtrack albums.  Dancing in Bollywood films, especially older films, is modeled on Indian dance: classical dance, dances of north-Indian courtesans (tawaif) or folk dances. In modern films, Indian dance blends with Western dance styles as seen on MTV or in Broadway musicals; Western pop and classical-dance numbers are commonly seen side-by-side in the same film. The hero (or heroine) often performs with a troupe of supporting dancers. Many song-and-dance routines in Indian films contain unrealistically-quick shifts of location or changes of costume between verses of a song. If the hero and heroine dance and sing a duet, it is often staged in natural surroundings or architecturally-grand settings.  Songs typically comment on the action taking place in the film. A song may be worked into the plot, so a character has a reason to sing. It may externalise a character's thoughts, or presage an event in the film (such as two characters falling in love). The songs are often referred to as a \"dream sequence\", with things happening which would not normally happen in the real world. Song and dance scenes were often filmed in Kashmir but, due to political unrest in Kashmir since the end of the 1980s,[153] they have been shot in western Europe (particularly Switzerland and Austria).[154][155]  Contemporary movie stars attracted popularity as dancers, including Madhuri Dixit, Hrithik Roshan, Aishwarya Rai Bachchan, Sridevi, Meenakshi Seshadri, Malaika Arora Khan, Shahid Kapoor, Katrina Kaif and Tiger Shroff. Older dancers include Helen[156] (known for her cabaret numbers), Madhubala, Vyjanthimala, Padmini, Hema Malini, Mumtaz, Cuckoo Moray,[157] Parveen Babi[158] , Waheeda Rahman,[159] Meena Kumari,[160] and Shammi Kapoor.[161]  Film producers have been releasing soundtracks (as tapes or CDs) before a film's release, hoping that the music will attract audiences; a soundtrack is often more popular than its film. Some producers also release music videos, usually (but not always) with a song from the film.  Bollywood films are multi-million dollar productions, with the most expensive productions costing up to ₹ 1 billion (about US$20 million). The science-fiction film Ra.One was made on a budget of ₹ 1.35 billion (about $27 million), making it the most expensive Bollywood film of all time.[162] Sets, costumes, special effects and cinematography were less than world-class, with some notable exceptions, until the mid-to-late 1990s. As Western films and television are more widely distributed in India, there is increased pressure for Bollywood films to reach the same production levels (particularly in action and special effects). Recent Bollywood films, like Krrish (2006), have employed international technicians such as Hong Kong-based action choreographer Tony Ching. The increasing accessibility of professional action and special effects, coupled with rising film budgets, have seen an increase in action and science-fiction films.  Since overseas scenes are attractive at the box office, Mumbai film crews are filming in Australia, Canada, New Zealand, the United Kingdom, the United States, Europe and elsewhere. Indian producers have also obtained funding for big-budget films shot in India, such as Lagaan and Devdas.  Funding for Bollywood films often comes from private distributors and a few large studios. Although Indian banks and financial institutions had been forbidden from lending to film studios, the ban has been lifted.[163] Finances are not regulated; some funding comes from illegitimate sources such as the Mumbai underworld, which is known to influence several prominent film personalities. Mumbai organised-crime hitmen shot Rakesh Roshan, a film director and father of star Hrithik Roshan, in January 2000. In 2001, the Central Bureau of Investigation seized all prints of Chori Chori Chupke Chupke after the film was found to be funded by members of the Mumbai underworld.[164]  Another problem facing Bollywood is widespread copyright infringement of its films. Often, bootleg DVD copies of movies are available before they are released in cinemas. Manufacturing of bootleg DVD, VCD, and VHS copies of the latest movie titles is an established small-scale industry in parts of south and southeast Asia. The Federation of Indian Chambers of Commerce and Industry (FICCI) estimates that the Bollywood industry loses $100 million annually from unlicensed home videos and DVDs. In addition to the homegrown market, demand for these copies is large amongst portions of the Indian diaspora. Bootleg copies are the only way people in Pakistan can watch Bollywood movies, since the Pakistani government has banned their sale, distribution and telecast. Films are frequently broadcast without compensation by small cable-TV companies in India and other parts of South Asia. Small convenience stores, run by members of the Indian diaspora in the US and the UK, regularly stock tapes and DVDs of dubious provenance; consumer copying adds to the problem. The availability of illegal copies of movies on the Internet also contributes to industry losses.  Satellite TV, television and imported foreign films are making inroads into the domestic Indian entertainment market. In the past, most Bollywood films could make money; now, fewer do. Most Bollywood producers make money, however, recouping their investments from many sources of revenue (including the sale of ancillary rights). There are increasing returns from theatres in Western countries like the United Kingdom, Canada, and the United States, where Bollywood is slowly being noticed. As more Indians migrate to these countries, they form a growing market for upscale Indian films. In 2002, Bollywood sold 3.6 billion tickets and had a total revenue (including theatre tickets, DVDs and television) of $1.3 billion; Hollywood films sold 2.6 billion tickets, and had a total revenue of $51 billion.  A number of Indian artists hand-painted movie billboards and posters. M. F. Husain painted film posters early in his career; human labour was found to be cheaper than printing and distributing publicity material.[165] Most of the large, ubiquitous billboards in India's major cities are now created with computer-printed vinyl. Old hand-painted posters, once considered ephemera, are collectible folk art.[165][166][167][168]  Releasing film music, or music videos, before a film's release may be considered a form of advertising. A popular tune is believed to help attract audiences.[169] Bollywood publicists use the Internet as a venue for advertising. Most bigger-budget films have a websites on which audiences can view trailers, stills and information on the story, cast, and crew.[170] Bollywood is also used to advertise other products. Product placement, used in Hollywood, is also common in Bollywood.[171]  Bollywood's increasing use of international settings such as Switzerland, London, Paris, New York, Mexico, Brazil and Singapore does not necessarily represent the people and cultures of those locales. Contrary to these spaces and geographies being filmed as they are, they are actually Indianised by adding Bollywood actors and Hindi speaking extras to them. While immersing in Bollywood films, viewers get to see their local experiences duplicated in different locations around the world.  According to Shakuntala Rao, \"Media representation can depict India's shifting relation with the world economy, but must retain its 'Indianness' in moments of dynamic hybridity\";[152] \"Indianness\" (cultural identity) poses a problem with Bollywood's popularity among varied diaspora audiences, but gives its domestic audience a sense of uniqueness from other immigrant groups.[172]  The Filmfare Awards are some of the most prominent awards given to Hindi films in India.[173] The Indian screen magazine Filmfare began the awards in 1954 (recognising the best films of 1953), and they were originally known as the Clare Awards after the magazine's editor. Modeled on the Academy of Motion Picture Arts and Sciences' poll-based merit format, individuals may vote in separate categories. A dual voting system was developed in 1956.[174]  The National Film Awards were also introduced in 1954. The Indian government has sponsored the awards, given by its Directorate of Film Festivals (DFF), since 1973. The DFF screens Bollywood films, films from the other regional movie industries, and independent\/art films. The awards are made at an annual ceremony presided over by the president of India. Unlike the Filmfare Awards, which are chosen by the public and a committee of experts, the National Film Awards are decided by a government panel.[175]  Other awards ceremonies for Hindi films in India are the Screen Awards (begun in 1995) and the Stardust Awards, which began in 2003. The International Indian Film Academy Awards (begun in 2000) and the Zee Cine Awards, begun in 1998, are held abroad in a different country each year.  In addition to their popularity among the Indian diaspora from Nigeria and Senegal to Egypt and Russia, generations of non-Indians have grown up with Bollywood.[176] Indian cinema's early contacts with other regions made inroads into the Soviet Union, the Middle East, Southeast Asia,[177] and China.[citation needed] Bollywood entered the consciousness of Western audiences and producers during the late 20th century,[95][178] and Western actors now seek roles in Bollywood films.[179]  Bollywood films are also popular in Pakistan, Bangladesh, and Nepal, where Hindustani is widely understood. Many Pakistanis understand Hindi, due to its linguistic similarity to Urdu.[180] Although Pakistan banned the import of Bollywood films in 1965, trade in unlicensed DVDs[181] and illegal cable broadcasts ensured their continued popularity. Exceptions to the ban were made for a few films, such as the colourised re-release of Mughal-e-Azam and Taj Mahal in 2006. Early in 2008, the Pakistani government permitted the import of 16 films.[182] More easing followed in 2009 and 2010. Although it is opposed by nationalists and representatives of Pakistan's small film industry, it is embraced by cinema owners who are making a profit after years of low receipts.[183] The most popular actors in Pakistan are the three Khans of Bollywood: Salman, Shah Rukh, and Aamir. The most popular actress is Madhuri Dixit;[184] at India-Pakistan cricket matches during the 1990s, Pakistani fans chanted \"Madhuri dedo, Kashmir lelo!\" (\"Give Madhuri, take Kashmir!\")[185] Bollywood films in Nepal earn more than Nepali films, and Salman Khan, Akshay Kumar and Shah Rukh Khan are popular in the country.  The films are also popular in Afghanistan due to its proximity to the Indian subcontinent and their cultural similarities, particularly in music. Popular actors include Shah Rukh Khan, Ajay Devgan, Sunny Deol, Aishwarya Rai, Preity Zinta, and Madhuri Dixit.[186] A number of Bollywood films were filmed in Afghanistan and some dealt with the country, including Dharmatma, Kabul Express, Khuda Gawah and Escape From Taliban.[187][188]  Bollywood films are popular in Southeast Asia, particularly in maritime Southeast Asia. The three Khans are very popular in the Malay world, including Indonesia, Malaysia, and Singapore. The films are also fairly popular in Thailand.[189]  India has cultural ties with Indonesia, and Bollywood films were introduced to the country at the end of World War II in 1945. The \"angry young man\" films of Amitabh Bachchan and Salim–Javed were popular during the 1970s and 1980s before Bollywood's popularity began gradually declining in the 1980s and 1990s. It experienced an Indonesian revival with the release of Shah Rukh Khan's Kuch Kuch Hota Hai (1998) in 2001, which was a bigger box-office success in the country than Titanic (1997). Bollywood has had a strong presence in Indonesia since then, particularly Shah Rukh Khan films such as Mohabbatein (2000), Kabhi Khushi Kabhie Gham... (2001), Kal Ho Naa Ho, Chalte Chalte and Koi... Mil Gaya (all 2003), and Veer-Zaara (2004).[190]  Some Bollywood films have been widely appreciated in China, Japan, and South Korea. Several Hindi films have been commercially successful in Japan, including Mehboob Khan's Aan (1952, starring Dilip Kumar) and Aziz Mirza's Raju Ban Gaya Gentleman (1992, starring Shah Rukh Khan). The latter sparked a two-year boom in Indian films after its 1997 release,[191] with Dil Se.. (1998) a beneficiary of the boom.[192] The highest-grossing Hindi film in Japan is 3 Idiots (2009), starring Aamir Khan,[193] which received a Japanese Academy Award nomination.[194] The film was also a critical and commercial success in South Korea.[195]  Dr. Kotnis Ki Amar Kahani, Awaara, and Do Bigha Zamin were successful in China during the 1940s and 1950s, and remain popular with their original audience. Few Indian films were commercially successful in the country during the 1970s and 1980s, among them Tahir Hussain's Caravan, Noorie and Disco Dancer.[196] Indian film stars popular in China included Raj Kapoor, Nargis,[197] and Mithun Chakraborty.[196] Hindi films declined significantly in popularity in China during the 1980s.[198] Films by Aamir Khan have recently been successful,[196][199] and Lagaan was the first Indian film with a nationwide Chinese release in 2011.[198][200] Chinese filmmaker He Ping was impressed by Lagaan (particularly its soundtrack), and hired its composer A. R. Rahman to score his Warriors of Heaven and Earth (2003).[201]  When 3 Idiots was released in China, China was the world's 15th-largest film market (partly due to its widespread pirate DVD distribution at the time). The pirate market introduced the film to Chinese audiences, however, and it became a cult hit. According to the Douban film-review site, 3 Idiots is China's 12th-most-popular film of all time; only one domestic Chinese film (Farewell My Concubine) ranks higher, and Aamir Khan acquired a large Chinese fan base as a result.[199] After 3 Idiots, several of Khan's other films (including 2007's Taare Zameen Par and 2008's Ghajini) also developed cult followings.[202] China became the world's second-largest film market (after the United States) by 2013, paving the way for Khan's box-office success with Dhoom 3 (2013), PK (2014), and Dangal (2016).[199] The latter is the 16th-highest-grossing film in China,[203] the fifth-highest-grossing non-English language film worldwide,[204] and the highest-grossing non-English foreign film in any market.[205][206][207] Several Khan films, including Taare Zameen Par, 3 Idiots, and Dangal, are highly rated on Douban.[208][209] His next film, Secret Superstar (2017, starring Zaira Wasim), broke Dangal's record for the highest-grossing opening weekend by an Indian film and cemented Khan's status[210] as \"a king of the Chinese box office\";[211] Secret Superstar was China's highest-grossing foreign film of 2018 to date.[212] Khan has become a household name in China,[213] with his success described as a form of Indian soft power[214] improving China–India relations despite political tensions.[197][210] With Bollywood competing with Hollywood in the Chinese market,[215] the success of Khan's films has driven up the price for Chinese distributors of Indian film imports.[216] Salman Khan's Bajrangi Bhaijaan and Irrfan Khan's Hindi Medium were also Chinese hits in early 2018.[217]  Although Bollywood is less successful on some Pacific islands such as New Guinea, it ranks second to Hollywood in Fiji (with its large Indian minority), Australia and New Zealand.[218] Australia also has a large South Asian diaspora, and Bollywood is popular amongst non-Asians in the country as well.[218] Since 1997, the country has been a backdrop for an increasing number of Bollywood films.[218] Indian filmmakers, attracted to Australia's diverse locations and landscapes, initially used the country as a setting for song-and-dance scenes;[218] however, Australian locations now figure in Bollywood film plots.[218] Hindi films shot in Australia usually incorporate Australian culture. Yash Raj Films' Salaam Namaste (2005), the first Indian film shot entirely in Australia, was the most successful Bollywood film of 2005 in that country.[219] It was followed by the box-office successes Heyy Babyy, (2007) Chak De! India (2007), and Singh Is Kinng (2008).[218] Prime Minister John Howard said during a visit to India after the release of Salaam Namaste that he wanted to encourage Indian filmmaking in Australia to increase tourism, and he appointed Steve Waugh as tourism ambassador to India.[220][failed verification] Australian actress Tania Zaetta, who appeared in Salaam Namaste and several other Bollywood films, was eager to expand her career in Bollywood.[221]  Bollywood films are popular in the former Soviet Union (Russia, Eastern Europe, and Central Asia),[222] and have been dubbed into Russian. Indian films were more popular in the Soviet Union than Hollywood films[223][224] and, sometimes, domestic Soviet films.[225] The first Indian film released in the Soviet Union was Dharti Ke Lal (1946), directed by Khwaja Ahmad Abbas and based on the Bengal famine of 1943, in 1949.[55] Three hundred Indian films were released in the Soviet Union after that;[226] most were Bollywood films with higher average audience figures than domestic Soviet productions.[224][227] Fifty Indian films had over 20 million viewers, compared to 41 Hollywood films.[228][229] Some, such as Awaara (1951) and Disco Dancer (1982), had more than 60 million viewers[230][231] and established actors Raj Kapoor, Nargis,[231] Rishi Kapoor[232] and Mithun Chakraborty in the country.[233]  According to diplomat Ashok Sharma, who served in the Commonwealth of Independent States,  The popularity of Bollywood in the CIS dates back to the Soviet days when the films from Hollywood and other Western cinema centers were banned in the Soviet Union. As there was no means of other cheap entertainment, the films from Bollywood provided the Soviets a cheap source of entertainment as they were supposed to be non-controversial and non-political. In addition, the Soviet Union was recovering from the onslaught of the Second World War. The films from India, which were also recovering from the disaster of partition and the struggle for freedom from colonial rule, were found to be a good source of providing hope with entertainment to the struggling masses. The aspirations and needs of the people of both countries matched to a great extent. These films were dubbed in Russian and shown in theatres throughout the Soviet Union. The films from Bollywood also strengthened family values, which was a big factor for their popularity with the government authorities in the Soviet Union.[234] After the collapse of the Soviet film-distribution system, Hollywood filled the void in the Russian film market and Bollywood's market share shrank.[222]  In Poland, Shah Rukh Khan has a large following. He was introduced to Polish audiences with the 2005 release of Kabhi Khushi Kabhie Gham... (2001) and his other films, including Dil Se.. (1998), Main Hoon Na (2004) and Kabhi Alvida Naa Kehna (2006), became hits in the country. Bollywood films are often covered in Gazeta Wyborcza, formerly Poland's largest newspaper.[235][236]  Squad (2021) is the first Indian film to be shot in Belarus. A majority of the film was shot at Belarusfilm studios, in Minsk.[237]  Hindi films have become popular in Arab countries,[238] and imported Indian films are usually subtitled in Arabic when they are released. Bollywood has progressed in Israel since the early 2000s, with channels dedicated to Indian films on cable television;[239] MBC Bollywood and Zee Aflam show Hindi movies and serials.[240]  In Egypt, Bollywood films were popular during the 1970s and 1980s. In 1987, however, they were restricted to a handful of films by the Egyptian government.[241][242] Amitabh Bachchan has remained popular in the country[243] and Indian tourists visiting Egypt are asked, \"Do you know Amitabh Bachchan?\"[184]  Bollywood movies are regularly screened in Dubai cinemas, and Bollywood is becoming popular in Turkey; Barfi! was the first Hindi film to have a wide theatrical release in that country.[244] Bollywood also has viewers in Central Asia (particularly Uzbekistan[245] and Tajikistan).[246]  Bollywood films are not influential in most of South America, although its culture and dance is recognised. Due to significant South Asian diaspora communities in Suriname and Guyana, however, Hindi-language movies are popular.[247] In 2006, Dhoom 2 became the first Bollywood film to be shot in Rio de Janeiro.[248] In January 2012, it was announced that UTV Motion Pictures would begin releasing films in Peru with Guzaarish.[249]  Hindi films were originally distributed to some parts of Africa by Lebanese businessmen.[176] In the 1950s, Hindi and Egyptian films were generally more popular than Hollywood films in East Africa. By the 1960s, East Africa was one of the largest overseas export markets for Indian films, accounting for about 20-50% of global earnings for many Indian films.[250]  Mother India (1957) continued to be screened in Nigeria decades after its release. Indian movies have influenced Hausa clothing, songs have been covered by Hausa singers, and stories have influenced Nigerian novelists. Stickers of Indian films and stars decorate taxis and buses in Nigeria's Northern Region, and posters of Indian films hang on the walls of tailoring shops and mechanics' garages. Unlike Europe and North America, where Indian films cater to the expatriate market, Bollywood films became popular in West Africa despite the lack of a significant Indian audience. One possible explanation is cultural similarity: the wearing of turbans, animals in markets; porters carrying large bundles, and traditional wedding celebrations. Within Muslim culture, Indian movies were said to show \"respect\" toward women; Hollywood movies were seen as having \"no shame\". In Indian movies, women are modestly dressed; men and women rarely kiss and there is no nudity, so the films are said to \"have culture\" which Hollywood lacks. The latter \"don't base themselves on the problems of the people\"; Indian films are based on socialist values and the reality of developing countries emerging from years of colonialism. Indian movies permitted a new youth culture without \"becoming Western.\"[176] The first Indian film shot in Mauritius was Souten, starring Rajesh Khanna, in 1983.[251]  In South Africa, film imports from India were watched by black and Indian audiences.[252] Several Bollywood figures have travelled to Africa for films and off-camera projects. Padmashree Laloo Prasad Yadav (2005) was filmed in South Africa.[253] Dil Jo Bhi Kahey... (2005) was also filmed almost entirely in Mauritius, which has a large ethnic-Indian population.  Bollywood, however, seems to be diminishing in popularity in Africa. New Bollywood films are more sexually explicit and violent. Nigerian viewers observed that older films (from the 1950s and 1960s) had more culture and were less Westernised.[176] The old days of India avidly \"advocating decolonization ... and India's policy was wholly influenced by his missionary zeal to end racial domination and discrimination in the African territories\" were replaced.[254] The emergence of Nollywood (West Africa's film industry) has also contributed to the declining popularity of Bollywood films, as sexualised Indian films became more like American films.  Kishore Kumar and Amitabh Bachchan have been popular in Egypt and Somalia.[255] In Ethiopia, Bollywood movies are shown with Hollywood productions in town square theatres such as the Cinema Ethiopia in Addis Ababa.[256] Less-commercial Bollywood films are also screened elsewhere in North Africa.[257]  The first Indian film to be released in the Western world and receive mainstream attention was Aan (1952), directed by Mehboob Khan and starring Dilip Kumar and Nimmi. It was subtitled in 17 languages and released in 28 countries,[252] including the United Kingdom,[258] the United States, and France.[259] Aan received significant praise from British critics, and The Times compared it favourably to Hollywood productions.[260] Mehboob Khan's later Academy Award-nominated Mother India (1957) was a success in overseas markets, including Europe,[260] Russia, the Eastern Bloc, French territories, and Latin America.[261]  Many Bollywood films have been commercially successful in the United Kingdom. The most successful Indian actor at the British box office has been Shah Rukh Khan, whose popularity in British Asian communities played a key role in introducing Bollywood to the UK[262] with films such as Darr (1993),[263] Dilwale Dulhaniya Le Jayenge (1995),[264] and Kuch Kuch Hota Hai (1998).[262] Dil Se (1998) was the first Indian film to enter the UK top ten.[262] A number of Indian films, such as Dilwale Dulhaniya Le Jayenge and Kabhi Khushi Kabhie Gham (2001), have been set in London.  Bollywood is also appreciated in France, Germany, the Netherlands,[265] and Scandinavia. Bollywood films are dubbed in German and shown regularly on the German television channel RTL II.[266] Germany is the second-largest European market for Indian films, after the United Kingdom. The most recognised Indian actor in Germany is Shah Rukh Khan, who has had box-office success in the country with films such as Don 2 (2011)[236] and Om Shanti Om (2007).[116] He has a large German fan base,[184] particularly in Berlin (where the tabloid Die Tageszeitung compared his popularity to that of the pope).[116]  Bollywood has experienced revenue growth in Canada and the United States, particularly in the South Asian communities of large cities such as Toronto, Chicago, and New York City.[95] Yash Raj Films, one of India's largest production houses and distributors, reported in September 2005 that Bollywood films in the United States earned about $100 million per year in theatre screenings, video sales and the sale of movie soundtracks;[95] Indian films earn more money in the United States than films from any other non-English speaking country.[95] Since the mid-1990s, a number of Indian films have been largely (or entirely) shot in New York, Los Angeles, Vancouver or Toronto. Films such as The Guru (2002) and Marigold: An Adventure in India (2007) attempted to popularise Bollywood for Hollywood.[citation needed]  Pressured by rushed production schedules and small budgets, some writers and musicians in Hindi cinema have been notorious to plagiarise.[267] Ideas, plot lines, tunes or riffs have been copied from other Indian film industries (including Telugu cinema, Tamil cinema, Malayalam cinema and others) or foreign films (including Hollywood and other Asian films) without acknowledging the source.[268]  Before the 1990s, plagiarism occurred with impunity. Copyright enforcement was lax in India, and few actors or directors saw an official contract.[269] The Hindi film industry was not widely known in the Global North (except in the Soviet states), who would be unaware that their material had been copied. Audiences may not have been aware of plagiarism, since many in India were unfamiliar with foreign films and music.[268] Although copyright enforcement in India is still somewhat lenient, Bollywood and other film industries are more aware of each other and Indian audiences are more familiar with foreign films and music.[citation needed] Organisations such as the India EU Film Initiative seek to foster a community between filmmakers and industry professionals in India and the European Union.[268]  A commonly-reported justification for plagiarism in Bollywood is that cautious producers want to remake popular Hollywood films in an Indian context. Although screenwriters generally produce original scripts, many are rejected due to uncertainty about whether a film will be successful.[268] Poorly-paid screenwriters have also been criticised for a lack of creativity.[270] Some filmmakers see plagiarism in Bollywood as an integral part of globalisation, with which Western (particularly American) culture is embedding itself into Indian culture.[270] Vikram Bhatt, director of Raaz (a remake of What Lies Beneath) and Kasoor (a remake of Jagged Edge), has spoken about the influence of American culture and Bollywood's desire to produce box-office hits based along the same lines: \"Financially, I would be more secure knowing that a particular piece of work has already done well at the box office. Copying is endemic everywhere in India. Our TV shows are adaptations of American programmes. We want their films, their cars, their planes, their Diet Cokes and also their attitude. The American way of life is creeping into our culture.\"[270] According to Mahesh Bhatt, \"If you hide the source, you're a genius. There's no such thing as originality in the creative sphere\".[270]  Although very few cases of film-copyright violations have been taken to court because of a slow legal process,[268] the makers of Partner (2007) and Zinda (2005) were targeted by the owners and distributors of the original films: Hitch and Oldboy.[271][272] The American studio 20th Century Fox brought Mumbai-based B. R. Films to court over the latter's forthcoming Banda Yeh Bindaas Hai, which Fox alleged was an illegal remake of My Cousin Vinny. B. R. Films eventually settled out of court for about $200,000, paving the way for its film's release.[273] Some studios comply with copyright law; in 2008, Orion Pictures secured the rights to remake Hollywood's Wedding Crashers.[274]  The Pakistani Qawwali musician Nusrat Fateh Ali Khan had a big impact on Hindi film music, inspiring numerous Indian musicians working in Bollywood, especially during the 1990s. However, there were many instances of Indian music directors plagiarising Khan's music to produce hit filmi songs.[275][276] Several popular examples include Viju Shah's hit song \"Tu Cheez Badi Hai Mast Mast\" in Mohra (1994) being plagiarised from Khan's popular Qawwali song \"Dam Mast Qalandar\",[275] \"Mera Piya Ghar Aya\" used in Yaarana (1995), and \"Sanoo Ek Pal Chain Na Aaye\" in Judaai (1997).[275] Despite the significant number of hit Bollywood songs plagiarised from his music, Nusrat Fateh Ali Khan was reportedly tolerant towards the plagiarism.[64][277] One of the Bollywood music directors who frequently plagiarised him, Anu Malik, claimed that he loved Khan's music and was actually showing admiration by using his tunes.[277] However, Khan was reportedly aggrieved when Malik turned his spiritual \"Allah Hoo, Allah Hoo\" into \"I Love You, I Love You\" in Auzaar (1997).[64] Khan said \"he has taken my devotional song Allahu and converted it into I love you. He should at least respect my religious songs.\"[277]  Bollywood soundtracks also plagiarised Guinean singer Mory Kanté, particularly his 1987 album Akwaba Beach. His song, \"Tama\", inspired two Bollywood songs: Bappi Lahiri's \"Tamma Tamma\" in Thanedaar (1990) and \"Jumma Chumma\" in Laxmikant–Pyarelal's soundtrack for Hum (1991). The latter also featured \"Ek Doosre Se\", which copied Kanté's \"Inch Allah\".[278] His song \"Yé ké yé ké\" was used as background music in the 1990 Bollywood film Agneepath, inspired the Bollywood song \"Tamma Tamma\" in Thanedaar.[278] "},"meta":{},"created_at":"2025-03-22T14:25:42.293407Z","updated_at":"2025-03-22T14:25:42.293407Z","inner_id":102,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":111,"annotations":[{"id":111,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.327895Z","updated_at":"2025-03-22T14:25:42.327895Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"dcc58dbd-76ee-47a5-a781-019a009264f7","import_id":null,"last_action":null,"bulk_created":false,"task":111,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  A documentary film (often described simply as a documentary) is a non-fictional motion picture intended to \"document reality, primarily for instruction, education or maintaining a historical record\".[1] The American author and media analyst Bill Nichols has characterized the documentary in terms of \"a filmmaking practice, a cinematic tradition, and mode of audience reception [that remains] a practice without clear boundaries\".[2]  Research into information gathering, as a behavior, and the sharing of knowledge, as a concept, has noted how documentary movies were preceded by the notable practice of documentary photography. This has involved the use of singular photographs to detail the complex attributes of historical events and continues to a certain degree to this day, with an example being the conflict-related photography achieved by popular figures such as Mathew Brady during the American Civil War. Documentary movies evolved from the creation of singular images in order to convey particular types of information in depth, using film as a medium.  Early documentary films, originally called \"actuality films\", briefly lasted for one minute or less in most cases. While faithfully depicting true events, these releases possessed no narrative structure per se and were of limited interest. Over time, documentaries have evolved to become longer in length and to include more categories of information. Some examples are explicitly educational, while others serve as observational works; docufiction movies notably include aspects of dramatic storytelling that are clearly fictional. Documentaries are informative at times, and certain types are often used within schools as a resource to teach various principles. Documentary filmmakers have a responsibility to be truthful to their vision of the world without intentionally misrepresenting a topic.  Social media organizations such as Dailymotion and YouTube, with many of these platforms receiving popular interest, have provided an avenue for the growth of documentaries as a particular film genre. Such platforms have increased the distribution area and ease-of-accessibility given the ability of online video sharing to spread to multiple audiences at once as well as to work past certain socio-political hurdles such as censorship.  Polish writer and filmmaker Bolesław Matuszewski was among those who identified the mode of documentary film. He wrote two of the earliest texts on cinema, Une nouvelle source de l'histoire (\"A New Source of History\") and La photographie animée (\"Animated photography\"). Both were published in 1898 in French and were among the earliest written works to consider the historical and documentary value of the film.[3] Matuszewski is also among the first filmmakers to propose the creation of a Film Archive to collect and keep safe visual materials.[4]  The word \"documentary\" was coined by Scottish documentary filmmaker John Grierson in his review of Robert Flaherty's film Moana (1926), published in the New York Sun on 8 February 1926, written by \"The Moviegoer\" (a pen name for Grierson).[5][6]  Grierson's principles of documentary were that cinema's potential for observing life could be exploited in a new art form; that the \"original\" actor and \"original\" scene are better guides than their fiction counterparts for interpreting the modern world; and that materials \"thus taken from the raw\" can be more real than the acted article. In this regard, Grierson's definition of documentary as \"creative treatment of actuality\"[7] has gained some acceptance; however, this position is at variance with Soviet film-maker Dziga Vertov's credos of provocation to present \"life as it is\" (that is, life filmed surreptitiously), and \"life caught unawares\" (life provoked or surprised by the camera).  The American film critic Pare Lorentz defines a documentary film as \"a factual film which is dramatic.\"[8] Others further state that a documentary stands out from the other types of non-fiction films for providing an opinion, and a specific message, along with the facts it presents.[9] Scholar Betsy McLane asserted that documentaries are for filmmakers to convey their views about historical events, people, and places which they find significant.[10] Therefore, the advantage of documentaries lies in introducing new perspectives which may not be prevalent in traditional media such as written publications and school curricula.[11]  Documentary practice is the complex process of creating documentary projects. It refers to what people do with media devices, content, form, and production strategies to address the creative, ethical, and conceptual problems and choices that arise as they make documentaries.  Documentary filmmaking can be used as a form of journalism, advocacy, or personal expression.  Early film (pre-1900) was dominated by the novelty of showing an event. Single-shot moments were captured on film, such as a train entering a station, a boat docking, or factory workers leaving work. These short films were called \"actuality\" films; the term \"documentary\" was not coined until 1926. Many of the first films, such as those made by Auguste and Louis Lumière, were a minute or less in length, due to technological limitations. Examples can be viewed on YouTube.  Films showing many people (for example, leaving a factory) were often made for commercial reasons: the people being filmed were eager to see, for payment, the film showing them. One notable film clocked in at over an hour and a half, The Corbett-Fitzsimmons Fight. Using pioneering film-looping technology, Enoch J. Rector presented the entirety of a famous 1897 prize-fight on cinema screens across the United States.  In May 1896, Bolesław Matuszewski recorded on film a few surgical operations in Warsaw and Saint Petersburg hospitals. In 1898, French surgeon Eugène-Louis Doyen invited Matuszewski and Clément Maurice to record his surgical operations. They started in Paris a series of surgical films sometime before July 1898.[12] Until 1906, the year of his last film, Doyen recorded more than 60 operations. Doyen said that his first films taught him how to correct professional errors he had been unaware of. For scientific purposes, after 1906, Doyen combined 15 of his films into three compilations, two of which survive, the six-film series Extirpation des tumeurs encapsulées (1906), and the four-film Les Opérations sur la cavité crânienne (1911). These and five other of Doyen's films survive.[13]  Between July 1898 and 1901, the Romanian professor Gheorghe Marinescu made several science films in his neurology clinic in Bucharest:[14] Walking Troubles of Organic Hemiplegy (1898), The Walking Troubles of Organic Paraplegies (1899), A Case of Hysteric Hemiplegy Healed Through Hypnosis (1899), The Walking Troubles of Progressive Locomotion Ataxy (1900), and Illnesses of the Muscles (1901). All these short films have been preserved. The professor called his works \"studies with the help of the cinematograph,\" and published the results, along with several consecutive frames, in issues of La Semaine Médicale magazine from Paris, between 1899 and 1902.[15] In 1924, Auguste Lumière recognized the merits of Marinescu's science films: \"I've seen your scientific reports about the usage of the cinematograph in studies of nervous illnesses, when I was still receiving La Semaine Médicale, but back then I had other concerns, which left me no spare time to begin biological studies. I must say I forgot those works and I am thankful to you that you reminded them to me. Unfortunately, not many scientists have followed your way.\"[16][17][18]  Travelogue films were very popular in the early part of the 20th century. They were often referred to by distributors as \"scenics\". Scenics were among the most popular sort of films at the time.[19] An important early film which moved beyond the concept of the scenic was In the Land of the Head Hunters (1914), which embraced primitivism and exoticism in a staged story presented as truthful re-enactments of the life of Native Americans.  Contemplation is a separate area.[further explanation needed] Pathé was the best-known global manufacturer of such films in the early 20th century. A vivid example is Moscow Clad in Snow (1909).  Biographical documentaries appeared during this time, such as the feature Eminescu-Veronica-Creangă (1914) on the relationship between the writers Mihai Eminescu, Veronica Micle and Ion Creangă (all deceased at the time of the production), released by the Bucharest chapter of Pathé.  Early color motion picture processes such as Kinemacolor (known for the feature With Our King and Queen Through India (1912)) and Prizma Color (known for Everywhere With Prizma (1919) and the five-reel feature Bali the Unknown (1921)) used travelogues to promote the new color processes. In contrast, Technicolor concentrated primarily on getting their process adopted by Hollywood studios for fiction feature films.  Also during this period, Frank Hurley's feature documentary film, South (1919) about the Imperial Trans-Antarctic Expedition was released. The film documented the failed Antarctic expedition led by Ernest Shackleton in 1914.  With Robert J. Flaherty's Nanook of the North in 1922, documentary film embraced romanticism. Flaherty filmed a number of heavily staged romantic documentary films during this time period, often showing how his subjects would have lived 100 years earlier and not how they lived right then. For instance, in Nanook of the North, Flaherty did not allow his subjects to shoot a walrus with a nearby shotgun, but had them use a harpoon instead. Some of Flaherty's staging, such as building a roofless igloo for interior shots, was done to accommodate the filming technology of the time.  Paramount Pictures tried to repeat the success of Flaherty's Nanook and Moana with two romanticized documentaries, Grass (1925) and Chang (1927), both directed by Merian C. Cooper and Ernest Schoedsack.  The \"city symphony\" sub film genre consisted of avant-garde films during the 1920s and 1930s. These films were particularly influenced by modern art, namely Cubism, Constructivism, and Impressionism.[20] According to art historian and author Scott MacDonald,[21] city symphony films can be described as, \"An intersection between documentary and avant-garde film: an avant-doc\"; however, A.L. Rees suggests regarding them as avant-garde films.[20]  Early titles produced within this genre include: Manhatta (New York; dir. Paul Strand, 1921); Rien que les heures\/Nothing But The Hours (France; dir. Alberto Cavalcanti, 1926); Twenty Four Dollar Island (dir. Robert J. Flaherty, 1927); Moscow (dir. Mikhail Kaufman, 1927); Études sur Paris (dir. André Sauvage, 1928); The Bridge (1928) and Rain (1929), both by Joris Ivens; São Paulo, Sinfonia da Metrópole (dir. Adalberto Kemeny, 1929), Berlin: Symphony of a Metropolis (dir. Walter Ruttmann, 1927); Man with a Movie Camera (dir. Dziga Vertov, 1929); Douro, Faina Fluvial (dir. Manoel de Oliveira, 1931); and Rhapsody in Two Languages (dir. Gordon Sparling, 1934).  A city symphony film, as the name suggests, is most often based around a major metropolitan city area and seeks to capture the life, events and activities of the city. It can use abstract cinematography (Walter Ruttman's Berlin) or may use Soviet montage theory (Dziga Vertov's, Man with a Movie Camera).  Most importantly, a city symphony film is a form of cinepoetry, shot and edited in the style of a \"symphony\".  The European continental tradition (See: Realism) focused on humans within human-made environments, and included the so-called city symphony films such as Walter Ruttmann's, Berlin: Symphony of a Metropolis (of which Grierson noted in an article[22] that Berlin, represented what a documentary should not be); Alberto Cavalcanti's, Rien que les heures; and Dziga Vertov's Man with a Movie Camera. These films tend to feature people as products of their environment, and lean towards the avant-garde.  Dziga Vertov was central to the Soviet Kino-Pravda (literally, \"cinematic truth\") newsreel series of the 1920s. Vertov believed the camera – with its varied lenses, shot-counter shot editing, time-lapse, ability to slow motion, stop motion and fast-motion – could render reality more accurately than the human eye, and created a film philosophy from it.  The newsreel tradition is important in documentary film. Newsreels at this time were sometimes staged but were usually re-enactments of events that had already happened, not attempts to steer events as they were in the process of happening. For instance, much of the battle footage from the early 20th century was staged; the cameramen would usually arrive on site after a major battle and re-enact scenes to film them.  The propagandist tradition consists of films made with the explicit purpose of persuading an audience of a point. One of the most celebrated and controversial propaganda films is Leni Riefenstahl's film Triumph of the Will (1935), which chronicled the 1934 Nazi Party Congress and was commissioned by Adolf Hitler. Leftist filmmakers Joris Ivens and Henri Storck directed Borinage (1931) about the Belgian coal mining region. Luis Buñuel directed a \"surrealist\" documentary Las Hurdes (1933).  Pare Lorentz's The Plow That Broke the Plains (1936) and The River (1938) and Willard Van Dyke's The City (1939) are notable New Deal productions, each presenting complex combinations of social and ecological awareness, government propaganda, and leftist viewpoints. Frank Capra's Why We Fight (1942–1944) series was a newsreel series in the United States, commissioned by the government to convince the U.S. public that it was time to go to war. Constance Bennett and her husband Henri de la Falaise produced two feature-length documentaries, Legong: Dance of the Virgins (1935) filmed in Bali, and Kilou the Killer Tiger (1936) filmed in Indochina.  In Canada, the Film Board, set up by John Grierson, was set up for the same propaganda reasons. It also created newsreels that were seen by their national governments as legitimate counter-propaganda to the psychological warfare of Nazi Germany orchestrated by Joseph Goebbels.  In Britain, a number of different filmmakers came together under John Grierson. They became known as the Documentary Film Movement. Grierson, Alberto Cavalcanti, Harry Watt, Basil Wright, and Humphrey Jennings amongst others succeeded in blending propaganda, information, and education with a more poetic aesthetic approach to documentary. Examples of their work include Drifters (John Grierson), Song of Ceylon (Basil Wright), Fires Were Started, and A Diary for Timothy (Humphrey Jennings). Their work involved poets such as W. H. Auden, composers such as Benjamin Britten, and writers such as J. B. Priestley. Among the best known films of the movement are Night Mail and Coal Face.  Calling Mr. Smith (1943) is an anti-Nazi color film[23][24][25] created by Stefan Themerson which is both a documentary and an avant-garde film against war. It was one of the first anti-Nazi films in history.[citation needed]  Cinéma vérité (or the closely related direct cinema) was dependent on some technical advances to exist: light, quiet and reliable cameras, and portable sync sound.  Cinéma vérité and similar documentary traditions can thus be seen, in a broader perspective, as a reaction against studio-based film production constraints. Shooting on location, with smaller crews, would also happen in the French New Wave, the filmmakers taking advantage of advances in technology allowing smaller, handheld cameras and synchronized sound to film events on location as they unfolded.  Although the terms are sometimes used interchangeably, there are important differences between cinéma vérité (Jean Rouch) and the North American \"direct cinema\" (or more accurately \"cinéma direct\"), pioneered by, among others, Canadians Michel Brault, Pierre Perrault and Allan King,[29] and Americans Robert Drew, Richard Leacock, Frederick Wiseman and Albert and David Maysles.  The directors of the movement take different viewpoints on their degree of involvement with their subjects. Kopple and Pennebaker, for instance, choose non-involvement (or at least no overt involvement), and Perrault, Rouch, Koenig, and Kroitor favor direct involvement or even provocation when they deem it necessary.  The films Chronicle of a Summer (Jean Rouch), Dont Look Back (D. A. Pennebaker), Grey Gardens (Albert and David Maysles), Titicut Follies (Frederick Wiseman), Primary and Crisis: Behind a Presidential Commitment (both produced by Robert Drew), Harlan County, USA (directed by Barbara Kopple), Lonely Boy (Wolf Koenig and Roman Kroitor) are all frequently deemed cinéma vérité films.  The fundamentals of the style include following a person during a crisis with a moving, often handheld, camera to capture more personal reactions. There are no sit-down interviews, and the shooting ratio (the amount of film shot to the finished product) is very high, often reaching 80 to one. From there, editors find and sculpt the work into a film. The editors of the movement – such as Werner Nold, Charlotte Zwerin, Muffie Meyer, Susan Froemke, and Ellen Hovde – are often overlooked, but their input to the films was so vital that they were often given co-director credits.  Famous cinéma vérité\/direct cinema films include Les Raquetteurs,[30] Showman, Salesman, Near Death, and The Children Were Watching.  In the 1960s and 1970s, documentary film was often regarded as a political weapon against neocolonialism and capitalism in general, especially in Latin America, but also in a changing society. La Hora de los hornos (The Hour of the Furnaces, from 1968), directed by Octavio Getino and Fernando Solanas, influenced a whole generation of filmmakers. Among the many political documentaries produced in the early 1970s was \"Chile: A Special Report\", public television's first in-depth expository look at the September 1973 overthrow of the Salvador Allende government in Chile by military leaders under Augusto Pinochet, produced by documentarians Ari Martinez and José Garcia.  A June 2020 article in The New York Times reviewed the political documentary And She Could Be Next, directed by Grace Lee and Marjan Safinia. The Times described the documentary not only as focusing on women in politics, but more specifically on women of color, their communities, and the significant changes they have wrought upon America.[31]  Box office analysts have noted that the documentary film genre has become increasingly successful in theatrical release with films such as Fahrenheit 9\/11, Super Size Me, Food, Inc., Earth, March of the Penguins, and An Inconvenient Truth among the most prominent examples. Compared to dramatic narrative films, documentaries typically have far lower budgets which makes them attractive to film companies because even a limited theatrical release can be highly profitable.  The nature of documentary films has expanded in the past 30 years from the cinéma vérité style introduced in the 1960s in which the use of portable camera and sound equipment allowed an intimate relationship between filmmaker and subject. The line blurs between documentary and narrative and some works are very personal, such as Marlon Riggs's Tongues Untied (1989) and Black Is...Black Ain't (1995), which mix expressive, poetic, and rhetorical elements and stresses subjectivities rather than historical materials.[32]  Historical documentaries, such as the landmark 14-hour Eyes on the Prize: America's Civil Rights Years (1986 – Part 1 and 1989 – Part 2) by Henry Hampton, 4 Little Girls (1997) by Spike Lee, The Civil War by Ken Burns, and UNESCO-awarded independent film on slavery 500 Years Later, express not only a distinctive voice but also a perspective and point of views. Some films such as The Thin Blue Line by Errol Morris incorporate stylized re-enactments, and Michael Moore's Roger & Me place far more interpretive control with the director. The commercial success of these documentaries may derive from this narrative shift in the documentary form, leading some critics to question whether such films can truly be called documentaries; critics sometimes refer to these works as \"mondo films\" or \"docu-ganda.\"[33] However, directorial manipulation of documentary subjects has been noted since the work of Flaherty, and may be endemic to the form due to problematic ontological foundations.  Documentary filmmakers are increasingly using social impact campaigns with their films.[34] Social impact campaigns seek to leverage media projects by converting public awareness of social issues and causes into engagement and action, largely by offering the audience a way to get involved.[35] Examples of such documentaries include Kony 2012, Salam Neighbor, Gasland, Living on One Dollar, and Girl Rising.  Although documentaries are financially more viable with the increasing popularity of the genre and the advent of the DVD, funding for documentary film production remains elusive. Within the past decade, the largest exhibition opportunities have emerged from within the broadcast market, making filmmakers beholden to the tastes and influences of the broadcasters who have become their largest funding source.[36]  Modern documentaries have some overlap with television forms, with the development of \"reality television\" that occasionally verges on the documentary but more often veers to the fictional or staged. The \"making-of\" documentary shows how a movie or a computer game was produced. Usually made for promotional purposes, it is closer to an advertisement than a classic documentary.  Modern lightweight digital video cameras and computer-based editing have greatly aided documentary makers, as has the dramatic drop in equipment prices. The first film to take full advantage of this change was Martin Kunert and Eric Manes' Voices of Iraq, where 150 DV cameras were sent to Iraq during the war and passed out to Iraqis to record themselves.  Films in the documentary form without words have been made. Listen to Britain, directed by Humphrey Jennings and Stuart McAllister in 1942, is a wordless meditation on wartime Britain. From 1982, the Qatsi trilogy and the similar Baraka could be described as visual tone poems, with music related to the images, but no spoken content. Koyaanisqatsi (part of the Qatsi trilogy) consists primarily of slow motion and time-lapse photography of cities and many natural landscapes across the United States. Baraka tries to capture the great pulse of humanity as it flocks and swarms in daily activity and religious ceremonies.  Bodysong was made in 2003 and won a British Independent Film Award for \"Best British Documentary.\"  The 2004 film Genesis shows animal and plant life in states of expansion, decay, sex, and death, with some, but little, narration.  The traditional style for narration is to have a dedicated narrator read a script which is dubbed onto the audio track. The narrator never appears on camera and may not necessarily have knowledge of the subject matter or involvement in the writing of the script.  This style of narration uses title screens to visually narrate the documentary. The screens are held for about 5–10 seconds to allow adequate time for the viewer to read them. They are similar to the ones shown at the end of movies based on true stories, but they are shown throughout, typically between scenes.  In this style, there is a host who appears on camera, conducts interviews, and who also does voice-overs.  The release of The Thin Blue Line (1988) directed by Errol Morris introduced possibilities for emerging forms of the hybrid documentary. Indeed, it was disqualified for an Academy Award because of the stylized recreations. Traditional documentary filmmaking typically removes signs of fictionalization to distinguish itself from fictional film genres. Audiences have recently become more distrustful of the media's traditional fact production, making them more receptive to experimental ways of telling facts. The hybrid documentary implements truth games to challenge traditional fact production. Although it is fact-based, the hybrid documentary is not explicit about what should be understood, creating an open dialogue between subject and audience.[37] Clio Barnard's The Arbor (2010), Joshua Oppenheimer's The Act of Killing (2012), Mads Brügger's The Ambassador, and Alma Har'el's Bombay Beach (2011) are a few notable examples.[37]  Docufiction is a hybrid genre from two basic ones, fiction film and documentary, practiced since the first documentary films were made.  Fake-fiction is a genre which deliberately presents real, unscripted events in the form of a fiction film, making them appear as staged. The concept was introduced[38] by Pierre Bismuth to describe his 2016 film Where is Rocky II?  A DVD documentary is a documentary film of indeterminate length that has been produced with the sole intent of releasing it for direct sale to the public on DVD, which is different from a documentary being made and released first on television or on a cinema screen (a.k.a. theatrical release) and subsequently on DVD for public consumption.  This form of documentary release is becoming more popular and accepted as costs and difficulty with finding TV or theatrical release slots increases. It is also commonly used for more \"specialist\" documentaries, which might not have general interest to a wider TV audience. Examples are military, cultural arts, transport, sports, animals, etc.  Compilation films were pioneered in 1927 by Esfir Schub with The Fall of the Romanov Dynasty. More recent examples include Point of Order! (1964), directed by Emile de Antonio about the McCarthy hearings. Similarly, The Last Cigarette combines the testimony of various tobacco company executives before the U.S. Congress with archival propaganda extolling the virtues of smoking.  Poetic documentaries, which first appeared in the 1920s, were a sort of reaction against both the content and the rapidly crystallizing grammar of the early fiction film. The poetic mode moved away from continuity editing and instead organized images of the material world by means of associations and patterns, both in terms of time and space. Well-rounded characters – \"lifelike people\" – were absent; instead, people appeared in these films as entities, just like any other, that are found in the material world. The films were fragmentary, impressionistic, lyrical. Their disruption of the coherence of time and space – a coherence favored by the fiction films of the day – can also be seen as an element of the modernist counter-model of cinematic narrative. The \"real world\" – Nichols calls it the \"historical world\" – was broken up into fragments and aesthetically reconstituted using film form. Examples of this style include Joris Ivens' Rain (1928), which records a passing summer shower over Amsterdam; László Moholy-Nagy's Play of Light: Black, White, Grey (1930), in which he films one of his own kinetic sculptures, emphasizing not the sculpture itself but the play of light around it; Oskar Fischinger's abstract animated films; Francis Thompson's N.Y., N.Y. (1957), a city symphony film; and Chris Marker's Sans Soleil (1982).  Expository documentaries speak directly to the viewer, often in the form of an authoritative commentary employing voiceover or titles, proposing a strong argument and point of view. These films are rhetorical, and try to persuade the viewer. (They may use a rich and sonorous male voice.) The (voice-of-God) commentary often sounds \"objective\" and omniscient. Images are often not paramount; they exist to advance the argument. The rhetoric insistently presses upon us to read the images in a certain fashion. Historical documentaries in this mode deliver an unproblematic and \"objective\" account and interpretation of past events.  Examples: TV shows and films like Biography, America's Most Wanted, many science and nature documentaries, Ken Burns' The Civil War (1990), Robert Hughes' The Shock of the New (1980), John Berger's Ways of Seeing (1972), Frank Capra's wartime Why We Fight series, and Pare Lorentz's The Plow That Broke the Plains (1936).  Observational documentaries attempt to spontaneously observe their subjects with minimal intervention. Filmmakers who worked in this subgenre often saw the poetic mode as too abstract and the expository mode as too didactic. The first observational docs date back to the 1960s; the technological developments which made them possible include mobile lightweight cameras and portable sound recording equipment for synchronized sound. Often, this mode of film eschewed voice-over commentary, post-synchronized dialogue and music, or re-enactments. The films aimed for immediacy, intimacy, and revelation of individual human character in ordinary life situations.  Participatory documentaries believe that it is impossible for the act of filmmaking to not influence or alter the events being filmed. What these films do is emulate the approach of the anthropologist: participant-observation. Not only is the filmmaker part of the film, we also get a sense of how situations in the film are affected or altered by their presence. Nichols: \"The filmmaker steps out from behind the cloak of voice-over commentary, steps away from poetic meditation, steps down from a fly-on-the-wall perch, and becomes a social actor (almost) like any other. (Almost like any other because the filmmaker retains the camera, and with it, a certain degree of potential power and control over events.)\" The encounter between filmmaker and subject becomes a critical element of the film. Rouch and Morin named the approach cinéma vérité, translating Dziga Vertov's kinopravda into French; the \"truth\" refers to the truth of the encounter rather than some absolute truth.  Reflexive documentaries do not see themselves as a transparent window on the world; instead, they draw attention to their own constructedness, and the fact that they are representations. How does the world get represented by documentary films? This question is central to this subgenre of films. They prompt us to \"question the authenticity of documentary in general.\" It is the most self-conscious of all the modes, and is highly skeptical of \"realism\". It may use Brechtian alienation strategies to jar us, in order to \"defamiliarize\" what we are seeing and how we are seeing it.  Performative documentaries stress subjective experience and emotional response to the world. They are strongly personal, unconventional, perhaps poetic and\/or experimental, and might include hypothetical enactments of events designed to make us experience what it might be like for us to possess a certain specific perspective on the world that is not our own, e.g. that of black, gay men in Marlon Riggs's Tongues Untied (1989) or Jenny Livingston's Paris Is Burning (1991). This subgenre might also lend itself to certain groups (e.g. women, ethnic minorities, gays and lesbians, etc.) to \"speak about themselves\". Often, a battery of techniques, many borrowed from fiction or avant-garde films, are used. Performative docs often link up personal accounts or experiences with larger political or historical realities.  Documentaries are shown in schools around the world in order to educate students. Used to introduce various topics to children, they are often used with a school lesson or shown many times to reinforce an idea.  There are several challenges associated with translation of documentaries. The main two are working conditions and problems with terminology.  Documentary translators very often have to meet tight deadlines. Normally, the translator has between five and seven days to hand over the translation of a 90-minute programme. Dubbing studios typically give translators a week to translate a documentary, but in order to earn a good salary, translators have to deliver their translations in a much shorter period, usually when the studio decides to deliver the final programme to the client sooner or when the broadcasting channel sets a tight deadline, e.g. on documentaries discussing the latest news.[39]  Another problem is the lack of postproduction script or the poor quality of the transcription. A correct transcription is essential for a translator to do their work properly, however many times the script is not even given to the translator, which is a major impediment since documentaries are characterised by \"the abundance of terminological units and very specific proper names\".[40] When the script is given to the translator, it is usually poorly transcribed or outright incorrect making the translation unnecessarily difficult and demanding because all of the proper names and specific terminology have to be correct in a documentary programme in order for it to be a reliable source of information, hence the translator has to check every term on their own. Such mistakes in proper names are for instance: \"Jungle Reinhard instead of Django Reinhart, Jorn Asten instead of Jane Austen, and Magnus Axle instead of Aldous Huxley\".[40]  The process of translation of a documentary programme requires working with very specific, often scientific terminology. Documentary translators are not usually specialists in a given field. Therefore, they are compelled to undertake extensive research whenever asked to make a translation of a specific documentary programme in order to understand it correctly and deliver the final product free of mistakes and inaccuracies. Generally, documentaries contain a large number of specific terms, with which translators have to familiarise themselves on their own, for example:  The documentary Beetles, Record Breakers makes use of 15 different terms to refer to beetles in less than 30 minutes (longhorn beetle, cellar beetle, stag beetle, burying beetle or gravediggers, sexton beetle, tiger beetle, bloody nose beetle, tortoise beetle, diving beetle, devil's coach horse, weevil, click beetle, malachite beetle, oil beetle, cockchafer), apart from mentioning other animals such as horseshoe bats or meadow brown butterflies.[41]  This poses a real challenge for the translators because they have to render the meaning, i.e. find an equivalent, of a very specific, scientific term in the target language and frequently the narrator uses a more general name instead of a specific term and the translator has to rely on the image presented in the programme to understand which term is being discussed in order to transpose it in the target language accordingly.[42] Additionally, translators of minorised languages often have to face another problem: some terms may not even exist in the target language. In such cases, they have to create new terminology or consult specialists to find proper solutions. Also, sometimes the official nomenclature differs from the terminology used by actual specialists, which leaves the translator to decide between using the official vocabulary that can be found in the dictionary, or rather opting for spontaneous expressions used by real experts in real life situations.[43] "},"meta":{},"created_at":"2025-03-22T14:25:42.293407Z","updated_at":"2025-03-22T14:25:42.293407Z","inner_id":103,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":112,"annotations":[{"id":112,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.327895Z","updated_at":"2025-03-22T14:25:42.327895Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"10449de4-0ea3-4a10-b923-8dbb68519ffa","import_id":null,"last_action":null,"bulk_created":false,"task":112,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  Private equity (PE) is stock in a private company that does not offer stock to the general public. Private equity is offered instead to specialized investment funds and limited partnerships that take an active role in the management and structuring of the companies. In casual usage, \"private equity\" can refer to these investment firms, rather than the companies in which they invest.[1]  Private-equity capital is invested into a target company either by an investment management company (private equity firm), a venture capital fund, or an angel investor; each category of investor has specific financial goals, management preferences, and investment strategies for profiting from their investments. Private equity provides working capital to finance a target company's expansion, including the development of new products and services, operational restructuring, management changes, and shifts in ownership and control.[2]  As a financial product, the private-equity fund is a type of private capital for financing a long-term investment strategy in an illiquid business enterprise.[3] Private equity fund investing has been described by the financial press as the superficial rebranding of investment management companies who specialized in the leveraged buyout of financially weak companies.[4]  Evaluations of the returns of private equity are mixed: some find that it outperforms public equity, but others find otherwise.[5]  Some key features of private equity investment include:  The strategies private-equity firms may use are as follows, leveraged buyout being the most common.  Leveraged buyout (LBO) refers to a strategy of making equity investments as part of a transaction in which a company, business unit, or business asset is acquired from the current shareholders typically with the use of financial leverage.[13] The companies involved in these transactions are typically mature and generate operating cash flows.[14]  Private-equity firms view target companies as either Platform companies, which have sufficient scale and a successful business model to act as a stand-alone entity, or as add-on \/ tuck-in \/ bolt-on acquisitions, which would include companies with insufficient scale or other deficits.[15][16]  Leveraged buyouts involve a financial sponsor agreeing to an acquisition without itself committing all the capital required for the acquisition. To do this, the financial sponsor will raise acquisition debt, which looks to the cash flows of the acquisition target to make interest and principal payments.[17] Acquisition debt in an LBO is often non-recourse to the financial sponsor and has no claim on other investments managed by the financial sponsor. Therefore, an LBO transaction's financial structure is particularly attractive to a fund's limited partners, allowing them the benefits of leverage, but limiting the degree of recourse of that leverage. This kind of financing structure leverage benefits an LBO's financial sponsor in two ways: (1) the investor only needs to provide a fraction of the capital for the acquisition, and (2) the returns to the investor will be enhanced, as long as the return on assets exceeds the cost of the debt.[18]  As a percentage of the purchase price for a leverage buyout target, the amount of debt used to finance a transaction varies according to the financial condition and history of the acquisition target, market conditions, the willingness of lenders to extend credit (both to the LBO's financial sponsors and the company to be acquired) and the interest costs and the ability of the company to cover those costs. Historically the debt portion of a LBO will range from 60 to 90% of the purchase price.[19] Between 2000 and 2005, debt averaged between 59.4% and 67.9% of total purchase price for LBOs in the United States.[20]  A private-equity fund, ABC Capital II, borrows $9bn from a bank (or other lender). To this, it adds $2bn of equity – money from its own partners and from limited partners. With this $11bn, it buys all the shares of an underperforming company, XYZ Industrial (after due diligence, i.e. checking the books). It replaces the senior management in XYZ Industrial, with others who set out to streamline it. The workforce is reduced, some assets are sold off, etc. The objective is to increase the valuation of the company for an early sale.  The stock market is experiencing a bull market, and XYZ Industrial is sold two years after the buy-out for $13bn, yielding a profit of $2bn. The original loan can now be paid off with interest of, say, $0.5bn. The remaining profit of $1.5bn is shared among the partners. Taxation of such gains is at the capital gains tax rates, which in the United States are lower than ordinary income tax rates.  Note that part of that profit results from turning the company around, and part results from the general increase in share prices in a buoyant stock market, the latter often being the greater component.[21]  Notes:  Growth capital refers to equity investments, most often minority investments, in relatively mature companies that are looking for capital to expand or restructure operations, enter new markets or finance a major acquisition without a change of control of the business.[24]  Companies that seek growth capital will often do so in order to finance a transformational event in their life cycle. These companies are likely to be more mature than venture capital-funded companies, able to generate revenue and operating profits, but unable to generate sufficient cash to fund major expansions, acquisitions or other investments. Because of this lack of scale, these companies generally can find few alternative conduits to secure capital for growth, so access to growth equity can be critical to pursue necessary facility expansion, sales and marketing initiatives, equipment purchases, and new product development.[25]  The primary owner of the company may not be willing to take the financial risk alone. By selling part of the company to private equity, the owner can take out some value and share the risk of growth with partners.[26] Capital can also be used to effect a restructuring of a company's balance sheet, particularly to reduce the amount of leverage (or debt) the company has on its balance sheet.[27]  A private investment in public equity (PIPE), refer to a form of growth capital investment made into a publicly traded company. PIPE investments are typically made in the form of a convertible or preferred security that is unregistered for a certain period of time.[28][29]  The Registered Direct (RD) is another common financing vehicle used for growth capital. A registered direct is similar to a PIPE, but is instead sold as a registered security.  Mezzanine capital refers to subordinated debt or preferred equity securities that often represent the most junior portion of a company's capital structure that is senior to the company's common equity. This form of financing is often used by private-equity investors to reduce the amount of equity capital required to finance a leveraged buyout or major expansion. Mezzanine capital, which is often used by smaller companies that are unable to access the high yield market, allows such companies to borrow additional capital beyond the levels that traditional lenders are willing to provide through bank loans.[30] In compensation for the increased risk, mezzanine debt holders require a higher return for their investment than secured or other more senior lenders.[31][32] Mezzanine securities are often structured with a current income coupon.  Venture capital[33] (VC) is a broad subcategory of private equity that refers to equity investments made, typically in less mature companies, for the launch of a seed or startup company, early-stage development, or expansion of a business. Venture investment is most often found in the application of new technology, new marketing concepts and new products that do not have a proven track record or stable revenue streams.[34][35]  Venture capital is often sub-divided by the stage of development of the company ranging from early-stage capital used for the launch of startup companies to late stage and growth capital that is often used to fund expansion of existing business that are generating revenue but may not yet be profitable or generating cash flow to fund future growth.[36]  Entrepreneurs often develop products and ideas that require substantial capital during the formative stages of their companies' life cycles.[37] Many entrepreneurs do not have sufficient funds to finance projects themselves, and they must, therefore, seek outside financing.[38] The venture capitalist's need to deliver high returns to compensate for the risk of these investments makes venture funding an expensive capital source for companies. Being able to secure financing is critical to any business, whether it is a startup seeking venture capital or a mid-sized firm that needs more cash to grow.[39] Venture capital is most suitable for businesses with large up-front capital requirements which cannot be financed by cheaper alternatives such as debt. Although venture capital is often most closely associated with fast-growing technology, healthcare and biotechnology fields, venture funding has been used for other more traditional businesses.[34][40]  Investors generally commit to venture capital funds as part of a wider diversified private-equity portfolio, but also to pursue the larger returns the strategy has the potential to offer. However, venture capital funds have produced lower returns for investors over recent years compared to other private-equity fund types, particularly buyout.  The category of distressed securities comprises financial strategies for the profitable investment of working capital into the corporate equity and the securities of financially weak companies.[41][42][43] The investment of private-equity capital into distressed securities is realised with two financial strategies:  Moreover, the private-equity investment strategies of hedge funds also include actively trading the loans held and the bonds issued by the financially-weak target companies.[46]  Secondary investments refer to investments made in existing private-equity assets. These transactions can involve the sale of private equity fund interests or portfolios of direct investments in privately held companies through the purchase of these investments from existing institutional investors.[47] By its nature, the private-equity asset class is illiquid, intended to be a long-term investment for buy and hold investors. Secondary investments allow institutional investors, particularly those new to the asset class, to invest in private equity from older vintages than would otherwise be available to them. Secondaries also typically experience a different cash flow profile, diminishing the j-curve effect of investing in new private-equity funds.[48][49] Often investments in secondaries are made through third-party fund vehicle, structured similar to a fund of funds although many large institutional investors have purchased private-equity fund interests through secondary transactions.[50] Sellers of private-equity fund investments sell not only the investments in the fund but also their remaining unfunded commitments to the funds.  Other strategies that can be considered private equity or a close adjacent market include:  and this to compensate for private equities not being traded on the public market, a private-equity secondary market has formed, where private-equity investors purchase securities and assets from other private equity investors.  The seeds of the US private-equity industry were planted in 1946 with the founding of two venture capital firms: American Research and Development Corporation (ARDC) and J.H. Whitney & Company.[58] Before World War II, venture capital investments (originally known as \"development capital\") were primarily the domain of wealthy individuals and families. In 1901 J.P. Morgan arguably managed the first leveraged buyout of the Carnegie Steel Company using private equity.[59] Modern era private equity, however, is credited to Georges Doriot, the \"father of venture capitalism\" with the founding of ARDC[60] and founder of INSEAD, with capital raised from institutional investors, to encourage private sector investments in businesses run by soldiers who were returning from World War II. ARDC is credited with the first major venture capital success story when its 1957 investment of $70,000 in Digital Equipment Corporation (DEC) would be valued at over $355 million after the company's initial public offering in 1968 (a return of over 5,000 times its investment and an annualized rate of return of 101%).[61][62][failed verification] It is commonly noted that the first venture-backed startup is Fairchild Semiconductor, which produced the first commercially practicable integrated circuit, funded in 1959 by what would later become Venrock Associates.[63]  The first leveraged buyout may have been the purchase by McLean Industries, Inc. of Pan-Atlantic Steamship Company in January 1955 and Waterman Steamship Corporation in May 1955[64] Under the terms of that transaction, McLean borrowed $42 million and raised an additional $7 million through an issue of preferred stock. When the deal closed, $20 million of Waterman cash and assets were used to retire $20 million of the loan debt.[65] Lewis Cullman's acquisition of Orkin Exterminating Company in 1964 is often cited as the first leveraged buyout.[66][67] Similar to the approach employed in the McLean transaction, the use of publicly traded holding companies as investment vehicles to acquire portfolios of investments in corporate assets was a relatively new trend in the 1960s popularized by the likes of Warren Buffett (Berkshire Hathaway) and Victor Posner (DWG Corporation) and later adopted by Nelson Peltz (Triarc), Saul Steinberg (Reliance Insurance) and Gerry Schwartz (Onex Corporation). These investment vehicles would utilize a number of the same tactics and target the same type of companies as more traditional leveraged buyouts and in many ways could be considered a forerunner of the later private-equity firms. Posner is often credited with coining the term \"leveraged buyout\" or \"LBO\".[68]  The leveraged buyout boom of the 1980s was conceived by a number of corporate financiers, most notably Jerome Kohlberg Jr. and later his protégé Henry Kravis. Working for Bear Stearns at the time, Kohlberg and Kravis along with Kravis' cousin George Roberts began a series of what they described as \"bootstrap\" investments. Many of these companies lacked a viable or attractive exit for their founders as they were too small to be taken public and the founders were reluctant to sell out to competitors and so a sale to a financial buyer could prove attractive.[69] In the following years the three Bear Stearns bankers would complete a series of buyouts including Stern Metals (1965), Incom (a division of Rockwood International, 1971), Cobblers Industries (1971), and Boren Clay (1973) and Thompson Wire, Eagle Motors and Barrows through their investment in Stern Metals.[70] By 1976, tensions had built up between Bear Stearns and Kohlberg, Kravis and Roberts leading to their departure and the formation of Kohlberg Kravis Roberts in that year.  In January 1982, former United States Secretary of the Treasury William E. Simon and a group of investors acquired Gibson Greetings, a producer of greeting cards, for $80 million, of which only $1 million was rumored to have been contributed by the investors. By mid-1983, just sixteen months after the original deal, Gibson completed a $290 million IPO and Simon made approximately $66 million.[71][72]  The success of the Gibson Greetings investment attracted the attention of the wider media to the nascent boom in leveraged buyouts. Between 1979 and 1989, it was estimated that there were over 2,000 leveraged buyouts valued in excess of $250 million.[73]  During the 1980s, constituencies within acquired companies and the media ascribed the \"corporate raid\" label to many private-equity investments, particularly those that featured a hostile takeover of the company, perceived asset stripping, major layoffs or other significant corporate restructuring activities. Among the most notable investors to be labeled corporate raiders in the 1980s included Carl Icahn, Victor Posner, Nelson Peltz, Robert M. Bass, T. Boone Pickens, Harold Clark Simmons, Kirk Kerkorian, Sir James Goldsmith, Saul Steinberg and Asher Edelman. Carl Icahn developed a reputation as a ruthless corporate raider after his hostile takeover of TWA in 1985.[74][75][76] Many of the corporate raiders were onetime clients of Michael Milken, whose investment banking firm, Drexel Burnham Lambert helped raise blind pools of capital with which corporate raiders could make a legitimate attempt to take over a company and provided high-yield debt (\"junk bonds\") financing of the buyouts.  One of the final major buyouts of the 1980s proved to be its most ambitious and marked both a high-water mark and a sign of the beginning of the end of the boom. In 1989, KKR (Kohlberg Kravis Roberts) closed in on a $31.1 billion takeover of RJR Nabisco. It was, at that time and for over 17 years, the largest leveraged buyout in history. The event was chronicled in the book (and later the movie), Barbarians at the Gate: The Fall of RJR Nabisco. KKR would eventually prevail in acquiring RJR Nabisco at $109 per share, marking a dramatic increase from the original announcement that Shearson Lehman Hutton would take RJR Nabisco private at $75 per share. A fierce series of negotiations and horse-trading ensued which pitted KKR against Shearson and later Forstmann Little & Co. Many of the major banking players of the day, including Morgan Stanley, Goldman Sachs, Salomon Brothers, and Merrill Lynch were actively involved in advising and financing the parties. After Shearson's original bid, KKR quickly introduced a tender offer to obtain RJR Nabisco for $90 per share—a price that enabled it to proceed without the approval of RJR Nabisco's management. RJR's management team, working with Shearson and Salomon Brothers, submitted a bid of $112, a figure they felt certain would enable them to outflank any response by Kravis's team. KKR's final bid of $109, while a lower dollar figure, was ultimately accepted by the board of directors of RJR Nabisco.[77] At $31.1 billion of transaction value, RJR Nabisco was by far the largest leveraged buyouts in history. In 2006 and 2007, a number of leveraged buyout transactions were completed that for the first time surpassed the RJR Nabisco leveraged buyout in terms of nominal purchase price. However, adjusted for inflation, none of the leveraged buyouts of the 2006–2007 period would surpass RJR Nabisco. By the end of the 1980s the excesses of the buyout market were beginning to show, with the bankruptcy of several large buyouts including Robert Campeau's 1988 buyout of Federated Department Stores, the 1986 buyout of the Revco drug stores, Walter Industries, FEB Trucking and Eaton Leonard. Additionally, the RJR Nabisco deal was showing signs of strain, leading to a recapitalization in 1990 that involved the contribution of $1.7 billion of new equity from KKR.[78] In the end, KKR lost $700 million on RJR.[79]  Drexel reached an agreement with the government in which it pleaded nolo contendere (no contest) to six felonies – three counts of stock parking and three counts of stock manipulation.[80] It also agreed to pay a fine of $650 million – at the time, the largest fine ever levied under securities laws. Milken left the firm after his own indictment in March 1989.[81][82] On 13 February 1990 after being advised by United States Secretary of the Treasury Nicholas F. Brady, the U.S. Securities and Exchange Commission (SEC), the New York Stock Exchange and the Federal Reserve, Drexel Burnham Lambert officially filed for Chapter 11 bankruptcy protection.[81]  The combination of decreasing interest rates, loosening lending standards and regulatory changes for publicly traded companies (specifically the Sarbanes–Oxley Act) would set the stage for the largest boom private equity had seen. Marked by the buyout of Dex Media in 2002, large multibillion-dollar U.S. buyouts could once again obtain significant high yield debt financing, and larger transactions could be completed. By 2004 and 2005, major buyouts were once again becoming common, including the acquisitions of Toys \"R\" Us,[83] The Hertz Corporation,[84][85] Metro-Goldwyn-Mayer[86] and SunGard[87] in 2005.  As 2006 began, new \"largest buyout\" records were set and surpassed several times; nine of the top ten buyouts by the end of 2007 had been announced in an 18-month period from the beginning of 2006 through the middle of 2007. In 2006, private-equity firms bought 654 U.S. companies for $375 billion, representing 18 times the level of transactions closed in 2003.[88] Additionally, U.S.-based private-equity firms raised $215.4 billion in investor commitments to 322 funds, surpassing the previous record set in 2000 by 22% and 33% higher than the 2005 fundraising total[89] The following year, despite the onset of turmoil in the credit markets in the summer, saw yet another record year of fundraising with $302 billion of investor commitments to 415 funds[90] Among the mega-buyouts completed during the 2006 to 2007 boom were: EQ Office, HCA,[91] Alliance Boots[92] and TXU.[93]  In July 2007, the turmoil that had been affecting the mortgage markets, spilled over into the leveraged finance and high-yield debt markets.[94][95] The markets had been highly robust during the first six months of 2007, with highly issuer friendly developments including PIK and PIK Toggle (interest is \"Payable In Kind\") and covenant light debt widely available to finance large leveraged buyouts. July and August saw a notable slowdown in issuance levels in the high yield and leveraged loan markets with few issuers accessing the market. Uncertain market conditions led to a significant widening of yield spreads, which coupled with the typical summer slowdown led many companies and investment banks to put their plans to issue debt on hold until the autumn. However, the expected rebound in the market after 1 May 2007 did not materialize, and the lack of market confidence prevented deals from pricing. By the end of September, the full extent of the credit situation became obvious as major lenders including Citigroup and UBS AG announced major writedowns due to credit losses. The leveraged finance markets came to a near standstill during a week in 2007.[96] As 2008 began, lending standards tightened and the era of \"mega-buyouts\" came to an end. Nevertheless, private equity continues to be a large and active asset class and the private-equity firms, with hundreds of billions of dollars of committed capital from investors are looking to deploy capital in new and different transactions.[citation needed]  As a result of the global financial crisis, private equity has become subject to increased regulation in Europe and is now subject, among other things, to rules preventing asset stripping of portfolio companies and requiring the notification and disclosure of information in connection with buy-out activity.[97][98]  From 2010 to 2014 KKR, Carlyle, Apollo and Ares went public. Starting from 2018 these companies converted from partnerships into corporations with more shareholder rights and the inclusion in stock indices and mutual fund portfolios.[99] But with the increased availability and scope of funding provided by private markets, many companies are staying private simply because they can. McKinsey & Company reports in its Global Private Markets Review 2018 that global private market fundraising increased by $28.2 billion from 2017, for a total of $748 billion in 2018.[100] Thus, given the abundance of private capital available, companies no longer require public markets for sufficient funding. Benefits may include avoiding the cost of an IPO, maintaining more control of the company, and having the 'legroom' to think long-term rather than focus on short-term or quarterly figures.[101][102]  A new feature in the 2020s: regulated platforms which fractionalize the assets, making possible individual investments of $10,000 or less.[103]  Private equity deal-making in the United Kingdom surged in 2024, with total investment reaching £63 billion, just 7% below the record high of £68 billion in 2021. According to Dealogic, there were 305 private equity deals in 2024, marking a significant increase from 229 deals in 2023. The uptick in activity was driven by improving financial conditions and a rebound in investor confidence after a period of high interest rates in 2022 and 2023, which had slowed deal flow.[104]  Notable acquisitions included:  The rapid pace of acquisitions also contributed to the decline in the number of listed companies in London, as private equity firms increasingly targeted publicly traded businesses. Research by Goldman Sachs showed that the London Stock Exchange experienced its fastest pace of shrinkage in over a decade due to private equity takeovers.[106]  However, concerns have been raised regarding the financial health of private equity-backed companies. The Bank of England issued a warning in 2024, stating that businesses owned by private equity firms were more vulnerable to default than other large businesses. The central bank’s research found that more than 2 million people in the UK were employed by firms engaged with private equity and that these companies were responsible for 15% of all corporate debt.[107]  Despite these risks, private equity interest in undervalued British companies has continued into 2025. As of early 2025, 19 deals worth a total of £2.9 billion have already been announced, highlighting the sector’s continued expansion.[108]  Although the capital for private equity originally came from individual investors or corporations, in the 1970s, private equity became an asset class in which various institutional investors allocated capital in the hopes of achieving risk-adjusted returns that exceed those possible in the public equity markets. In the 1980s, insurers were major private-equity investors. Later, public pension funds and university and other endowments became more significant sources of capital.[109] For most institutional investors, private-equity investments are made as part of a broad asset allocation that includes traditional assets (e.g., public equity and bonds) and other alternative assets (e.g., hedge funds, real estate, commodities).  US, Canadian and European public and private pension schemes have invested in the asset class since the early 1980s to diversify away from their core holdings (public equity and fixed income).[110] Today pension investment in private equity accounts for more than a third of all monies allocated to the asset class, ahead of other institutional investors such as insurance companies, endowments, and sovereign wealth funds.  Most institutional investors do not invest directly in privately held companies, lacking the expertise and resources necessary to structure and monitor the investment. Instead, institutional investors will invest indirectly through a private equity fund. Certain institutional investors have the scale necessary to develop a diversified portfolio of private-equity funds themselves, while others will invest through a fund of funds to allow a portfolio more diversified than one a single investor could construct.  Returns on private-equity investments are created through one or a combination of three factors that include: debt repayment or cash accumulation through cash flows from operations, operational improvements that increase earnings over the life of the investment and multiple expansion, selling the business for a higher price than was originally paid. A key component of private equity as an asset class for institutional investors is that investments are typically realized after some period of time, which will vary depending on the investment strategy. Private-equity investment returns are typically realized through one of the following avenues:  Large institutional asset owners such as pension funds (with typically long-dated liabilities), insurance companies, sovereign wealth and national reserve funds have a generally low likelihood of facing liquidity shocks in the medium term, and thus can afford the required long holding periods characteristic of private-equity investment.[110]  The median horizon for a LBO transaction is eight years.[111]  The private-equity secondary market (also often called private-equity secondaries) refers to the buying and selling of pre-existing investor commitments to private equity and other alternative investment funds. Sellers of private-equity investments sell not only the investments in the fund but also their remaining unfunded commitments to the funds. By its nature, the private-equity asset class is illiquid, intended to be a long-term investment for buy-and-hold investors. For the vast majority of private-equity investments, there is no listed public market; however, there is a robust and maturing secondary market available for sellers of private-equity assets.  Increasingly, secondaries are considered a distinct asset class with a cash flow profile that is not correlated with other private-equity investments. As a result, investors are allocating capital to secondary investments to diversify their private-equity programs. Driven by strong demand for private-equity exposure, a significant amount of capital has been committed to secondary investments from investors looking to increase and diversify their private-equity exposure.  Investors seeking access to private equity have been restricted to investments with structural impediments such as long lock-up periods, lack of transparency, unlimited leverage, concentrated holdings of illiquid securities and high investment minimums.  Secondary transactions can be generally divided into two primary categories:  This is the most common type of secondary transaction, involving the sale of an investor’s interest in a private-equity fund or a portfolio of multiple fund interests. Transactions may take several forms:  Also known as GP-Centered, secondary directs or synthetic secondaries, these transactions involve the sale of a portfolio of direct investments in portfolio companies. Subcategories include:  According to Private Equity International's latest PEI 300 ranking,[114] the largest private-equity firm in the world today is The Blackstone Group based on the amount of private-equity direct-investment capital raised over a five-year window.  As ranked by the PEI 300, the 15 largest private-equity firms in the world in 2024 were:  Because private-equity firms are continuously in the process of raising, investing and distributing their private equity funds, capital raised can often be the easiest to measure. Other metrics can include the total value of companies purchased by a firm or an estimate of the size of a firm's active portfolio plus capital available for new investments. As with any list that focuses on size, the list does not provide any indication as to relative investment performance of these funds or managers.  Preqin, an independent data provider, ranks the 25 largest private-equity investment managers. Among the larger firms in the 2017 ranking were AlpInvest Partners, Ardian (formerly AXA Private Equity), AIG Investments, and Goldman Sachs Capital Partners. Invest Europe publishes a yearbook which analyses industry trends derived from data disclosed by over 1,300 European private-equity funds.[115] Finally, websites such as AskIvy.net[116] provide lists of London-based private-equity firms.  The investment strategies of private-equity firms differ from those of hedge funds. Typically, private-equity investment groups are geared towards long-hold, multiple-year investment strategies in illiquid assets (whole companies, large-scale real estate projects, or other tangibles not easily converted to cash) where they have more control and influence over operations or asset management to influence their long-term returns. Hedge funds usually focus on short or medium term liquid securities which are more quickly convertible to cash, and they do not have direct control over the business or asset in which they are investing.[117] Both private-equity firms and hedge funds often specialize in specific types of investments and transactions. Private-equity specialization is usually in specific industry sector asset management while hedge fund specialization is in industry sector risk capital management. Private-equity strategies can include wholesale purchase of a privately held company or set of assets, mezzanine financing for startup projects, growth capital investments in existing businesses or leveraged buyout of a publicly held asset converting it to private control.[118] Finally, private-equity firms only take long positions, for short selling is not possible in this asset class.  Private-equity fundraising refers to the action of private-equity firms seeking capital from investors for their funds. Typically an investor will invest in a specific fund managed by a firm, becoming a limited partner in the fund, rather than an investor in the firm itself. As a result, an investor will only benefit from investments made by a firm where the investment is made from the specific fund in which it has invested.  As fundraising has grown over the past few years, so too has the number of investors in the average fund. In 2004, there were 26 investors in the average private-equity fund, this figure has now grown to 42 according to Preqin ltd. (formerly known as Private Equity Intelligence).  The managers of private-equity funds will also invest in their own vehicles, typically providing between 1–5% of the overall capital.  Often private-equity fund managers will employ the services of external fundraising teams known as placement agents in order to raise capital for their vehicles. The use of placement agents has grown over the past few years, with 40% of funds closed in 2006 employing their services, according to Preqin ltd. Placement agents will approach potential investors on behalf of the fund manager, and will typically take a fee of around 1% of the commitments that they are able to garner.  The amount of time that a private-equity firm spends raising capital varies depending on the level of interest among investors, which is defined by current market conditions and also the track record of previous funds raised by the firm in question. Firms can spend as little as one or two months raising capital when they are able to reach the target that they set for their funds relatively easily, often through gaining commitments from existing investors in their previous funds, or where strong past performance leads to strong levels of investor interest. Other managers may find fundraising taking considerably longer, with managers of less popular fund types finding the fundraising process more tough. It can take up to two years to raise capital, although the majority of fund managers will complete fundraising within nine months to fifteen months.  Once a fund has reached its fundraising target, it will have a final close. After this point it is not normally possible for new investors to invest in the fund, unless they were to purchase an interest in the fund on the secondary market.  The state of the industry around the end of 2011 was as follows.[120]  Private-equity assets under management probably exceeded $2 trillion at the end of March 2012, and funds available for investment totaled $949bn (about 47% of overall assets under management).  Approximately $246bn of private equity was invested globally in 2011, down 6% on the previous year and around two-thirds below the peak activity in 2006 and 2007. Following on from a strong start, deal activity slowed in the second half of 2011 due to concerns over the global economy and sovereign debt crisis in Europe. There was $93bn in investments during the first half of this year as the slowdown persisted into 2012. This was down a quarter on the same period in the previous year. Private-equity backed buyouts generated some 6.9% of global M&A volume in 2011 and 5.9% in the first half of 2012. This was down on 7.4% in 2010 and well below the all-time high of 21% in 2006.  Global exit activity totalled $252bn in 2011, practically unchanged from the previous year, but well up on 2008 and 2009 as private-equity firms sought to take advantage of improved market conditions at the start of the year to realise investments. Exit activity however, has lost momentum following a peak of $113bn in the second quarter of 2011. TheCityUK estimates total exit activity of some $100bn in the first half of 2012, well down on the same period in the previous year.  The fund raising environment remained stable for the third year running in 2011 with $270bn in new funds raised, slightly down on the previous year's total. Around $130bn in funds was raised in the first half of 2012, down around a fifth on the first half of 2011. The average time for funds to achieve a final close fell to 16.7 months in the first half of 2012, from 18.5 months in 2011. Private-equity funds available for investment (\"dry powder\") totalled $949bn at the end of q1-2012, down around 6% on the previous year. Including unrealised funds in existing investments, private-equity funds under management probably totalled over $2.0 trillion.  Public pensions are a major source of capital for private-equity funds. Increasingly, sovereign wealth funds are growing as an investor class for private equity.[121]  Private Equity was invested in 13% of the Pharma 1000 in 2021 according to Torreya with Eight Roads Ventures having the highest number of investments in this industry.[122]  Due to limited disclosure, studying the returns to private equity is relatively difficult. Unlike mutual funds, private-equity funds need not disclose performance data. And, as they invest in private companies, it is difficult to examine the underlying investments. It is challenging to compare private-equity performance to public-equity performance, in particular because private-equity fund investments are drawn and returned over time as investments are made and subsequently realized.  An oft-cited academic paper (Kaplan and Schoar, 2005)[123] suggests that the net-of-fees returns to PE funds are roughly comparable to the S&P 500 (or even slightly under). This analysis may actually overstate the returns because it relies on voluntarily reported data and hence suffers from survivorship bias (i.e. funds that fail will not report data). One should also note that these returns are not risk-adjusted. A 2012 paper by Harris, Jenkinson and Kaplan, 2012[124] found that average buyout fund returns in the U.S. have actually exceeded that of public markets. These findings were supported by earlier work, using a data set from Robinson and Sensoy in 2011.[125]  Commentators have argued that a standard methodology is needed to present an accurate picture of performance, to make individual private-equity funds comparable and so the asset class as a whole can be matched against public markets and other types of investment. It is also claimed that PE fund managers manipulate data to present themselves as strong performers, which makes it even more essential to standardize the industry.[126]  Two other findings in Kaplan and Schoar in 2005: First, there is considerable variation in performance across PE funds. Second, unlike the mutual fund industry, there appears to be performance persistence in PE funds. That is, PE funds that perform well over one period, tend to also perform well the next period. Persistence is stronger for VC firms than for LBO firms.  The application of the Freedom of Information Act (FOIA) in certain states in the United States has made certain performance data more readily available. Specifically, FOIA has required certain public agencies to disclose private-equity performance data directly on their websites.[127]  In the United Kingdom, the second largest market for private equity, more data has become available since the 2007 publication of the David Walker Guidelines for Disclosure and Transparency in Private Equity.[128]  Below is a partial list of billionaires who acquired their wealth through private equity.  Income to private equity firms is primarily in the form of \"carried interest\", typically 20% of the profits generated by investments made by the firm, and a \"management fee\", often 2% of the principal invested in the firm by the outside investors whose money the firm holds. As a result of a tax loophole enshrined in the U.S. tax code, carried interest that accrues to private equity firms is treated as capital gains, which is taxed at a lower rate than is ordinary income. Currently, the long term capital gains tax rate is 20% compared with the 37% top ordinary income tax rate for individuals. This loophole has been estimated to cost the government $130 billion over the next decade in unrealized revenue. Armies of corporate lobbyists and huge private equity industry donations to political campaigns in the United States have ensured that this powerful industry receives this favorable tax treatment by the government. Private equity firms retain close to 200 lobbyists and over the last decade have made almost $600 million in political campaign contributions.[144]  In addition, through an accounting maneuver called \"fee waiver\", private equity firms often also treat management fee income as capital gains. The U.S. Internal Revenue Service (IRS) lacks the manpower and the expertise that would be necessary to track compliance with even these already quite favorable legal requirements. In fact, the IRS conducts nearly no income tax audits of the industry. As a result of the complexity of the accounting that arises from the fact that most private equity firms are organized as large partnerships, such that the firm's profits are apportioned to each of the many partners, a number of private equity firms fail to comply with tax laws, according to industry whistleblowers.[144]  When a private equity entity invests in a company, industry or public service, there have been reports of reduced quality, both in terms of services and goods produced.[145][146] While a private equity investment into a business might result in short-term improvements, such as new staff and equipment, the incentive is to maximize profits, not necessarily the quality of products or services. Over time, cost-cutting has also been common, and deferring further investments. Private equity investors may also be incentivized to make short-term gains by selling a company once a certain level of profitability is achieved or simply selling off its assets if that is not possible. Both of these situations, and others, can result in a loss of innovation and quality.[147][148][149][150][146][145]  There is a debate around the distinction between private equity and foreign direct investment (FDI), and whether to treat them separately. The difference is blurred on account of private equity not entering the country through the stock market. Private equity generally flows to unlisted firms and to firms where the percentage of shares is smaller than the promoter- or investor-held shares (also known as free-floating shares). The main point of contention is that FDI is used solely for production, whereas in the case of private equity the investor can reclaim their money after a revaluation period and make investments in other financial assets. At present, most countries report private equity as a part of FDI.[151]  Some studies have shown that private-equity investments in health care and related services, such as nursing homes and hospitals, have decreased the quality of care while driving up costs. Researchers at the Becker Friedman Institute of the University of Chicago found that private-equity ownership of nursing homes increased the short-term mortality of Medicare patients by 10%.[152] Treatment by private-equity owned health care providers tends to be associated with a higher rate of \"surprise bills\".[153] Private equity ownership of dermatology practices has led to pressure to increase profitability, concerns about up-charging and patient safety.[154][155] In a 2024 study of 51 private equity–acquired hospitals matched with 250 controls, the former had a 25% increase in hospital-acquired conditions, such as falls and central line-associated bloodstream infections.[156]  According to conservative Oren Cass, private equity captures wealth rather than creating it, and this capture can be \"zero-sum, or even value-destroying, in aggregate.\" He describes \"assets get shuffled and reshuffled, profits get made, but relatively little flows toward actual productive uses.\"[157]  Bloomberg Businessweek states that: PE may contribute to inequality in several ways. First, it offers investors higher returns than those available in public stocks and bonds markets. Yet, to enjoy those returns, it helps to already be rich. Private-equity funds are open solely to \"qualified\" (read: high-net-worth) individual investors and to institutions such as endowments. Only some workers get indirect exposure via pension funds. Second, PE puts pressure on the lower end of the wealth divide. Companies can be broken up, merged, or generally restructured to increase efficiency and productivity, which inevitably means job cuts.[4]"},"meta":{},"created_at":"2025-03-22T14:25:42.293407Z","updated_at":"2025-03-22T14:25:42.293407Z","inner_id":104,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":113,"annotations":[{"id":113,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.327895Z","updated_at":"2025-03-22T14:25:42.327895Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"d38c9c25-7f4d-446b-8deb-ef71639153b4","import_id":null,"last_action":null,"bulk_created":false,"task":113,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Corporate governance refers to the mechanisms, processes, practices, and relations by which corporations are controlled and operated by their boards of directors, managers, shareholders, and stakeholders.  \"Corporate governance\" may be defined, described or delineated in diverse ways, depending on the writer's purpose. Writers focused on a disciplinary interest or context (such as accounting, finance, law, or management) often adopt narrow definitions that appear purpose-specific. Writers concerned with regulatory policy in relation to corporate governance practices often use broader structural descriptions. A broad (meta) definition that encompasses many adopted definitions is \"Corporate governance describes the processes, structures, and mechanisms that influence the control and direction of corporations.\"[1]  This meta definition accommodates both the narrow definitions used in specific contexts and the broader descriptions that are often presented as authoritative. The latter include the structural definition from the Cadbury Report, which identifies corporate governance as \"the system by which companies are directed and controlled\" (Cadbury 1992, p. 15); and the relational-structural view adopted by the Organisation for Economic Cooperation and Development (OECD) of \"Corporate governance involves a set of relationships between a company's management, board, shareholders and stakeholders. Corporate governance also provides the structure and systems through which the company is directed and its objectives are set, and the means of attaining those objectives and monitoring performance are determined\" (OECD 2023, p. 6).[2]  Examples of narrower definitions in particular contexts include:  The firm itself is modelled as a governance structure acting through the mechanisms of contract.[5][6][7][8]  Here corporate governance may include its relation to corporate finance.[9][10][11]  Contemporary discussions of corporate governance tend to refer to principles raised in three documents released since 1990: The Cadbury Report (UK, 1992), the Principles of Corporate Governance (OECD, 1999, 2004, 2015 and 2023), and the Sarbanes–Oxley Act of 2002 (US, 2002). The Cadbury and Organisation for Economic Co-operation and Development (OECD) reports present general principles around which businesses are expected to operate to assure proper governance. The Sarbanes–Oxley Act, informally referred to as Sarbox or Sox, is an attempt by the federal government in the United States to legislate several of the principles recommended in the Cadbury and OECD reports.  Some concerns regarding governance follows from the potential for conflicts of interests that are a consequence of the non-alignment of preferences between: shareholders and upper management (principal–agent problems); and among shareholders (principal–principal problems),[22] although also other stakeholder relations are affected and coordinated through corporate governance.  In large firms where there is a separation of ownership and management, the principal–agent problem[23] can arise between upper-management (the \"agent\") and the shareholder(s) (the \"principals\"). The shareholders and upper management may have different interests. The shareholders typically desire returns on their investments through profits and dividends, while upper management may also be influenced by other motives, such as management remuneration or wealth interests, working conditions and perquisites, or relationships with other parties within (e.g., management-worker relations) or outside the corporation, to the extent that these are not necessary for profits. Those pertaining to self-interest are usually emphasized in relation to principal-agent problems. The effectiveness of corporate governance practices from a shareholder perspective might be judged by how well those practices align and coordinate the interests of the upper management with those of the shareholders.  However, corporations sometimes undertake initiatives, such as climate activism and voluntary emission reduction, that seems to contradict the idea that rational self-interest drives shareholders' governance goals.[24]: 3   An example of a possible conflict between shareholders and upper management materializes through stock repurchases (treasury stock). Executives may have incentive to divert cash surpluses to buying treasury stock to support or increase the share price. However, that reduces the financial resources available to maintain or enhance profitable operations. As a result, executives can sacrifice long-term profits for short-term personal gain. Shareholders may have different perspectives in this regard, depending on their own time preferences, but it can also be viewed as a conflict with broader corporate interests (including preferences of other stakeholders and the long-term health of the corporation).  The principal–agent problem can be intensified when upper management acts on behalf of multiple shareholders—which is often the case in large firms (see Multiple principal problem).[22] Specifically, when upper management acts on behalf of multiple shareholders, the multiple shareholders face a collective action problem in corporate governance, as individual shareholders may lobby upper management or otherwise have incentives to act in their individual interests rather than in the collective interest of all shareholders.[25] As a result, there may be free-riding in steering and monitoring of upper management,[26] or conversely, high costs may arise from duplicate steering and monitoring of upper management.[27] Conflict may break out between principals,[28] and this all leads to increased autonomy for upper management.[22]  Ways of mitigating or preventing these conflicts of interests include the processes, customs, policies, laws, and institutions which affect the way a company is controlled—and this is the challenge of corporate governance.[29][30] To solve the problem of governing upper management under multiple shareholders, corporate governance scholars have figured out that the straightforward solution of appointing one or more shareholders for governance is likely to lead to problems because of the information asymmetry it creates.[31][32][33] Shareholders' meetings are necessary to arrange governance under multiple shareholders, and it has been proposed that this is the solution to the problem of multiple principals due to median voter theorem: shareholders' meetings lead power to be devolved to an actor that approximately holds the median interest of all shareholders, thus causing governance to best represent the aggregated interest of all shareholders.[22]  An important theme of governance is the nature and extent of corporate accountability. A related discussion at the macro level focuses on the effect of a corporate governance system on economic efficiency, with a strong emphasis on shareholders' welfare.[8] This has resulted in a literature focused on economic analysis.[34][35][36] A comparative assessment of corporate governance principles and practices across countries was published by Aguilera and Jackson in 2011.[37]  Different models of corporate governance differ according to the variety of capitalism in which they are embedded. The Anglo-American \"model\" tends to emphasize the interests of shareholders. The coordinated or multistakeholder model associated with Continental Europe and Japan also recognizes the interests of workers, managers, suppliers, customers, and the community. A related distinction is between market-oriented and network-oriented models of corporate governance.[38]  Some continental European countries, including Germany, Austria, and the Netherlands, require a two-tiered board of directors as a means of improving corporate governance.[39] In the two-tiered board, the executive board, made up of company executives, generally runs day-to-day operations while the supervisory board, made up entirely of non-executive directors who represent shareholders and employees, hires and fires the members of the executive board, determines their compensation, and reviews major business decisions.[40]  Germany, in particular, is known for its practice of co-determination, founded on the German Codetermination Act of 1976, in which workers are granted seats on the board as stakeholders, separate from the seats accruing to shareholder equity.  The so-called \"Anglo-American model\" of corporate governance emphasizes the interests of shareholders. It relies on a single-tiered board of directors that is normally dominated by non-executive directors elected by shareholders. Because of this, it is also known as \"the unitary system\".[41][42] Within this system, many boards include some executives from the company (who are ex officio members of the board). Non-executive directors are expected to outnumber executive directors and hold key posts, including audit and compensation committees. In the United Kingdom, the CEO generally does not also serve as chairman of the board, whereas in the US having the dual role has been the norm, despite major misgivings regarding the effect on corporate governance.[43] The number of US firms combining both roles is declining, however.[44]  In the United States, corporations are directly governed by state laws, while the exchange (offering and trading) of securities in corporations (including shares) is governed by federal legislation. Many US states have adopted the Model Business Corporation Act, but the dominant state law for publicly traded corporations is Delaware General Corporation Law, which continues to be the place of incorporation for the majority of publicly traded corporations.[45] Individual rules for corporations are based upon the corporate charter and, less authoritatively, the corporate bylaws.[45] Shareholders cannot initiate changes in the corporate charter although they can initiate changes to the corporate bylaws.[45]  It is sometimes colloquially stated that in the US and the UK that \"the shareholders own the company.\" This is, however, a misconception as argued by Eccles and Youmans (2015) and Kay (2015).[46] The American system has long been based on a belief in the potential of shareholder democracy to efficiently allocate capital.  The Japanese model of corporate governance has traditionally held a broad view that firms should account for the interests of a range of stakeholders. For instance, managers do not have a fiduciary responsibility to shareholders. This framework is rooted in the belief that a balance among stakeholder interests can lead to a superior allocation of resources for society. The Japanese model includes several key principles:[47]  An article published by the Australian Institute of Company Directors called \"Do Boards Need to become more Entrepreneurial?\" considered the need for founder centrism behaviour at board level to appropriately manage disruption.[48]  Corporations are created as legal persons by the laws and regulations of a particular jurisdiction. These may vary in many respects between countries, but a corporation's legal person status is fundamental to all jurisdictions and is conferred by statute. This allows the entity to hold property in its own right without reference to any real person. It also results in the perpetual existence that characterizes the modern corporation. The statutory granting of corporate existence may arise from general purpose legislation (which is the general case) or from a statute to create a specific corporation. Now, the formation of business corporations in most jurisdictions requires government legislation that facilitates incorporation. This legislation is often in the form of Companies Act or Corporations Act, or similar. Country-specific regulatory devices are summarized below.  It is generally perceived that regulatory attention on the corporate governance practices of publicly listed corporations, particularly in relation to transparency and accountability, increased in many jurisdictions following the high-profile corporate scandals in 2001–2002, many of which involved accounting fraud; and then again after the financial crisis in 2008. For example, in the U.S., these included scandals surrounding Enron and MCI Inc. (formerly WorldCom). Their demise led to the enactment of the Sarbanes–Oxley Act in 2002, a U.S. federal law intended to improve corporate governance in the United States. Comparable failures in Australia (HIH, One.Tel) are linked to with the eventual passage of the CLERP 9 reforms there (2004), that similarly aimed to improve corporate governance.[49] Similar corporate failures in other countries stimulated increased regulatory interest (e.g., Parmalat in Italy). Also see  In addition to legislation the facilitates incorporation, many jurisdictions have some major regulatory devices that impact on corporate governance. This includes statutory laws concerned with the functioning of stock or securities markets (also see Security (finance), consumer and competition (antitrust) laws, labour or employment laws, and environmental protection laws, which may also entail disclosure requirements. In addition to the statutory laws of the relevant jurisdiction, corporations are subject to common law in some countries.  In most jurisdictions, corporations also have some form of a corporate constitution that provides individual rules that govern the corporation and authorize or constrain its decision-makers. This constitution is identified by a variety of terms; in English-speaking jurisdictions, it is sometimes known as the corporate charter or articles of association (which also be accompanied by a memorandum of association).  Incorporation in Australia originated under state legislation but has been under federal legislation since 2001. Also see Australian corporate law. Other significant legislation includes:  Incorporation in Canada can be done either under either federal or provincial legislation. See Canadian corporate law.  Dutch corporate law is embedded in the ondernemingsrecht and, specifically for limited liability companies, in the vennootschapsrecht.  In addition The Netherlands has adopted a Corporate Governance Code in 2016, which has been updated twice since. In the latest version (2022),[50] the Executive Board of the company is held responsible for the continuity of the company and its sustainable long-term value creation. The executive board considers the impact of corporate actions on People and Planet and takes the effects on corporate stakeholders into account.[51]  In the Dutch two-tier system, the Supervisory Board monitors and supervises the executive board in this respect.  Polish Corporate Law is regulated in Code of Commercial Companies.[52] The code regulates most of the aspects of corporate governance, incl. rules of incorporation and liquidation, it defines rights, obligations and rules of operations of corporate bodies (Management Board, Supervisory Board, Shareholders Meeting).[53]  The UK has a single jurisdiction for incorporation. Also see United Kingdom company law Other significant legislation includes:  The UK passed the Bribery Act in 2010. This law made it illegal to bribe either government or private citizens or make facilitating payments (i.e., payment to a government official to perform their routine duties more quickly). It also required corporations to establish controls to prevent bribery.  Incorporation in the US is under state level legislation, but there important federal acts. in particular, see Securities Act of 1933, Securities Exchange Act of 1934, and Uniform Securities Act.  The Sarbanes–Oxley Act of 2002 (SOX) was enacted in the wake of a series of high-profile corporate scandals, which cost investors billions of dollars.[54] It established a series of requirements that affect corporate governance in the US and influenced similar laws in many other countries. SOX contained many other elements, but provided for several changes that are important to corporate governance practices:  The U.S. passed the Foreign Corrupt Practices Act (FCPA) in 1977, with subsequent modifications. This law made it illegal to bribe government officials and required corporations to maintain adequate accounting controls. It is enforced by the U.S. Department of Justice and the Securities and Exchange Commission (SEC). Substantial civil and criminal penalties have been levied on corporations and executives convicted of bribery.[56]  Corporate governance principles and codes have been developed in different countries and issued from stock exchanges, corporations, institutional investors, or associations (institutes) of directors and managers with the support of governments and international organizations. As a rule, compliance with these governance recommendations is not mandated by law, although the codes linked to stock exchange listing requirements may have a coercive effect.  One of the most influential guidelines on corporate governance are the G20\/OECD Principles of Corporate Governance, first published as the OECD Principles in 1999, revised in 2004, in 2015 when endorsed by the G20, and in 2023.[57] The Principles are often referenced by countries developing local codes or guidelines. Building on the work of the OECD, other international organizations, private sector associations and more than 20 national corporate governance codes formed the United Nations Intergovernmental Working Group of Experts on International Standards of Accounting and Reporting (ISAR) to produce their Guidance on Good Practices in Corporate Governance Disclosure.[58] This internationally agreed[59] benchmark consists of more than fifty distinct disclosure items across five broad categories:[60]  The OECD Guidelines on Corporate Governance of State-Owned Enterprises[61] complement the G20\/OECD Principles of Corporate Governance,[62] providing guidance tailored to the corporate governance challenges of state-owned enterprises.  Companies listed on the New York Stock Exchange (NYSE) and other stock exchanges are required to meet certain governance standards. For example, the NYSE Listed Company Manual requires, among many other elements:  The investor-led organisation International Corporate Governance Network (ICGN) was set up by individuals centred around the ten largest pension funds in the world in 1995. The aim is to promote global corporate governance standards. The network is led by investors that manage US$77 trillion, and members are located in fifty different countries. ICGN has developed a suite of global guidelines ranging from shareholder rights to business ethics.[63]  The World Business Council for Sustainable Development (WBCSD) has done work on corporate governance, particularly on accounting and reporting.[64] In 2009, the International Finance Corporation and the UN Global Compact released a report, \"Corporate Governance: the Foundation for Corporate Citizenship and Sustainable Business\",[65] linking the environmental, social and governance responsibilities of a company to its financial performance and long-term sustainability.  Most codes are largely voluntary. An issue raised in the U.S. since the 2005 Disney decision[66] is the degree to which companies manage their governance responsibilities; in other words, do they merely try to supersede the legal threshold, or should they create governance guidelines that ascend to the level of best practice. For example, the guidelines issued by associations of directors, corporate managers and individual companies tend to be wholly voluntary, but such documents may have a wider effect by prompting other companies to adopt similar practices.[citation needed]  In 2021, the first ever international standard, ISO 37000, was published as guidance for good governance.[67] The guidance places emphasis on purpose which is at the heart of all organizations, i.e. a meaningful reason to exist. Values inform both the purpose and the way the purpose is achieved.[68]  Robert E. Wright argued in Corporation Nation (2014) that the governance of early U.S. corporations, of which over 20,000 existed by the Civil War of 1861–1865, was superior to that of corporations in the late 19th and early 20th centuries because early corporations governed themselves like \"republics\", replete with numerous \"checks and balances\" against fraud and against usurpation of power by managers or by large shareholders.[69] (The term \"robber baron\" became particularly associated with US corporate figures in the Gilded Age—the late 19th century.)  In the immediate aftermath of the Wall Street crash of 1929 legal scholars such as Adolf Augustus Berle, Edwin Dodd, and Gardiner C. Means pondered on the changing role of the modern corporation in society.[70] From the Chicago school of economics, Ronald Coase[71] introduced the notion of transaction costs into the understanding of why firms are founded and how they continue to behave.[72]  US economic expansion through the emergence of multinational corporations after World War II (1939–1945) saw the establishment of the managerial class. Several Harvard Business School management professors studied and wrote about the new class: Myles Mace (entrepreneurship), Alfred D. Chandler, Jr. (business history), Jay Lorsch (organizational behavior) and Elizabeth MacIver (organizational behavior). According to Lorsch and MacIver \"many large corporations have dominant control over business affairs without sufficient accountability or monitoring by their board of directors\".[citation needed]  In the 1980s, Eugene Fama and Michael Jensen[73] established the principal–agent problem as a way of understanding corporate governance: the firm is seen as a series of contracts.[74]  In the period from 1977 to 1997, corporate directors' duties in the U.S. expanded beyond their traditional legal responsibility of duty of loyalty to the corporation and to its shareholders.[75] [vague]  In the first half of the 1990s, the issue of corporate governance in the U.S. received considerable press attention due to a spate of CEO dismissals (for example, at IBM, Kodak, and Honeywell) by their boards. The California Public Employees' Retirement System (CalPERS) led a wave of institutional shareholder activism (something only very rarely seen before), as a way of ensuring that corporate value would not be destroyed by the now traditionally cozy relationships between the CEO and the board of directors (for example, by the unrestrained issuance of stock options, not infrequently back-dated).  In the early 2000s, the massive bankruptcies (and criminal malfeasance) of Enron and Worldcom, as well as lesser corporate scandals (such as those involving Adelphia Communications, AOL, Arthur Andersen, Global Crossing, and Tyco) led to increased political interest in corporate governance. This was reflected in the passage of the Sarbanes–Oxley Act of 2002. Other triggers for continued interest in the corporate governance of organizations included the financial crisis of 2008\/9 and the level of CEO pay.[76]  Some corporations have tried to burnish their ethical image by creating whistle-blower protections, such as anonymity. This varies significantly by justification, company and sector.  In 1997 the East Asian Financial Crisis severely affected the economies of Thailand, Indonesia, South Korea, Malaysia, and the Philippines through the exit of foreign capital after property assets collapsed. The lack of corporate governance mechanisms in these countries highlighted the weaknesses of the institutions in their economies.[citation needed]  In the 1990s, China established the Shanghai and Shenzhen Stock Exchanges and the China Securities Regulatory Commission (CSRC) to improve corporate governance. Despite these efforts, state ownership concentration and governance issues such as board independence and insider trading persisted.[77]  In November 2006 the Capital Market Authority (Saudi Arabia) (CMA) issued a corporate governance code in the Arabic language.[78] The Kingdom of Saudi Arabia has made considerable progress with respect to the implementation of viable and culturally appropriate governance mechanisms (Al-Hussain & Johnson, 2009).[79][need quotation to verify]  Al-Hussain, A. and Johnson, R. (2009) found a strong relationship between the efficiency of corporate governance structure and Saudi bank performance when using return on assets as a performance measure with one exception—that government and local ownership groups were not significant. However, using rate of return as a performance measure revealed a weak positive relationship between the efficiency of corporate governance structure and bank performance.[80]  Key parties involved in corporate governance include stakeholders such as the board of directors, management and shareholders. External stakeholders such as creditors, auditors, customers, suppliers, government agencies, and the community at large also exert influence. The agency view of the corporation posits that the shareholder forgoes decision rights (control) and entrusts the manager to act in the shareholders' best (joint) interests. Partly as a result of this separation between the two investors and managers, corporate governance mechanisms include a system of controls intended to help align managers' incentives with those of shareholders. Agency concerns (risk) are necessarily lower for a controlling shareholder.[81]  In private for-profit corporations, shareholders elect the board of directors to represent their interests. In the case of nonprofits, stakeholders may have some role in recommending or selecting board members, but typically the board itself decides who will serve on the board as a 'self-perpetuating' board.[82] The degree of leadership that the board has over the organization varies; in practice at large organizations, the executive management, principally the CEO, drives major initiatives with the oversight and approval of the board.[83]  Former Chairman of the Board of General Motors John G. Smale wrote in 1995: \"The board is responsible for the successful perpetuation of the corporation.  That responsibility cannot be relegated to management.\"[84] A board of directors is expected to play a key role in corporate governance. The board has responsibility for: CEO selection and succession; providing feedback to management on the organization's strategy; compensating senior executives; monitoring financial health, performance and risk; and ensuring accountability of the organization to its investors and authorities. Boards typically have several committees (e.g., Compensation, Nominating and Audit) to perform their work.[85]  The OECD Principles of Corporate Governance (2025) describe the responsibilities of the board; some of these are summarized below:[57]  All parties, not just shareholders, to corporate governance have an interest, whether direct or indirect, in the financial performance of the corporation.[86] Directors, workers and management receive salaries, benefits and reputation, while investors expect to receive financial returns. For lenders, it is specified interest payments, while returns to equity investors arise from dividend distributions or capital gains on their stock. Customers are concerned with the certainty of the provision of goods and services of an appropriate quality; suppliers are concerned with compensation for their goods or services, and possible continued trading relationships. These parties provide value to the corporation in the form of financial, physical, human and other forms of capital. Many parties may also be concerned with corporate social performance.[86]  A key factor in a party's decision to participate in or engage with a corporation is their confidence that the corporation will deliver the party's expected outcomes. When categories of parties (stakeholders) do not have sufficient confidence that a corporation is being controlled and directed in a manner consistent with their desired outcomes, they are less likely to engage with the corporation. When this becomes an endemic system feature, the loss of confidence and participation in markets may affect many other stakeholders, and increases the likelihood of political action. There is substantial interest in how external systems and institutions, including markets, influence corporate governance.[87]  In 2016 the director of the World Pensions Council (WPC) said that \"institutional asset owners now seem more eager to take to task [the] negligent CEOs\" of the companies whose shares they own.[88]  This development is part of a broader trend towards more fully exercised asset ownership—notably from the part of the boards of directors ('trustees') of large UK, Dutch, Scandinavian and Canadian pension investors:  No longer 'absentee landlords', [pension fund] trustees have started to exercise more forcefully their governance prerogatives across the boardrooms of Britain, Benelux and America: coming together through the establishment of engaged pressure groups […] to 'shift the [whole economic] system towards sustainable investment'.[88] This could eventually put more pressure on the CEOs of publicly listed companies, as \"more than ever before, many [North American,] UK and European Union pension trustees speak enthusiastically about flexing their fiduciary muscles for the UN's Sustainable Development Goals\", and other ESG-centric investment practices.[89]  In Britain, \"The widespread social disenchantment that followed the [2008–2012] great recession had an impact\" on all stakeholders, including pension fund board members and investment managers.[90]  Many of the UK's largest pension funds are thus already active stewards of their assets, engaging with corporate boards and speaking up when they think it is necessary.[90]  Control and ownership structure refers to the types and composition of shareholders in a corporation. In some countries such as most of Continental Europe, ownership is not necessarily equivalent to control due to the existence of e.g. dual-class shares, ownership pyramids, voting coalitions, proxy votes and clauses in the articles of association that confer additional voting rights to long-term shareholders.[91] Ownership is typically defined as the ownership of cash flow rights whereas control refers to ownership of control or voting rights.[91] Researchers often \"measure\" control and ownership structures by using some observable measures of control and ownership concentration or the extent of inside control and ownership. Some features or types of control and ownership structure involving corporate groups include pyramids, cross-shareholdings, rings, and webs. German \"concerns\" (Konzern) are legally recognized corporate groups with complex structures. Japanese keiretsu (系列) and South Korean chaebol (which tend to be family-controlled) are corporate groups which consist of complex interlocking business relationships and shareholdings. Cross-shareholding is an essential feature of keiretsu and chaebol groups. Corporate engagement with shareholders and other stakeholders can differ substantially across different control and ownership structures.  In smaller companies founder‐owners often play a pivotal role in shaping corporate value systems that influence companies for years to come. In larger companies that separate ownership and control, managers and boards come to play an influential role.[92] This is in part due to the distinction between employees and shareholders in large firms, where labour forms part of the corporate organization to which it belongs whereas shareholders, creditors and investors act outside of the organization of interest.  Family interests dominate ownership and control structures of some corporations, and it has been suggested that the oversight of family-controlled corporations are superior to corporations \"controlled\" by institutional investors (or with such diverse share ownership that they are controlled by management). A 2003 Business Week study said: \"Forget the celebrity CEO. Look beyond Six Sigma and the latest technology fad. One of the biggest strategic advantages a company can have, it turns out, is blood lines.\"[93] A 2007 study by Credit Suisse found that European companies in which \"the founding family or manager retains a stake of more than 10 per cent of the company's capital enjoyed a superior performance over their respective sectoral peers\", reported Financial Times.[94] Since 1996, this superior performance amounted to 8% per year.[94]  The significance of institutional investors varies substantially across countries. In developed Anglo-American countries (Australia, Canada, New Zealand, U.K., U.S.), institutional investors dominate the market for stocks in larger corporations. While the majority of the shares in the Japanese market are held by financial companies and industrial corporations, these are not institutional investors if their holdings are largely with-on group.[citation needed]  The largest funds of invested money or the largest investment management firm for corporations are designed to maximize the benefits of diversified investment by investing in a very large number of different corporations with sufficient liquidity. The idea is this strategy will largely eliminate individual firm financial or other risk. A consequence of this approach is that these investors have relatively little interest in the governance of a particular corporation. It is often assumed that, if institutional investors pressing for changes decide they will likely be costly because of \"golden handshakes\" or the effort required, they will simply sell out their investment.[citation needed]  Particularly in the United States, proxy access allows shareholders to nominate candidates which appear on the proxy statement, as opposed to restricting that power to the nominating committee. The SEC had attempted a proxy access rule for decades,[95] and the United States Dodd–Frank Wall Street Reform and Consumer Protection Act specifically allowed the SEC to rule on this issue, however, the rule was struck down in court.[95] Beginning in 2015, proxy access rules began to spread driven by initiatives from major institutional investors, and as of 2018, 71% of S&P 500 companies had a proxy access rule.[95]  Corporate governance mechanisms and controls are designed to reduce the inefficiencies that arise from moral hazard and adverse selection. There are both internal monitoring systems and external monitoring systems.[96] Internal monitoring can be done, for example, by one (or a few) large shareholder(s) in the case of privately held companies or a firm belonging to a business group. Furthermore, the various board mechanisms provide for internal monitoring. External monitoring of managers' behavior occurs when an independent third party (e.g. the external auditor) attests the accuracy of information provided by management to investors. Stock analysts and debt holders may also conduct such external monitoring. An ideal monitoring and control system should regulate both motivation and ability, while providing incentive alignment toward corporate goals and objectives. Care should be taken that incentives are not so strong that some individuals are tempted to cross lines of ethical behavior, for example by manipulating revenue and profit figures to drive the share price of the company up.[72]  Internal corporate governance controls monitor activities and then take corrective actions to accomplish organisational goals. Examples include:  In publicly traded U.S. corporations, boards of directors are largely chosen by the president\/CEO, and the president\/CEO often takes the chair of the board position for him\/herself (which makes it much more difficult for the institutional owners to \"fire\" him\/her). The practice of the CEO also being the chair of the Board is fairly common in large American corporations.[99]  While this practice is common in the U.S., it is relatively rare elsewhere. In the U.K., successive codes of best practice have recommended against duality.[citation needed]  External corporate governance controls the external stakeholders' exercise over the organization. Examples include:  The board of directors has primary responsibility for the corporation's internal and external financial reporting functions. The chief executive officer and chief financial officer are crucial participants, and boards usually have a high degree of reliance on them for the integrity and supply of accounting information. They oversee the internal accounting systems, and are dependent on the corporation's accountants and internal auditors.  Current accounting rules under International Accounting Standards and U.S. GAAP allow managers some choice in determining the methods of measurement and criteria for recognition of various financial reporting elements. The potential exercise of this choice to improve apparent performance increases the information risk for users. Financial reporting fraud, including non-disclosure and deliberate falsification of values also contributes to users' information risk. To reduce this risk and to enhance the perceived integrity of financial reports, corporation financial reports must be audited by an independent external auditor who issues a report that accompanies the financial statements.  One area of concern is whether the auditing firm acts as both the independent auditor and management consultant to the firm they are auditing. This may result in a conflict of interest which places the integrity of financial reports in doubt due to client pressure to appease management. The power of the corporate client to initiate and terminate management consulting services and, more fundamentally, to select and dismiss accounting firms contradicts the concept of an independent auditor. Changes enacted in the United States in the form of the Sarbanes–Oxley Act (following numerous corporate scandals, culminating with the Enron scandal) prohibit accounting firms from providing both auditing and management consulting services. Similar provisions are in place under clause 49 of Standard Listing Agreement in India.  A basic comprehension of corporate positioning on the market can be found by looking at which market area or areas a corporation acts in, and which stages of the respective value chain for that market area or areas it encompasses.[100][101]  A corporation may from time to time decide to alter or change its market positioning – through M&A activity for example – however it may loose some or all of its market efficiency in the process due to commercial operations depending to a large extent on its ability to account for a specific positioning on the market.[102]  Well-designed corporate governance policies also support the sustainability and resilience of corporations and in turn, may contribute to the sustainability and resilience of the broader economy. Investors have increasingly expanded their focus on companies' financial performance to include the financial risks and opportunities posed by broader economic, environmental and societal challenges, and companies' resilience to and management of those risks. In some jurisdictions, policy makers also focus on how companies' operations may contribute to addressing such challenges. A sound framework for corporate governance with respect to sustainability matters can help companies recognise and respond to the interests of shareholders and different stakeholders, as well as contribute to their own long-term success. Such a framework should include the disclosure of material sustainability-related information that is reliable, consistent and comparable, including related to climate change. In some cases, jurisdictions may interpret concepts of sustainability-related disclosure and materiality in terms of applicable standards articulating information that a reasonable shareholder needs in order to make investment or voting decisions.  Increasing attention and regulation (as under the Swiss referendum \"against corporate rip-offs\" of 2013) has been brought to executive pay levels since the financial crisis of 2007–2008. Research on the relationship between firm performance and executive compensation does not identify consistent and significant relationships between executives' remuneration and firm performance. Not all firms experience the same levels of agency conflict, and external and internal monitoring devices may be more effective for some than for others.[76][104] Some researchers have found that the largest CEO performance incentives came from ownership of the firm's shares, while other researchers found that the relationship between share ownership and firm performance was dependent on the level of ownership. The results suggest that increases in ownership above 20% cause management to become more entrenched, and less interested in the welfare of their shareholders.[104]  Some argue that firm performance is positively associated with share option plans and that these plans direct managers' energies and extend their decision horizons toward the long-term, rather than the short-term, performance of the company. However, that point of view came under substantial criticism circa in the wake of various security scandals including mutual fund timing episodes and, in particular, the backdating of option grants as documented by University of Iowa academic Erik Lie[105] and reported by James Blander and Charles Forelle of the Wall Street Journal.[104][106]  Even before the negative influence on public opinion caused by the 2006 backdating scandal, use of options faced various criticisms. A particularly forceful and long running argument concerned the interaction of executive options with corporate stock repurchase programs. Numerous authorities (including U.S. Federal Reserve Board economist Weisbenner) determined options may be employed in concert with stock buybacks in a manner contrary to shareholder interests. These authors argued that, in part, corporate stock buybacks for U.S. Standard & Poor's 500 companies surged to a $500 billion annual rate in late 2006 because of the effect of options.[107]  A combination of accounting changes and governance issues led options to become a less popular means of remuneration as 2006 progressed, and various alternative implementations of buybacks surfaced to challenge the dominance of \"open market\" cash buybacks as the preferred means of implementing a share repurchase plan.  Shareholders elect a board of directors, who in turn hire a chief executive officer (CEO) to lead management. The primary responsibility of the board relates to the selection and retention of the CEO. However, in many U.S. corporations the CEO and chairman of the board roles are held by the same person. This creates an inherent conflict of interest between management and the board.  Critics of combined roles argue the two roles that should be separated to avoid the conflict of interest and more easily enable a poorly performing CEO to be replaced. Warren Buffett wrote in 2014: \"In my service on the boards of nineteen public companies, however, I've seen how hard it is to replace a mediocre CEO if that person is also Chairman. (The deed usually gets done, but almost always very late.)\"[108]  Advocates argue that empirical studies do not indicate that separation of the roles improves stock market performance and that it should be up to shareholders to determine what corporate governance model is appropriate for the firm.[109]  In 2004, 73.4% of U.S. companies had combined roles; this fell to 57.2% by May 2012. Many U.S. companies with combined roles have appointed a \"Lead Director\" to improve independence of the board from management. German and UK companies have generally split the roles in nearly 100% of listed companies. Empirical evidence does not indicate one model is superior to the other in terms of performance. However, one study indicated that poorly performing firms tend to remove separate CEOs more frequently than when the CEO\/Chair roles are combined.[110]  Certain groups of shareholders may become disinterested in the corporate governance process, potentially creating a power vacuum in corporate power. Insiders, other shareholders, and stakeholders may take advantage of these situations to exercise greater influence and extract rents from the corporation. Shareholder apathy may result from the increasing popularity of passive investing, diversification, and investment vehicles such as mutual funds and ETFs. "},"meta":{},"created_at":"2025-03-22T14:25:42.293407Z","updated_at":"2025-03-22T14:25:42.293407Z","inner_id":105,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":114,"annotations":[{"id":114,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.327895Z","updated_at":"2025-03-22T14:25:42.327895Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"b2bf9123-858b-47fd-9b16-f79e105aab34","import_id":null,"last_action":null,"bulk_created":false,"task":114,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"Capital budgeting in corporate finance, corporate planning and accounting is an area of capital management that concerns the planning process used to determine whether an organization's long term capital investments such as new machinery, replacement of machinery, new plants, new products, and research  development projects are worth the funding of cash through the firm's capitalization structures (debt, equity or retained earnings). It is the process of allocating resources for major capital, or investment, expenditures.[1]   An underlying goal, consistent with the overall approach in corporate finance,[2] is to increase the value of the firm to the shareholders.   Capital budgeting is typically considered a non-core business activity as it is not part of the revenue model or models of most types of firms, or even a part of daily operations. It holds a strategic financial function within a business. One example of a firm type where capital budgeting is possibly a part of the core business activities is with investment banks, as their revenue model or models rely on financial strategy to a considerable degree.[3][4][5][6]  For the budget allocated to ongoing expenses and revenue, see operating budget.  Many formal methods are used in capital budgeting, including the techniques such as  These methods use the incremental cash flows from each potential investment, or project. Techniques based on accounting earnings and accounting rules are sometimes used - though economists consider this to be improper -  such as the accounting rate of return, and \"return on investment.\"  Simplified and hybrid methods are used as well, such as payback period and discounted payback period.  Cash flows are discounted at the cost of capital to give the net present value (NPV) added to the firm. Unless capital is constrained, or there are dependencies between projects, in order to maximize the value added to the firm, the firm would accept all projects with positive NPV. This method accounts for the time value of money. For the mechanics of the valuation here, see Valuation using discounted cash flows.  Mutually exclusive projects are a set of projects from which at most one will be accepted, for example, a set of projects which accomplish the same task. Thus when choosing between mutually exclusive projects, more than one of the projects may satisfy the capital budgeting criterion, but only one project can be accepted; see below #Ranked projects.  The internal rate of return (IRR) is the discount rate that gives a net present value (NPV) of zero. It is a widely used measure of investment efficiency. To maximize return, sort projects in order of IRR.  Many projects have a simple cash flow structure, with a negative cash flow at the start, and subsequent cash flows are positive. In such a case, if the IRR is greater than the cost of capital, the NPV is positive, so for non-mutually exclusive projects in an unconstrained environment, applying this criterion will result in the same decision as the NPV method.  An example of a project with cash flows which do not conform to this pattern is a loan, consisting of a positive cash flow at the beginning, followed by negative cash flows later. The greater the IRR of the loan, the higher the rate the borrower must pay, so clearly, a lower IRR is preferable in this case. Any such loan with IRR less than the cost of capital has a positive NPV.  Excluding such cases, for investment projects, where the pattern of cash flows is such that the higher the IRR, the higher the NPV, for mutually exclusive projects, the decision rule of taking the project with the highest IRR will maximize the return, but it may select a project with a lower NPV.  In some cases, several solutions to the equation NPV = 0 may exist, meaning there is more than one possible IRR. The IRR exists and is unique if one or more years of net investment (negative cash flow) are followed by years of net revenues.  But if the signs of the cash flows change more than once, there may be several IRRs.  The IRR equation generally cannot be solved analytically but only via iterations.  IRR is the return on capital invested, over the sub-period it is invested. It may be impossible to reinvest intermediate cash flows at the same rate as the IRR. Accordingly, a measure called Modified Internal Rate of Return (MIRR) is designed to overcome this issue, by simulating reinvestment of cash flows at a second rate of return.  Despite a strong academic preference for maximizing the value of the firm according to NPV, surveys indicate that executives prefer to maximize returns[citation needed].  The equivalent annuity method expresses the NPV as an annualized cash flow by dividing it by the present value of the annuity factor.  It is often used when assessing only the costs of specific projects that have the same cash inflows.  In this form, it is known as the equivalent annual cost (EAC) method and is the cost per year of owning and operating an asset over its entire lifespan.  It is often used when comparing investment projects of unequal lifespans.  For example, if project A has an expected lifetime of 7 years, and project B has an expected lifetime of 11 years it would be improper to simply compare the net present values (NPVs) of the two projects, unless the projects could not be repeated.  The use of the EAC method implies that the project will be replaced by an identical project.  Alternatively, the chain method can be used with the NPV method under the assumption that the projects will be replaced with the same cash flows each time. To compare projects of unequal length, say, 3 years and 4 years, the projects are chained together, i.e. four repetitions of the 3-year project are compare to three repetitions of the 4-year project.  The chain method and the EAC method give mathematically equivalent answers.  The assumption of the same cash flows for each link in the chain is essentially an assumption of zero inflation, so a real interest rate rather than a nominal interest rate is commonly used in the calculations.  Real options analysis has become important since the 1970s as option pricing models have gotten more sophisticated.  The discounted cash flow methods essentially value projects as if they were risky bonds, with the promised cash flows known.  But managers will have many choices of how to increase future cash inflows, or to decrease future cash outflows.  In other words, managers get to manage the projects - not simply accept or reject them.  Real options analysis tries to value the choices - the option value - that the managers will have in the future and adds these values to the NPV.  The real value of capital budgeting is to rank projects.  Most organizations have many projects that could potentially be financially rewarding.  Once it has been determined that a particular project has exceeded its hurdle, then it should be ranked against peer projects (e.g. - highest Profitability index to lowest Profitability index).  The highest ranking projects should be implemented until the budgeted capital has been expended.  Capital budgeting investments and projects must be funded through excess cash provided through the raising of debt capital, equity capital, or the use of retained earnings.   Debt capital is borrowed cash, usually in the form of bank loans, or bonds issued to creditors.   Equity capital are investments made by shareholders, who purchase shares in the company's stock.   Retained earnings are excess cash surplus from the company's present and past earnings.  Each of these sources has its own characteristics re (i) the required rate of return expected by capital providers, with the consequent impact on overall cost of capital, as well as (ii) implications for cash flow.  The \"financing mix\" selected will thus effect the valuation of the firm: Corporate finance § Capitalization structure discusses these two interrelated considerations . "},"meta":{},"created_at":"2025-03-22T14:25:42.293407Z","updated_at":"2025-03-22T14:25:42.293407Z","inner_id":106,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":115,"annotations":[{"id":115,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.327895Z","updated_at":"2025-03-22T14:25:42.327895Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"bfa735f3-0ba3-46cc-b48c-8eef7d355667","import_id":null,"last_action":null,"bulk_created":false,"task":115,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"In finance, a futures contract (sometimes called futures) is a standardized legal contract to buy or sell something at a predetermined price for delivery at a specified time in the future, between parties not yet known to each other. The item transacted is usually a commodity or financial instrument. The predetermined price of the contract is known as the forward price or delivery price. The specified time in the future when delivery and payment occur is known as the delivery date. Because it derives its value from the value of the underlying asset, a futures contract is a derivative.  Contracts are traded at futures exchanges, which act as a marketplace between buyers and sellers. The buyer of a contract is said to be the long position holder and the selling party is said to be the short position holder.[1] As both parties risk their counter-party reneging if the price goes against them, the contract may involve both parties lodging as security a margin of the value of the contract with a mutually trusted third party. For example, in gold futures trading, the margin varies between 2% and 20% depending on the volatility of the spot market.[2]  A stock future is a cash-settled futures contract on the value of a particular stock market index. Stock futures are one of the high risk trading instruments in the market. Stock market index futures are also used as indicators to determine market sentiment.[3]  The first futures contracts were negotiated for agricultural commodities, and later futures contracts were negotiated for natural resources such as oil. Financial futures were introduced in 1972, and in recent decades, currency futures, interest rate futures, stock market index futures, and cryptocurrency inverse futures and perpetual futures have played an increasingly large role in the overall futures markets. Even organ futures have been proposed to increase the supply of transplant organs.[4]  The original use of futures contracts mitigates the risk of price or exchange rate movements by allowing parties to fix prices or rates in advance for future transactions. This could be advantageous when (for example) a party expects to receive payment in foreign currency in the future and wishes to guard against an unfavorable movement of the currency in the interval before payment is received.[5]  However, futures contracts also offer opportunities for speculation in that a trader who predicts that the price of an asset will move in a particular direction can contract to buy or sell it in the future at a price which (if the prediction is correct) will yield a profit. In particular, if the speculator is able to profit, then the underlying commodity that the speculator traded would have been saved during a time of surplus and sold during a time of need, offering the consumers of the commodity a more favorable distribution of commodity over time.[2]  The Dōjima Rice Exchange, first established in 1697 in Osaka, is considered by some to be the first futures exchange market, to meet the needs of samurai who—being paid in rice—needed a stable conversion to coin after a series of bad harvests.[6]  The Chicago Board of Trade (CBOT) listed the first-ever standardized 'exchange traded' forward contracts in 1864, which were called futures contracts. This contract was based on grain trading, and started a trend that saw contracts created on a number of different standardized futures contracts based on commodities, as well as a number of futures exchanges set up in countries around the world.[7] By 1875 cotton futures were being traded in Bombay in India and within a few years this had expanded to futures on edible oilseeds complex, raw jute and jute goods and bullion.[8]  In the 1930s two thirds of all futures was in wheat.[9]  The 1972 creation of the International Monetary Market (IMM) by the Chicago Mercantile Exchange was the world's first financial futures exchange, and launched currency futures. In 1976, the IMM added interest rate futures on US treasury bills, and in 1982 they added stock market index futures.[10]  Although futures contracts are oriented towards a future time point, their main purpose is to mitigate the risk of default by either party in the intervening period. In this vein, the futures exchange requires both parties to put up initial cash, or a performance bond, known as the margin. Margins, sometimes set as a percentage of the value of the futures contract, must be maintained throughout the life of the contract to guarantee the agreement, as over this time the price of the contract can vary as a function of supply and demand, causing one side of the exchange to lose money at the expense of the other.  To mitigate the risk of default, the product is marked to market on a daily basis where the difference between the initial agreed-upon price and the actual daily futures price is re-evaluated daily. This is sometimes known as the variation margin, where the futures exchange will draw money out of the losing party's margin account and put it into that of the other party, ensuring the correct loss or profit is reflected daily. If the margin account goes below a certain value set by the exchange, then a margin call is made and the account owner must replenish the margin account.  On the delivery date, the amount exchanged is not the specified price on the contract but the spot value, since any gain or loss has already been previously settled by marking to market.  To minimize counterparty risk to traders, trades executed on regulated futures exchanges are guaranteed by a clearing house. The clearing house becomes the buyer to each seller, and the seller to each buyer, so that in the event of a counterparty default the clearer assumes the risk of loss. This enables traders to transact without performing due diligence on their counterparty.  Margin requirements are waived or reduced in some cases for hedgers who have physical ownership of the covered commodity or spread traders who have offsetting contracts balancing the position.  Clearing margin are financial safeguards to ensure that companies or corporations perform on their customers' open futures and options contracts. Clearing margins are distinct from customer margins that individual buyers and sellers of futures and options contracts are required to deposit with brokers.  Customer margin Within the futures industry, financial guarantees required of both buyers and sellers of futures contracts and sellers of options contracts to ensure fulfillment of contract obligations. Futures Commission Merchants are responsible for overseeing customer margin accounts. Margins are determined on the basis of market risk and contract value. Also referred to as performance bond margin.  Initial margin is the equity required to initiate a futures position. This is a type of performance bond. The maximum exposure is not limited to the amount of the initial margin, however, the initial margin requirement is calculated based on the maximum estimated change in contract value within a trading day. The initial margin is set by the exchange.  If a position involves an exchange-traded product, the amount or percentage of the initial margin is set by the exchange concerned.  In case of loss or if the value of the initial margin is being eroded, the broker will make a margin call in order to restore the amount of initial margin available. Often referred to as \"variation margin\", margin called, for this reason, is usually done on a daily basis, however, in times of high volatility, a broker can make a margin call intra-day.  Margin calls are usually expected to be paid and received on the same day. If not, the broker has the right to close sufficient positions to meet the amount called by way of margin. After the position is closed out the client is liable for any resulting deficit in the client's account.  Some U.S. exchanges also use the term \"maintenance margin\", which in effect defines how much the value of the initial margin can reduce before a margin call is made. However, most non-US brokers only use the term \"initial margin\" and \"variation margin\".  The Initial Margin requirement is established by the Futures exchange, in contrast to other securities' Initial Margin (which is set by the Federal Reserve in the U.S. Markets).  A futures account is marked to market daily. If the margin drops below the margin maintenance requirement established by the exchange listing the futures, a margin call will be issued to bring the account back up to the required level.  Maintenance margin A set minimum margin per outstanding futures contract that a customer must maintain in their margin account.  Margin-equity ratio is a term used by speculators, representing the amount of their trading capital that is being held as margin at any particular time. The low margin requirements of futures results in substantial leverage of the investment. However, the exchanges require a minimum amount that varies depending on the contract and the trader. The broker may set the requirement higher, but may not set it lower. A trader, of course, can set it above that, if he does not want to be subject to margin calls.  Performance bond margin The amount of money deposited by both a buyer and seller of a futures contract or an options seller to ensure the performance of the term of the contract. Margin in commodities is not a payment of equity or down payment on the commodity itself, but rather it is a security deposit.  Return on margin (ROM) is often used to judge performance because it represents the gain or loss compared to the exchange's perceived risk as reflected in the required margin. ROM may be calculated (realized return) \/ (initial margin). The Annualized ROM is equal to (ROM+1)(year\/trade_duration)-1. For example, if a trader earns 10% on margin in two months, that would be about 77% annualized.  Expiry (or \"expiration\" in the U.S.) is the time and the day that a particular delivery month of a futures contract stops trading, as well as the final settlement price for that contract. For many equity index futures and interest rate futures as well as for most equity (index) options, this happens on the third Friday of certain trading months. On this day the back month futures contract becomes the front-month futures contract, and the front-month futures contract becomes the back month futures contract.   For example, for most CME and CBOT contracts, at the expiration of the December contract, the March futures become the nearest contract. During a short period (perhaps 30 minutes) the underlying cash price and the futures prices sometimes struggle to converge. At this moment the futures and the underlying assets are extremely liquid and any disparity between an index and an underlying asset is quickly traded by arbitrageurs. At this moment also, the increase in volume is caused by traders rolling over positions to the next contract or, in the case of equity index futures, purchasing underlying components of those indexes to hedge against current index positions.   On the expiry date, a European equity arbitrage trading desk in London or Frankfurt will see positions expire in as many as eight major markets every approximate half hour. Exchanges implement strict limits on how much exposure an entity may have closer to expiration as an effort to avoid any volatility around final settlement.  Final settlement is the act of consummating the contract, and can be done in one of two ways, as specified per type of futures contract:  Final settlement is distinct from trade settlement, which confirms that the security has been fully paid for and delivered, and mark-to-market settlement, which keeps the price of the contract commensurate with the price of the underlying asset.  When the deliverable asset exists in plentiful supply or may be freely created, then the price of a futures contract is determined via arbitrage arguments. This is typical for stock index futures, treasury bond futures, and futures on physical commodities when they are in supply (e.g. agricultural crops after the harvest). However, when the deliverable commodity is not in plentiful supply or when it does not yet exist—for example on crops before the harvest or on Eurodollar Futures or Federal funds rate futures (in which the supposed underlying instrument is to be created upon the delivery date)—the futures price cannot be fixed by arbitrage. In this scenario, there is only one force setting the price, which is simple supply and demand for the asset in the future, as expressed by supply and demand for the futures contract.  Arbitrage arguments (\"rational pricing\") apply when the deliverable asset exists in plentiful supply or may be freely created. Here, the forward price represents the expected future value of the underlying discounted at the risk-free rate—as any deviation from the theoretical price will afford investors a risk-free profit opportunity and should be arbitraged away. We define the forward price to be the strike K such that the contract has 0 value at the present time. Assuming interest rates are constant the forward price of the futures is equal to the forward price of the forward contract with the same strike and maturity. It is also the same if the underlying asset is uncorrelated with interest rates. Otherwise, the difference between the forward price on the futures (futures price) and the forward price on the asset, is proportional to the covariance between the underlying asset price and interest rates. For example, a futures contract on a zero-coupon bond will have a futures price lower than the forward price. This is called the futures \"convexity correction\".  Thus, assuming constant rates, for a simple, non-dividend paying asset, the value of the futures\/forward price, F(t,T), will be found by compounding the present value S(t) at time t to maturity T by the rate of risk-free return r.  or, with continuous compounding  This relationship may be modified for storage costs u, dividend or income yields q, and convenience yields y. Storage costs are costs involved in storing a commodity to sell at the futures price. Investors selling the asset at the spot price to arbitrage a futures price earns the storage costs they would have paid to store the asset to sell at the futures price. Convenience yields are benefits of holding an asset for sale at the futures price beyond the cash received from the sale. Such benefits could include the ability to meet unexpected demand, or the ability to use the asset as an input in production.[12] Investors pay or give up the convenience yield when selling at the spot price because they give up these benefits. Such a relationship can be summarized as:  The convenience yield is not easily observable or measured, so y is often calculated, when r and u are known, as the extraneous yield paid by investors selling at spot to arbitrage the futures price.[13] Dividend or income yields q are more easily observed or estimated, and can be incorporated in the same way:[14]  In a perfect market, the relationship between futures and spot prices depends only on the above variables; in practice, there are various market imperfections (transaction costs, differential borrowing, and lending rates, restrictions on short selling) that prevent complete arbitrage. Thus, the futures price in fact varies within arbitrage boundaries around the theoretical price.  When the deliverable commodity is not in plentiful supply (or when it does not yet exist) rational pricing cannot be applied, as the arbitrage mechanism is not applicable. Here the price of the futures is determined by today's supply and demand for the underlying asset in the future.  In an efficient market, supply and demand would be expected to balance out at a futures price that represents the present value of an unbiased expectation of the price of the asset at the delivery date. This relationship can be represented as[15]::  By contrast, in a shallow and illiquid market, or in a market in which large quantities of the deliverable asset have been deliberately withheld from market participants (an illegal action known as cornering the market), the market clearing price for the futures may still represent the balance between supply and demand but the relationship between this price and the expected future price of the asset can break down.  The expectation-based relationship will also hold in a no-arbitrage setting when we take expectations with respect to the risk-neutral probability. In other words: a futures price is a martingale with respect to the risk-neutral probability. With this pricing rule, a speculator is expected to break even when the futures market fairly prices the deliverable commodity.  The situation where the price of a commodity for future delivery is higher than the expected spot price is known as contango. Markets are said to be normal when futures prices are above the current spot price and far-dated futures are priced above near-dated futures. The reverse, where the price of a commodity for future delivery is lower than the expected spot price is known as backwardation.  Similarly, markets are said to be inverted when futures prices are below the current spot price and far-dated futures are priced below near-dated futures.  There are many different kinds of futures contracts, reflecting the many different kinds of \"tradable\" assets about which the contract may be based such as commodities, securities (such as single-stock futures), currencies or intangibles such as interest rates and indexes. For information on futures markets in specific underlying commodity markets, follow the links. For a list of tradable commodities futures contracts, see List of traded commodities. See also the futures exchange article.  Trading on commodities began in Japan in the 18th century with the trading of rice and silk, and similarly in Holland with tulip bulbs. Trading in the US began in the mid 19th century when central grain markets were established and a marketplace was created for farmers to bring their commodities and sell them either for immediate delivery (also called spot or cash market) or for forward delivery. These forward contracts were private contracts between buyers and sellers and became the forerunner to today's exchange-traded futures contracts. Although contract trading began with traditional commodities such as grains, meat, and livestock, exchange trading has expanded to include metals, energy, currency and currency indexes, equities and equity indexes, government interest rates, and private interest rates.  Contracts on financial instruments were introduced in the 1970s by the Chicago Mercantile Exchange (CME) and these instruments became hugely successful and quickly overtook commodities futures in terms of trading volume and global accessibility to the markets. This innovation led to the introduction of many new futures exchanges worldwide, such as the London International Financial Futures Exchange in 1982 (now Euronext. liffe), Deutsche Terminbörse (now Eurex) and the Tokyo Commodity Exchange (TOCOM). Today, there are more than 90 futures and futures options exchanges worldwide trading to include:  Most futures contract codes are five characters. The first two characters identify the contract type, the third character identifies the month and the last two characters identify the year.  On CME Group markets, third (month) futures contract codes are:[17] Contracts expire after the listing month. Therefore traders must roll over their positions into the next month code.  Example: CLX14 is a Crude Oil (CL), November (X) 2014 (14) contract.  Futures traders are traditionally placed in one of two groups: hedgers and speculators. Hedgers have an interest in the underlying asset (which could include an intangible such as an index or interest rate) and are seeking to hedge out the risk of price changes. Speculators, by contrast, seek to make a profit by predicting market moves and opening a derivative contract related to the asset \"on paper\", while they have no practical use for or intent to actually take or make delivery of the underlying asset. In other words, speculators are seeking exposure to the asset in long futures contracts or the opposite effect via short futures contracts.  Hedgers typically include producers and consumers of a commodity or the owner of an asset or assets subject to certain influences such as an interest rate.  For example, in traditional commodity markets, farmers often sell futures contracts for the crops and livestock they produce to guarantee a certain price, making it easier for them to plan. Similarly, livestock producers often purchase futures to cover their feed costs, so that they can plan on a fixed cost for feed. In modern (financial) markets, \"producers\" of interest rate swaps or equity derivative products will use financial futures or equity index futures to reduce or remove the risk on the swap.  Those that buy or sell commodity futures need to be careful. If a company buys contracts hedging against price increases, but in fact, the market price of the commodity is substantially lower at the time of delivery, they could find themselves disastrously non-competitive (for example see: VeraSun Energy).  Investment fund managers at the portfolio and the fund sponsor level can use financial asset futures to manage portfolio interest rate risk, or duration, without making cash purchases or sales using bond futures.[18] Invest firms that receive capital calls or capital inflows in a different currency than their base currency could use currency futures to hedge the currency risk of that inflow in the future.[19]  Speculators typically fall into three categories: position traders, day traders, and swing traders (swing trading), though many hybrid types and unique styles exist. With many investors pouring into the futures markets in recent years controversy has risen about whether speculators are responsible for increased volatility in commodities like oil, and experts are divided on the matter.[20]  An example that has both hedge and speculative notions involves a mutual fund or separately managed account whose investment objective is to track the performance of a stock index such as the S&P 500 stock index. The portfolio manager often \"equitizes\" unintended cash holdings or cash inflows in an easy and cost-effective manner by investing in (opening long) S&P 500 stock index futures. This gains the portfolio exposure to the index which is consistent with the fund or account investment objective without having to buy an appropriate proportion of each of the individual 500 stocks just yet. This also preserves balanced diversification, maintains a higher degree of the percent of assets invested in the market and helps reduce tracking error in the performance of the fund\/account. When it is economically feasible (an efficient amount of shares of every individual position within the fund or account can be purchased), the portfolio manager can close the contract and make purchases of each individual stock.[21]  The social utility of futures markets is considered to be mainly in the transfer of risk, and increased liquidity between traders with different risk and time preferences, from a hedger to a speculator, for example.[1]  In many cases, options are traded on futures, sometimes called simply \"futures options\". A put is the option to sell a futures contract, and a call is the option to buy a futures contract. For both, the option strike price is the specified futures price at which the futures is traded if the option is exercised. Futures are often used since they are delta one instruments. Calls and options on futures may be priced similarly to those on traded assets by using an extension of the Black-Scholes formula, namely the Black model. For options on futures, where the premium is not due until unwound, the positions are commonly referred to as a fution, as they act like options, however, they settle like futures.  Investors can either take on the role of option seller (or \"writer\") or the option buyer. Option sellers are generally seen as taking on more risk because they are contractually obligated to take the opposite futures position if the buyer of the option exercises their right to the futures position specified in the option. The price of an option is determined by supply and demand principles and consists of the option premium, or the price paid to the option seller for offering the option and taking on risk.[22]  Where as futures often matures on a quarterly or monthly basis, their options expires more frequent (i.e. daily). Examples are options on futures with the underlying in gold (XAU), index (Nasdaq, S&P 500) or commodities (oil, VIX). The stock exchanges and their clearing houses provide overviews of these products (CME,[23] COMEX, NYMEX).  All futures transactions in the United States are regulated by the Commodity Futures Trading Commission (CFTC), an independent agency of the United States government. The commission has the right to impose fines and other punishments on individuals or companies that break rules. Although by law the commission regulates all transactions, each exchange can have its own rule, and under contract can fine companies for different things or extend the fine that the CFTC hands out.  The CFTC publishes weekly reports containing details of the open interest of market participants for each market segment that has more than 20 participants. These reports are released every Friday (including data from the previous Tuesday) and contain data on open interest split by reportable and non-reportable open interest as well as commercial and non-commercial open interest. This type of report is referred to as the 'Commitments of Traders Report', COT-Report, or simply COTR.  The following definition from Björk[24] describes a futures contract with delivery of item J at time T:  A closely related contract is a forward contract. A forward is like a futures in that it specifies the exchange of goods for a specified price at a specified future date. However, a forward is not traded on an exchange and thus does not have the interim partial payments due to marking to market.  While futures and forward contracts are both contracts to deliver an asset on a future date at a prearranged price, they are different in two main respects:  Forwards have credit risk, but futures do not because a clearing house guarantees against default risk by taking both sides of the trade and marking to market their positions every night. Forwards are basically unregulated, while futures contracts are regulated at the central government level.  The Futures Industry Association (FIA) estimates that 6.97 billion futures contracts were traded in 2007, an increase of nearly 32% over the 2006 figure.  Futures are always traded on an exchange, whereas forwards always trade over-the-counter,[25] or can simply be a signed contract between two parties. Therefore:  A variety of trades have developed that involve an exchange of a futures contract for either a over-the-counter, physical commodity, or cash asset position that meets certain criteria such as scale and correspondence to the underlying commodity risk.[26][27][28]  Futures are margined daily to the daily spot price of a forward with the same agreed-upon delivery price and the underlying asset (based on mark to market).  Forwards do not have a standard. More typical would be for the parties to agree to true up, for example, every quarter. The fact that forwards are not margined daily means that, due to movements in the price of the underlying asset, a large differential can build up between the forward's delivery price and the settlement price, and in any event, an unrealized gain (loss) can build up.  Again, this differs from futures which get 'trued-up' typically daily by a comparison of the market value of the futures to the collateral securing the contract to keep it in line with the brokerage margin requirements. This true-ing up occurs by the \"loss\" party providing additional collateral; so if the buyer of the contract incurs a drop in value, the shortfall or variation margin would typically be shored up by the investor wiring or depositing additional cash in the brokerage account.  In a forward though, the spread in exchange rates is not trued up regularly but, rather, it builds up as unrealized gain (loss) depending on which side of the trade being discussed. This means that entire unrealized gain (loss) becomes realized at the time of delivery (or as what typically occurs, the time the contract is closed prior to expiration)—assuming the parties must transact at the underlying currency's spot price to facilitate receipt\/delivery.  The result is that forwards have higher credit risk than futures, and that funding is charged differently.  The situation for forwards, however, where no daily true-up takes place, in turn, creates credit risk for forwards, but not so much for futures. Simply put, the risk of a forward contract is that the supplier will be unable to deliver the referenced asset, or that the buyer will be unable to pay for it on the delivery date or the date at which the opening party closes the contract.  The margining of futures eliminates much of this credit risk by forcing the holders to update daily to the price of an equivalent forward purchased that day. This means that there will usually be very little additional money due on the final day to settle the futures contract: only the final day's gain or loss, not the gain or loss over the life of the contract.  In addition, the daily futures-settlement failure risk is borne by an exchange, rather than an individual party, further limiting credit risk in futures.  Example: Consider a futures contract with a $100 price: Let's say that on day 50, a futures contract with a $100 delivery price (on the same underlying asset as the future) costs $88. On day 51, that futures contract costs $90. This means that the \"mark-to-market\" calculation would require the holder of one side of the futures to pay $2 on day 51 to track the changes of the forward price (\"post $2 of margin\"). This money goes, via margin accounts, to the holder of the other side of the future. That is the loss party wires cash to the other party.  A forward-holder, however, may pay nothing until settlement on the final day, potentially building up a large balance; this may be reflected in the mark by an allowance for credit risk. So, except for tiny effects of convexity bias (due to earning or paying interest on margin), futures and forwards with equal delivery prices result in the same total loss or gain, but holders of futures experience that loss\/gain in daily increments which track the forward's daily price changes, while the forward's spot price converges to the settlement price. Thus, while under mark to market accounting, for both assets the gain or loss accrues over the holding period; for a futures this gain or loss is realized daily, while for a forward contract the gain or loss remains unrealized until expiry.  With an exchange-traded future, the clearing-house interposes itself on every trade. Thus there is no risk of counterparty default. The only risk is that the clearing house defaults (e.g. become bankrupt), which is considered very unlikely.[citation needed] "},"meta":{},"created_at":"2025-03-22T14:25:42.293407Z","updated_at":"2025-03-22T14:25:42.293407Z","inner_id":107,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":116,"annotations":[{"id":116,"completed_by":1,"result":[{"value":{"choices":["finance"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.327895Z","updated_at":"2025-03-22T14:25:42.327895Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"4274ea04-a768-4543-90d7-c117b504c261","import_id":null,"last_action":null,"bulk_created":false,"task":116,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"In finance, a bond is a type of security under which the issuer (debtor) owes the holder (creditor) a debt, and is obliged – depending on the terms – to provide cash flow to the creditor (e.g. repay the principal (i.e. amount borrowed) of the bond at the maturity date and interest (called the coupon) over a specified amount of time.[1]) The timing and the amount of cash flow provided varies, depending on the economic value that is emphasized upon, thus giving rise to different types of bonds.[1] The interest is usually payable at fixed intervals: semiannual, annual, and less often at other periods. Thus, a bond is a form of loan or IOU. Bonds provide the borrower with external funds to finance long-term investments or, in the case of government bonds, to finance current expenditure.  Bonds and stocks are both securities, but the major difference between the two is that (capital) stockholders have an equity stake in a company (i.e. they are owners), whereas bondholders have a creditor stake in a company (i.e. they are lenders). As creditors, bondholders have priority over stockholders. This means they will be repaid in advance of stockholders, but will rank behind secured creditors, in the event of bankruptcy. Another difference is that bonds usually have a defined term, or maturity, after which the bond is redeemed, whereas stocks typically remain outstanding indefinitely. An exception is an irredeemable bond, which is a perpetuity, that is, a bond with no maturity. Certificates of deposit (CDs) or short-term commercial paper are classified as money market instruments and not bonds: the main difference is the length of the term of the instrument.  The most common forms include municipal, corporate, and government bonds. Very often the bond is negotiable, that is, the ownership of the instrument can be transferred in the secondary market. This means that once the transfer agents at the bank medallion-stamp the bond, it is highly liquid on the secondary market.[2] The price of a bond in the secondary market may differ substantially from the principal due to various factors in bond valuation.  Bonds are often identified by their international securities identification number, or ISIN, which is a 12-digit alphanumeric code that uniquely identifies debt securities.  In English, the word \"bond\" relates to the etymology of \"bind\". The use of the word \"bond\" in this sense of an \"instrument binding one to pay a sum to another\" dates from at least the 1590s.[3][4]  Bonds are issued by public authorities, credit institutions, companies and supranational institutions in the primary markets. The most common process for issuing bonds is through underwriting. When a bond issue is underwritten, one or more securities firms or banks, forming a syndicate, buy the entire issue of bonds from the issuer and resell them to investors. The security firm takes the risk of being unable to sell on the issue to end investors. Primary issuance is arranged by bookrunners who arrange the bond issue, have direct contact with investors and act as advisers to the bond issuer in terms of timing and price of the bond issue. The bookrunner is listed first among all underwriters participating in the issuance in the tombstone ads commonly used to announce bonds to the public. The bookrunners' willingness to underwrite must be discussed prior to any decision on the terms of the bond issue as there may be limited demand for the bonds.  In contrast, government bonds are usually issued in an auction. In some cases, both members of the public and banks may bid for bonds. In other cases, only market makers may bid for bonds. The overall rate of return on the bond depends on both the terms of the bond and the price paid.[5] The terms of the bond, such as the coupon, are fixed in advance and the price is determined by the market.  In the case of an underwritten bond, the underwriters will charge a fee for underwriting. An alternative process for bond issuance, which is commonly used for smaller issues and avoids this cost, is the private placement bond. Bonds sold directly to buyers may not be tradeable in the bond market.[6]  Historically, an alternative practice of issuance was for the borrowing government authority to issue bonds over a period of time, usually at a fixed price, with volumes sold on a particular day dependent on market conditions. This was called a tap issue or bond tap.  Nominal, principal, par, or face amount is the amount on which the issuer pays interest, and which, most commonly, has to be repaid at the end of the term. Some structured bonds can have a redemption amount which is different from the face amount and can be linked to the performance of particular assets.  The issuer is obligated to repay the nominal amount on the maturity date. As long as all due payments have been made, the issuer has no further obligations to the bond holders after the maturity date. The length of time until the maturity date is often referred to as the term or tenor or maturity of a bond. The maturity can be any length of time, although debt securities with a term of less than one year are generally designated money market instruments rather than bonds. Most bonds have a term shorter than 30 years. Some bonds have been issued with terms of 50 years or more, and historically there have been some issues with no maturity date (irredeemable). In the market for United States Treasury securities, there are four categories of bond maturities:  The coupon is the interest rate that the issuer pays to the holder. For fixed rate bonds, the coupon is fixed throughout the life of the bond. For floating rate notes, the coupon varies throughout the life of the bond and is based on the movement of a money market reference rate (historically this was generally LIBOR, but with its discontinuation the market reference rate has transitioned to SOFR).  Historically, coupons were physical attachments to the paper bond certificates, with each coupon representing an interest payment. On the interest due date, the bondholder would hand in the coupon to a bank in exchange for the interest payment. Today, interest payments are almost always paid electronically. Interest can be paid at different frequencies: generally semi-annual (every six months) or annual.  The yield is the rate of return received from investing in the bond. It usually refers to one of the following:  The quality of the issue refers to the probability that the bondholders will receive the amounts promised at the due dates. In other words, credit quality tells investors how likely the borrower is going to default. This will depend on a wide range of factors. High-yield bonds are bonds that are rated below investment grade by the credit rating agencies. As these bonds are riskier than investment grade bonds, investors expect to earn a higher yield. These bonds are also called junk bonds.  The market price of a tradable bond will be influenced, among other factors, by the amounts, currency and timing of the interest payments and capital repayment due, the quality of the bond, and the available redemption yield of other comparable bonds which can be traded in the markets.  The price can be quoted as clean or dirty. \"Dirty\" includes the present value of all future cash flows, including accrued interest, and is most often used in Europe. \"Clean\" does not include accrued interest, and is most often used in the U.S.  The issue price at which investors buy the bonds when they are first issued will typically be approximately equal to the nominal amount. The net proceeds that the issuer receives are thus the issue price, less issuance fees. The market price of the bond will vary over its life: it may trade at a premium (above par, usually because market interest rates have fallen since issue), or at a discount (price below par, if market rates have risen or there is a high probability of default on the bond).  Bonds can be categorised in several ways, such as the type of issuer, the currency, the term of the bond (length of time to maturity) and the conditions applying to the bond. The following descriptions are not mutually exclusive, and more than one of them may apply to a particular bond:  The nature of the issuer will affect the security (certainty of receiving the contracted payments) offered by the bond, and sometimes the tax treatment.  Some companies, banks, governments, and other sovereign entities may decide to issue bonds in foreign currencies as the foreign currency may appear to potential investors to be more stable and predictable than their domestic currency. Issuing bonds denominated in foreign currencies also gives issuers the ability to access investment capital available in foreign markets.  A downside is that the government loses the option to reduce its bond liabilities by inflating its domestic currency.  The proceeds from the issuance of these bonds can be used by companies to break into foreign markets, or can be converted into the issuing company's local currency to be used on existing operations through the use of foreign exchange swap hedges. Foreign issuer bonds can also be used to hedge foreign exchange rate risk. Some foreign issuer bonds are called by their nicknames, such as the \"samurai bond\". These can be issued by foreign issuers looking to diversify their investor base away from domestic markets. These bond issues are generally governed by the law of the market of issuance, e.g., a samurai bond, issued by an investor based in Europe, will be governed by Japanese law. Not all of the following bonds are restricted for purchase by investors in the market of issuance.  The market price of a bond is the present value of all expected future interest and principal payments of the bond, here discounted at the bond's yield to maturity (i.e. rate of return). That relationship is the definition of the redemption yield on the bond, which is likely to be close to the current market interest rate for other bonds with similar characteristics, as otherwise there would be arbitrage opportunities. The yield and price of a bond are inversely related so that when market interest rates rise, bond prices fall and vice versa. For a discussion of the mathematics see Bond valuation.  The bond's market price is usually expressed as a percentage of nominal value: 100% of face value, \"at par\", corresponds to a price of 100; prices can be above par (bond is priced at greater than 100), which is called trading at a premium, or below par (bond is priced at less than 100), which is called trading at a discount. The market price of a bond may be quoted including the accrued interest since the last coupon date. (Some bond markets include accrued interest in the trading price and others add it on separately when settlement is made.) The price including accrued interest is known as the \"full\" or \"dirty price\". (See also Accrual bond.) The price excluding accrued interest is known as the \"flat\" or \"clean price\".  Most government bonds are denominated in units of $1000 in the United States, or in units of £100 in the United Kingdom. Hence, a deep discount US bond, selling at a price of 75.26, indicates a selling price of $752.60 per bond sold. (Often, in the US, bond prices are quoted in points and thirty-seconds of a point, rather than in decimal form.) Some short-term bonds, such as the U.S. Treasury bill, are always issued at a discount, and pay par amount at maturity rather than paying coupons. This is called a discount bond.  Although bonds are not necessarily issued at par (100% of face value, corresponding to a price of 100), their prices will move towards par as they approach maturity (if the market expects the maturity payment to be made in full and on time) as this is the price the issuer will pay to redeem the bond. This is referred to as \"pull to par\". At the time of issue of the bond, the coupon paid, and other conditions of the bond, will have been influenced by a variety of factors, such as current market interest rates, the length of the term and the creditworthiness of the issuer. These factors are likely to change over time, so the market price of a bond will vary after it is issued. (The position is a bit more complicated for inflation-linked bonds.)  The interest payment (\"coupon payment\") divided by the current price of the bond is called the current yield (this is the nominal yield multiplied by the par value and divided by the price). There are other yield measures that exist such as the yield to first call, yield to worst, yield to first par call, yield to put, cash flow yield and yield to maturity. The relationship between yield and term to maturity (or alternatively between yield and the weighted mean term allowing for both interest and capital repayment) for otherwise identical bonds derives the yield curve, a graph plotting this relationship.  If the bond includes embedded options, the valuation is more difficult and combines option pricing with discounting. Depending on the type of option, the option price as calculated is either added to or subtracted from the price of the \"straight\" portion. See further under Bond option § Embedded options. This total is then the value of the bond. More sophisticated lattice- or simulation-based techniques may (also) be employed.  Bond markets, unlike stock or share markets, sometimes do not have a centralized exchange or trading system. Rather, in most developed bond markets such as the U.S., Japan and western Europe, bonds trade in decentralized, dealer-based over-the-counter markets. In such a market, liquidity is provided by dealers and other market participants committing risk capital to trading activity. In the bond market, when an investor buys or sells a bond, the counterparty to the trade is almost always a bank or securities firm acting as a dealer. In some cases, when a dealer buys a bond from an investor, the dealer carries the bond \"in inventory\", i.e. holds it for their own account. The dealer is then subject to risks of price fluctuation. In other cases, the dealer immediately resells the bond to another investor.  Bonds are bought and traded mostly by institutions like central banks, sovereign wealth funds, pension funds, insurance companies, hedge funds, and banks. Insurance companies and pension funds have liabilities which essentially include fixed amounts payable on predetermined dates. They buy the bonds to match their liabilities, and may be compelled by law to do this. Most individuals who want to own bonds do so through bond funds. Still, in the U.S., nearly 10% of all bonds outstanding are held directly by households.  The volatility of bonds (especially short and medium dated bonds) is lower than that of equities (stocks). Thus, bonds are generally viewed as safer investments than equities, but this perception is only partially correct. Bonds do suffer from less day-to-day volatility than stocks, and bonds' interest payments are sometimes higher than the general level of dividend payments. Bonds are often liquid – it is often fairly easy for an institution to sell a large quantity of bonds without affecting the price much, which may be more difficult for equities – and the comparative certainty of a fixed interest payment twice a year and a fixed lump sum at maturity is attractive. Bondholders also enjoy a measure of legal protection: under the law of most countries, if a company goes bankrupt, its bondholders will often receive some money back (the recovery amount), whereas the company's equity stock often ends up valueless. However, bonds can also be risky but less risky than stocks:  Bonds are also subject to various other risks such as call and prepayment risk, credit risk, reinvestment risk, liquidity risk, event risk, exchange rate risk, volatility risk, inflation risk, sovereign risk and yield curve risk. Again, some of these will only affect certain classes of investors.  Price changes in a bond will immediately affect mutual funds that hold these bonds. If the value of the bonds in their trading portfolio falls, the value of the portfolio also falls. This can be damaging for professional investors such as banks, insurance companies, pension funds and asset managers (irrespective of whether the value is immediately \"marked to market\" or not). If there is any chance a holder of individual bonds may need to sell their bonds and \"cash out\", interest rate risk could become a real problem, conversely, bonds' market prices would increase if the prevailing interest rate were to drop, as it did from 2001 through 2003. One way to quantify the interest rate risk on a bond is in terms of its duration. Efforts to control this risk are called immunization or hedging.  There is no guarantee of how much money will remain to repay bondholders. As an example, after an accounting scandal and a Chapter 11 bankruptcy at the giant telecommunications company Worldcom, in 2004 its bondholders ended up being paid 35.7 cents on the dollar.[16] In a bankruptcy involving reorganization or recapitalization, as opposed to liquidation, bondholders may end up having the value of their bonds reduced, often through an exchange for a smaller number of newly issued bonds.  A number of bond indices exist for the purposes of managing portfolios and measuring performance, similar to the S&P 500 or Russell Indexes for companies' shares. The most common American benchmarks are the Bloomberg Barclays US Aggregate (ex Lehman Aggregate), Citigroup BIG and Merrill Lynch Domestic Master. Most indices are parts of families of broader indices that can be used to measure global bond portfolios, or may be further subdivided by maturity or sector for managing specialized portfolios.  Market specific  General "},"meta":{},"created_at":"2025-03-22T14:25:42.293407Z","updated_at":"2025-03-22T14:25:42.293407Z","inner_id":108,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":117,"annotations":[{"id":117,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.327895Z","updated_at":"2025-03-22T14:25:42.327895Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"9d35a2ad-c358-4931-9d83-f6c5560a7d0e","import_id":null,"last_action":null,"bulk_created":false,"task":117,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  The art of motion-picture making within Spain or by Spanish filmmakers abroad is collectively known as Spanish Cinema.  Only a small portion of box office sales in Spain are generated by domestic films. The different Spanish governments have therefore implemented measures aimed at supporting local film production and the movie theaters, which currently include the assurance of funding from the main television broadcasters. Nowadays, the Instituto de la Cinematografía y de las Artes Audiovisuales (ICAA) is the State agency in charge of regulating the allocation of public funds to the domestic film industry.  The first Spanish film exhibition took place on 5 May 1895, in Barcelona. Exhibitions of Lumière films were screened in Madrid, Málaga and Barcelona in May and December 1896, respectively.  The matter of which Spanish film came first is in dispute.[4] The first was either Salida de la misa de doce de la Iglesia del Pilar de Zaragoza –Exit of the Twelve O'Clock Mass from the Church of El Pilar of Zaragoza– (Eduardo Jimeno Peromarta), Plaza del puerto en Barcelona –Plaza of the Port of Barcelona– (Alexandre Promio) or Llegada de un tren de Teruel a Segorbe –Arrival of a Train from Teruel in Segorbe– (anonymous). It is also possible that the first film was Riña en un café (Fructuós Gelabert). These films were all released in 1897.  The first Spanish film director to achieve great success internationally was Segundo de Chomón, who worked in France and Italy but made several famous fantasy films in Spain, such as El hotel eléctrico (1908).  In 1914, Barcelona was the center of the nation's film industry.  The españoladas (historical Spanish epics) predominated until the 1960s. Prominent among these were the films of Florián Rey, starring Imperio Argentina, and the first version of Nobleza Baturra (Juan Vila Vilamala, 1925). Historical dramas such as Vida de Cristóbal Colón y su Descubrimiento de América –The Life of Christopher Columbus and His Discovery of America– (Gérard Bourgeois, 1917), adaptations of newspaper serials such as Los misterios de Barcelona –The Mysteries of Barcelona– (starring Joan Maria Codina, 1916), and of stage plays such as Don Juan Tenorio (Ricardo de Baños [es], 1922) and zarzuelas (comedic operettas), were also produced. Even the Nobel Prize-winning playwright Jacinto Benavente, who said that \"in film they pay me the scraps,\" would shoot film versions of his theatrical works.  In 1928, Ernesto Giménez Caballero and Luis Buñuel founded the first cine-club, in Madrid. By that point, Madrid was already the primary center of the industry; forty-four of the fifty-eight films released up until that point had been produced there.  The rural drama La aldea maldita –The Cursed Village– (Florian Rey, 1929) was a hit in Paris, where, at the same time, Buñuel and Salvador Dalí premiered Un chien andalou. Un chien andalou has become one of the most well-known avant-garde films of that era.  By 1931, the introduction of foreign sound films had hurt the Spanish film industry to the point where only a single title was released that year.  In 1935, Manuel Casanova founded the Compañía Industrial Film Española S.A. (Cifesa) and introduced sound to Spanish film-making. Cifesa would grow to become the biggest production company to ever exist in Spain. Sometimes criticized as an instrument of the right wing, it nevertheless supported young filmmakers such as Buñuel and his pseudo-documentary Las Hurdes: Tierra Sin Pan (1933). In 1933 it was responsible for filming seventeen motion pictures and in 1934, twenty-one. The most notable success was Paloma Fair (Benito Perojo, 1935). They were also responsible for Don Quijote de la Mancha (Rafael Gil, 1947), the most elaborate version of the Cervantes classic up to that time. By 1935 production had risen to thirty-seven films.  The Civil War devastated the silent film era: only ten per cent of all silent films made before 1936 survived the war. Films were also destroyed for their celluloid content and made into goods.[5]  Around 1936, both sides of the Civil War began to use cinema as a means of propaganda. A typical example of this is España 1936 (Buñuel, 1937), which also contains much rare newsreel footage. The pro-Franco side founded the National Department of Cinematography, causing many actors to go into exile.  The new regime then began to impose censorship and the obligatory dubbing to Spanish to all films released. Highlights in this era are El difunto es un vivo (Ignacio F. Iquino, 1941), Traces of Light (Rafael Gil, 1941), Madness for Love (Juan de Orduña, 1948), Last Stand in the Philippines (Antonio Román, 1945), Raza (José Luis Sáenz de Heredia with screenplay by Franco himself, 1942), and The Tower of the Seven Hunchbacks (Edgar Neville, 1944).  Cifesa produced Ella, él y sus millones (de Orduña, 1944) as well as Fedra (Manuel Mur Oti, 1956).  A policy of autarky tried to keep foreign currency in the country and establish a domestic film industry. If the distributors wanted licences to import and dub foreign films (audiences preferred American films), they would have to acquire them from producers of local films. The number of licences depended on the merits (artistic, moral, cultural, political) acknowledged by the government to each local film. The American distributors of the MPAA tried to open the market removing the local producers. To that end, they embargoed Spain since May 1951. The embargo goes into 1952 due to complications with American studios outside MPAA and reorganizations within the Spanish government. Spanish producers, lacking the income from the dubbing licences and with an uncertain future, greatly diminished their production as well. An agreement between Spain and the United States was finally reached.[6]  On the other hand, Miracle of Marcelino (Ladislao Vajda, 1955) is the first Spanish film to obtain worldwide recognition from critics and public, winning the Silver Bear award at the 5th Berlin International Film Festival. This film would trigger a trend of child actors, such as Joselito, Marisol, Rocío Dúrcal or Pili y Mili starring in popular musical films.  In 1951, the regime instituted the Ministry of Information and Tourism to safeguard and develop the Spanish brand, the social imagery and the public image under the slogan \"Spain is different\" which was launched in the 1920s and then internationally spread in the 1960s.[8] Its main purpose was to promote the Spanish tourist industry and a massive inflow of people who came from all the Europe towards the Andalusia, looking for what they saw in the Spanish films: sun and sea, comfortable transports and hotels, good ethnic cuisine, passion and adventure, and the so called españoladas (bulls, castanets, flamenco, Gitano culture and folklore).[8] Fog and Sun (José María Forqué, 1951) was the first movies belonging to the genre of the \"touristic cinema\". It was followed by Veraneo en España (Miguel Iglesias, 1958) and by Spain Again (Jaime Camino, 1969).[8]  Musical films The Last Torch Song (de Orduña, 1957) and The Violet Seller (Luis César Amadori, 1958), both starring Sara Montiel, were huge international commercial successes, making Montiel the first worldwide famous film star –and the highest paid– of Spanish cinema.[9]  In the 1950s, the influence of neorealism became evident in the works of a number of rather young film directors, such as Furrows (José Antonio Nieves Conde, 1951), Reckless (Nieves Conde, 1951), We're All Necessary (Nieves Conde, 1956), Pride (Mur Oti, 1955), Death of a Cyclist (Juan Antonio Bardem, 1955), Calle Mayor (Bardem, 1956), El pisito (Marco Ferreri, 1959), El cochecito (Ferreri, 1960), Welcome Mr. Marshall! (Luis García Berlanga, 1953), or Plácido (García Berlanga, 1961), ranged from melodrama to esperpento or black comedy, but all of them showed a strong social criticism, unexpected under a political censorship, like the one featured by Franco`s regime. From the amorality and selfishness of the upper middle class or the ridiculousness and mediocrity of the small town people to the hopelessness of the impoverished working class, every social stratum of the contemporary Spain was shown up.  Luis Buñuel in turn returned to Spain to film the shocking Viridiana (1961) and Tristana (1970).  A 1954 report by Eduardo Moya from the Ministry of Trade remarked that the Spanish cinema industry had to become competitive at home and abroad. Co-productions with France and Italy could bring the equipment and skills needed.[6]  Numerous co-productions with France and, most of all, Italy along the 1950s–1970s invigorated Spanish cinema both industrially and artistically. Actually the just mentioned Buñuel's movies were co-productions: Viridiana (1961) was Spanish-Mexican, and Tristana (1970) Spanish-French-Italian. Also, the hundreds of Spaghetti-westerns and sword and sandal films shot in southern Spain by mixed Spanish-Italian teams were co-productions.  Under the Spanish-American agreements, part of the foreign profits locked in Spain since the war were invested in runaway productions to be distributed abroad. Several American epic-scale superproductions or blockbusters were shot in Spain, produced either by Samuel Bronston, such as King of Kings (Nicholas Ray, 1961), El Cid (Anthony Mann, 1961), 55 Days at Peking (Ray, 1963), The Fall of the Roman Empire (Mann, 1964), and Circus World (Henry Hathaway, 1964); or by others, such as Alexander the Great (Robert Rossen, 1956), The Pride and the Passion (Stanley Kramer, 1957), Solomon and Sheba (King Vidor, 1959), Lawrence of Arabia (David Lean, 1962), Doctor Zhivago (Lean, 1965), The Trojan Women (Michael Cacoyannis, 1971). These movies employed many Spanish technical professionals, and as a byproduct caused that some film stars, like Ava Gardner and Orson Welles lived in Spain for years. Actually Welles, with Mr. Arkadin (1955), in fact a French-Spanish-Swiss co-production, was one of the first American filmmakers to devise Spain as location for his shootings, and he did it again for Chimes at Midnight (1966), this time a Spanish-Swiss co-production.   Warner Bros., an American studio had opened its local headquarters in Spain in the early 1970s under the name of Warner Española S.A. Warner Española,  alongside releasing Warner Bros. films (as well as films by Disney theatrically in the late 1980s-90s) is also involved in distribution of Spanish films such as Ensalada Baudelaire (1978), Adios Pequeña (1986) and most of 1990s Pedro Almodóvar's films such as  High Heels (1991), Kika (1993), and Live Flesh (1997).  Many international actors starred in Spanish films: Italians Vittorio de Sica, Vittorio Gassman and Rossano Brazzi with Mexican María Félix in The Black Crown (Luis Saslavsky, 1951); Italian couple Raf Vallone and Elena Varzi in The Eyes Leave a Trace (Sáenz de Heredia, 1952), Mexican Arturo de Córdova in The Red Fish (Nieves Conde, 1955), Americans Betsy Blair in Calle Mayor (Bardem, 1956); Edmund Gwenn in Calabuch (García Berlanga, 1956), or Richard Basehart in Miracles of Thursday (García Berlanga, 1957) among many others. All the foreign actors were dubbed into Spanish. Mexican actor Gael García Bernal has also recently received international notoriety in films by Spanish directors.  In 1962, José María García Escudero [es] became the Director General of Cinematography and Theatre, propelling forward state efforts and the Official Cinema School, from which emerged the majority of new directors, generally from the political left and those opposed to the Franco government. Among these were Mario Camus, Miguel Picazo, Francisco Regueiro, Manuel Summers, and, above all, Carlos Saura. Apart from this line of directors, Fernando Fernán Gómez made El extraño viaje (1964)[10][11] and Life Goes On (1965),[12] Víctor Erice The Spirit of the Beehive (1973), and Jaime de Armiñan My Dearest Senorita (1971).  From the so-called Escuela de Barcelona, originally more experimentalist and cosmopolitan, come Jacinto Esteva, Pere Portabella, Joaquin Jordan, Vicente Aranda, Jaime Camino, and Gonzalo Suárez, who made their master works in the 1980s.  In the Basque country the directors Fernando Larruquert, Nestor Basterretxea, José María Zabalza and the producer Elías Querejeta stood out.  The 1968–1980 period saw the golden age of Spanish B-Movie horror, underpinning the term fantaterror to convey the set of films blending supernatural and horror themes that originated as an answer to European and American exploitation titles.[13]  In the 1960s (and 1970s), a new sort of españolada different from the previous one brought the formulation of an \"Iberian\" model of masculininity associated to casticismo [es], represented by a male star system consisting of the likes of José Luis López Vázquez, Alfredo Landa, Andrés Pajares, and Fernando Esteso.[14] A new wave of popular and reactionary mainstream comedy films came to be collectively known as landismo [es] –after Alfredo Landa, a recurring appearance in many of those films playing foreign-women-preying \"Latin lover\" types–,[15] which was a cultural phenomenon in the 1970s.[16]  With the end of dictatorship in the mid 1970s, censorship was greatly loosened and cultural works were permitted in other languages spoken in Spain besides Spanish, resulting in the founding of the Centro Galego de Artes da Imaxe or the Institut del Cinema Català [ca], among others. Also with the end of censorship and repression, a commercial cinema –of low quality and minimal cost– with a high erotic content and gratuitous nudity –mostly feminine– appeared, which was called cine de destape [es] and which lasted until the early 1980s.[17]  In the context of the Transition, the so-called cine quinqui –of which Eloy de la Iglesia and José Antonio de la Loma [es] were prominent representatives–, particularly popular from 1977 to 1987,[19] approached taboo issues from a sensationalist angle, criminalizing the lumpenproletariat.[20] These films (whose lead performers sometimes were delinquent themselves)[21] also ended up contributing to the promotion of an imaginary of symbolic violence associated to the naturalization of the punitive and non-rehabilitating function of the prison system.[22] In the view of Germán Labrador Méndez [es], many of the quinqui films underpinned a true allegory of the Transition, conveying \"the mythical domestication of the non-consensual socio-political forces embodied by the quinquis, as children of the working class and, above all, as young people\".[23]  During the democracy, a whole new series of directors base their films either on controversial topics or on revising the country's history. Jaime Chávarri, Víctor Erice, José Luis Garci, Manuel Gutiérrez Aragón, Eloy de la Iglesia, Pilar Miró and Pedro Olea were some of these who directed great films. Montxo Armendáriz or Juanma Bajo Ulloa's \"new Basque cinema\" has also been outstanding; another prominent Basque director is Julio Médem.  The Spanish cinema, however, depends on the great hits of the so-called comedia madrileña by Fernando Colomo or Fernando Trueba, the sophisticated melodramas by Pedro Almodóvar, Alex de la Iglesia and Santiago Segura's black humour or Alejandro Amenábar's works, in such a manner that, according to producer José Antonio Félez [es], \"fifty per cent of total box office revenues comes from five titles, and between eight and ten films give eighty per cent of the total\" during the year 2004.  Foreign films often dominate box offices in Spain, with average monthly receipts of €35–50 million, making Spain the tenth largest country in the world for international theatrical release, with a total gross of USD 193,304,925 in 2020, thus giving Spain a worldwide market share of 1.8%.[24]  The San Sebastian International Film Festival is a major film festival supervised by the FIAPF. It was started in 1953, and it takes place in San Sebastián every year. Alfred Hitchcock, Audrey Hepburn, Steven Spielberg, Gregory Peck, Elizabeth Taylor are some of the stars that have participated in this festival, the most important in Spain.  The Sitges Film Festival, now known as the Sitges International Fantastic Film Festival of Catalonia, was started in 1967. It is considered one of the best cinematographic contests in Europe, and is the best in the specialty of science fiction film.  There are several other film festivals with important prizes for the industry such as the Valladolid International Film Festival, and the Seville European Film Festival from September to November, –Autumn has become the season par excellence for the debut of Spanish films in the domestic commercial circuit–.[25] Meanwhile the Málaga Film Festival, focused on Spanish and Ibero-American films, is generally held in early Spring.[26]  The Goya Awards are the main film awards in Spain. They were established in 1987,[27] a year after the founding of the Academy of Cinematographic Arts and Sciences of Spain, and recognize excellence in many aspects of Spanish motion picture making such as acting, directing and screenwriting. The first ceremony took place on 16 March 1987 at the Lope de Vega Theatre, Madrid. The ceremony continues to take place annually around the end of January, and awards are given to films produced during the previous year. The award itself is a small bronze bust of Francisco de Goya created by the sculptor José Luis Fernández.  In 2013,[28] the Feroz Awards were established by the Asociación de Informadores Cinematográficos de España, an association of entertainment journalists and critics.  Awards recognising the excellence in the regional cinema (and\/or wider audiovisual industry) include the Mestre Mateo Awards (from Galicia; presented by the Academia Galega do Audiovisual [gl]),[29] the Gaudí Awards (from Catalonia; presented by the Catalan Film Academy),[30] the Lola Gaos Awards (from the Valencian Community, presented by the Acadèmia Valenciana de l'Audiovisual)[31] or the Carmen Awards (from Andalusia, presented by the Academia de Cine de Andalucía).[32]  A large part of the funding of Spanish-produced films is covered in advance of the theatrical window by pre-sales to public (RTVE) or private (Atresmedia or Mediaset) broadcasters, subsidies (from ICAA, from regional or provincial administrations, or from tax rebates) and from pre-sales to streaming platforms.[33] Pre-sales may cover up to a 60–70% of the budget of a film with an average budget of €2.5 million.[33] This system, which favours  the attempt to approach the break-even point before the first window of theatrical exhibition, has received criticism from within the industry because it might discourage the pursuit of \"commercial success\".[33] The AIE (agrupación de interés económico; transl. 'economic interest grouping') legal form is used as a tax vehicle to take advantage of rebates.[33]  The ten highest-grossing Spanish films of all time (1965–2023)[a] by domestic box office gross revenue are listed as follows:[35][36] "},"meta":{},"created_at":"2025-03-22T14:25:42.293407Z","updated_at":"2025-03-22T14:25:42.293407Z","inner_id":109,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]},{"id":118,"annotations":[{"id":118,"completed_by":1,"result":[{"value":{"choices":["film"]},"from_name":"label","to_name":"text","type":"labels"}],"was_cancelled":false,"ground_truth":true,"created_at":"2025-03-22T14:25:42.327895Z","updated_at":"2025-03-22T14:25:42.327895Z","draft_created_at":null,"lead_time":null,"prediction":{},"result_count":0,"unique_id":"5582d865-0110-46ea-b65e-10edcd8f20ba","import_id":null,"last_action":null,"bulk_created":false,"task":118,"project":5,"updated_by":null,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"efc8061e-label_studio_input.json","drafts":[],"predictions":[],"data":{"text":"  The cinema of Japan (日本映画, Nihon eiga), also known domestically as hōga (邦画, \"domestic cinema\"), has a history that spans more than 100 years. Japan has one of the oldest and largest film industries in the world; as of 2022, it was the fourth largest by number of feature films produced, producing 634 films, and third largest in terms of box office revenue, standing at $1.5 billion.[4] Films have been produced in Japan since 1897.  During the 1950s, a period dubbed the \"Golden Age of Japanese cinema\", the jidaigeki films of Akira Kurosawa as well as the science fiction films of Ishirō Honda and Eiji Tsuburaya gained Japanese cinema international praise and made these directors universally renown and highly influential. Some of the Japanese films of this period are now rated some of the greatest of all time: Tokyo Story (1953) ranked number three in Sight & Sound critics' list of the 100 greatest films of all time[5] and also topped the 2012 Sight & Sound directors' poll of The Top 50 Greatest Films of All Time, dethroning Citizen Kane,[6][7] while Akira Kurosawa's Seven Samurai (1954) was voted the greatest foreign-language film of all time in BBC's 2018 poll of 209 critics in 43 countries.[8] Japan has also won the Academy Award for the Best International Feature Film[nb 1] five times,[nb 2] more than any other Asian country.[11]  Japan's Big Four film studios are Toho, Toei, Shochiku and Kadokawa, which are the only members of the Motion Picture Producers Association of Japan (MPPAJ). The annual Japan Academy Film Prize hosted by the Nippon Academy-shō Association is considered to be the Japanese equivalent of the Academy Awards.  The kinetoscope, first shown commercially by Thomas Edison in the United States in 1894, was first shown in Japan in November 1896. The Vitascope and the Lumière Brothers' Cinematograph were first presented in Japan in early 1897,[12] by businessmen such as Inabata Katsutaro.[13] Lumière cameramen were the first to shoot films in Japan.[14] Moving pictures, however, were not an entirely new experience for the Japanese because of their rich tradition of pre-cinematic devices such as gentō (utsushi-e) or the magic lantern.[15][16] The first successful Japanese film in late 1897 showed sights in Tokyo.[17]  In 1898, some ghost films were made, such as the Shirō Asano shorts Bake Jizo (Jizo the Spook \/ 化け地蔵) and Shinin no sosei (Resurrection of a Corpse).[18] The first documentary, the short Geisha no teodori (芸者の手踊り), was made in June 1899. Tsunekichi Shibata made a number of early films, including Momijigari, an 1899 record of two famous actors performing a scene from a well-known kabuki play. Early films were influenced by traditional theater – for example, kabuki and bunraku.  At the dawn of the 20th century, theaters in Japan hired benshi, storytellers who sat next to the screen and narrated silent movies. They were descendants of kabuki jōruri, kōdan storytellers, theater barkers and other forms of oral storytelling.[19] Benshi could be accompanied by music like silent films from cinema of the West. With the advent of sound in the early 1930s, the benshi gradually declined.  In 1908, Shōzō Makino, considered the pioneering director of Japanese film, began his influential career with Honnōji gassen (本能寺合戦), produced for Yokota Shōkai. Shōzō recruited Matsunosuke Onoe, a former kabuki actor, to star in his productions. Onoe became Japan's first film star, appearing in over 1,000 films, mostly shorts, between 1909 and 1926. The pair pioneered the jidaigeki genre.[20] Tokihiko Okada was a popular romantic lead of the same era.  The first Japanese film production studio was built in 1909 by the Yoshizawa Shōten company in Tokyo.[21]  The first female Japanese performer to appear in a film professionally was the dancer\/actress Tokuko Nagai Takagi, who appeared in four shorts for the American-based Thanhouser Company between 1911 and 1914.[22]  Among intellectuals, critiques of Japanese cinema grew in the 1910s and eventually developed into a movement that transformed Japanese film. Film criticism began with early film magazines such as Katsudō shashinkai (begun in 1909) and a full-length book written by Yasunosuke Gonda in 1914, but many early film critics often focused on chastising the work of studios like Nikkatsu and Tenkatsu for being too theatrical (using, for instance, elements from kabuki and shinpa such as onnagata) and for not utilizing what were considered more cinematic techniques to tell stories, instead relying on benshi. In what was later named the Pure Film Movement, writers in magazines such as Kinema Record called for a broader use of such cinematic techniques. Some of these critics, such as Norimasa Kaeriyama, went on to put their ideas into practice by directing such films as The Glow of Life (1918), which was one of the first films to use actresses (in this case, Harumi Hanayagi). There were parallel efforts elsewhere in the film industry. In his 1917 film The Captain's Daughter (based on the play by Choji Nakauchi, based in turn on the German film, Gendarm Möbius), Masao Inoue started using techniques new to the silent film era, such as the close-up and cut back. The Pure Film Movement was central in the development of the gendaigeki and scriptwriting.[23]  New studios established around 1920, such as Shochiku and Taikatsu, aided the cause for reform. At Taikatsu, Thomas Kurihara directed films scripted by the novelist Junichiro Tanizaki, who was a strong advocate of film reform.[24] Even Nikkatsu produced reformist films under the direction of Eizō Tanaka. By the mid-1920s, actresses had replaced onnagata and films used more of the devices pioneered by Inoue. Some of the most discussed silent films from Japan are those of Kenji Mizoguchi, whose later works (including Ugetsu\/Ugetsu Monogatari) retain a very high reputation.  Japanese films gained popularity in the mid-1920s against foreign films, in part fueled by the popularity of movie stars and a new style of jidaigeki. Directors such as Daisuke Itō and Masahiro Makino made samurai films like A Diary of Chuji's Travels and Roningai featuring rebellious antiheroes in fast-cut fight scenes that were both critically acclaimed and commercial successes.[25] Some stars, such as Tsumasaburo Bando, Kanjūrō Arashi, Chiezō Kataoka, Takako Irie and Utaemon Ichikawa, were inspired by Makino Film Productions and formed their own independent production companies where directors such as Hiroshi Inagaki, Mansaku Itami and Sadao Yamanaka honed their skills. Director Teinosuke Kinugasa created a production company to produce the experimental masterpiece A Page of Madness, starring Masao Inoue, in 1926.[26] Many of these companies, while surviving during the silent era against major studios like Nikkatsu, Shochiku, Teikine, and Toa Studios, could not survive the cost involved in converting to sound.  With the rise of left-wing political movements and labor unions at the end of the 1920s, there arose so-called tendency films with left-leaning tendencies. Directors Kenji Mizoguchi, Daisuke Itō, Shigeyoshi Suzuki, and Tomu Uchida were prominent examples. In contrast to these commercially produced 35 mm films, the Marxist Proletarian Film League of Japan (Prokino) made works independently in smaller gauges (such as 9.5mm and 16mm), with more radical intentions.[27] Tendency films suffered from severe censorship heading into the 1930s, and Prokino members were arrested and the movement effectively crushed. Such moves by the government had profound effects on the expression of political dissent in 1930s cinema. Films from this period include: Sakanaya Honda, Jitsuroku Chushingura, Horaijima, Orochi, Maboroshi, Kurutta Ippeji, Jujiro, Kurama Tengu: Kyōfu Jidai, and Kurama Tengu.[28]   The 1923 earthquake, the bombing of Tokyo during World War II, and the natural effects of time and Japan's humidity on flammable and unstable nitrate film have resulted in a great dearth of surviving films from this period.Ref?  Unlike in the West, silent films were still being produced in Japan well into the 1930s; as late as 1938, a third of Japanese films were silent.[29] For instance, Yasujirō Ozu's An Inn in Tokyo (1935), considered a precursor to the neorealism genre, was a silent film. A few Japanese sound shorts were made in the 1920s and 1930s, but Japan's first feature-length talkie was Fujiwara Yoshie no furusato (1930), which used the Mina Talkie System. Notable talkies of this period include Mikio Naruse's Wife, Be Like A Rose! (Tsuma Yo Bara No Yoni, 1935), which was one of the first Japanese films to gain a theatrical release in the U.S.; Kenji Mizoguchi's Sisters of the Gion (Gion no shimai, 1936); Osaka Elegy (1936); The Story of the Last Chrysanthemums (1939); and Sadao Yamanaka's Humanity and Paper Balloons (1937).  Film criticism shared this vitality, with many film journals such as Kinema Junpo and newspapers printing detailed discussions of the cinema of the day, both at home and abroad. A cultured \"impressionist\" criticism pursued by critics such as Tadashi Iijima, Fuyuhiko Kitagawa, and Matsuo Kishi was dominant, but opposed by leftist critics such as Akira Iwasaki and Genjū Sasa who sought an ideological critique of films.[30]  The 1930s also saw increased government involvement in cinema, which was symbolized by the passing of the Film Law, which gave the state more authority over the film industry, in 1939. The government encouraged some forms of cinema, producing propaganda films and promoting documentary films (also called bunka eiga or \"culture films\"), with important documentaries being made by directors such as Fumio Kamei.[31] Realism was in favor; film theorists such as Taihei Imamura and Heiichi Sugiyama advocated for documentary or realist drama, while directors such as Hiroshi Shimizu and Tomotaka Tasaka produced fiction films that were strongly realistic in style. Films reinforced the importance of traditional Japanese values against the rise of the Westernised modern girl, a character epitomised by Shizue Tatsuta in Ozu's 1930 film Young Lady.[32]  Because of World War II and the weak economy, unemployment became widespread in Japan, and the cinema industry suffered.  During this period, when Japan was expanding its empire, the Japanese government saw cinema as a propaganda tool to show the glory and invincibility of the Empire of Japan. Thus, many films from this period depict patriotic and militaristic themes. However unlike most wartime films the Japanese tended to tell it like it is, showing the hardships soldiers face everyday in battle, marching through mud and staying in small unknown towns. In 1942, Kajiro Yamamoto's film The War at Sea from Hawaii to Malaya portrayed the attack on Pearl Harbor; the film made use of special effects directed by Eiji Tsuburaya, including a miniature scale model of Pearl Harbor itself.  Kamishibai (紙芝居) or paper theater was a popular form of street entertainment, especially for the children. Kamishibai was often used to tell stories of Buddhist deities and the history of some Buddhist temples. In 1920 it started out as normal storytelling for the children, but in about 1932, it started to lean more to a militaristic viewpoint.  Yoshiko Yamaguchi was a very popular actress. She rose to international stardom with 22 wartime movies. The Manchukuo Film Association let her use the Chinese name Li Xianglan so she could represent Chinese roles in Japanese propaganda movies. After the war she used her official Japanese name and starred in an additional 29 movies. She was elected as a member of the Japanese parliament in the 1970s and served for 18 years.  Akira Kurosawa made his feature film debut with Sugata Sanshiro in 1943.  After the surrender of Japan in 1945, wartime controls and restrictions on the Japanese film industry were abolished, and the Supreme Commander for the Allied Powers (SCAP) established the Civil Information and Education Section (CIE), which came to manage the industry. All film proposals and screenplays were to be processed and approved by CIE. The script would then be processed by the Civil Censorship Detachment (CCD), which was under the direct control of American military.[33] Pre-war and wartime films were also subject to review, and over 500 were condemned, with half of them being burned. In addition, Toho and Daiei pre-emptively destroyed films they thought to be incriminating.[34] In November 1945, CIE announced that it would forbid films deemed to be:   A major consequence of these restrictions was that the production of jidaigeki films, especially those involving samurai, became effectively impossible.[36] A notable case of censorship was of the war film Escape at Dawn, written by Akira Kurosawa and Senkichi Taniguchi, which was re-written over a dozen times at the request of CIE, largely erasing the original content of the story.[37] On the other hand, the CIE favored the production of films that reflected the policies of the Occupation, such as agricultural reform and the organization of labor unions, and promoted the peaceful redevelopment of Japan and the rights of individuals.  Significant movies among them are, Setsuko Hara appeared in Akira Kurosawa's No Regrets for Our Youth (1946), Kōzaburō Yoshimura's A Ball at the Anjo House (1947), Tadashi Imai's Aoi sanmyaku (1949), etc. It gained national popularity as a star symbolizing the beginning of a new era. In Yasushi Sasaki's Hatachi no Seishun (1946), the first kiss scene of a Japanese movie was filmed. The Mainichi Film Award was also created in 1946.[38]  The first movie released after the war was Soyokaze, directed by Yasushi Sasaki, and the theme song Ringo no Uta was a big hit.[39]  The first collaborations between Akira Kurosawa and actor Toshiro Mifune were Drunken Angel in 1948 and Stray Dog in 1949. Yasujirō Ozu directed the critically and commercially successful Late Spring in 1949.  In the later half of the Occupation, the Reverse Course came into effect. Left-wing filmmakers displaced from the major studios in the Red Purge joined those displaced after suppression of the Toho strikes, forming a new independent film movement. Directors such as Fumio Kamei, Tadashi Imai and Satsuo Yamamoto were members of the Japanese Communist Party. Independent social realist dramas saw a small and temporary boom amid the wave of sentimental war dramas produced after the end of Occupation.[37]  The 1950s are widely considered the Golden Age of Japanese cinema.[40] Three Japanese films from this decade (Rashomon, Seven Samurai and Tokyo Story) appeared in the top ten of Sight & Sound's critics' and directors' polls for the best films of all time in 2002.[41] They also appeared in the 2012 polls,[42][43] with Tokyo Story (1953) dethroning Citizen Kane at the top of the 2012 directors' poll.[43]  War movies covering themes previously restricted by SCAP began to be produced, such as Hideo Sekigawa's Listen to the Voices of the Sea (1950), Tadashi Imai's Himeyuri no Tô (Tower of the Lilies, 1953), Keisuke Kinoshita's Twenty-Four Eyes (1954) and Kon Ichikawa's The Burmese Harp (1956). Works showcasing tragic and sentimental retrospectives of the war experience became a public phenomenon. Other films produced include Battleship Yamato (1953) and Eagle of the Pacific (1953). Under these circumstances, movies such as Emperor Meiji and the Russo-Japanese War (明治天皇と日露大戦争, 1957), where Kanjūrō Arashi played Emperor Meiji, also appeared. It was a situation that was unthinkable before the war, the commercialization of the Emperor who was supposed to be sacred and inviolable.  The period after the American Occupation led to a rise in diversity in movie distribution thanks to the increased output and popularity of the film studios of Toho, Daiei, Shochiku, Nikkatsu, and Toei. This period gave rise to the six great artists of Japanese cinema: Masaki Kobayashi, Akira Kurosawa, Ishirō Honda, Eiji Tsuburaya, Kenji Mizoguchi, and Yasujirō Ozu. Each director dealt with the effects the war and subsequent occupation by America in unique and innovative ways. During this decade, the works of Kurosawa, Honda, and Tsuburaya would become the first Japanese films to be widely distributed in foreign theaters.  The decade started with Akira Kurosawa's Rashomon (1950), which won the Golden Lion at the Venice Film Festival in 1951 and the Academy Honorary Award for Best Foreign Language Film in 1952, and marked the entrance of Japanese cinema onto the world stage. It was also the breakout role for legendary star Toshiro Mifune.[44] In 1953, Entotsu no mieru basho by Heinosuke Gosho was in competition at the 3rd Berlin International Film Festival.  The first Japanese film in color was Carmen Comes Home directed by Keisuke Kinoshita and released in 1951. There was also a black-and-white version of this film available. Tokyo File 212 (1951) was the first American feature film to be shot entirely in Japan. The lead roles were played by Florence Marly and Robert Peyton. It featured the geisha Ichimaru in a short cameo. Suzuki Ikuzo's Tonichi Enterprises Company co-produced the film.[45] Gate of Hell, a 1953 film by Teinosuke Kinugasa, was the first movie that filmed using Eastmancolor film, Gate of Hell was both Daiei's first color film and the first Japanese color movie to be released outside Japan, receiving an Academy Honorary Award in 1954 for Best Costume Design by Sanzo Wada and an Honorary Award for Best Foreign Language Film. It also won the Palme d'Or at the Cannes Film Festival, the first Japanese film to achieve that honour.  The year 1954 saw two of Japan's most influential films released. The first was the Kurosawa epic Seven Samurai, about a band of hired samurai who protect a helpless village from a rapacious gang of thieves. The same year, Kurosawa's friend and colleague Ishirō Honda directed the anti-nuclear monster-drama Godzilla, featuring award-winning effects by Eiji Tsuburaya. The latter film was first ever Japanese film to be given a wide release throughout the United States,[46] where it was heavily re-edited, and featured new footage with actor Raymond Burr for its distribution in 1956 as Godzilla, King of the Monsters!.[47] Although it was edited for its Western release, Godzilla became an international icon of Japan and spawned an entire subgenre of kaiju films,[48] as well as the longest-running film franchise in history.[49] Also in 1954, another Kurosawa film, Ikiru was in competition at the 4th Berlin International Film Festival.  In 1955, Hiroshi Inagaki won an Academy Honorary Award for Best Foreign Language Film for Part I of his Samurai trilogy and in 1958 won the Golden Lion at the Venice Film Festival for Rickshaw Man. Kon Ichikawa directed two anti-war dramas: The Burmese Harp (1956), which was nominated for Best Foreign Language Film at the Academy Awards, and Fires on the Plain (1959), along with Enjo (1958), which was adapted from Yukio Mishima's novel Temple of The Golden Pavilion. Masaki Kobayashi made three films which would collectively become known as The Human Condition Trilogy: No Greater Love (1959), and The Road to Eternity (1959). The trilogy was completed in 1961, with A Soldier's Prayer.  Kenji Mizoguchi, who died in 1956, ended his career with a series of masterpieces including The Life of Oharu (1952), Ugetsu (1953) and Sansho the Bailiff (1954). He won the Silver Lion at the Venice Film Festival for Ugetsu. Mizoguchi's films often deal with the tragedies inflicted on women by Japanese society. Mikio Naruse made Repast (1950), Late Chrysanthemums (1954), Sound of the Mountain (1954) and Floating Clouds (1955). Yasujirō Ozu began directing color films beginning with Equinox Flower (1958), and later Good Morning (1959) and Floating Weeds (1958), which was adapted from his earlier silent A Story of Floating Weeds (1934), and was shot by Rashomon and Sansho the Bailiff cinematographer Kazuo Miyagawa.  The Blue Ribbon Awards were established in 1950. The first winner for Best Film was Until We Meet Again by Tadashi Imai.  The number of films produced, and the cinema audience reached a peak in the 1960s.[50] Most films were shown in double bills, with one half of the bill being a \"program picture\" or B movie. A typical program picture was shot in four weeks. The demand for these program pictures in quantity meant the growth of film series such as The Hoodlum Soldier or Akumyo.  The huge level of activity of 1960s Japanese cinema also resulted in many classics. Akira Kurosawa directed the 1961 classic Yojimbo. Yasujirō Ozu made his final film, An Autumn Afternoon, in 1962. Mikio Naruse directed the wide screen melodrama When a Woman Ascends the Stairs in 1960; his final film was 1967's Scattered Clouds.  Kon Ichikawa captured the watershed 1964 Olympics in his three-hour documentary Tokyo Olympiad (1965). Seijun Suzuki was fired by Nikkatsu for \"making films that don't make any sense and don't make any money\" after his surrealist yakuza flick Branded to Kill (1967).  The 1960s were the peak years of the Japanese New Wave movement, which began in the 1950s and continued through the early 1970s. Nagisa Oshima, Kaneto Shindo, Masahiro Shinoda, Susumu Hani and Shohei Imamura emerged as major filmmakers during the decade. Oshima's Cruel Story of Youth, Night and Fog in Japan and Death by Hanging, along with Shindo's Onibaba, Hani's Kanojo to kare and Imamura's The Insect Woman, became some of the better-known examples of Japanese New Wave filmmaking. Documentary played a crucial role in the New Wave, as directors such as Hani, Kazuo Kuroki, Toshio Matsumoto, and Hiroshi Teshigahara moved from documentary into fiction film, while feature filmmakers like Oshima and Imamura also made documentaries. Shinsuke Ogawa and Noriaki Tsuchimoto became the most important documentarists: \"two figures [that] tower over the landscape of Japanese documentary.\"[51]  Teshigahara's Woman in the Dunes (1964) won the Special Jury Prize at the Cannes Film Festival, and was nominated for Best Director and Best Foreign Language Film Oscars. Masaki Kobayashi's Kwaidan (1965) also picked up the Special Jury Prize at Cannes and received a nomination for Best Foreign Language Film at the Academy Awards. Bushido, Samurai Saga by Tadashi Imai won the Golden Bear at the 13th Berlin International Film Festival. Immortal Love by Keisuke Kinoshita and Twin Sisters of Kyoto and Portrait of Chieko, both by Noboru Nakamura, also received nominations for Best Foreign Language Film at the Academy Awards. Lost Spring, also by Nakamura, was in competition for the Golden Bear at the 17th Berlin International Film Festival.  The 1970s saw the cinema audience drop due to the spread of television. Total audience declined from 1.2 billion in 1960 to 0.2 billion in 1980.[52] Film companies refused to hire top actors and directors, not even the companies' production skills to the television industry, thereby making the film companies losing money. [53]  Film companies fought back in various ways, such as the bigger budget films of Kadokawa Pictures, or including increasingly sexual or violent content and language which could not be shown on television. The resulting pink film industry became the stepping stone for many young independent filmmakers. The seventies also saw the start of the \"idol eiga\", films starring young \"idols\", who would bring in audiences due to their fame and popularity.  Toshiya Fujita made the revenge film Lady Snowblood in 1973. In the same year, Yoshishige Yoshida made the film Coup d'État, a portrait of Ikki Kita, the leader of the Japanese coup of February 1936. Its experimental cinematography and mise-en-scène, as well as its avant-garde score by Toshi Ichiyanagi, garnered it wide critical acclaim within Japan.  In 1976, the Hochi Film Award was created. The first winner for Best Film was The Inugamis by Kon Ichikawa. Nagisa Oshima directed In the Realm of the Senses (1976), a film detailing a crime of passion involving Sada Abe set in the 1930s. Controversial for its explicit sexual content, it has never been seen uncensored in Japan.  Kinji Fukasaku completed the epic Battles Without Honor and Humanity series of yakuza films. Yoji Yamada introduced the commercially successful Tora-San series, while also directing other films, notably the popular The Yellow Handkerchief, which won the first Japan Academy Prize for Best Film in 1978. New wave filmmakers Susumu Hani and Shōhei Imamura retreated to documentary work, though Imamura made a dramatic return to feature filmmaking with Vengeance Is Mine (1979).  Dodes'ka-den by Akira Kurosawa and Sandakan No. 8 by Kei Kumai were nominated to the Academy Award for Best Foreign Language Film.  The 1980s saw the decline of the major Japanese film studios and their associated chains of cinemas, with major studios Toho and Toei barely staying in business, Shochiku supported almost solely by the Otoko wa tsurai yo films, and Nikkatsu declining even further.  Of the older generation of directors, Akira Kurosawa directed Kagemusha (1980), which won the Palme d'Or at the 1980 Cannes Film Festival, and Ran (1985). Seijun Suzuki made a comeback beginning with Zigeunerweisen in 1980. Shohei Imamura won the Palme d'Or at the Cannes Film Festival for The Ballad of Narayama (1983). Yoshishige Yoshida made A Promise (1986), his first film since 1973's Coup d'État.  New directors who appeared in the 1980s include actor Juzo Itami, who directed his first film, The Funeral, in 1984, and achieved critical and box office success with Tampopo in 1985. Shinji Sōmai, an artistically inclined populist director who made films like the youth-focused Typhoon Club, and the critically acclaimed Roman porno Love Hotel among others. Kiyoshi Kurosawa, who would generate international attention beginning in the mid-1990s, made his initial debut with pink films and genre horror.  During the 1980s, anime rose in popularity, with new animated movies released every summer and winter, often based upon popular anime television series. Mamoru Oshii released his landmark Angel's Egg in 1985 while Hayao Miyazaki adapted his manga series, Nausicaä of the Valley of Wind, into a feature film of the same name in 1984. Katsuhiro Otomo followed suit by adapting his own manga Akira into a feature film of the same name in 1988.  Eventually the home video made it possible to where the Japanese civilians, and eventually citizens in other countries, could watch these films individually. This would increase sales in the direct-to-video film industry, allowing for further developments, such as DVD's and eventual streaming services, to develop.  Mini theaters, a type of independent movie theater characterized by a smaller size and seating capacity in comparison to larger movie theaters, gained popularity during the 1980s.[54] Mini theaters helped bring independent and arthouse films from other countries, as well as films produced in Japan by unknown Japanese filmmakers, to Japanese audiences.[54]  Because of economic recessions, the number of movie theaters in Japan had been steadily decreasing since the 1960s. The number of cinemas was under 2,000 in 1993 compared to more than 7,000 in 1960.[53] The 1990s saw the reversal of this trend and the introduction of the multiplex in Japan. At the same time, the popularity of mini theaters continued.[54][55]  Takeshi Kitano emerged as a significant filmmaker with works such as Sonatine (1993), Kids Return (1996) and Hana-bi (1997), which was given the Golden Lion at the Venice Film Festival. Shōhei Imamura again won the Palme d'Or (shared with Iranian director Abbas Kiarostami), this time for The Eel (1997). He became the fifth two-time recipient, joining Alf Sjöberg, Francis Ford Coppola, Emir Kusturica and Bille August.  Kiyoshi Kurosawa gained international recognition following the release of Cure (1997). Takashi Miike launched a prolific career with titles such as Audition (1999), Dead or Alive (1999) and The Bird People in China (1998). Former documentary filmmaker Hirokazu Koreeda launched an acclaimed feature career with Maborosi (1996) and After Life (1999).  Hayao Miyazaki directed two mammoth box office and critical successes, Porco Rosso (1992) – which beat E.T. the Extra-Terrestrial (1982) as the highest-grossing film in Japan – and Princess Mononoke (1997), which also claimed the top box office spot until Titanic (1997).  Several new anime directors rose to widespread recognition, bringing with them notions of anime as not only entertainment, but modern art. Mamoru Oshii released the internationally acclaimed philosophical science fiction action film Ghost in the Shell in 1996. Satoshi Kon directed the award-winning psychological thriller Perfect Blue. Hideaki Anno also gained considerable recognition with The End of Evangelion in 1997.  In the beginning of 21st century, Japan has been referenced numerous times in popular culture, which was a relatively successful one for Japanese film industry, returning to the idea of a second Japanese New Wave” in their cinematic releases. The country has appeared as a setting and topic multiple times in film, poetry, television, and music. The number of films being shown in Japan steadily increased, with about 821 films released in 2006. Films based on Japanese television series were especially popular during this period. Anime films now accounted for 60 percent of Japanese film production and would become one of the world’s leading producers of animated cinema. [56] The 1990s and 2000s are considered to be \"Japanese Cinema's Second Golden Age\", due to the immense popularity of anime, both within Japan and overseas.[40]  Although not a commercial success, All About Lily Chou-Chou directed by Shunji Iwai was honored at the Berlin, the Yokohama and the Shanghai Film Festivals in 2001. Takeshi Kitano appeared in Battle Royale and directed and starred in Dolls and Zatoichi. Beginning in the late 1990s, the J-horror film genre began to boom, as films such as Ringu, Kairo, Dark Water, Yogen, the Grudge series and One Missed Call met with commercial success. [56] In 2004, Godzilla: Final Wars, directed by Ryuhei Kitamura, was released to celebrate the 50th anniversary of Godzilla. In 2005, director Seijun Suzuki made his 56th film, Princess Raccoon. Hirokazu Koreeda claimed film festival awards around the world with two of his films Distance and Nobody Knows. Female film director Naomi Kawase's film The Mourning Forest won the Grand Prix at the Cannes Film Festival in 2007. Yoji Yamada, director of the Otoko wa Tsurai yo series, made a trilogy of acclaimed revisionist samurai films, 2002's Twilight Samurai, followed by The Hidden Blade in 2004 and Love and Honor in 2006. In 2008, Departures won the Academy Award for best foreign language film.  In anime, Hayao Miyazaki directed Spirited Away in 2001, breaking Japanese box office records and winning several awards—including the Academy Award for Best Animated Feature in 2003[57]—followed by Howl's Moving Castle and Ponyo in 2004 and 2008 respectively. In 2004, Mamoru Oshii released the anime movie Ghost in the Shell 2: Innocence which received critical praise around the world. His 2008 film The Sky Crawlers was met with similarly positive international reception. Satoshi Kon also released three quieter, but nonetheless highly successful films: Millennium Actress, Tokyo Godfathers, and Paprika. Katsuhiro Otomo released Steamboy, his first animated project since the 1995 short film compilation Memories, in 2004. In collaboration with Studio 4C, American director Michael Arias released Tekkon Kinkreet in 2008, to international acclaim. After several years of directing primarily lower-key live-action films, Hideaki Anno formed his own production studio and revisited his still-popular Evangelion franchise with the Rebuild of Evangelion tetralogy, a new series of films providing an alternate retelling of the original story.  Some Hollywood directors have turned to Tokyo as a backdrop for movies set in Japan. Post-war period examples include: Tokyo Joe, My Geisha, Tokyo Story and the James Bond film You Only Live Twice; recent examples include Lost in Translation and The Last Samurai (both in 2003), Kill Bill: Volume 1 and 2 and The Day After Tomorrow (all in 2004), Memoirs of a Geisha (2005), The Fast and the Furious: Tokyo Drift and Babel (both in 2006), The Day the Earth Stood Still (2008), 2012 (2009), Inception (2010), Emperor (2012), Pacific Rim and The Wolverine (both in 2013), Geostorm (2017), and Avengers: Endgame (2019).  Since February 2000, the Japan Film Commission Promotion Council was established. On November 16, 2001, the Japanese Foundation for the Promotion of the Arts laws were presented to the House of Representatives. These laws were intended to promote the production of media arts, including film scenery, and stipulate that the government – on both the national and local levels – must lend aid in order to preserve film media. The laws were passed on November 30 and came into effect on December 7. In 2003, at a gathering for the Agency of Cultural Affairs, twelve policies were proposed in a written report to allow public-made films to be promoted and shown at the Film Center of the National Museum of Modern Art.  Japanese cinema has always been perceived as either having feminist male directors as well as female directors, furthering the notion of the importance of women in Japanese cinema. This is proven most profusely in 2009, where a symposium for the Nippon Connection Festival was held, which the entire meeting was devoted to women: as a subject, as female directors, and as their importance to Japanese cinema. [58] The impact of women is seen in various film festivals, including ‘Peaches,’ where Japanese women graduates were allowed to display their achievements in the cinematic field willingly. [59] Through these interpretations and diverse views of males in cinema, women have a major impact on Japanese cinema, about politically and socially: they themselves are a part of the Japanese narrative and their stories need to be studied in films for audiences to fully grasp their stories.  Four films have so far received international recognition by being selected to compete in major film festivals: Caterpillar by Kōji Wakamatsu was in competition for the Golden Bear at the 60th Berlin International Film Festival and won the Silver Bear for Best Actress, Outrage by Takeshi Kitano was In Competition for the Palme d'Or at the 2010 Cannes Film Festival, Himizu by Sion Sono was in competition for the Golden Lion at the 68th Venice International Film Festival.  In March 2011, Japanese film and television industry was afflicted by the Tohoku earthquake and tsunami and the subsequent Fukushima nuclear disaster, which was greatly suffered due to ongoing triple disaster. [60] However, many Japanese studios were officially closed or reorganized to prevent the triple disaster. As of result, many of Japanese studios began to reopen and production rates have increased.  In October 2011 (after fully reopening of Japanese film and television industry), Takashi Miike's Hara-Kiri: Death of a Samurai was In Competition for the Palme d'Or at the 2012 Cannes Film Festival, the first 3D film ever to screen In Competition at Cannes. The film was co-produced by British independent producer Jeremy Thomas, who had successfully broken Japanese titles such as Nagisa Oshima's Merry Christmas, Mr Lawrence and  Taboo, Takeshi Kitano's Brother, and Miike's 13 Assassins onto the international stage as producer.  In 2018, Hirokazu Kore-eda won the Palme d'Or for his movie Shoplifters at the 71st Cannes Film Festival, a festival that also featured Ryūsuke Hamaguchi's Asako I & II in competition.  The 2020 Japanese epic disaster drama film Fukushima 50, released on 6 March 2020, directed by Setsurō Wakamatsu and written by Yōichi Maekawa. The film is based on the book by Ryusho Kadota, titled On the Brink: The Inside Story of Fukushima Daiichi, and it is the first Japanese film to depict the disaster.   In early 2020, the Japanese film and television industry was afflicted by the COVID-19 pandemic, which greatly suffered due to health requirements. This gave the nation its worst day of film and television industry impacted by health crises since the end of World War II. From the first (of many) 'health lockdowns' until the end of September 2021, many Japanese studios were closed or reorganized to suit the legal requirements for spread prevention which ultimately resulted in the suspension of filming for many movies, however, it did not stop from people wanting to see movies. [61] Despite this pandemic occurring, many films were slowly being reintroduced to Japanese cinemas, which changed how Japan would approach cinema within the following years. From 2021-2022, there was the reinstating of Japanese cinema to Japanese audiences, as theater attendance had increased from the original 54.5% from 2020,[61] to about 78% by 2022.[62] In 2022 alone, though there was a significant decrease from 2019’s numbers, there were 590 movie theatres that were open and available to the public, allowing for the public to reengage with normal activities while being amid the pandemic.[63]  In October 2020 (after the reopening film industry), a Japanese anime film Demon Slayer: Mugen Train based on the Demon Slayer: Kimetsu no Yaiba manga series broke all box-office records in the country, becoming the highest-grossing film of all time in Japan, the highest-grossing Japanese film of all time and the highest-grossing film of 2020.  In October 2021, a Japanese drama-road film Drive My Car won Best Foreign Language Film at the 79th Golden Globe Awards and received the Academy Award for Best International Feature Film at the 94th Academy Awards.[64][65]  In May 2023, a Japanese drama film Perfect Days won Best Actor and Ecumenical Jury at the 76th Cannes Film Festival.[66] Besides that a Japanese psychological dramatic mystery thriller film Monster won Best Screenplay as well as the Queer Palm at the same festival.[67]  In September 2023, a Japanese drama mystery film Evil Does Not Exist won Grand Jury and FIPRESCI Award at the 80th Venice International Film Festival and also awarded Best Film at the 2023 BFI London Film Festival.[68][69]  Hayao Miyazaki's The Boy and the Heron and Takashi Yamazaki's Godzilla Minus One (both released in 2023) each won an award at the 96th Academy Awards and garnered critical acclaim.[70][71] The Boy and the Heron also won Best Animated Feature Film at the 81st Golden Globe Awards, the first non-English-language animated film to do so.[72] Likewise, Godzilla Minus One became the first foreign-language film to win the Academy Award for Best Visual Effects.[70] "},"meta":{},"created_at":"2025-03-22T14:25:42.293407Z","updated_at":"2025-03-22T14:25:42.293407Z","inner_id":110,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":5,"updated_by":null,"comment_authors":[]}]